<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
        "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html>
<head>
<title>Kafka Connect: tvorba producentů a konzumentů bez nutnosti udržovat zdrojový kód (2.část)</title>
<meta name="Author" content="Pavel Tisnovsky" />
<meta name="Generator" content="vim" />
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<style type="text/css">
         body {color:#000000; background:#ffffff;}
         h1  {font-family: arial, helvetica, sans-serif; color:#ffffff; background-color:#c00000; text-align:center; padding-left:1em}
         h2  {font-family: arial, helvetica, sans-serif; color:#ffffff; background-color:#0000c0; padding-left:1em; text-align:left}
         h3  {font-family: arial, helvetica, sans-serif; color:#000000; background-color:#c0c0c0; padding-left:1em; text-align:left}
         h4  {font-family: arial, helvetica, sans-serif; color:#000000; background-color:#e0e0e0; padding-left:1em; text-align:left}
         a   {font-family: arial, helvetica, sans-serif;}
         li  {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify; width:450px;}
         ol  {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify; width:450px;}
         ul  {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify; width:450px;}
         p   {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify;}
         pre {background:#e0e0e0}
</style>
</head>

<body>

<h1>Kafka Connect: tvorba producentů a konzumentů bez nutnosti udržovat zdrojový kód (2.část)</h1>

<h3>Pavel Tišnovský</h3>

<p></p>

<h1>Úvodník</h1>

<p>Na úvodní článek o frameworku Kafka Connect dnes navážeme. V první části textu se budeme zabývat způsobem definice a kontroly schématu zpráv, což je v oblasti heterogenních architektur založených na mikroslužbách velmi užitečná a žádaná vlastnost. Ukážeme si i konektor pro zápis zpráv do relační databáze.</p>



<h2>Obsah</h2>

<p><a href="#k01">1. Kafka Connect: tvorba producentů a konzumentů bez nutnosti udržovat zdrojový kód (2.část)</a></p>
<p><a href="#k02">2. Konektor typu <i>sink</i>, který kontroluj schéma zpráv</a></p>
<p><a href="#k03">3. Poslání celého souboru JSON do zvoleného tématu</a></p>
<p><a href="#k04">4. Chování konektoru pro zprávy bez schématu</a></p>
<p><a href="#k05">5. Přidání schématu přímo do zprávy</a></p>
<p><a href="#k06">*** 6. Atribut typu &bdquo;celé číslo&ldquo;</a></p>
<p><a href="#k07">*** 7. Povinné a volitelné atributy</a></p>
<p><a href="#k08">*** 8. </a></p>
<p><a href="#k09">*** 9. Zalogování konkrétního problému při zpracování zprávy v&nbsp;DLQ</a></p>
<p><a href="#k10">*** 10. </a></p>
<p><a href="#k11">*** 11. </a></p>
<p><a href="#k12">*** 12. </a></p>
<p><a href="#k13">*** 13. </a></p>
<p><a href="#k14">*** 14. </a></p>
<p><a href="#k15">*** 15. </a></p>
<p><a href="#k16">*** 16. </a></p>
<p><a href="#k17">*** 17. </a></p>
<p><a href="#k18">*** 18. </a></p>
<p><a href="#k19">19. Repositář s&nbsp;demonstračními příklady, konfiguračními soubory a testovacími zprávami</a></p>
<p><a href="#k20">20. Odkazy na Internetu</a></p>



<p><a name="k01"></a></p>
<h2 id="k01">1. Kontrola formátu zpráv vůči schématu</h2>

<p><i>&bdquo;When I started my journey with Apache Kafka, JSON was already
everywhere. From Javascript UIs, through API calls, and even databases &ndash;
it became a lingua franca of data exchange.&ldquo;</i></p>

<p>Na <a
href="https://www.root.cz/clanky/kafka-connect-tvorba-producentu-a-konzumentu-bez-zdrojoveho-kodu/">úvodní
článek</a> o frameworku <i>Kafka Connect</i> dnes navážeme. V&nbsp;úvodní části
dnešního textu se budeme zabývat způsobem definice a kontroly schématu zpráv,
což je v&nbsp;oblasti heterogenních architektur založených na mikroslužbách
(které se samy postupně vyvíjí) velmi užitečná a žádaná vlastnost.</p>

<p>Připomeňme si nejdříve, jakým způsobem je možné definovat konektor pro
technologii <i>Kafka Connect</i>, který slouží pro definici konzumenta zpráv.
Konzumované zprávy jsou ukládány do souboru nazvaného
<strong>test.sink4.jsons</strong> a přitom se provádí kontrola, zda jak klíč
zprávy (pokud existuje), tak i vlastní tělo zprávy jsou uloženy ve formátu
JSON. Pokud tomu tak není, je zpráva přeposlána do takzvané <i>dead letter
queue</i>, což je v&nbsp;našem případě konkrétně téma nazvané
<strong>dlq_bad_jsons</strong>:</p>

<pre>
name=local-file-sink-json
connector.class=FileStreamSink
tasks.max=1
file=<strong>test.sink4.jsons</strong>
topics=connect-test-json
<strong>key.converter=org.apache.kafka.connect.json.JsonConverter</strong>
<strong>value.converter=org.apache.kafka.connect.json.JsonConverter</strong>
<strong>key.converter.schemas.enable=false</strong>
<strong>value.converter.schemas.enable=false</strong>
errors.tolerance=all
<strong>errors.deadletterqueue.topic.name=dlq_bad_jsons</strong>
errors.deadletterqueue.topic.replication.factor=1
</pre>

<p>Tento konektor se spustí příkazem:</p>

<pre>
$ <strong>bin/connect-standalone.sh config/connect-standalone.properties config/connect-file-sink-4.properties</strong>
</pre>

<p>Obsah DLQ si můžeme vypsat příkazem:</p>

<pre>
$ <strong>bin/kafka-console-consumer.sh  --bootstrap-server localhost:9092 --topic dlq_bad_jsons --partition 0 --offset earliest</strong>
</pre>

<p>V&nbsp;tomto případě je ovšem výhodnější použít nástroj <i>Kafkacat</i> a
netrápit se s&nbsp;uváděním offsetů a oddílů:</p>

<pre>
$ <strong>kafkacat -b localhost:9092 -t dlq_bad_jsons -C</strong>
</pre>



<p><a name="k02"></a></p>
<h2 id="k02">2. Konektor typu <i>sink</i>, který kontroluj schéma zpráv</h2>

<p>Další konektor, který nakonfigurujeme v&nbsp;rámci této kapitoly, se od
konektoru popsaného <a href="#k01">v&nbsp;úvodní kapitole</a> odlišuje
především tím, že má vlastnosti <strong>key.converter.schemas.enable</strong> a
<strong>value.converter.schemas.enable</strong> nastaveny na hodnotu
<strong>true</strong>, což znamená, že se konektor bude snažit zjistit, zda
zpracovávané zprávy odpovídají schématu (samotné schéma prozatím ovšem nebudeme
mít nikde definováno):</p>

<pre>
name=local-file-sink-json-checked
connector.class=FileStreamSink
tasks.max=1
file=test.sink5.jsons
topics=connect-test-json
<strong>key.converter=org.apache.kafka.connect.json.JsonConverter</strong>
<strong>value.converter=org.apache.kafka.connect.json.JsonConverter</strong>
<strong>key.converter.schemas.enable=true</strong>
<strong>value.converter.schemas.enable=true</strong>
errors.tolerance=all
errors.deadletterqueue.topic.name=dlq_bad_jsons
errors.deadletterqueue.topic.replication.factor=1
</pre>

<p>Tento konektor, jenž je uložen v&nbsp;souboru
<strong>connect-file-sink-5.properties</strong>, se spustí příkazem:</p>

<pre>
$ <strong>bin/connect-standalone.sh config/connect-standalone.properties config/connect-file-sink-5.properties</strong>
</pre>



<p><a name="k03"></a></p>
<h2 id="k03">3. Poslání celého souboru JSON do zvoleného tématu</h2>

<p>Nyní se pokusíme do tématu se jménem <strong>connect-test-json</strong>
předat zprávu ve formátu JSON, ovšem bez specifikace schématu:</p>

<pre>
{
  "ID": 1,
  "Name": "Linus",
  "Surname": "Torvalds"
}
</pre>

<p>Jak tuto operaci provedeme, pokud je zpráva uložena v&nbsp;souboru?
V&nbsp;tomto případě <i>nemůžeme</i> obsah souboru přesměrovat přímo
producentovi, protože ten by obsah chápal jako několik uložených zpráv &ndash;
každá zpráva na jednom řádku. To znamená, že následující příkaz je
nekorektní:</p>

<pre>
$ <strong>cat bad_msg.json | bin/kafka-console-producer.sh --broker-list localhost:9092 --topic connect-test-json</strong>
</pre>

<p>V&nbsp;tomto případě si budeme muset pomoci malým trikem, například použitím
nástroje <strong>jq</strong>, který dokáže zprávu ve formátu JSON převést do
kompaktního (jednořádkového) tvaru, pokud použijeme přepínač
<strong>-c</strong>:</p>

<p>Korektní způsob poslání obsahu souboru jako jediné zprávy tedy bude vypadat
takto:</p>

<pre>
$ <strong>jq -c . msg3.json | bin/kafka-console-producer.sh --broker-list localhost:9092 --topic connect-test-json</strong>
</pre>

<p><div class="rs-tip-major">Poznámka: nástroj <strong>jq</strong>, který je
v&nbsp;praxi velmi užitečný, jsme si popsali v&nbsp;článku <a
href="https://www.root.cz/clanky/zpracovani-dat-reprezentovanych-ve-formatu-json-nastrojem-jq/">Zpracování
dat reprezentovaných ve formátu JSON nástrojem jq</a>.</div></p>



<p><a name="k04"></a></p>
<h2 id="k04">4. Chování konektoru pro zprávy bez schématu</h2>

<p>Nyní si otestujme, jak se bude konektor chovat v&nbsp;případě, že mu pošleme
zprávu <a href="#k03">z&nbsp;předchozí kapitoly</a>, tedy správu bez
schématu:</p>

<pre>
{
  "ID": 1,
  "Name": "Linus",
  "Surname": "Torvalds"
}
</pre>

<p>Po poslání zprávy <strong>bad_msg.json</strong> do námi používaného tématu
zůstane soubor <strong>test.sink5.jsons</strong> prázdný:</p>

<pre>
$ <strong>file test.sink5.jsons </strong>
&nbsp;
test.sink5.jsons: empty
</pre>

<p>To musí nutně znamenat, že zpráva byla uložena do DLQ, o čemž se ostatně
snadno přesvědčíme zadáním příkazu:</p>

<pre>
kafkacat -b localhost:9092 -t dlq_bad_jsons -C \
  -f '\nKey (%K bytes): %k
  Value (%S bytes): %s
  Timestamp: %T
  Partition: %p
  Offset: %o
  Headers: %h\n'
</pre>

<p>S&nbsp;tímto výsledkem (pro větší přehlednost vynecháme záznamy do DLQ
provedené v&nbsp;rámci předchozího článku):</p>

<pre>
Key (-1 bytes): 
  Value (44 bytes): {"ID":1,"Name":"Linus","Surname":"Torvalds"}
  Timestamp: 1676301868176
  Partition: 0
  Offset: 163
  Headers: 
</pre>



<p><a name="k05"></a></p>
<h2 id="k05">5. Přidání schématu přímo do zprávy</h2>

<p>Jedním z&nbsp;možných řešení &bdquo;schematizace&ldquo; zpráv je přidání
schématu přímo do vlastní zprávy. V&nbsp;tomto případě se tělo zprávy skládá
z&nbsp;JSONu, který obsahuje dvojici atributů nazvaných <strong>schema</strong>
a <strong>payload</strong>, přičemž význam obou atributů je zřejmý už
z&nbsp;jejich názvu &ndash; první z&nbsp;atributů obsahuje schéma, druhý
vlastní data. Z&nbsp;následující ukázky je zřejmé, jak může vypadat schéma
zprávy předepisující JSON objekt se třemi atributy (<i>fields</i>), které jsou
povinné a jsou typu řetězec:</p>

<pre>
{
  "schema": {
    "type": "struct",
    "optional": false,
    "version": 1,
    "fields": [
      {
        "field": "ID",
        "type": "string",
        "optional": false
      },
      {
        "field": "Name",
        "type": "string",
        "optional": false
      },
      {
        "field": "Surname",
        "type": "string",
        "optional": false
      }
    ]
  },
  "payload": {
    "ID": "1",
    "Name": "Linus",
    "Surname": "Torvalds"
  }
}
</pre>

<p>Vyzkoušejme nyní do tématu tuto zprávu poslat:</p>

<pre>
$ <strong>jq -c . msg3.json | bin/kafka-console-producer.sh --broker-list localhost:9092 --topic connect-test-json</strong>
</pre>

<p>Nyní již bude soubor zachycující zprávy obsahovat tento řádek:</p>

<pre>
$ <strong>cat test.sink5.jsons </strong>
&nbsp;
Struct{ID=1,Name=Linus,Surname=Torvalds}
</pre>

<p><div class="rs-tip-major">Poznámka: povšimněte si, jak se změnil formát
zprávy &ndash; nyní se jedná o strukturu s&nbsp;pojmenovanými atributy a bez
schématu.</div></p>



<p><a name="k06"></a></p>
<h2 id="k06">6. Atribut typu &bdquo;celé číslo&ldquo;</h2>

<p></p>

<pre>
{
  "schema": {
    "type": "struct",
    "optional": false,
    "version": 1,
    "fields": [
      {
        "field": "ID",
        "type": "int64",
        "optional": false
      },
      {
        "field": "Name",
        "type": "string",
        "optional": false
      },
      {
        "field": "Surname",
        "type": "string",
        "optional": false
      }
    ]
  },
  "payload": {
    "ID": 1,
    "Name": "Linus",
    "Surname": "Torvalds"
  }
}
</pre>




<p><a name="k07"></a></p>
<h2 id="k07">7. Povinné a volitelné atributy</h2>

<p></p>

<pre>
{
  "schema": {
    "type": "struct",
    "optional": false,
    "version": 1,
    "fields": [
      {
        "field": "ID",
        "type": "int64",
        "optional": false
      },
      {
        "field": "Name",
        "type": "string",
        "optional": false
      },
      {
        "field": "Surname",
        "type": "string",
        "optional": false
      }
    ]
  },
  "payload": {
    "ID": 1,
    "Surname": "Torvalds"
  }
}
</pre>

<pre>
{
  "schema": {
    "type": "struct",
    "optional": false,
    "version": 1,
    "fields": [
      {
        "field": "ID",
        "type": "int64",
        "optional": false
      },
      {
        "field": "Name",
        "type": "string",
        "optional": true
      },
      {
        "field": "Surname",
        "type": "string",
        "optional": false
      }
    ]
  },
  "payload": {
    "ID": 1,
    "Surname": "Torvalds"
  }
}
</pre>



<p><a name="k08"></a></p>
<h2 id="k08">8. </h2>

<p></p>



<p><a name="k09"></a></p>
<h2 id="k09">9. Zalogování konkrétního problému při zpracování zprávy v&nbsp;DLQ</h2>

<p></p>

<pre>
name=local-file-sink-json-checked
connector.class=FileStreamSink
tasks.max=1
file=test.sink6.jsons
topics=connect-test-json
key.converter=org.apache.kafka.connect.json.JsonConverter
value.converter=org.apache.kafka.connect.json.JsonConverter
key.converter.schemas.enable=true
value.converter.schemas.enable=true
errors.tolerance=all
errors.deadletterqueue.topic.name=dlq_bad_jsons
errors.deadletterqueue.topic.replication.factor=1
errors.deadletterqueue.context.headers.enable=true
</pre>



<p><a name="k10"></a></p>
<h2 id="k10">10. </h2>

<p></p>

<pre>
Key (-1 bytes): 
  Value (44 bytes): {"ID":1,"Name":"Linus","Surname":"Torvalds"}
  Timestamp: 1676302494257
  Partition: 0
  Offset: 164
  Headers: __connect.errors.topic=connect-test-json,__connect.errors.partition=0,__connect.errors.offset=113,__connect.errors.connector.name=local-file-sink-json-checked,__connect.errors.task.id=0,__connect.errors.stage=VALUE_CONVERTER,__connect.errors.class.name=org.apache.kafka.connect.json.JsonConverter,__connect.errors.exception.class.name=org.apache.kafka.connect.errors.DataException,__connect.errors.exception.message=JsonConverter with schemas.enable requires "schema" and "payload" fields and may not contain additional fields. If you are trying to deserialize plain JSON data, set schemas.enable=false in your converter configuration.,__connect.errors.exception.stacktrace=org.apache.kafka.connect.errors.DataException: JsonConverter with schemas.enable requires "schema" and "payload" fields and may not contain additional fields. If you are trying to deserialize plain JSON data, set schemas.enable=false in your converter configuration.
	at org.apache.kafka.connect.json.JsonConverter.toConnectData(JsonConverter.java:328)
	at org.apache.kafka.connect.storage.Converter.toConnectData(Converter.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.lambda$convertAndTransformRecord$4(WorkerSinkTask.java:516)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndRetry(RetryWithToleranceOperator.java:173)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:207)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execute(RetryWithToleranceOperator.java:149)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertAndTransformRecord(WorkerSinkTask.java:516)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertMessages(WorkerSinkTask.java:493)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:332)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:234)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:203)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:189)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:244)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)


Key (-1 bytes): 
  Value (258 bytes): {"schema":{"type":"struct","optional":false,"version":1,"fields":[{"field":"ID","type":"int64","optional":false},{"field":"Name","type":"string","optional":false},{"field":"Surname","type":"string","optional":false}]},"payload":{"ID":1,"Surname":"Torvalds"}}
  Timestamp: 1676302522399
  Partition: 0
  Offset: 165
  Headers: __connect.errors.topic=connect-test-json,__connect.errors.partition=0,__connect.errors.offset=116,__connect.errors.connector.name=local-file-sink-json-checked,__connect.errors.task.id=0,__connect.errors.stage=VALUE_CONVERTER,__connect.errors.class.name=org.apache.kafka.connect.json.JsonConverter,__connect.errors.exception.class.name=org.apache.kafka.connect.errors.DataException,__connect.errors.exception.message=Invalid null value for required STRING field,__connect.errors.exception.stacktrace=org.apache.kafka.connect.errors.DataException: Invalid null value for required STRING field
	at org.apache.kafka.connect.json.JsonConverter.convertToConnect(JsonConverter.java:685)
	at org.apache.kafka.connect.json.JsonConverter.lambda$static$11(JsonConverter.java:133)
	at org.apache.kafka.connect.json.JsonConverter.convertToConnect(JsonConverter.java:730)
	at org.apache.kafka.connect.json.JsonConverter.toConnectData(JsonConverter.java:343)
	at org.apache.kafka.connect.storage.Converter.toConnectData(Converter.java:88)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.lambda$convertAndTransformRecord$4(WorkerSinkTask.java:516)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndRetry(RetryWithToleranceOperator.java:173)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:207)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execute(RetryWithToleranceOperator.java:149)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertAndTransformRecord(WorkerSinkTask.java:516)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertMessages(WorkerSinkTask.java:493)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:332)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:234)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:203)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:189)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:244)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
</pre>



<p><a name="k11"></a></p>
<h2 id="k11">11. Zápis přijatých zpráv do relační databáze</h2>

<p></p>



<p><a name="k12"></a></p>
<h2 id="k12">12. </h2>

<p></p>

<pre>
name=db-sink
connector.class=io.confluent.connect.jdbc.JdbcSinkConnector
tasks.max=1
topics=connect-test-db
key.converter=org.apache.kafka.connect.json.JsonConverter
value.converter=org.apache.kafka.connect.json.JsonConverter
key.converter.schemas.enable=true
value.converter.schemas.enable=true
connection.url=jdbc:postgresql://localhost:5432/kafka_sink?user=postgres&amp;password=postgres
auto.create=true
delete.enabled=false
</pre>



<p><a name="k13"></a></p>
<h2 id="k13">13. </h2>

<p></p>

<pre>
name=db-sink
connector.class=io.confluent.connect.jdbc.JdbcSinkConnector
tasks.max=1
topics=test_table
key.converter=org.apache.kafka.connect.json.JsonConverter
value.converter=org.apache.kafka.connect.json.JsonConverter
key.converter.schemas.enable=true
value.converter.schemas.enable=true
connection.url=jdbc:postgresql://localhost:5432/kafka_sink?user=postgres&amp;password=postgres
auto.create=true
delete.enabled=false
</pre>



<p><a name="k14"></a></p>
<h2 id="k14">14. </h2>

<p></p>

<pre>
[2023-02-13 19:38:35,693] INFO [db-sink|task-0] Checking PostgreSql dialect for existence of TABLE "test_table" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:586)
[2023-02-13 19:38:35,698] INFO [db-sink|task-0] Using PostgreSql dialect TABLE "test_table" absent (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:594)
[2023-02-13 19:38:35,700] INFO [db-sink|task-0] Creating table with sql: CREATE TABLE "test_table" (
"ID" TEXT NOT NULL,
"Name" TEXT NOT NULL,
"Surname" TEXT NOT NULL) (io.confluent.connect.jdbc.sink.DbStructure:122)
[2023-02-13 19:38:35,710] INFO [db-sink|task-0] Checking PostgreSql dialect for existence of TABLE "test_table" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:586)
[2023-02-13 19:38:35,715] INFO [db-sink|task-0] Using PostgreSql dialect TABLE "test_table" present (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:594)
[2023-02-13 19:38:35,727] INFO [db-sink|task-0] Checking PostgreSql dialect for type of TABLE "test_table" (io.confluent.connect.jdbc.dialect.GenericDatabaseDialect:880)
[2023-02-13 19:38:35,730] INFO [db-sink|task-0] Setting metadata for table "test_table" to Table{name='"test_table"', type=TABLE columns=[Column{'Name', isPrimaryKey=false, allowsNull=false, sqlType=text}, Column{'Surname', isPrimaryKey=false, allowsNull=false, sqlType=text}, Column{'ID', isPrimaryKey=false, allowsNull=false, sqlType=text}]} (io.confluent.connect.jdbc.util.TableDefinitions:64)
</pre>

<pre>
kafka_sink=# \d
              List of relations
 Schema |      Name       | Type  |  Owner   
--------+-----------------+-------+----------
 public | connect-test-db | table | postgres
 public | test_table      | table | postgres
(2 rows)
</pre>

<pre>
kafka_sink=# \d+ test_table
                     Table "public.test_table"
 Column  | Type | Modifiers | Storage  | Stats target | Description 
---------+------+-----------+----------+--------------+-------------
 ID      | text | not null  | extended |              | 
 Name    | text | not null  | extended |              | 
 Surname | text | not null  | extended |              | 
</pre>

<pre>
kafka_sink=# select * from test_table;
 ID | Name  | Surname  
----+-------+----------
 1  | Linus | Torvalds
(1 row)
</pre>



<p><a name="k15"></a></p>
<h2 id="k15">15. </h2>

<p></p>



<p><a name="k16"></a></p>
<h2 id="k16">16. </h2>

<p></p>



<p><a name="k17"></a></p>
<h2 id="k17">17. </h2>

<p></p>



<p><a name="k18"></a></p>
<h2 id="k18">18. </h2>

<p></p>



<p><a name="k19"></a></p>
<h2 id="k19">19. Repositář s&nbsp;demonstračními příklady, konfiguračními soubory a testovacími zprávami</h2>

<p>Demonstrační příklady ukázané <a
href="https://www.root.cz/clanky/kafka-connect-tvorba-producentu-a-konzumentu-bez-zdrojoveho-kodu/">minule</a>
byly společně s&nbsp;konfiguracemi konektorů určenými pro <i>Kafka Connect</i>
a ukázkovými zprávami uloženy do repositáře, jenž je dostupný na adrese <a
href="https://github.com/tisnik/slides/">https://github.com/tisnik/slides/</a>.
V&nbsp;případě, že nebudete chtít klonovat celý repositář, můžete namísto toho
použít odkazy na jednotlivé demonstrační příklady i další soubory, které
naleznete v&nbsp;následující tabulce:</p>

<table>
<tr><th> #</th><th>Soubor</th><th>Stručný popis</th><th>Adresa</th></tr>
<tr><td> 1</td><td>producer1.go</td><td>jednoduchý producent zpráv pro Apache Kafku naprogramovaný v&nbsp;jazyku Go (Sarama)</td><td><a href="https://github.com/tisnik/slides/blob/master/files/kafka/producer1.go">https://github.com/tisnik/slides/blob/master/files/kafka/producer1.go</a></td></tr>
<tr><td> 2</td><td>producer2.go</td><td>jednoduchý producent zpráv pro Apache Kafku naprogramovaný v&nbsp;jazyku Go (Sarama)</td><td><a href="https://github.com/tisnik/slides/blob/master/files/kafka/producer2.go">https://github.com/tisnik/slides/blob/master/files/kafka/producer2.go</a></td></tr>
<tr><td> 3</td><td>consumer1.go</td><td>jednoduchý konzument zpráv pro Apache Kafku naprogramovaný v&nbsp;jazyku Go (Confluent Kafka)</td><td><a href="https://github.com/tisnik/slides/blob/master/files/kafka/consumer1.go">https://github.com/tisnik/slides/blob/master/files/kafka/consumer1.go</a></td></tr>
<tr><td> 4</td><td>consumer2.go</td><td>jednoduchý konzument zpráv pro Apache Kafku naprogramovaný v&nbsp;jazyku Go (Confluent Kafka)</td><td><a href="https://github.com/tisnik/slides/blob/master/files/kafka/consumer2.go">https://github.com/tisnik/slides/blob/master/files/kafka/consumer2.go</a></td></tr>
<tr><td> 5</td><td>producer.py</td><td>jednoduchý producent zpráv pro Apache Kafku naprogramovaný v&nbsp;jazyku Python</td><td><a href="https://github.com/tisnik/slides/blob/master/files/kafka/producer.py">https://github.com/tisnik/slides/blob/master/files/kafka/producer.py</a></td></tr>
<tr><td> 6</td><td>consumer.py</td><td>jednoduchý konzument zpráv pro Apache Kafku naprogramovaný v&nbsp;jazyku Python</td><td><a href="https://github.com/tisnik/slides/blob/master/files/kafka/consumer.py">https://github.com/tisnik/slides/blob/master/files/kafka/consumer.py</a></td></tr>
<tr><td> 7</td><td>SimpleProducer.java</td><td>jednoduchý producent zpráv pro Apache Kafku naprogramovaný v&nbsp;jazyku Java</td><td><a href="https://github.com/tisnik/slides/blob/master/files/kafka/SimpleProducer.java">https://github.com/tisnik/slides/blob/master/files/kafka/SimpleProducer.java</a></td></tr>
<tr><td> 8</td><td>SimpleConsumer.java</td><td>jednoduchý konzument zpráv npro Apache Kafku aprogramovaný v&nbsp;jazyku Java</td><td><a href="https://github.com/tisnik/slides/blob/master/files/kafka/SimpleConsumer.java">https://github.com/tisnik/slides/blob/master/files/kafka/SimpleConsumer.java</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td> 9</td><td>connect-file-sink.properties</td><td>konfigurace konektoru technologie Kafka Connect: zápis přijatých zpráv (v&nbsp;řetězcovém formátu) do souboru</td><td><a href="https://github.com/tisnik/slides/blob/master/files/kafka/connect-file-sink.properties">https://github.com/tisnik/slides/blob/master/files/kafka/connect-file-sink.properties</a></td></tr>
<tr><td>10</td><td>connect-file-sink-2.properties</td><td>konfigurace konektoru technologie Kafka Connect: zápis přijatých zpráv ve formátu JSON bez kontroly schématu</td><td><a href="https://github.com/tisnik/slides/blob/master/files/kafka/connect-file-sink-2.properties">https://github.com/tisnik/slides/blob/master/files/kafka/connect-file-sink-2.properties</a></td></tr>
<tr><td>11</td><td>connect-file-sink-3.properties</td><td>konfigurace konektoru technologie Kafka Connect: dtto, ale konektor není ukončen po přijetí nekorektní zprávy</td><td><a href="https://github.com/tisnik/slides/blob/master/files/kafka/connect-file-sink-3.properties">https://github.com/tisnik/slides/blob/master/files/kafka/connect-file-sink-3.properties</a></td></tr>
<tr><td>12</td><td>connect-file-sink-4.properties</td><td>konfigurace konektoru technologie Kafka Connect: nekorektní zprávy budou uloženy do DLQ</td><td><a href="https://github.com/tisnik/slides/blob/master/files/kafka/connect-file-sink-4.properties">https://github.com/tisnik/slides/blob/master/files/kafka/connect-file-sink-4.properties</a></td></tr>
<tr><td>13</td><td>connect-file-sink-5.properties</td><td>konfigurace konektoru technologie Kafka Connect: kontrola zpráv oproti schématu</td><td><a href="https://github.com/tisnik/slides/blob/master/files/kafka/connect-file-sink-5.properties">https://github.com/tisnik/slides/blob/master/files/kafka/connect-file-sink-5.properties</a></td></tr>
<tr><td>14</td><td>connect-file-sink-6.properties</td><td>konfigurace konektoru technologie Kafka Connect: zápis konkrétní chyby do DLQ</td><td><a href="https://github.com/tisnik/slides/blob/master/files/kafka/connect-file-sink-6.properties">https://github.com/tisnik/slides/blob/master/files/kafka/connect-file-sink-6.properties</a></td></tr>
<tr><td>15</td><td>db-sink-1.properties</td><td>konfigurace konektoru technologie Kafka Connect: uložení zpráv do databáze</td><td><a href="https://github.com/tisnik/slides/blob/master/files/kafka/db-sink-1.properties">https://github.com/tisnik/slides/blob/master/files/kafka/db-sink-1.properties</a></td></tr>
<tr><td>16</td><td>db-sink-2.properties</td><td>konfigurace konektoru technologie Kafka Connect: uložení zpráv do databáze (vhodnější jméno tabulky)</td><td><a href="https://github.com/tisnik/slides/blob/master/files/kafka/db-sink-2.properties">https://github.com/tisnik/slides/blob/master/files/kafka/db-sink-2.properties</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>17</td><td>msg1.json</td><td>zpráva uložená ve formátu JSON obsahující i schéma</td><td><a href="https://github.com/tisnik/slides/blob/master/files/kafka/msg1.json">https://github.com/tisnik/slides/blob/master/files/kafka/msg1.json</a></td></tr>
<tr><td>18</td><td>msg2.json</td><td>schéma s&nbsp;jedním celočíselným atributem, včetně vlastního těla zprávy</td><td><a href="https://github.com/tisnik/slides/blob/master/files/kafka/msg2.json">https://github.com/tisnik/slides/blob/master/files/kafka/msg2.json</a></td></tr>
<tr><td>19</td><td>msg3.json</td><td>nekorektní zpráva: chybějící povinné atributy předepsané schématem</td><td><a href="https://github.com/tisnik/slides/blob/master/files/kafka/msg3.json">https://github.com/tisnik/slides/blob/master/files/kafka/msg3.json</a></td></tr>
<tr><td>20</td><td>msg4.json</td><td>schéma s&nbsp;nepovinnými atributy + korektní zpráva, která odpovídá schématu</td><td><a href="https://github.com/tisnik/slides/blob/master/files/kafka/msg4.json">https://github.com/tisnik/slides/blob/master/files/kafka/msg4.json</a></td></tr>
<tr><td>21</td><td>bad_msg.json</td><td>zpráva ve formátu JSON, která ovšem neobsahuje schéma</td><td><a href="https://github.com/tisnik/slides/blob/master/files/kafka/bad_msg.json">https://github.com/tisnik/slides/blob/master/files/kafka/bad_msg.json</a></td></tr>
</table>



<p><a name="k20"></a></p>
<h2 id="k20">20. Odkazy na Internetu</h2>

<ol>

<li>Kafka Connect and Schemas<br />
<a href="https://rmoff.net/2020/01/22/kafka-connect-and-schemas/">https://rmoff.net/2020/01/22/kafka-connect-and-schemas/</a>
</li>

<li>JSON and schemas<br />
<a href="https://www.confluent.io/blog/kafka-connect-deep-dive-converters-serialization-explained/#json-schemas">https://www.confluent.io/blog/kafka-connect-deep-dive-converters-serialization-explained/#json-schemas</a>
</li>

<li>What, why, when to use Apache Kafka, with an example<br />
<a href="https://www.startdataengineering.com/post/what-why-and-how-apache-kafka/">https://www.startdataengineering.com/post/what-why-and-how-apache-kafka/</a>
</li>

<li>When NOT to use Apache Kafka?<br />
<a href="https://www.kai-waehner.de/blog/2022/01/04/when-not-to-use-apache-kafka/">https://www.kai-waehner.de/blog/2022/01/04/when-not-to-use-apache-kafka/</a>
</li>

<li>Microservices: The Rise Of Kafka<br />
<a href="https://movio.co/blog/microservices-rise-kafka/">https://movio.co/blog/microservices-rise-kafka/</a>
</li>

<li>Building a Microservices Ecosystem with Kafka Streams and KSQL<br />
<a href="https://www.confluent.io/blog/building-a-microservices-ecosystem-with-kafka-streams-and-ksql/">https://www.confluent.io/blog/building-a-microservices-ecosystem-with-kafka-streams-and-ksql/</a>
</li>

<li>An introduction to Apache Kafka and microservices communication<br />
<a href="https://medium.com/@ulymarins/an-introduction-to-apache-kafka-and-microservices-communication-bf0a0966d63">https://medium.com/@ulymarins/an-introduction-to-apache-kafka-and-microservices-communication-bf0a0966d63</a>
</li>

<li>kappa-architecture.com<br />
<a href="http://milinda.pathirage.org/kappa-architecture.com/">http://milinda.pathirage.org/kappa-architecture.com/</a>
</li>

<li>Questioning the Lambda Architecture<br />
<a href="https://www.oreilly.com/ideas/questioning-the-lambda-architecture">https://www.oreilly.com/ideas/questioning-the-lambda-architecture</a>
</li>

<li>Lambda architecture<br />
<a href="https://en.wikipedia.org/wiki/Lambda_architecture">https://en.wikipedia.org/wiki/Lambda_architecture</a>
</li>

<li>Kafka &ndash; ecosystem (Wiki)<br />
<a href="https://cwiki.apache.org/confluence/display/KAFKA/Ecosystem">https://cwiki.apache.org/confluence/display/KAFKA/Ecosystem</a>
</li>

<li>The Kafka Ecosystem - Kafka Core, Kafka Streams, Kafka Connect, Kafka REST Proxy, and the Schema Registry<br />
<a href="http://cloudurable.com/blog/kafka-ecosystem/index.html">http://cloudurable.com/blog/kafka-ecosystem/index.html</a>
</li>

<li>A Kafka Operator for Kubernetes<br />
<a href="https://github.com/krallistic/kafka-operator">https://github.com/krallistic/kafka-operator</a>
</li>

<li>Kafka Streams<br />
<a href="https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Streams">https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Streams</a>
</li>

<li>Kafka Streams<br />
<a href="http://kafka.apache.org/documentation/streams/">http://kafka.apache.org/documentation/streams/</a>
</li>

<li>Kafka Streams (FAQ)<br />
<a href="https://cwiki.apache.org/confluence/display/KAFKA/FAQ#FAQ-Streams">https://cwiki.apache.org/confluence/display/KAFKA/FAQ#FAQ-Streams</a>
</li>

<li>Event stream processing<br />
<a href="https://en.wikipedia.org/wiki/Event_stream_processing">https://en.wikipedia.org/wiki/Event_stream_processing</a>
</li>

<li>Part 1: Apache Kafka for beginners - What is Apache Kafka?<br />
<a href="https://www.cloudkarafka.com/blog/2016-11-30-part1-kafka-for-beginners-what-is-apache-kafka.html">https://www.cloudkarafka.com/blog/2016-11-30-part1-kafka-for-beginners-what-is-apache-kafka.html</a>
</li>

<li>What are some alternatives to Apache Kafka?<br />
<a href="https://www.quora.com/What-are-some-alternatives-to-Apache-Kafka">https://www.quora.com/What-are-some-alternatives-to-Apache-Kafka</a>
</li>

<li>What is the best alternative to Kafka?<br />
<a href="https://www.slant.co/options/961/alternatives/~kafka-alternatives">https://www.slant.co/options/961/alternatives/~kafka-alternatives</a>
</li>

<li>A super quick comparison between Kafka and Message Queues<br />
<a href="https://hackernoon.com/a-super-quick-comparison-between-kafka-and-message-queues-e69742d855a8?gi=e965191e72d0">https://hackernoon.com/a-super-quick-comparison-between-kafka-and-message-queues-e69742d855a8?gi=e965191e72d0</a>
</li>

<li>Kafka Queuing: Kafka as a Messaging System<br />
<a href="https://dzone.com/articles/kafka-queuing-kafka-as-a-messaging-system">https://dzone.com/articles/kafka-queuing-kafka-as-a-messaging-system</a>
</li>

<li>Configure Self-Managed Connectors<br />
<a href="https://docs.confluent.io/kafka-connectors/self-managed/configuring.html#configure-self-managed-connectors">https://docs.confluent.io/kafka-connectors/self-managed/configuring.html#configure-self-managed-connectors</a>
</li>

<li>Schema Evolution and Compatibility<br />
<a href="https://docs.confluent.io/platform/current/schema-registry/avro.html#schema-evolution-and-compatibility">https://docs.confluent.io/platform/current/schema-registry/avro.html#schema-evolution-and-compatibility</a>
</li>

<li>Configuring Key and Value Converters<br />
<a href="https://docs.confluent.io/kafka-connectors/self-managed/userguide.html#configuring-key-and-value-converters">https://docs.confluent.io/kafka-connectors/self-managed/userguide.html#configuring-key-and-value-converters</a>
</li>

<li>Introduction to Kafka Connectors<br />
<a href="https://www.baeldung.com/kafka-connectors-guide">https://www.baeldung.com/kafka-connectors-guide</a>
</li>

<li>Kafka CLI: command to list all consumer groups for a topic?<br />
<a href="https://stackoverflow.com/questions/63883999/kafka-cli-command-to-list-all-consumer-groups-for-a-topic">https://stackoverflow.com/questions/63883999/kafka-cli-command-to-list-all-consumer-groups-for-a-topic</a>
</li>

<li>Java Property File Processing<br />
<a href="https://www.w3resource.com/java-tutorial/java-propertyfile-processing.php">https://www.w3resource.com/java-tutorial/java-propertyfile-processing.php</a>
</li>

<li>Skipping bad records with the Kafka Connect JDBC sink connector<br />
<a href="https://rmoff.net/2019/10/15/skipping-bad-records-with-the-kafka-connect-jdbc-sink-connector/">https://rmoff.net/2019/10/15/skipping-bad-records-with-the-kafka-connect-jdbc-sink-connector/</a>
</li>

<li>Kafka Connect Deep Dive – Error Handling and Dead Letter Queues<br />
<a href="https://www.confluent.io/blog/kafka-connect-deep-dive-error-handling-dead-letter-queues/">https://www.confluent.io/blog/kafka-connect-deep-dive-error-handling-dead-letter-queues/</a>
</li>

<li>Errors and Dead Letter Queues<br />
<a href="https://developer.confluent.io/learn-kafka/kafka-connect/error-handling-and-dead-letter-queues/">https://developer.confluent.io/learn-kafka/kafka-connect/error-handling-and-dead-letter-queues/</a>
</li>

<li>Confluent Cloud Dead Letter Queue<br />
<a href="https://docs.confluent.io/cloud/current/connectors/dead-letter-queue.html">https://docs.confluent.io/cloud/current/connectors/dead-letter-queue.html</a>
</li>

<li>Dead Letter Queues (DLQs) in Kafka<br />
<a href="https://medium.com/@sannidhi.s.t/dead-letter-queues-dlqs-in-kafka-afb4b6835309">https://medium.com/@sannidhi.s.t/dead-letter-queues-dlqs-in-kafka-afb4b6835309</a>
</li>

<li>Deserializer<br />
<a href="https://docs.confluent.io/platform/current/schema-registry/serdes-develop/serdes-json.html#json-schema-serializer-and-deserializer">https://docs.confluent.io/platform/current/schema-registry/serdes-develop/serdes-json.html#json-schema-serializer-and-deserializer</a>
</li>

<li>JSON, Kafka, and the need for schema<br />
<a href="https://mikemybytes.com/2022/07/11/json-kafka-and-the-need-for-schema/">https://mikemybytes.com/2022/07/11/json-kafka-and-the-need-for-schema/</a>
</li>

<li>Using Kafka Connect with Schema Registry<br />
<a href="https://docs.confluent.io/platform/current/schema-registry/connect.html">https://docs.confluent.io/platform/current/schema-registry/connect.html</a>
</li>

<li>Zpracování dat reprezentovaných ve formátu JSON nástrojem jq<br />
<a href="https://www.root.cz/clanky/zpracovani-dat-reprezentovanych-ve-formatu-json-nastrojem-jq/">https://www.root.cz/clanky/zpracovani-dat-reprezentovanych-ve-formatu-json-nastrojem-jq/</a>
</li>

<li>Repositář projektu jq (GitHub)<br />
<a href="https://github.com/stedolan/jq">https://github.com/stedolan/jq</a>
</li>

<li>GitHub stránky projektu jq<br />
<a href="https://stedolan.github.io/jq/">https://stedolan.github.io/jq/</a>
</li>

<li>5 modern alternatives to essential Linux command-line tools<br />
<a href="https://opensource.com/ar­ticle/20/6/modern-linux-command-line-tools">https://opensource.com/ar­ticle/20/6/modern-linux-command-line-tools</a>
</li>

<li>Návod k nástroji jq<br />
<a href="https://stedolan.github.i­o/jq/tutorial/">https://stedolan.github.i­o/jq/tutorial/</a>
</li>

<li>jq Manual (development version)<br />
<a href="https://stedolan.github.io/jq/manual/">https://stedolan.github.io/jq/manual/</a>
</li>

<li>Introducing JSON<br />
<a href="https://www.json.org/json-en.html">https://www.json.org/json-en.html</a>
</li>

</ol>



<p></p><p></p>
<p><small>Autor: <a href="http://www.fit.vutbr.cz/~tisnovpa">Pavel Tišnovský</a> &nbsp; 2023</small></p>
</body>
</html>

