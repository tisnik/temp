<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
        "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html>
<head>
<title></title>
<meta name="Author" content="Pavel Tisnovsky" />
<meta name="Generator" content="vim" />
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<style type="text/css">
         body {color:#000000; background:#ffffff;}
         h1  {font-family: arial, helvetica, sans-serif; color:#ffffff; background-color:#c00000; text-align:center; padding-left:1em}
         h2  {font-family: arial, helvetica, sans-serif; color:#ffffff; background-color:#0000c0; padding-left:1em; text-align:left}
         h3  {font-family: arial, helvetica, sans-serif; color:#000000; background-color:#c0c0c0; padding-left:1em; text-align:left}
         h4  {font-family: arial, helvetica, sans-serif; color:#000000; background-color:#e0e0e0; padding-left:1em; text-align:left}
         a   {font-family: arial, helvetica, sans-serif;}
         li  {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify; width:450px;}
         ol  {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify; width:450px;}
         ul  {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify; width:450px;}
         p   {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify;}
         pre {background:#e0e0e0}
</style>
</head>

<body>

<h1></h1>

<h3>Pavel Tišnovský</h3>

<p></p>

<h1>Úvodník</h1>

<p></p>



<h2>Obsah</h2>

<p><a href="#k01">1. Nejjednodušší prakticky použitelná neuronová síť: výběr jednoho vstupu ze dvou dostupných</a></p>
<p><a href="#k02">2. Výsledky získané otestování jednoduché neuronové sítě</a></p>
<p><a href="#k03">3. Odstranění náhody z&nbsp;procesu tréninku a testování neuronové sítě</a></p>
<p><a href="#k04">4. Skript pro trénink a otestování neuronové sítě se stabilními výsledky</a></p>
<p><a href="#k05">5. Stabilní výsledky získané předchozím skriptem</a></p>
<p><a href="#k06">6. Vylepšení tréninku neuronové sítě zvýšením počtu trénovacích vzorků</a></p>
<p><a href="#k07">7. Skript, který postupně zvětšuje počet vzorků použitých pro trénink neuronové sítě</a></p>
<p><a href="#k08">8. Výsledky běhu předchozího skriptu</a></p>
<p><a href="#k09">9. Grafické znázornění závislosti MSE, vah neuronů a biasu na počtu vzorků</a></p>
<p><a href="#k10">10. Výsledky získané po běhu skriptu</a></p>
<p><a href="#k11">11. Zrychlení tréninku neuronové sítě snížením počtu iterací</a></p>
<p><a href="#k12">12. Výsledky získané po běhu skriptu: snížení počtu iterací při tréninku neuronové sítě</a></p>
<p><a href="#k13">13. Vyšší míra změny vah na vstupu neuronů při tréninku</a></p>
<p><a href="#k14">14. Výsledky získané po běhu skriptu: vyšší míra změna vah neuronů</a></p>
<p><a href="#k15">15. Riziko příliš vysoké hodnoty <strong>learning_rate_init</strong></a></p>
<p><a href="#k16">16. Výsledky získané po běhu skriptu: vysoká míra změny vah neuronů</a></p>
<p><a href="#k17">*** 17. Opačný extrém &ndash; příliš malá hodnota <strong>learning_rate_init</strong></a></p>
<p><a href="#k18">*** 18. </a></p>
<p><a href="#k19">*** 19. Repositář s&nbsp;demonstračními příklady</a></p>
<p><a href="#k20">20. Odkazy na Internetu</a></p>



<p><a name="k01"></a></p>
<h2 id="k01">1. Nejjednodušší prakticky použitelná neuronová síť: výběr jednoho vstupu ze dvou dostupných</h2>

<p>Pro pochopení toho, jaký vliv mají hyperparametry modelu na výslednou
neuronovou síť si vytvoříme tu nejjednodušší ještě prakticky použitelnou síť,
která bude mít dva vstupy a jediný výstup. Bude se jednat o regresní síť, která
bude natrénována tak, aby z&nbsp;obou vstupů vybrala vždy hodnotu
z&nbsp;jednoho předem určeného vstupu (určení, o který vstup se jedná, se
provádí tréninkem). Například síť natrénujeme takovým způsobem, aby vždy
vybrala hodnotu druhého vstupu a tu poslala na výstup (jedná se o
&bdquo;analogovou&ldquo; hodnotu, ne o logické hradlo
resp.&nbsp;demultiplexor). A navíc síť zjednodušíme do nejmenší možné
velikosti, protože bude mít pouze dva neurony ve vstupní vrstvě a jediný neuron
ve vrstvě výstupní. Skryté vrstvy nebudou existovat. To tedy znamená, že
namísto relativně složité sítě typu:</p>

*** image ***
<p><i>Obrázek 1: Neuronová síť s&nbsp;více skrytými vrstvami.</i></p>

<p>Budeme mít síť s&nbsp;celkem pouze třemi neurony:</p>

*** image ***
<p><i>Obrázek 2: Neuronová síť bez skrytých vrstev; vstupní vrstva má dva
neurony, výstupní vrstva neuron jediný.</i></p>

<p>Skript, který si připraví data pro trénink a testování (začneme na 10
záznamech), zkonstruuje síť a následně ověří její (ne)funkčnost, může vypadat
následovně. Naprostou většinu konceptů použitých v&nbsp;tomto skriptu již
známe:</p>

<pre>
import numpy as np
&nbsp;
import random
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
&nbsp;
<i># model zalozeny na neuronove siti</i>
from sklearn.neural_network import MLPRegressor
&nbsp;
<i># velikost vstupu</i>
MAX_N = 10
&nbsp;
<i># X je matice, y je vektor</i>
X = np.zeros( (MAX_N, 2) )   <i># kombinace vstupu</i>
y = np.zeros( (MAX_N, ))     <i># vektor vysledku</i>
&nbsp;
<i># priprava dat pro trenink i otestovani neuronove site</i>
for i in range(0, MAX_N):
    X[i, 0] = random.randint(-10, 10)
    X[i, 1] = random.randint(-10, 10)
    y[i] = X[i, 1]
&nbsp;
<i># rozdeleni dat na treninkovou a testovaci mnozinu</i>
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
&nbsp;
<i># konstrukce modelu</i>
nn = MLPRegressor(max_iter=5000, hidden_layer_sizes=())
&nbsp;
<i># trénink modelu</i>
nn.fit(X_train, y_train)
&nbsp;
<i># predikce modelu</i>
y_pred = nn.predict(X_test)
&nbsp;
<i># chyba predikce</i>
print("Mean squared error: %.2f" % mean_squared_error(y_test, y_pred))
&nbsp;
<i># 1 = nejlepší predikce modelu</i>
print("Coefficient of determination: %.2f" % r2_score(y_test, y_pred))
&nbsp;
<i># zobrazit parametry neuronove site</i>
print(f"Features: {nn.n_features_in_}")
print(f"Layers:   {nn.n_layers_}")
print(f"Outputs:  {nn.n_outputs_}")
print("Weights:")
&nbsp;
<i># vahy neuronu</i>
for layer, weights in enumerate(nn.coefs_):
    print("\t", layer, weights.shape)
    print(weights)
print()
&nbsp;
<i># posuny (dalsi vstup do neuronu)</i>
print("Biases:")
for layer, biases in enumerate(nn.intercepts_):
    print("\t", layer, biases.shape)
    print(biases)
print()
&nbsp;
<i># test neuronove site na (potencialne) nezname vstupy</i>
inputs = []
for i in range(0, MAX_N):
    for j in range(0, MAX_N):
        inputs.append([i, j])
predicted = nn.predict(inputs)
&nbsp;
print()
&nbsp;
<i># odhady neuronove site po zaokrouhleni</i>
all = 0
wrong = 0
&nbsp;
<i># projit vsemi odhady a najit spatne vystupy site</i>
for i, p in zip(inputs, predicted):
    <i># spatny odhad?</i>
    if i[1] != round(p):
        print(f"{i[0]:2}, {i[1]:2} = {round(p):2}")
        wrong += 1
    all += 1
&nbsp;
<i># vysledna statistika</i>
print(f"{wrong}/{all}")
</pre>



<p><a name="k02"></a></p>
<h2 id="k02">2. Výsledky získané otestování jednoduché neuronové sítě</h2>

<p>Užitečné bude zjistit, jak se bude výše nakonfigurovaná neuronová síť
chovat. Ovšem vzhledem k&nbsp;tomu, že se v&nbsp;průběhu tréninku používají
náhodná čísla, nebudou výsledky vždy totožné. Ovšem poměrně často je síť
natrénována tak vhodným způsobem, že má prakticky stoprocentní odpovědi:</p>

<pre>
Mean squared error: 0.10
Coefficient of determination: 0.00
</pre>

<p>Interní struktura takové sítě vypadá následovně:</p>

<pre>
Features: 2
Layers:   2
Outputs:  1
Weights:
	 0 (2, 1)
[[-0.00311307]
 [ 0.96168339]]
&nbsp;
Biases:
	 0 (1,)
[-0.08298607]
</pre>

<p>Povšimněte si toho, že váhy na vstupu neuronu ve výstupní vrstvě jsou
poměrně blízko hodnotám 0,0 a 1,0, což odpovídá požadované funkci sítě. A bias
(tedy váha pro posuny hodnot na vstupu tohoto neuronu) je taky prakticky
nulový.  Neuron tedy skutečně vybere druhou vstupní hodnotu, kterou předá do
aktivační funkce. Ta je pro kladné hodnoty funkcí lineární, takže ji můžeme
ignorovat. Ze vztahu:</p>

<p>
y = f(<strong>w<sub>0</sub></strong> + w<sub>1</sub>x<sub>1</sub> + w<sub>2</sub>x<sub>2</sub> + ... + w<sub>n</sub>x<sub>n</sub>)
</p>

<p>Tedy získáme tento výpočet, který neuron provádí:</p>

<p>
y = <strong>w<sub>0</sub></strong> + w<sub>1</sub>x<sub>1</sub> + w<sub>2</sub>x<sub>2</sub>
</p>

<p>A po dosazení ideálních hodnot:</p>

<p>
y = 0x<sub>1</sub> + 1x<sub>2</sub> = x<sub>2</sub>
</p>

<p>Ideálně natrénované neuronové síti odpovídá tento výsledek:</p>

<pre>
rounded:
0/100
</pre>

<p><div class="rs-tip-major">Poznámka: naše síť sice není natrénována ideálně,
ale pro zvolenou testovací sadu jsme tuto vlastnost (zatím)
neodhalili.</div></p>

<p>Ovšem v&nbsp;některých případech dopadne trénink odlišně, a to opět kvůli
(pseudo)náhodným hodnotám, které do celého procesu vstupují. Povšimněte si
odlišných vah a o snahu sítě vše &bdquo;vyvážit&ldquo; vyšším biasem:</p>

<pre>
Mean squared error: 0.49
Coefficient of determination: 0.92
Features: 2
Layers:   2
Outputs:  1
Weights:
	 0 (2, 1)
[[0.04786446]
 [0.95183809]]
&nbsp;
Biases:
	 0 (1,)
[0.47868983]
&nbsp;
&nbsp;
rounded:
 1,  0 =  1
 2,  0 =  1
 2,  1 =  2
 3,  0 =  1
 3,  1 =  2
 3,  2 =  3
 4,  0 =  1
 4,  1 =  2
 4,  2 =  3
 4,  3 =  4
 5,  0 =  1
 5,  1 =  2
 5,  2 =  3
 5,  3 =  4
 5,  4 =  5
 6,  0 =  1
 6,  1 =  2
 6,  2 =  3
 6,  3 =  4
 6,  4 =  5
 6,  5 =  6
 7,  0 =  1
 7,  1 =  2
 7,  2 =  3
 7,  3 =  4
 7,  4 =  5
 7,  5 =  6
 7,  6 =  7
 8,  0 =  1
 8,  1 =  2
 8,  2 =  3
 8,  3 =  4
 8,  4 =  5
 8,  5 =  6
 8,  6 =  7
 8,  7 =  8
 9,  0 =  1
 9,  1 =  2
 9,  2 =  3
 9,  3 =  4
 9,  4 =  5
 9,  5 =  6
 9,  6 =  7
 9,  7 =  8
 9,  8 =  9
45/100
</pre>



<p><a name="k03"></a></p>
<h2 id="k03">3. Odstranění náhody z&nbsp;procesu tréninku a testování neuronové sítě</h2>

<p>Abychom se vyhnuli tomu, že po každém spuštění procesu tréninku a testování
získáme poněkud odlišné hodnoty, pokusíme se odstranit všechny náhody (náhodná
čísla), která jsou používána. Nejprve nastavíme konstantní
&bdquo;semínko&ldquo; (<i>seed</i>) použité pro inicializaci generátoru
pseudonáhodných čísel. Tím zajistíme, že stejné pořadí volání funkce
<strong>random.randint()</strong> bude vracet stejné pořadí výsledků:</p>

<pre>
<i># zadne skutecne nahodne hodnoty</i>
<strong>random.seed(19)</strong>
&nbsp;
<i># priprava dat pro trenink i otestovani neuronove site</i>
for i in range(0, MAX_N):
    X[i, 0] = <strong>random.randint(-10, 10)</strong>
    X[i, 1] = <strong>random.randint(-10, 10)</strong>
    y[i] = X[i, 1]
</pre>

<p>Náhodu odstraníme i z&nbsp;procesu rozdělení datové sady na trénovací a
validační data:</p>

<pre>
<i># rozdeleni dat na treninkovou a testovaci mnozinu</i>
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, <strong>random_state=42</strong>)
</pre>

<p>A konečně předáme &bdquo;náhodný stav&ldquo;, který ovšem evidentně náhodný
není, i jako hyperparametr modelu:</p>

<pre>
<i># konstrukce modelu</i>
nn = MLPRegressor(max_iter=5000, hidden_layer_sizes=(), <strong>random_state=1000</strong>)
</pre>



<p><a name="k04"></a></p>
<h2 id="k04">4. Skript pro trénink a otestování neuronové sítě se stabilními výsledky</h2>

<p>Po úpravě zdrojového kódu tak, aby se v&nbsp;něm nevyskytovaly žádné náhodné
hodnoty, dojdeme k&nbsp;následujícímu skriptu, jenž by měl při každém spuštění
vždy odpovědět stejně:</p>

<pre>
import numpy as np
&nbsp;
import random
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
&nbsp;
<i># model zalozeny na neuronove siti</i>
from sklearn.neural_network import MLPRegressor
&nbsp;
<i># velikost vstupu</i>
MAX_N = 20
&nbsp;
<i># X je matice, y je vektor</i>
X = np.zeros( (MAX_N, 2) )   <i># kombinace vstupu</i>
y = np.zeros( (MAX_N, ))     <i># vektor vysledku</i>
&nbsp;
<i># zadne skutecne nahodne hodnoty</i>
random.seed(19)
&nbsp;
<i># priprava dat pro trenink i otestovani neuronove site</i>
for i in range(0, MAX_N):
    X[i, 0] = random.randint(-10, 10)
    X[i, 1] = random.randint(-10, 10)
    y[i] = X[i, 1]
&nbsp;
&nbsp;
<i># rozdeleni dat na treninkovou a testovaci mnozinu</i>
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
&nbsp;
<i># konstrukce modelu</i>
nn = MLPRegressor(max_iter=5000, hidden_layer_sizes=(), random_state=1000)
&nbsp;
<i># trénink modelu</i>
nn.fit(X_train, y_train)
&nbsp;
<i># predikce modelu</i>
y_pred = nn.predict(X_test)
&nbsp;
<i># chyba predikce</i>
print("Mean squared error: %.2f" % mean_squared_error(y_test, y_pred))
&nbsp;
<i># 1 = nejlepší predikce modelu</i>
print("Coefficient of determination: %.2f" % r2_score(y_test, y_pred))
&nbsp;
<i># zobrazit parametry neuronove site</i>
print(f"Features: {nn.n_features_in_}")
print(f"Layers:   {nn.n_layers_}")
print(f"Outputs:  {nn.n_outputs_}")
print("Weights:")
&nbsp;
<i># vahy neuronu</i>
for layer, weights in enumerate(nn.coefs_):
    print("\t", layer, weights.shape)
    print(weights)
print()
&nbsp;
<i># posuny (dalsi vstup do neuronu)</i>
print("Biases:")
for layer, biases in enumerate(nn.intercepts_):
    print("\t", layer, biases.shape)
    print(biases)
print()
&nbsp;
<i># test neuronove site na (potencialne) nezname vstupy</i>
inputs = []
for i in range(0, MAX_N):
    for j in range(0, MAX_N):
        inputs.append([i, j])
predicted = nn.predict(inputs)
&nbsp;
print()
&nbsp;
<i># odhady neuronove site po zaokrouhleni</i>
all = 0
wrong = 0
&nbsp;
for i, p in zip(inputs, predicted):
    <i># spatny odhad?</i>
    if i[1] != round(p):
        print(f"{i[0]:2}, {i[1]:2} = {round(p):2}")
        wrong += 1
    all += 1
&nbsp;
<i># vysledna statistika</i>
print(f"{wrong}/{all}")
</pre>



<p><a name="k05"></a></p>
<h2 id="k05">5. Stabilní výsledky získané předchozím skriptem</h2>

<p>Výsledky, které získáme po <i>každém</i> spuštění skriptu popsaného <a
href="#k04">v&nbsp;předchozí kapitole</a>, vypadají na mém počítači následovně
(teoreticky se totiž mohou výsledky na jiném počítači odlišovat, pokud je
použitý jiný algoritmus generování náhodných čísel):</p>

<pre>
Mean squared error: 0.08
Coefficient of determination: 1.00
Features: 2
Layers:   2
Outputs:  1
Weights:
	 0 (2, 1)
[[0.01515987]
 [0.96449027]]
&nbsp;
Biases:
	 0 (1,)
[0.08740665]
</pre>

<p>Povšimněte si, že síť nebyla natrénována zcela ideálně, protože váha u
druhého vstupu neuronu by mohla být blíže jedničce a naopak bias by mohl být
blíže k&nbsp;nule. Ne zcela ideálnímu natrénovaní odpovídá i několik chybných
odhadů nalezených při testování, ovšem není jich příliš mnoho &ndash; tři
procenta:</p>

<pre>
 0, 17 = 16
 0, 18 = 17
 0, 19 = 18
 1, 17 = 16
 1, 18 = 17
 1, 19 = 18
 2, 18 = 17
 2, 19 = 18
 3, 18 = 17
 3, 19 = 18
 4, 19 = 18
 5, 19 = 18
12/400
</pre>



<p><a name="k06"></a></p>
<h2 id="k06">6. Vylepšení tréninku neuronové sítě zvýšením počtu trénovacích vzorků</h2>

<p>Jak můžeme vylepšit chování neuronové sítě s&nbsp;pouhými dvěma vrstvami
(vstupní a výstupní) a třemi neurony? Z&nbsp;teoretického pohledu nemá smysl
zvyšovat počet vrstev ani neuronů, protože i jediný neuron dokáže jeden ze
vstupů vynásobit váhou 0 a druhý vstup vynásobit jedničkou &ndash; což je
přesně to, co od této sítě očekáváme. Ovlivnit však můžeme natrénování sítě,
tj.&nbsp;dosažení toho, aby váhové faktory byly skutečně nastaveny na [0.0,
1.0] a bias na 0.0. První způsob je ten nejznámější &ndash; jednoduše zvýšíme
počet vzorků použitých při tréninku neuronové sítě. Jaký vliv počet vzorků pro
trénink má, si lze relativně snadno ověřit (ovšem opět se vyhneme použití
náhodných čísel).</p>



<p><a name="k07"></a></p>
<h2 id="k07">7. Skript, který postupně zvětšuje počet vzorků použitých pro trénink neuronové sítě</h2>

<p>Podívejme se nyní na to, jak snadno bylo možné upravit skript <a
href="#k04">ze čtvrté kapitoly</a> takovým způsobem, že se postupně zvětšuje
počet vzorků určených pro trénink (vzorky pro ověření sítě se nemění).
Jednoduše jsme do funkce <strong>train_and_test_nn</strong> předáme
celočíselnou hodnotu, kterou použijeme pro &bdquo;vyseknutí&ldquo; trénovacích
a validačních dat z&nbsp;připravené sady čtyřiceti vzorků. Využijeme
k&nbsp;tomu operaci řezu (<i>slice</i>):</p>

<pre>
import numpy as np
&nbsp;
import random
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
&nbsp;
<i># model zalozeny na neuronove siti</i>
from sklearn.neural_network import MLPRegressor
&nbsp;
<i># velikost vstupu</i>
MAX_N = 40
&nbsp;
<i># X je matice, y je vektor</i>
X = np.zeros( (MAX_N, 2) )   <i># kombinace vstupu</i>
y = np.zeros( (MAX_N, ))     <i># vektor vysledku</i>
&nbsp;
<i># zadne skutecne nahodne hodnoty</i>
random.seed(19)
&nbsp;
<i># priprava dat pro trenink i otestovani neuronove site</i>
for i in range(0, MAX_N):
    X[i, 0] = random.randint(-10, 10)
    X[i, 1] = random.randint(-10, 10)
    y[i] = X[i, 1]
&nbsp;
&nbsp;
<i># rozdeleni dat na treninkovou a testovaci mnozinu</i>
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
&nbsp;
&nbsp;
def <strong>train_and_test_nn</strong>(size: int):
    X_train_ = X_train[:size]
    y_train_ = y_train[:size]
&nbsp;
    <i># konstrukce modelu</i>
    nn = MLPRegressor(max_iter=5000, hidden_layer_sizes=(), random_state=1000)
&nbsp;
    <i># trénink modelu</i>
    nn.fit(X_train_, y_train_)
&nbsp;
    <i># predikce modelu</i>
    y_pred = nn.predict(X_test)
&nbsp;
    <i># chyba predikce</i>
    <i># 1 = nejlepší predikce modelu</i>
    print("%2d" % size, "%.2f" % mean_squared_error(y_test, y_pred), "%.2f" % r2_score(y_test, y_pred))
&nbsp;
&nbsp;
<i># postupne zvetsovani datove sady</i>
for i in range(1, MAX_N+1):
    train_and_test_nn(i)
</pre>



<p><a name="k08"></a></p>
<h2 id="k08">8. Výsledky běhu předchozího skriptu</h2>

<p>V&nbsp;případě, že skript popsaný <a href="#k07">v&nbsp;předchozí
kapitole</a> spustíme, poměrně snadno zjistíme, že už pro relativně malý počet
trénovacích vzorků se neuronová síť (tedy přesněji řečeno váhy na vstupech
neuronů) natrénuje takovým způsobem, že se zmenší MSE, tedy odchylka odpovědí
sítě od očekávané odpovědi (připomeňme si, že pro regresní síť se jedná o
dvojici reálných čísel, které od sebe odečteme a vypočteme čtverec jejich
rozdílu). Už pro cca jedenáct trénovacích vzorků se přibližně dosáhne maximální
(v&nbsp;této chvíli možné) přesnosti natrénování:</p>

<pre>
 1 106.64 -1.06
 2 88.05 -0.70
 3 0.15 1.00
 4 0.15 1.00
 5 0.14 1.00
 6 0.18 1.00
 7 0.16 1.00
 8 0.17 1.00
 9 0.20 1.00
10 0.12 1.00
11 0.08 1.00
12 0.08 1.00
13 0.06 1.00
14 0.07 1.00
15 0.08 1.00
16 0.09 1.00
17 0.07 1.00
18 0.08 1.00
19 0.09 1.00
20 0.06 1.00
21 0.07 1.00
22 0.08 1.00
23 0.08 1.00
24 0.09 1.00
25 0.07 1.00
26 0.07 1.00
27 0.07 1.00
28 0.08 1.00
29 0.09 1.00
30 0.09 1.00
31 0.08 1.00
32 0.08 1.00
33 0.08 1.00
34 0.08 1.00
35 0.08 1.00
36 0.08 1.00
37 0.08 1.00
38 0.08 1.00
39 0.08 1.00
40 0.08 1.00
</pre>



<p><a name="k09"></a></p>
<h2 id="k09">9. Grafické znázornění závislosti MSE, vah neuronů a biasu na počtu vzorků</h2>

<p>Fakt, že se neuronová síť dokáže kvalitněji naučit v&nbsp;případě, že jí
předáme na trénink větší množství vzorků, jsme si již ověřili, a to dokonce
několikrát. Nyní nás ovšem bude zajímat, jak konkrétně toto vylepšení
vlastností neuronové sítě vypadá interně, tedy jak (a zda vůbec) se mění váhy a
biasy na vstupech neuronů. Pro naši jednoduchou síť se třemi neurony tedy
budeme sledovat dvě váhy (vstup do posledního neuronu) a jeho bias. Pro větší
přehlednost si necháme vykreslit i průběh MSE a hodnoty R2 score, tedy veličin,
které souhrnně určují kvalitu odpovědí sítě (pro klasifikační sítě jsou však
lepší matice záměn):</p>

<pre>
from types import NoneType
import matplotlib.pyplot as plt
import numpy as np
&nbsp;
import random
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
&nbsp;
<i># model zalozeny na neuronove siti</i>
from sklearn.neural_network import MLPRegressor
&nbsp;
<i># velikost vstupu</i>
MAX_N = 50
&nbsp;
<i># X je matice, y je vektor</i>
X = np.zeros( (MAX_N, 2) )   <i># kombinace vstupu</i>
y = np.zeros( (MAX_N, ))     <i># vektor vysledku</i>
&nbsp;
<i># zadne skutecne nahodne hodnoty</i>
random.seed(19)
&nbsp;
<i># priprava dat pro trenink i otestovani neuronove site</i>
for i in range(0, MAX_N):
    X[i, 0] = random.randint(-10, 10)
    X[i, 1] = random.randint(-10, 10)
    y[i] = X[i, 1]
&nbsp;
&nbsp;
<i># rozdeleni dat na treninkovou a testovaci mnozinu</i>
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
&nbsp;
&nbsp;
def <strong>train_and_test_nn</strong>(size: int):
    X_train_ = X_train[:size]
    y_train_ = y_train[:size]
&nbsp;
    <i># konstrukce modelu</i>
    nn = MLPRegressor(max_iter=5000, hidden_layer_sizes=(), random_state=1000)
&nbsp;
    <i># trénink modelu</i>
    nn.fit(X_train_, y_train_)
&nbsp;
    <i># predikce modelu</i>
    y_pred = nn.predict(X_test)
&nbsp;
    <i># chyba predikce</i>
    <i># 1 = nejlepší predikce modelu</i>
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    print("%2d" % size, "%.2f" % mse, "%.2f" % r2)
&nbsp;
    <i># vahy na vstupu neuronu ve vystupni vrstve</i>
    w = nn.coefs_[0]
&nbsp;
    <i># bias na vstupu neuronu ve vystupni vrstve</i>
    b = nn.intercepts_[0]
&nbsp;
    <i># vratit obe vahy i bias</i>
    return mse, r2, w[0][0], w[1][0], b[0]
&nbsp;
&nbsp;
<i># trening site az do poctu prvku MAX_N</i>
r = range(1, MAX_N+1)
&nbsp;
weights1 = []
weights2 = []
biases = []
mses = []
r2s = []
&nbsp;
<i># postupne provest trenink site, vyplneni poli s vahami a biasy</i>
for i in r:
    mse, r2, weight1, weight2, bias = train_and_test_nn(i)
    mses.append(mse)
    r2s.append(r2)
    weights1.append(weight1)
    weights2.append(weight2)
    biases.append(bias)
&nbsp;
print(weights1)
print(weights2)
print(biases)
&nbsp;
plt.plot(r, mses, r, r2s)
plt.legend(["MSE", "R2 score"])
plt.savefig("mse_r2.png")
plt.show()
&nbsp;
plt.plot(r, weights1, r, weights2)
plt.legend(["weight1", "weight2"])
plt.savefig("weights.png")
plt.show()
&nbsp;
plt.plot(r, biases)
plt.legend(["bias"])
plt.savefig("biases.png")
plt.show()
</pre>

<p><div class="rs-tip-major">Poznámka: povšimněte si, že síť vždy trénujeme od
začátku. Bylo by sice možné síť &bdquo;dotrénovávat&ldquo; (použitá knihovna
<i>scikit-learn</i> to umožňuje), ale výsledky nebudou v&nbsp;takovém případě
stejné s&nbsp;výsledky získanými tak, že síť budeme vždy učit znovu, bez
předchozích znalostí. To může být poměrně problematická vlastnost, protože
neuronové sítě se &bdquo;dotrénovávají&ldquo; běžně &ndash; například tehdy,
pokud se trénovaná data nevejdou do operační paměti.</div></p>



<p><a name="k10"></a></p>
<h2 id="k10">10. Výsledky získané po běhu skriptu</h2>

<p>Podívejme se nyní, jak vypadá trénink neuronové sítě po jeho vizualizaci do
grafů:</p>

*** image ***
<p><i>Obrázek 3: Změna vah na vstupu jediného neuronu ve výstupní vrstvě. Po
několika kolech učení se síť skutečně naučila správné váhy 0,0 a 1,0.</i></p>

*** image ***
<p><i>Obrázek 4: Změna biasu na vstupu jediného neuronu ve výstupní vrstvě.
Opět zde můžeme vidět, že po několika iteracích tréninku se bias ustálil na
očekávané nulové hodnotě.</i></p>

*** image ***
<p><i>Obrázek 5: Četnost chybných odpovědí, které síť při testování produkuje,
se postupně zmenšuje a odpovídá předchozím dvěma grafům.</i></p>



<p><a name="k11"></a></p>
<h2 id="k11">11. Zrychlení tréninku neuronové sítě snížením počtu iterací</h2>

<p>Hyperparametrem nazvaným <strong>max_iter</strong> lze modifikovat počet
iterací při tréninku neuronové sítě. Prozatím jsme používali výchozí hodnotu
5000, takže konstrukce neuronové sítě vypadala následovně:</p>

<pre>
<i># konstrukce modelu</i>
nn = MLPRegressor(<strong>max_iter=5000</strong>, hidden_layer_sizes=(), random_state=1000)
</pre>

<p>Ovšem celý proces učení můžeme urychlit zadáním menší hodnoty. Samotná
topologie neuronové sítě se vůbec nezmění:</p>

<pre>
<i># konstrukce modelu</i>
nn = MLPRegressor(<strong>max_iter=1000</strong>, hidden_layer_sizes=(), random_state=1000)
</pre>


<p>Opět se podívejme na to, jak může vypadat skript, který neuronovou síť
natrénuje a použije přitom menší počet iterací, než je výchozí hodnota 5000.
Oproti předchozímu skriptu se změnil jen jediný řádek se specifikací
hyperparametrů modelu:</p>

<pre>
from types import NoneType
import matplotlib.pyplot as plt
import numpy as np
&nbsp;
import random
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
&nbsp;
<i># model zalozeny na neuronove siti</i>
from sklearn.neural_network import MLPRegressor
&nbsp;
<i># velikost vstupu</i>
MAX_N = 50
&nbsp;
<i># X je matice, y je vektor</i>
X = np.zeros( (MAX_N, 2) )   <i># kombinace vstupu</i>
y = np.zeros( (MAX_N, ))     <i># vektor vysledku</i>
&nbsp;
random.seed(19)
&nbsp;
for i in range(0, MAX_N):
    X[i, 0] = random.randint(-10, 10)
    X[i, 1] = random.randint(-10, 10)
    y[i] = X[i, 1]
&nbsp;
&nbsp;
<i># rozdeleni dat na treninkovou a testovaci mnozinu</i>
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
&nbsp;
&nbsp;
def <strong>train_and_test_nn</strong>(size: int):
    X_train_ = X_train[:size]
    y_train_ = y_train[:size]
&nbsp;
    <i># konstrukce modelu</i>
    nn = MLPRegressor(max_iter=1000, hidden_layer_sizes=(), random_state=1000)
&nbsp;
    <i># trénink modelu</i>
    nn.fit(X_train_, y_train_)
&nbsp;
    <i># predikce modelu</i>
    y_pred = nn.predict(X_test)
&nbsp;
    <i># chyba predikce</i>
    <i># 1 = nejlepší predikce modelu</i>
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    print("%2d" % size, "%.2f" % mse, "%.2f" % r2)
&nbsp;
    <i># vahy na vstupu neuronu ve vystupni vrstve</i>
    w = nn.coefs_[0]
&nbsp;
    <i># bias na vstupu neuronu ve vystupni vrstve</i>
    b = nn.intercepts_[0]
&nbsp;
    <i># vratit obe vahy i bias</i>
    return mse, r2, w[0][0], w[1][0], b[0]
&nbsp;
&nbsp;
<i># trening site az do poctu prvku MAX_N</i>
r = range(1, MAX_N+1)
&nbsp;
weights1 = []
weights2 = []
biases = []
mses = []
r2s = []
&nbsp;
<i># postupne provest treing site, vyplneni poli s vahami a biasy</i>
for i in r:
    mse, r2, weight1, weight2, bias = train_and_test_nn(i)
    mses.append(mse)
    r2s.append(r2)
    weights1.append(weight1)
    weights2.append(weight2)
    biases.append(bias)
&nbsp;
print(weights1)
print(weights2)
print(biases)
&nbsp;
plt.plot(r, mses, r, r2s)
plt.legend(["MSE", "R2 score"])
plt.savefig("mse_r2.png")
plt.show()
&nbsp;
plt.plot(r, weights1, r, weights2)
plt.legend(["weight1", "weight2"])
plt.savefig("weights.png")
plt.show()
&nbsp;
plt.plot(r, biases)
plt.legend(["bias"])
plt.savefig("biases.png")
plt.show()
</pre>



<p><a name="k12"></a></p>
<h2 id="k12">12. Výsledky získané po běhu skriptu: snížení počtu iterací při tréninku neuronové sítě</h2>

<p>Výsledky nyní budou vypadat odlišně:</p>

*** image ***
<p><i>Obrázek 6: Změna vah na vstupu jediného neuronu ve výstupní vrstvě.
Neuronová síť se sice učí rychleji, váhy neuronů jsou ale nastaveny špatně. A
navíc větší počet trénovacích dat pravděpodobně již nepomůže, což naznačuje
pravá strana grafu).</i></p>

*** image ***
<p><i>Obrázek 7: Změna biasu na vstupu jediného neuronu ve výstupní vrstvě.
Opět si povšimněte, jak špatně se síť naučila &ndash; evidentně skončila
v&nbsp;lokálním maximu, nikoli v&nbsp;maximu globálním.</p>

*** image ***
<p><i>Obrázek 8: Četnost chybných odpovědí je oproti původní síti mnohem větší
&ndash; síť nedokáže odpovídat korektně.</i></p>



<p><a name="k13"></a></p>
<h2 id="k13">13. Vyšší míra změny vah na vstupu neuronů při tréninku</h2>

<p>Kromě počtu iterací můžeme modifikovat i míru změny vah na vstupech
jednotlivých neuronů při tréninku. Příliš velká míra změny může znamenat, že
sít bude &bdquo;vyskakovat&ldquo; z&nbsp;optimálního stavu (každý nový vstup
při tréninku ji bude z&nbsp;tohoto stavu posouvat), na druhou stranu se však
může trénink urychlit a dokonce lze překonat lokální maxima. Výchozí hodnotou
<strong>learning_rate_init</strong> je 0.001 a důležité je, že se změna projeví
jen u trénovacích algoritmů &bdquo;adam&ldquo; a &bdquo;sgd&ldquo; (viz též
předchozí článek).</p>

<p>Pokusme se tuto hodnotu padesátkrát zvýšit:</p>

<pre>
<i># konstrukce modelu</i>
nn = MLPRegressor(max_iter=5000, hidden_layer_sizes=(), random_state=1000, <strong>learning_rate_init=0.05</strong>)
</pre>

<p>Upravený skript bude vypadat následovně:</p>

<pre>
from types import NoneType
import matplotlib.pyplot as plt
import numpy as np
&nbsp;
import random
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
&nbsp;
<i># model zalozeny na neuronove siti</i>
from sklearn.neural_network import MLPRegressor
&nbsp;
<i># velikost vstupu</i>
MAX_N = 50
&nbsp;
<i># X je matice, y je vektor</i>
X = np.zeros( (MAX_N, 2) )   <i># kombinace vstupu</i>
y = np.zeros( (MAX_N, ))     <i># vektor vysledku</i>
&nbsp;
random.seed(19)
&nbsp;
for i in range(0, MAX_N):
    X[i, 0] = random.randint(-10, 10)
    X[i, 1] = random.randint(-10, 10)
    y[i] = X[i, 1]
&nbsp;
&nbsp;
<i># rozdeleni dat na treninkovou a testovaci mnozinu</i>
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
&nbsp;
&nbsp;
def <strong>train_and_test_nn</strong>(size: int):
    X_train_ = X_train[:size]
    y_train_ = y_train[:size]
&nbsp;
    <i># konstrukce modelu</i>
    nn = MLPRegressor(max_iter=5000, hidden_layer_sizes=(), random_state=1000, learning_rate_init=0.05)
&nbsp;
    <i># trénink modelu</i>
    nn.fit(X_train_, y_train_)
&nbsp;
    <i># predikce modelu</i>
    y_pred = nn.predict(X_test)
&nbsp;
    <i># chyba predikce</i>
    <i># 1 = nejlepší predikce modelu</i>
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    print("%2d" % size, "%.2f" % mse, "%.2f" % r2)
&nbsp;
    <i># vahy na vstupu neuronu ve vystupni vrstve</i>
    w = nn.coefs_[0]
&nbsp;
    <i># bias na vstupu neuronu ve vystupni vrstve</i>
    b = nn.intercepts_[0]
&nbsp;
    <i># vratit obe vahy i bias</i>
    return mse, r2, w[0][0], w[1][0], b[0]
&nbsp;
&nbsp;
<i># trening site az do poctu prvku MAX_N</i>
r = range(1, MAX_N+1)
&nbsp;
weights1 = []
weights2 = []
biases = []
mses = []
r2s = []
&nbsp;
<i># postupne provest treing site, vyplneni poli s vahami a biasy</i>
for i in r:
    mse, r2, weight1, weight2, bias = train_and_test_nn(i)
    mses.append(mse)
    r2s.append(r2)
    weights1.append(weight1)
    weights2.append(weight2)
    biases.append(bias)
&nbsp;
print(weights1)
print(weights2)
print(biases)
&nbsp;
plt.plot(r, mses, r, r2s)
plt.legend(["MSE", "R2 score"])
plt.savefig("mse_r2.png")
plt.show()
&nbsp;
plt.plot(r, weights1, r, weights2)
plt.legend(["weight1", "weight2"])
plt.savefig("weights.png")
plt.show()
&nbsp;
plt.plot(r, biases)
plt.legend(["bias"])
plt.savefig("biases.png")
plt.show()
</pre>



<p><a name="k14"></a></p>
<h2 id="k14">14. Výsledky získané po běhu skriptu: vyšší míra změna vah neuronů</h2>

<p>Podívejme se nyní, jak vypadá trénink neuronové sítě po jeho vizualizaci do
grafů:</p>

*** image ***
<p><i>Obrázek 9: Změna vah na vstupu jediného neuronu ve výstupní vrstvě. Po
několika kolech učení se síť skutečně naučila správné váhy 0,0 a 1,0.</i></p>

*** image ***
<p><i>Obrázek 10: Změna biasu na vstupu jediného neuronu ve výstupní vrstvě.
Opět zde můžeme vidět, že po několika iteracích tréninku se bias ustálil na
očekávané nulové hodnotě.</i></p>

*** image ***
<p><i>Obrázek 11: Četnost chybných odpovědí, které síť při testování produkuje,
se postupně zmenšuje a odpovídá předchozím dvěma grafům.</i></p>

<p><div class="rs-tip-major">Poznámka: nyní jsme tedy měli štěstí, protože se
síť natrénovala nejenom korektně, ale i velmi rychle. Ovšem další zvyšování
hodnoty <strong>learning_rate_init</strong> s&nbsp;sebou nese poměrně velká
rizika.</div></p>



<p><a name="k15"></a></p>
<h2 id="k15">15. Riziko příliš vysoké hodnoty <strong>learning_rate_init</strong></h2>

<p>Hodnotu <strong>learning_rate_init</strong> samozřejmě můžeme dále zvyšovat
ve smyslu logiky &bdquo;když poněkud vyšší hodnota pomohla, ještě vyšší hodnota
povede k&nbsp;mnohem lepším výsledkům&ldquo;. Pokusme se tedy tuto hodnotu
zvýšit na 0,2, což je v&nbsp;praxi již příliš mnoho (jak posléze uvidíme):</p>

<pre>
<i># rozdeleni dat na treninkovou a testovaci mnozinu</i>
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
</pre>

<p>Celý skript:</p>

<pre>
from types import NoneType
import matplotlib.pyplot as plt
import numpy as np
&nbsp;
import random
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
&nbsp;
<i># model zalozeny na neuronove siti</i>
from sklearn.neural_network import MLPRegressor
&nbsp;
<i># velikost vstupu</i>
MAX_N = 50
&nbsp;
<i># X je matice, y je vektor</i>
X = np.zeros( (MAX_N, 2) )   <i># kombinace vstupu</i>
y = np.zeros( (MAX_N, ))     <i># vektor vysledku</i>
&nbsp;
random.seed(19)
&nbsp;
for i in range(0, MAX_N):
    X[i, 0] = random.randint(-10, 10)
    X[i, 1] = random.randint(-10, 10)
    y[i] = X[i, 1]
&nbsp;
&nbsp;
<i># rozdeleni dat na treninkovou a testovaci mnozinu</i>
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
&nbsp;
&nbsp;
def <strong>train_and_test_nn</strong>(size: int):
    X_train_ = X_train[:size]
    y_train_ = y_train[:size]
&nbsp;
    <i># konstrukce modelu</i>
    nn = MLPRegressor(max_iter=5000, hidden_layer_sizes=(), random_state=1000, learning_rate_init=0.2)
&nbsp;
    <i># trénink modelu</i>
    nn.fit(X_train_, y_train_)
&nbsp;
    <i># predikce modelu</i>
    y_pred = nn.predict(X_test)
&nbsp;
    <i># chyba predikce</i>
    <i># 1 = nejlepší predikce modelu</i>
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    print("%2d" % size, "%.2f" % mse, "%.2f" % r2)
&nbsp;
    <i># vahy na vstupu neuronu ve vystupni vrstve</i>
    w = nn.coefs_[0]
&nbsp;
    <i># bias na vstupu neuronu ve vystupni vrstve</i>
    b = nn.intercepts_[0]
&nbsp;
    <i># vratit obe vahy i bias</i>
    return mse, r2, w[0][0], w[1][0], b[0]
&nbsp;
&nbsp;
<i># trening site az do poctu prvku MAX_N</i>
r = range(1, MAX_N+1)
&nbsp;
weights1 = []
weights2 = []
biases = []
mses = []
r2s = []
&nbsp;
<i># postupne provest treing site, vyplneni poli s vahami a biasy</i>
for i in r:
    mse, r2, weight1, weight2, bias = train_and_test_nn(i)
    mses.append(mse)
    r2s.append(r2)
    weights1.append(weight1)
    weights2.append(weight2)
    biases.append(bias)
&nbsp;
print(weights1)
print(weights2)
print(biases)
&nbsp;
plt.plot(r, mses, r, r2s)
plt.legend(["MSE", "R2 score"])
plt.savefig("mse_r2.png")
plt.show()
&nbsp;
plt.plot(r, weights1, r, weights2)
plt.legend(["weight1", "weight2"])
plt.savefig("weights.png")
plt.show()
&nbsp;
plt.plot(r, biases)
plt.legend(["bias"])
plt.savefig("biases.png")
plt.show()
</pre>



<p><a name="k16"></a></p>
<h2 id="k16">16. Výsledky získané po běhu skriptu: vysoká míra změny vah neuronů</h2>

<p>Neuronová síť již pro hodnotu <strong>learning_rate_init==0.2</strong>
nebude dobře natrénovaná, což je jasně patrné z&nbsp;následujících průběhů:</p>

*** image ***
<p><i>Obrázek 12: Změna vah na vstupu jediného neuronu ve výstupní vrstvě. Nyní
je síť naučena (zcela) špatně, protože váhy nejsou rovny 0,0 a 1,0.</i></p>

*** image ***
<p><i>Obrázek 13: Změna biasu na vstupu jediného neuronu ve výstupní vrstvě.
Opět zde můžeme vidět, že po několika iteracích tréninku se bias ustálil, ovšem
na nekorektní (nenulové) hodnotě.</i></p>

*** image ***
<p><i>Obrázek 14: Četnost chybných odpovědí, které síť při testování produkuje,
se již nikdy nezmenší na nulu.</i></p>



<p><a name="k17"></a></p>
<h2 id="k17">17. Opačný extrém &ndash; příliš malá hodnota <strong>learning_rate_init</strong></h2>

<pre>
from types import NoneType
import matplotlib.pyplot as plt
import numpy as np

import random
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

<i># model zalozeny na neuronove siti</i>
from sklearn.neural_network import MLPRegressor

<i># velikost vstupu</i>
MAX_N = 50

<i># X je matice, y je vektor</i>
X = np.zeros( (MAX_N, 2) )   <i># kombinace vstupu</i>
y = np.zeros( (MAX_N, ))     <i># vektor vysledku</i>

random.seed(19)

for i in range(0, MAX_N):
    X[i, 0] = random.randint(-10, 10)
    X[i, 1] = random.randint(-10, 10)
    y[i] = X[i, 1]


<i># rozdeleni dat na treninkovou a testovaci mnozinu</i>
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


def train_and_test_nn(size: int):
    X_train_ = X_train[:size]
    y_train_ = y_train[:size]

    <i># konstrukce modelu</i>
    nn = MLPRegressor(max_iter=5000, hidden_layer_sizes=(), random_state=1000, learning_rate_init=0.0001)

    <i># trénink modelu</i>
    nn.fit(X_train_, y_train_)

    <i># predikce modelu</i>
    y_pred = nn.predict(X_test)

    <i># chyba predikce</i>
    <i># 1 = nejlepší predikce modelu</i>
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    print("%2d" % size, "%.2f" % mse, "%.2f" % r2)

    <i># vahy na vstupu neuronu ve vystupni vrstve</i>
    w = nn.coefs_[0]

    <i># bias na vstupu neuronu ve vystupni vrstve</i>
    b = nn.intercepts_[0]

    <i># vratit obe vahy i bias</i>
    return mse, r2, w[0][0], w[1][0], b[0]


<i># trening site az do poctu prvku MAX_N</i>
r = range(1, MAX_N+1)

weights1 = []
weights2 = []
biases = []
mses = []
r2s = []

<i># postupne provest treing site, vyplneni poli s vahami a biasy</i>
for i in r:
    mse, r2, weight1, weight2, bias = train_and_test_nn(i)
    mses.append(mse)
    r2s.append(r2)
    weights1.append(weight1)
    weights2.append(weight2)
    biases.append(bias)

print(weights1)
print(weights2)
print(biases)

plt.plot(r, mses, r, r2s)
plt.legend(["MSE", "R2 score"])
plt.savefig("mse_r2.png")
plt.show()

plt.plot(r, weights1, r, weights2)
plt.legend(["weight1", "weight2"])
plt.savefig("weights.png")
plt.show()

plt.plot(r, biases)
plt.legend(["bias"])
plt.savefig("biases.png")
plt.show()
</pre>



<p><a name="k19"></a></p>
<h2 id="k19">19. Repositář s&nbsp;demonstračními příklady</h2>

<p>Všechny demonstrační příklady využívající knihovnu Scikit-learn lze nalézt
v&nbsp;repositáři <a
href="https://github.com/tisnik/most-popular-python-libs">https://github.com/tisnik/most-popular-python-libs</a>.
Následují odkazy na jednotlivé příklady i na (Jupyter) diáře s&nbsp;postupem
výpočtů a analýz:</p>

<table>
<tr><th>#<th>Příklad</th><th>Stručný popis</th><th>Adresa příkladu</th></tr></i>
<tr><td> 1</td><td>01_show_matrix.py</td><td>kooperace mezi knihovnami Matplotlib a NumPy: vizualizace obsahu 2D matice</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/01_show_matrix.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/01_show_matrix.py</a></td></tr>
<tr><td> 2</td><td>02_get_digits.py</td><td>datová množina obsahující naskenované ručně napsané číslice</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/02_get_digits.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/02_get_digits.py</a></td></tr>
<tr><td> 3</td><td>03_get_features.py</td><td>další atributy datové množiny, které použijeme při trénování</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/03_get_features.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/03_get_features.py</a></td></tr>
<tr><td> 4</td><td>04_get_images.py</td><td>přečtení a následné vykreslení jednotlivých ručně nakreslených číslic</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/04_get_images.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/04_get_images.py</a></td></tr>
<tr><td> 5</td><td>05_show_grayscale_matrix.py</td><td>odstranění umělé aplikované barvové palety (obrázky ve stupních šedi)</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/05_show_grayscale_matrix.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/05_show_grayscale_matrix.py</a></td></tr>
<tr><td> 6</td><td>06_grayscale_images.py</td><td>vykreslení ručně nakreslených číslic ve formě obrázků ve stupních šedi</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/06_grayscale_images.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/06_grayscale_images.py</a></td></tr>
<tr><td> 7</td><td>07_multiplot.py</td><td>rozdělení plochy grafu do oblastí; vykreslení více obrázků do jediného grafu</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/07_multiplot.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/07_multiplot.py</a></td></tr>
<tr><td> 8</td><td>08_model_preperation_1.py</td><td>obrázky s&nbsp;jejich ohodnocením</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/08_model_preperation_1.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/08_model_preperation_1.py</a></td></tr>
<tr><td> 9</td><td>09_training_set.py</td><td>příprava dat pro trénink</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/09_training_set.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/09_training_set.py</a></td></tr>
<tr><td>10</td><td>10_classification.py</td><td>klasifikace obrázků</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/10_classification.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/10_classification.py</a></td></tr>
<tr><td>11</td><td>11_results.py</td><td>vykreslení obrázků společně s&nbsp;jejich klasifikací</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/11_results.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/11_results.py</a></td></tr>
<tr><td>12</td><td>12_change_training_set.py</td><td>změna poměru rozdělení dat na tréninkovou a testovací množinu</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/12_change_training_set.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/12_change_training_set.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>13</td><td>13_blobs.py</td><td>použití funkce <strong>make_blobs</strong> pro vygenerování sady bodů v&nbsp;rovině sdružených do oblastí</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/13_blobs.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/13_blobs.py</a></td></tr>
<tr><td>14</td><td>14_swap_coords.py</td><td>úprava předchozího příkladu: prohození souřadnic na osách</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/14_swap_coords.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/14_swap_coords.py</a></td></tr>
<tr><td>15</td><td>15_blobs_scatter_plot.py</td><td>základní podoba bodového diagramu (<i>scatter plot</i>)</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/15_blobs_scatter_plot.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/15_blobs_scatter_plot.py</a></td></tr>
<tr><td>16</td><td>16_blobs_scatter_plot.py</td><td>úprava bodového diagramu při zobrazení většího množství bodů</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/16_blobs_scatter_plot.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/16_blobs_scatter_plot.py</a></td></tr>
<tr><td>17</td><td>17_colorized_blobs.py</td><td>obarvení bodů podle oblastí</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/17_colorized_blobs.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/17_colorized_blobs.py</a></td></tr>
<tr><td>18</td><td>18_k-means.py</td><td>základní použití algoritmu K-means pro clustering</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/18_k-means.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/18_k-means.py</a></td></tr>
<tr><td>19</td><td>19_combination.py</td><td>zobrazení centroidů společně s&nbsp;původními body</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/19_combination.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/19_combination.py</a></td></tr>
<tr><td>20</td><td>20_combinations.py</td><td>vizualizace clusteringu původní množiny bodů</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/20_combinations.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/20_combinations.py</a></td></tr>
<tr><td>21</td><td>21_other_settings.py</td><td>vizualizace clusteringu původní množiny bodů pro odlišnou množinu</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/21_other_settings.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/21_other_settings.py</a></td></tr>
<tr><td>22</td><td>22_random_points.py</td><td>clustering pro náhodná data</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/22_random_points.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/22_random_points.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>23</td><td>23_circles.py</td><td>pseudonáhodné rozmístění bodů do kružnic, menší náhodnost výsledku</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/23_circles.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/23_circles.py</a></td></tr>
<tr><td>24</td><td>24_more_noise_circles.py</td><td>pseudonáhodné rozmístění bodů do kružnic, větší náhodnost výsledku</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/24_more_noise_circles.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/24_more_noise_circles.py</a></td></tr>
<tr><td>25</td><td>25_moons.py</td><td>pseudonáhodné rozmístění bodů do tvaru dvou půlměsíců, menší náhodnost</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/25_moons.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/25_moons.py</a></td></tr>
<tr><td>26</td><td>26_more_noisy_moons.py</td><td>pseudonáhodné rozmístění bodů do tvaru dvou půlměsíců, větší náhodnost</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/26_more_noisy_moons.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/26_more_noisy_moons.py</a></td></tr>
<tr><td>27</td><td>27_circles_kmeans.py</td><td>výsledek clusteringu provedeného algoritmem K-means na &bdquo;kružnice&ldquo;</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/27_circles_kmeans.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/27_circles_kmeans.py</a></td></tr>
<tr><td>28</td><td>28_moons_kmeans.py</td><td>výsledek clusteringu provedeného algoritmem K-means na &bdquo;půlměsíce&ldquo;</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/28_moons_kmeans.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/28_moons_kmeans.py</a></td></tr>
<tr><td>29</td><td>29_blobs_spectral_clustering.py</td><td>spectral clustering pro body rozmístěné pomocí <strong>make_blobs</strong></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/29_blobs_spectral_clustering.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/29_blobs_spectral_clustering.py</a></td></tr>
<tr><td>30</td><td>30_circles_spectral_clustering.py</td><td>spectral clustering pro body rozmístěné do kružnic</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/30_circles_spectral_clustering.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/30_circles_spectral_clustering.py</a></td></tr>
<tr><td>31</td><td>31_moons_spectral_clustering.py</td><td>spectral clustering pro body rozmístěné do půlměsíců </td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/31_moons_spectral_clustering.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/31_moons_spectral_clustering.py</a></td></tr>
<tr><td>32</td><td>32_moons_spectral_clustering_limits.py</td><td>vyhledání limitů algoritmu spectral clustering</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/32_moons_spectral_clustering_limits.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/32_moons_spectral_clustering_limits.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>33</td><td>33_particles_load.py</td><td>načtení souřadnic částic uložených v&nbsp;souboru formátu CSV</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/33_particles_load.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/33_particles_load.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>34</td><td>34_lorenz_attractor.py</td><td>zobrazení Lorenzova atraktoru formou bodů propojených úsečkami</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/34_lorenz_attractor.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/34_lorenz_attractor.py</a></td></tr>
<tr><td>35</td><td>35_lorenz_attractor_points.py</td><td>Lorenzův atraktor vykreslený formou jednotlivých bodů s&nbsp;definovaným stylem zobrazení a velikostí stopy</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/35_lorenz_attractor_points.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/35_lorenz_attractor_points.py</a></td></tr>
<tr><td>36</td><td>36_blobs_3d.py</td><td>vygenerování a zobrazení sady bodů v&nbsp;3D prostoru</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/36_blobs_3d.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/36_blobs_3d.py</a></td></tr>
<tr><td>37</td><td>37_spread_blobs_3d.py</td><td>vygenerování a zobrazení sady bodů v&nbsp;3D prostoru, odlišné parametry při generování</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/37_spread_blobs_3d.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/37_spread_blobs_3d.py</a></td></tr>
<tr><td>38</td><td>38_views.py</td><td>různé pohledy na 3D graf</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/38_views.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/38_views.py</a></td></tr>
<tr><td>39</td><td>39_colorized_3d_blobs.py</td><td>obarvení bodů v&nbsp;prostoru na základě vstupních dat</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/39_colorized_3d_blobs.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/39_colorized_3d_blobs.py</a></td></tr>
<tr><td>40</td><td>40_kmeans_3d_blobs.py</td><td>shluková analýza v&nbsp;3D prostoru</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/40_kmeans_3d_blobs.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/40_kmeans_3d_blobs.py</a></td></tr>
<tr><td>41</td><td>41_kmeans_spread_3d_blobs.py</td><td>shluková analýza v&nbsp;3D prostoru pro odlišnou množinu bodů</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/41_kmeans_spread_3d_blobs.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/41_kmeans_spread_3d_blobs.py</a></td></tr>
<tr><td>42</td><td>42_kmeans_random_3d.py</td><td>shluková analýza pro body rozmístěné zcela náhodně v&nbsp;omezeném prostoru</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/42_kmeans_random_3d.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/42_kmeans_random_3d.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>43</td><td>43_speed_measurements.py</td><td>benchmark pro postupně rostoucí počet bodů tvořících shluky</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/43_speed_measurements.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/43_speed_measurements.py</a></td></tr>
<tr><td>44</td><td>44_speed_measurements.py</td><td>benchmark pro postupně rostoucí počet bodů rozmístěných náhodně</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/44_speed_measurements.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/44_speed_measurements.py</a></td></tr>
<tr><td>45</td><td>45_speed_measurements.py</td><td>benchmark pro stále stejný počet bodů, u jejichž rozmístění v&nbsp;prostoru se používá stále větší směrodatná odchylka</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/45_speed_measurements.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/45_speed_measurements.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>46</td><td>46_iris_dataset.py</td><td>načtení datové kolekce</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/46_iris_dataset.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/46_iris_dataset.py</a></td></tr>
<tr><td>47</td><td>47_iris_description.py</td><td>metadata o datové kolekci</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/47_iris_description.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/47_iris_description.py</a></td></tr>
<tr><td>48</td><td>48_iris_data.py</td><td>tvar dat &ndash; počet záznamů a počet proměnných</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/48_iris_data.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/48_iris_data.py</a></td></tr>
<tr><td>49</td><td>49_iris_targets.py</td><td>jména atributů, vztah mezi numerickou hodnotou atributu a jeho jménem</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/49_iris_targets.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/49_iris_targets.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>50</td><td>50_iris_scatter_plot_1.py</td><td>korelační diagram pro dvojici vybraných proměnných</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/50_iris_scatter_plot_1.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/50_iris_scatter_plot_1.py</a></td></tr>
<tr><td>51</td><td>51_iris_scatter_plot_2.py</td><td>příprava pro tvorbu složitějších grafů</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/51_iris_scatter_plot_2.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/51_iris_scatter_plot_2.py</a></td></tr>
<tr><td>52</td><td>52_iris_mutliplot.py</td><td>mřížka obsahující více korelačních diagramů</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/52_iris_mutliplot.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/52_iris_mutliplot.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>53</td><td>53_iris_histograms.py</td><td>zobrazení základního histogramu pro data v&nbsp;sadě Iris</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/53_iris_histograms.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/53_iris_histograms.py</a></td></tr>
<tr><td>54</td><td>54_iris_histograms.py</td><td>úprava histogramu</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/54_iris_histograms.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/54_iris_histograms.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>55</td><td>55_pca.py</td><td>analýza hlavních komponent (PCA), výsledek zobrazený v&nbsp;2D grafu</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/55_pca.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/55_pca.py</a></td></tr>
<tr><td>56</td><td>56_pca_3d.py</td><td>analýza hlavních komponent (PCA), výsledek zobrazený v&nbsp;3D grafu</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/56_pca_3d.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/56_pca_3d.py</a></td></tr>
<tr><td>57</td><td>57_kmeans.py</td><td>základní shluková analýza</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/57_kmeans.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/57_kmeans.py</a></td></tr>
<tr><td>58</td><td>58_multiple_kmeans.py</td><td>větší množství výsledků shlukové analýzy pro různé atributy</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/58_multiple_kmeans.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/58_multiple_kmeans.py</a></td></tr>
<tr><td>59</td><td>59_kmeans_errors.py</td><td>korektní a nekorektní výsledky základní shlukové analýzy</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/59_kmeans_errors.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/59_kmeans_errors.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>60</td><td>60_basic_classifier.py</td><td>aplikace jednoduchého modelu</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/60_basic_classifier.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/60_basic_classifier.py</a></td></tr>
<tr><td>61</td><td>61_changed_model_parameters.py</td><td>změna parametrů modelu pro zjištění druhů rostil</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/61_changed_model_parameters.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/61_changed_model_parameters.py</a></td></tr>
<tr><td>62</td><td>62_different_model.py</td><td>použití odlišného modelu pro zjištění druhů rostlin</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/62_different_model.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/62_different_model.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
</table>

160_nn_wrong_predictions.py
161_activation_function.py
162_solver.py
163_best_combination.py
164_simplest_nn.py
165_no_randomization.py
166_step_by_step.py
167_weights_biases.py
168_maxiter.py
169_learning_rate.py
170_too_fast_rate.py
171_too_slow_rate.py

<p>V&nbsp;repositáři nalezneme taktéž projektový soubor a Jupyter Notebook
s&nbsp;vysvětlením, jak lze modely využít pro rozpoznávání obsahu rastrových
obrázků:</p>

<table>
<tr><th>#<th>Příklad</th><th>Stručný popis</th><th>Adresa příkladu</th></tr></i>
<tr><td>1</td><td>pyproject.toml</td><td>projektový soubor (pro PDM) se všemi závislostmi</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/pyproject.toml">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/pyproject.toml</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>2</td><td>pdm.lock</td><td>lock soubor s&nbsp;konkrétními verzemi všech přímých i tranzitivních závislostí</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/pdm.lock">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/pdm.lock</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>3</td><td>Rozpoznání_obrazu_scikit-learn.ipynb</td><td>Jupyter notebook s&nbsp;celým postupem</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/Rozpoznání_obrazu_scikit-learn.ipynb">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/Rozpoznání_obrazu_scikit-learn.ipynb</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>4</td><td>particle_life.py</td><td>emergence: příklad vzniku struktury</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/particles/particle_life.py">https://github.com/tisnik/most-popular-python-libs/blob/master/particles/particle_life.py</a></td></tr>
</table>



<p><a name="k20"></a></p>
<h2 id="k20">20. Odkazy na Internetu</h2>

<ol>

<li>Shluková analýza (clustering) a knihovna Scikit-learn<br />
<a href="https://www.root.cz/clanky/shlukova-analyza-clustering-a-knihovna-scikit-learn/">https://www.root.cz/clanky/shlukova-analyza-clustering-a-knihovna-scikit-learn/</a>
</li>

<li>Shluková analýza (clustering) a knihovna Scikit-learn (2)<br />
<a href="https://www.root.cz/clanky/shlukova-analyza-clustering-a-knihovna-scikit-learn-2/">https://www.root.cz/clanky/shlukova-analyza-clustering-a-knihovna-scikit-learn-2/</a>
</li>

<li>Shluková analýza (clustering) a knihovna Scikit-learn (z plochy do 3D prostoru)<br />
<a href="https://www.root.cz/clanky/shlukova-analyza-clustering-a-knihovna-scikit-learn-z-plochy-do-3d-prostoru/">https://www.root.cz/clanky/shlukova-analyza-clustering-a-knihovna-scikit-learn-z-plochy-do-3d-prostoru/</a>
</li>

<li>Rozpoznávání obrázků knihovnou Scikit-learn: první kroky<br />
<a href="https://www.root.cz/clanky/rozpoznavani-obrazku-knihovnou-scikit-learn-prvni-kroky/">https://www.root.cz/clanky/rozpoznavani-obrazku-knihovnou-scikit-learn-prvni-kroky/</a>
</li>

<li>scikit-learn: Machine Learning in Python<br />
<a href="https://scikit-learn.org/stable/index.html">https://scikit-learn.org/stable/index.html</a>
</li>

<li>Sklearn-pandas<br />
<a href="https://github.com/scikit-learn-contrib/sklearn-pandas">https://github.com/scikit-learn-contrib/sklearn-pandas</a>
</li>

<li>sklearn-xarray<br />
<a href="https://github.com/phausamann/sklearn-xarray/">https://github.com/phausamann/sklearn-xarray/</a>
</li>

<li>Clustering<br />
<a href="https://scikit-learn.org/stable/modules/clustering.html">https://scikit-learn.org/stable/modules/clustering.html</a>
</li>

<li>Cluster analysis (Wikipedia)<br />
<a href="https://en.wikipedia.org/wiki/Cluster_analysis">https://en.wikipedia.org/wiki/Cluster_analysis</a>
</li>

<li>Shluková analýza (Wikipedia)<br />
<a href="https://cs.wikipedia.org/wiki/Shlukov%C3%A1_anal%C3%BDza">https://cs.wikipedia.org/wiki/Shlukov%C3%A1_anal%C3%BDza</a>
</li>

<li>K-means<br />
<a href="https://cs.wikipedia.org/wiki/K-means">https://cs.wikipedia.org/wiki/K-means</a>
</li>

<li>k-means clustering<br />
<a href="https://en.wikipedia.org/wiki/K-means_clustering">https://en.wikipedia.org/wiki/K-means_clustering</a>
</li>

<li>Spectral clustering<br />
<a href="https://en.wikipedia.org/wiki/Spectral_clustering">https://en.wikipedia.org/wiki/Spectral_clustering</a>
</li>

<li>Emergence<br />
<a href="https://cs.wikipedia.org/wiki/Emergence">https://cs.wikipedia.org/wiki/Emergence</a>
</li>

<li>Particle Life: Vivid structures from rudimentary rules<br />
<a href="https://particle-life.com/">https://particle-life.com/</a>
</li>

<li>Hertzsprungův–Russellův diagram<br />
<a href="https://cs.wikipedia.org/wiki/Hertzsprung%C5%AFv%E2%80%93Russell%C5%AFv_diagram">https://cs.wikipedia.org/wiki/Hertzsprung%C5%AFv%E2%80%93Russell%C5%AFv_diagram</a>
</li>

<li>Using Machine Learning in an HR Diagram<br />
<a href="https://cocalc.com/share/public_paths/08b6e03583cbdef3cdb9813a54ec68ff773c747f">https://cocalc.com/share/public_paths/08b6e03583cbdef3cdb9813a54ec68ff773c747f</a>
</li>

<li>Gaia H-R diagrams: Querying Gaia data for one million nearby stars<br />
<a href="https://vlas.dev/post/gaia-dr2-hrd/">https://vlas.dev/post/gaia-dr2-hrd/</a>
</li>

<li>The Hertzsprung–Russell diagram<br />
<a href="https://scipython.com/book2/chapter-9-data-analysis-with-pandas/problems/p92/the-hertzsprung-russell-diagram/">https://scipython.com/book2/chapter-9-data-analysis-with-pandas/problems/p92/the-hertzsprung-russell-diagram/</a>
</li>

<li>Animated Hertzsprung-Russell Diagram with 119,614 datapoints<br />
<a href="https://github.com/zonination/h-r-diagram">https://github.com/zonination/h-r-diagram</a>
</li>

<li>Neuraxle Pipelines<br />
<a href="https://github.com/Neuraxio/Neuraxle">https://github.com/Neuraxio/Neuraxle</a>
</li>

<li>scikit-learn: Getting Started<br />
<a href="https://scikit-learn.org/stable/getting_started.html">https://scikit-learn.org/stable/getting_started.html</a>
</li>

<li>Support Vector Machines<br />
<a href="https://scikit-learn.org/stable/modules/svm.html">https://scikit-learn.org/stable/modules/svm.html</a>
</li>

<li>Use Deep Learning to Detect Programming Languages<br />
<a href="http://searene.me/2017/11/26/use-neural-networks-to-detect-programming-languages/">http://searene.me/2017/11/26/use-neural-networks-to-detect-programming-languages/</a>
</li>

<li>Natural-language processing<br />
<a href="https://en.wikipedia.org/wiki/Natural-language_processing">https://en.wikipedia.org/wiki/Natural-language_processing</a>
</li>

<li>THE MNIST DATABASE of handwritten digits<br />
<a href="http://yann.lecun.com/exdb/mnist/">http://yann.lecun.com/exdb/mnist/</a>
</li>

<li>MNIST database (Wikipedia)<br />
<a href="https://en.wikipedia.org/wiki/MNIST_database">https://en.wikipedia.org/wiki/MNIST_database</a>
</li>

<li>MNIST For ML Beginners<br />
<a href="https://www.tensorflow.org/get_started/mnist/beginners">https://www.tensorflow.org/get_started/mnist/beginners</a>
</li>

<li>Stránka projektu Torch<br />
<a href="http://torch.ch/">http://torch.ch/</a>
</li>

<li>Torch: Serialization<br />
<a href="https://github.com/torch/torch7/blob/master/doc/serialization.md">https://github.com/torch/torch7/blob/master/doc/serialization.md</a>
</li>

<li>Torch: modul image<br />
<a href="https://github.com/torch/image/blob/master/README.md">https://github.com/torch/image/blob/master/README.md</a>
</li>

<li>Data pro neuronové sítě<br />
<a href="http://archive.ics.uci.edu/ml/index.php">http://archive.ics.uci.edu/ml/index.php</a>
</li>

<li>Torch na GitHubu (několik repositářů)<br />
<a href="https://github.com/torch">https://github.com/torch</a>
</li>

<li>Torch (machine learning), Wikipedia<br />
<a href="https://en.wikipedia.org/wiki/Torch_%28machine_learning%29">https://en.wikipedia.org/wiki/Torch_%28machine_learning%29</a>
</li>

<li>Torch Package Reference Manual<br />
<a href="https://github.com/torch/torch7/blob/master/README.md">https://github.com/torch/torch7/blob/master/README.md</a>
</li>

<li>Torch Cheatsheet<br />
<a href="https://github.com/torch/torch7/wiki/Cheatsheet">https://github.com/torch/torch7/wiki/Cheatsheet</a>
</li>

<li>Neural network containres (Torch)<br />
<a href="https://github.com/torch/nn/blob/master/doc/containers.md">https://github.com/torch/nn/blob/master/doc/containers.md</a>
</li>

<li>Simple layers<br />
<a href="https://github.com/torch/nn/blob/master/doc/simple.md#nn.Linear">https://github.com/torch/nn/blob/master/doc/simple.md#nn.Linear</a>
</li>

<li>Transfer Function Layers<br />
<a href="https://github.com/torch/nn/blob/master/doc/transfer.md#nn.transfer.dok">https://github.com/torch/nn/blob/master/doc/transfer.md#nn.transfer.dok</a>
</li>

<li>Feedforward neural network<br />
<a href="https://en.wikipedia.org/wiki/Feedforward_neural_network">https://en.wikipedia.org/wiki/Feedforward_neural_network</a>
</li>

<li>Biologické algoritmy (4) - Neuronové sítě<br />
<a href="https://www.root.cz/clanky/biologicke-algoritmy-4-neuronove-site/">https://www.root.cz/clanky/biologicke-algoritmy-4-neuronove-site/</a>
</li>

<li>Biologické algoritmy (5) - Neuronové sítě<br />
<a href="https://www.root.cz/clanky/biologicke-algoritmy-5-neuronove-site/">https://www.root.cz/clanky/biologicke-algoritmy-5-neuronove-site/</a>
</li>

<li>Umělá neuronová síť (Wikipedia)<br />
<a href="https://cs.wikipedia.org/wiki/Um%C4%9Bl%C3%A1_neuronov%C3%A1_s%C3%AD%C5%A5">https://cs.wikipedia.org/wiki/Um%C4%9Bl%C3%A1_neuronov%C3%A1_s%C3%AD%C5%A5</a>
</li>

<li>PyTorch<br />
<a href="http://pytorch.org/">http://pytorch.org/</a>
</li>

<li>JupyterLite na PyPi<br />
<a href="https://pypi.org/project/jupyterlite/">https://pypi.org/project/jupyterlite/</a>
</li>

<li>JupyterLite na GitHubu<br />
<a href="https://github.com/jupyterlite/jupyterlite">https://github.com/jupyterlite/jupyterlite</a>
</li>

<li>Dokumentace k&nbsp;projektu JupyterLite<br />
<a href="https://github.com/jupyterlite/jupyterlite">https://github.com/jupyterlite/jupyterlite</a>
</li>

<li>Matplotlib Home Page<br />
<a href="http://matplotlib.org/">http://matplotlib.org/</a>
</li>

<li>Matplotlib (Wikipedia)<br />
<a href="https://en.wikipedia.org/wiki/Matplotlib">https://en.wikipedia.org/wiki/Matplotlib</a>
</li>

<li>Popis barvových map modulu matplotlib.cm<br />
<a href="https://gist.github.com/endolith/2719900#id7">https://gist.github.com/endolith/2719900#id7</a>
</li>

<li>Ukázky (palety) barvových map modulu matplotlib.cm<br />
<a href="http://matplotlib.org/examples/color/colormaps_reference.html">http://matplotlib.org/examples/color/colormaps_reference.html</a>
</li>

<li>Galerie grafů vytvořených v&nbsp;Matplotlibu<br />
<a href="https://matplotlib.org/3.2.1/gallery/">https://matplotlib.org/3.2.1/gallery/</a>
</li>

<li>3D rendering<br />
<a href="https://en.wikipedia.org/wiki/3D_rendering">https://en.wikipedia.org/wiki/3D_rendering</a>
</li>

<li>3D computer graphics<br />
<a href="https://en.wikipedia.org/wiki/3D_computer_graphics">https://en.wikipedia.org/wiki/3D_computer_graphics</a>
</li>

<li>Primary 3D view planes<br />
<a href="https://matplotlib.org/stable/gallery/mplot3d/view_planes_3d.html">https://matplotlib.org/stable/gallery/mplot3d/view_planes_3d.html</a>
</li>

<li>Getting started in scikit-learn with the famous iris dataset<br />
<a href="https://www.youtube.com/watch?v=hd1W4CyPX58">https://www.youtube.com/watch?v=hd1W4CyPX58</a>
</li>

<li>Training a machine learning model with scikit-learn<br />
<a href="https://www.youtube.com/watch?v=RlQuVL6-qe8">https://www.youtube.com/watch?v=RlQuVL6-qe8</a>
</li>

<li>Iris (plant)<br />
<a href="https://en.wikipedia.org/wiki/Iris_(plant)">https://en.wikipedia.org/wiki/Iris_(plant)</a>
</li>

<li>Kosatec<br />
<a href="https://cs.wikipedia.org/wiki/Kosatec">https://cs.wikipedia.org/wiki/Kosatec</a>
</li>

<li>Iris setosa<br />
<a href="https://en.wikipedia.org/wiki/Iris_setosa">https://en.wikipedia.org/wiki/Iris_setosa</a>
</li>

<li>Iris versicolor<br />
<a href="https://en.wikipedia.org/wiki/Iris_versicolor">https://en.wikipedia.org/wiki/Iris_versicolor</a>
</li>

<li>Iris virginica<br />
<a href="https://en.wikipedia.org/wiki/Iris_virginica">https://en.wikipedia.org/wiki/Iris_virginica</a>
</li>

<li>Druh<br />
<a href="https://cs.wikipedia.org/wiki/Druh">https://cs.wikipedia.org/wiki/Druh</a>
</li>

<li>Iris subg. Limniris<br />
<a href="https://en.wikipedia.org/wiki/Iris_subg._Limniris">https://en.wikipedia.org/wiki/Iris_subg._Limniris</a>
</li>

<li>Iris Dataset Classification with Python: A Tutorial<br />
<a href="https://www.pycodemates.com/2022/05/iris-dataset-classification-with-python.html">https://www.pycodemates.com/2022/05/iris-dataset-classification-with-python.html</a>
</li>

<li>Iris flower data set<br />
<a href="https://en.wikipedia.org/wiki/Iris_flower_data_set">https://en.wikipedia.org/wiki/Iris_flower_data_set</a>
</li>

<li>List of datasets for machine-learning research<br />
<a href="https://en.wikipedia.org/wiki/List_of_datasets_for_machine-learning_research">https://en.wikipedia.org/wiki/List_of_datasets_for_machine-learning_research</a>
</li>

<li>Analýza hlavních komponent<br />
<a href="https://cs.wikipedia.org/wiki/Anal%C3%BDza_hlavn%C3%ADch_komponent">https://cs.wikipedia.org/wiki/Anal%C3%BDza_hlavn%C3%ADch_komponent</a>
</li>

<li>Principal component analysis<br />
<a href="https://en.wikipedia.org/wiki/Principal_component_analysis">https://en.wikipedia.org/wiki/Principal_component_analysis</a>
</li>

<li>Scikit-learn Crash Course - Machine Learning Library for Python<br />
<a href="https://www.youtube.com/watch?v=0B5eIE_1vpU">https://www.youtube.com/watch?v=0B5eIE_1vpU</a>
</li>

<li>calm-notebooks<br />
<a href="https://github.com/koaning/calm-notebooks">https://github.com/koaning/calm-notebooks</a>
</li>

<li>Should you teach Python or R for data science?<br />
<a href="https://www.dataschool.io/python-or-r-for-data-science/">https://www.dataschool.io/python-or-r-for-data-science/</a>
</li>

<li>nbviewer: A simple way to share Jupyter Notebooks<br />
<a href="https://nbviewer.org/">https://nbviewer.org/</a>
</li>

<li>AI vs Machine Learning (Youtube)<br />
<a href="https://www.youtube.com/watch?v=4RixMPF4xis">https://www.youtube.com/watch?v=4RixMPF4xis</a>
</li>

<li>Machine Learning | What Is Machine Learning? | Introduction To Machine Learning | 2024 | Simplilearn (Youtube)<br />
<a href="https://www.youtube.com/watch?v=ukzFI9rgwfU">https://www.youtube.com/watch?v=ukzFI9rgwfU</a>
</li>

<li>A Gentle Introduction to Machine Learning (Youtube)<br />
<a href="https://www.youtube.com/watch?v=Gv9_4yMHFhI">https://www.youtube.com/watch?v=Gv9_4yMHFhI</a>
</li>

<li>Machine Learning vs Deep Learning<br />
<a href="https://www.youtube.com/watch?v=q6kJ71tEYqM">https://www.youtube.com/watch?v=q6kJ71tEYqM</a>
</li>

<li>Umělá inteligence (slajdy)<br />
<a href="https://slideplayer.cz/slide/12119218/">https://slideplayer.cz/slide/12119218/</a>
</li>

<li>Úvod do umělé inteligence<br />
<a href="https://slideplayer.cz/slide/2505525/">https://slideplayer.cz/slide/2505525/</a>
</li>

<li>Umělá inteligence I / Artificial Intelligence I<br />
<a href="https://ktiml.mff.cuni.cz/~bartak/ui/">https://ktiml.mff.cuni.cz/~bartak/ui/</a>
</li>

<li>Matplotlib vs. seaborn vs. Plotly vs. MATLAB vs. ggplot2 vs. pandas<br />
<a href="https://ritza.co/articles/matplotlib-vs-seaborn-vs-plotly-vs-MATLAB-vs-ggplot2-vs-pandas/">https://ritza.co/articles/matplotlib-vs-seaborn-vs-plotly-vs-MATLAB-vs-ggplot2-vs-pandas/</a>
</li>

<li>Matplotlib, Seaborn or Plotnine?<br />
<a href="https://www.reddit.com/r/datascience/comments/jvrqxt/matplotlib_seaborn_or_plotnine/">https://www.reddit.com/r/datascience/comments/jvrqxt/matplotlib_seaborn_or_plotnine/</a>
</li>

<li>@Rabeez: Rabeez/plotting_comparison.ipynb<br />
<a href="https://gist.github.com/Rabeez/ffc0b59d4a41e20fa8d944c44a96adbc">https://gist.github.com/Rabeez/ffc0b59d4a41e20fa8d944c44a96adbc</a>
</li>

<li>Matplotlib, Seaborn, Plotly and Plotnine Comparison<br />
<a href="https://python.plainenglish.io/matplotlib-seaborn-plotly-and-plotnine-comparison-baf2db5a9c40">https://python.plainenglish.io/matplotlib-seaborn-plotly-and-plotnine-comparison-baf2db5a9c40</a>
</li>

<li>Data Visualization 101: How to Choose a Python Plotting Library<br />
<a href="https://towardsdatascience.com/data-visualization-101-how-to-choose-a-python-plotting-library-853460a08a8a">https://towardsdatascience.com/data-visualization-101-how-to-choose-a-python-plotting-library-853460a08a8a</a>
</li>

<li>Data science in Python: pandas, seaborn, scikit-learn<br />
<a href="https://www.youtube.com/watch?v=3ZWuPVWq7p4">https://www.youtube.com/watch?v=3ZWuPVWq7p4</a>
</li>

<li>7.2. Real world datasets<br />
<a href="https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset">https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset</a>
</li>

<li>7.2.7. California Housing dataset<br />
<a href="https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset">https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset</a>
</li>

<li>Comprehensive Guide to Classification Models in Scikit-Learn<br />
<a href="https://www.geeksforgeeks.org/comprehensive-guide-to-classification-models-in-scikit-learn/">https://www.geeksforgeeks.org/comprehensive-guide-to-classification-models-in-scikit-learn/</a>
</li>

<li>Tidy Data Visualization: ggplot2 vs seaborn<br />
<a href="https://blog.tidy-intelligence.com/posts/ggplot2-vs-seaborn/">https://blog.tidy-intelligence.com/posts/ggplot2-vs-seaborn/</a>
</li>

<li>seaborn: statistical data visualization<br />
<a href="https://seaborn.pydata.org/">https://seaborn.pydata.org/</a>
</li>

<li>Linear regression (Wikipedia)<br />
<a href="https://en.wikipedia.org/wiki/Linear_regression">https://en.wikipedia.org/wiki/Linear_regression</a>
</li>

<li>Lineární regrese (Wikipedia)<br />
<a href="https://cs.wikipedia.org/wiki/Line%C3%A1rn%C3%AD_regrese">https://cs.wikipedia.org/wiki/Line%C3%A1rn%C3%AD_regrese</a>
</li>

<li>Iris Flower Classification with MLP Classifier<br />
<a href="https://www.metriccoders.com/post/iris-flower-classification-with-mlp-classifier">https://www.metriccoders.com/post/iris-flower-classification-with-mlp-classifier</a>
</li>

</ol>



<p></p><p></p>
<p><small>Autor: <a href="http://www.fit.vutbr.cz/~tisnovpa">Pavel Tišnovský</a> &nbsp; 2024</small></p>
</body>
</html>

