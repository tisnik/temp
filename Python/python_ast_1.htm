<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
        "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html>
<head>
<title>Lexikální a syntaktická analýza zdrojových kódů programovacího jazyka Python</title>
<meta name="Author" content="Pavel Tisnovsky" />
<meta name="Generator" content="vim" />
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<style type="text/css">
         body {color:#000000; background:#ffffff;}
         h1  {font-family: arial, helvetica, sans-serif; color:#ffffff; background-color:#c00000; text-align:center; padding-left:1em}
         h2  {font-family: arial, helvetica, sans-serif; color:#ffffff; background-color:#0000c0; padding-left:1em; text-align:left}
         h3  {font-family: arial, helvetica, sans-serif; color:#000000; background-color:#c0c0c0; padding-left:1em; text-align:left}
         h4  {font-family: arial, helvetica, sans-serif; color:#000000; background-color:#e0e0e0; padding-left:1em; text-align:left}
         a   {font-family: arial, helvetica, sans-serif;}
         li  {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify; width:450px;}
         ol  {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify; width:450px;}
         ul  {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify; width:450px;}
         p   {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify;}
         pre {background:#e0e0e0}
</style>
</head>

<body>

<h1>Lexikální a syntaktická analýza zdrojových kódů programovacího jazyka Python</h1>

<h3>Pavel Tišnovský</h3>

<p></p>

<h1>Úvodník</h1>

<p>V tomto článku o jazyku Python si řekneme, jak je možné s využitím standardní knihovny Pythonu provádět lexikální i syntaktickou analýzu zdrojových kódů napsaných v Pythonu, včetně konstrukce a zobrazení AST (abstraktního syntaktického stromu). </p>



<h2>Obsah</h2>

<p><a href="#k01">1. Lexikální a syntaktická analýza zdrojových kódů programovacího jazyka Python</a></p>
<p><a href="#k02">2. Od volně zapsaného zdrojového kódu k&nbsp;AST</a></p>
<p><a href="#k03">3. Lexémy a tokeny (tokenizace)</a></p>
<p><a href="#k04">4. Malá odbočka: tokenizace a programovací jazyk BASIC</a></p>
<p><a href="#k05">5. Příklad tokenizace jednoduchého kódu napsaného v&nbsp;Pythonu</a></p>
<p><a href="#k06">6. Tokenizace standardním modulem <strong>tokenize</strong></a></p>
<p><a href="#k07">7. Tokenizace jednoduchého výrazu</a></p>
<p><a href="#k08">8. Tokenizace kódu s&nbsp;funkcí, vnořenými smyčkami a podmínkou</a></p>
<p><a href="#k09">9. Tokenizace kódu s&nbsp;klíčovými slovy <strong>async</strong> a <strong>await</strong></a></p>
<p><a href="#k10">10. Rozlišení operátorů v&nbsp;sekvenci tokenů</a></p>
<p><a href="#k11">11. Zobrazení AST pro část zdrojového kódu Pythonu</a></p>
<p><a href="#k12">*** 12. Zobrazení AST pro soubor se zdrojovým kódem</a></p>
<p><a href="#k13">13. Obsah navazujícího článku</a></p>
<p><a href="#k14">14. Repositář s&nbsp;demonstračními příklady</a></p>
<p><a href="#k15">15. Odkazy na Internetu</a></p>



<p><a name="k01"></a></p>
<h2 id="k01">1. Lexikální a syntaktická analýza zdrojových kódů programovacího jazyka Python</h2>

<p>Na trojici článků, v&nbsp;nichž jsme se zabývali problematikou lexikální a
syntaktické analýzy zdrojových kódů jazyka Go [<a
href="https://www.root.cz/clanky/lexikalni-a-syntakticka-zdrojovych-kodu-jazyka-go/">1</a>]
[<a
href="https://www.root.cz/clanky/lexikalni-a-syntakticka-analyza-zdrojovych-kodu-jazyka-go-2-cast/">2</a>]
[<a
href="https://www.root.cz/clanky/lexikalni-a-syntakticka-analyza-zdrojovych-kodu-jazyka-go-dokonceni/">3</a>]
dnes do jisté míry navážeme, ovšem přesuneme se <a
href="https://www.root.cz/serialy/programovaci-jazyk-go/">z&nbsp;jazyka Go</a>
k&nbsp;pravděpodobně nejpopulárnějšímu jazyku současnosti &ndash; <a
href="https://www.root.cz/n/python/">k&nbsp;Pythonu</a>. Ukážeme si, jak lze
provést takzvanou <i>tokenizaci</i> a následně <i>parsing</i> zdrojových kódů,
jehož výsledkem je AST neboli abstraktní syntaktický strom (<i>Abstract Syntax
Tree</i>). A samozřejmě se zmíníme i o tom, jakým způsobem lze abstraktním
syntaktickým stromem procházet či jinak manipulovat.</p>

<p>K&nbsp;čemu je však získání AST pro zadaný zdrojový kód vhodné? Uveďme si
několik příkladů:</p>

<ul>
<li>Statická analýza kódu s&nbsp;vyhledáváním potenciálně problematických částí</li>
<li>Formátování zdrojového kódu</li>
<li>Vyhledávání <i>strukturálních</i> změn ve dvou verzích kódu</li>
<li>AST používají i mnohé generátor kódu (<i>code generators</i>)</li>
<li>Nástroje typu <i>Hypothesis</i> taktéž pracují s&nbsp;AST</li>
<li>Většina <i>transpilerů</i> manipuluje právě s&nbsp;AST (<a href="https://transcrypt.org/">Transcrypt</a>)</li>
<li>Optimalizace typu <i>constant folding</i></li>
<li>Některé šablonovací nástroje taktéž pracují s&nbsp;AST</li>
<li>A samozřejmě AST slouží v&nbsp;samotném Pythonu pro konstrukci CFG (<i>Control Flow Graph</i>) a následně k&nbsp;produkci bajtkódu</li>
</ul>

<p>V&nbsp;případě samotného Pythonu je cesta od zdrojového kódu k&nbsp;bajtkódu
(dosti zjednodušeně řečeno) následující:</p>

<pre>
zdrojový kód &rarr; parse tree &rarr; AST &rarr; CFG &rarr; bajtkód
</pre>

<p><div class="rs-tip-major">Poznámka: pro Python existují i podpůrné knihovny
a nástroje pro manipulaci s&nbsp;AST, například <strong>astor</strong>,
<strong>meta</strong> a <strong>codegen</strong>. Ještě se k&nbsp;nim
vrátíme.</div></p>



<p><a name="k02"></a></p>
<h2 id="k02">2. Od volně zapsaného zdrojového kódu k&nbsp;AST</h2>

<p>Při zpracování zdrojových kódů se postupně provádí jednotlivé dílčí kroky, a
to jak v&nbsp;klasických překladačích (Go), tak i v&nbsp;jazycích, které
provádí překlad &bdquo;jen&ldquo; do bajtkódu s&nbsp;jeho pozdější interpretací
(Python). Díky rozdělení celého zpracování do několika konfigurovatelných kroků
je zajištěna velká flexibilita a možnost případného relativně snadného
rozšiřování o další syntaktické prvky, existuje možnost použití jediné sady
nástrojů více jazyky, lze přidat podporu pro různé výstupní formáty (překlad do
nativního kódu nebo do WebAssembly atd.), podporu speciální filtry apod.
(nehledě na to, že každá činnost je založena na odlišné teorii). Celý průběh
zpracování vypadá při určitém zjednodušení následovně:</p>

<ol>

<li>Na začátku zpracování se nachází takzvaný <i>lexer</i>, který postupně
načítá jednotlivé znaky ze vstupního řetězce (resp.&nbsp;ze vstupního souboru)
a vytváří z&nbsp;nich lexikální <i>tokeny</i>. Teoreticky se pro každý
programovací jazyk používá odlišný lexer a samozřejmě je možné v&nbsp;případě
potřeby si napsat lexer vlastní. V&nbsp;případě Pythonu můžeme použít
standardní modul <a
href="https://docs.python.org/3.8/library/tokenize.html">tokenizer</a>, nebo
lze alternativně použít například projekt <i>Pygments</i>, jenž obsahuje lexery
pro mnoho dalších programovacích jazyků.</li>

<li>Výstup z&nbsp;lexeru může procházet libovolným počtem <i>filtrů</i>
sloužících pro odstranění nebo (častěji) modifikaci jednotlivých tokenů; ať již
jejich typů či přímo textu, který tvoří hodnotu tokenu. Díky existenci filtrů
je například možné nechat si zvýraznit vybrané bílé znaky, slova se speciálním
významem v&nbsp;komentářích (TODO:, FIX:) apod. Některé lexery obsahují filtr
přímo ve svém modulu.</li>

<li>Sekvence <i>tokenů</i> tvoří základ pro syntaktickou analýzu. Nástroj,
který syntaktickou analýzu provádí, se většinou nazývá <i>parser</i> a proto se
taktéž někdy setkáme s&nbsp;pojmem <i>parsing</i> (tento termín je ovšem chybně
používán i v&nbsp;těch případech, kdy se provádí &bdquo;pouze&ldquo; lexikální
analýza). Výsledkem činnosti parseru je vhodně zvolená datová struktura,
typicky abstraktní syntaktický strom (AST); někdy též strom derivační.
V&nbsp;případě Pythonu vypadá postupné zpracování vstupního zdrojového textu
takto: lexer &rarr; derivační strom (<i>parse tree</i>) &rarr; AST.</li>

</ol>

<p><div class="rs-tip-major">Poznámka: díky tomu, že se prakticky veškeré
zpracování zdrojových textů odehrává na úrovni tokenů, není nutné, aby byl celý
zpracovávaný zdrojový kód (nebo jeho tokenizovaná podoba) uložen
v&nbsp;operační paměti. Je tedy možné zpracovávat i velmi rozsáhlé zdrojové
texty, a to bez větších nároků na operační paměť. Tento princip je použit
například v&nbsp;již popsaném balíčku <strong>go/scanner</strong>, ovšem
v&nbsp;případě Pythonu a jeho standardních modulů s&nbsp;lexerem a parserem
<i>jsme</i> omezeni dostupnou kapacitou paměti (což v&nbsp;praxi nevadí, kromě
extrémních případů).</div></p>



<p><a name="k03"></a></p>
<h2 id="k03">3. Lexémy a tokeny (tokenizace)</h2>

<p>První část zpracování zdrojových textů je nejzajímavější, a to jak
z&nbsp;hlediska použití, tak i z&nbsp;hlediska implementace (zde ovšem záleží
na tom, jaké techniky jsou použity pro implementaci lexeru). <i>Lexer</i> totiž
musí v&nbsp;sekvenci znaků tvořících zdrojový text najít takzvané
<i>lexémy</i>, tj.&nbsp;skupiny (sousedních) znaků odpovídajících nějakému
vzorku (použít lze gramatiku, regulární výraz či ad-hoc testy). Z&nbsp;lexémů
se posléze tvoří již zmíněné lexikální <i>tokeny</i>, což je &ndash; poněkud
zjednodušeně řečeno &ndash; typicky dvojice obsahující typ tokenu (někdy se
namísto &bdquo;typ&ldquo; používá označení &bdquo;jméno&ldquo;) a řetězec ze
vstupního zdrojového souboru (jak uvidíme dále, obsahují tokeny generované
standardním Pythonovským lexerem mnohem více informací, například i celý řádek
zdrojového kódu atd.). Převodu zdrojového textu na sekvenci tokenů se někdy
říká <i>tokenizace</i>. Účelem tokenizace může být:</p>

<ul>

<li>Transformace zdrojového textu do podoby, která může být dále zpracovávána
dalším modulem překladače (syntaktická analýza popř.&nbsp;nějaké prvotní
optimalizace). V&nbsp;takovém případě se však některé tokeny mohou zahazovat;
příkladem mohou být komentáře, tokeny představující bílé znaky apod. Spojením
lexeru a modulu pro syntaktickou analýzu vznikne <i>parser</i> (jeho typickým
výsledkem je AST, mezivýsledkem pak již zmíněný derivační strom).</li>

<li>Transformace zdrojového kódu pro účely zvýraznění syntaxe
v&nbsp;programátorských textových editorech či prohlížečích. V&nbsp;tomto
případě se žádné tokeny nezahazují, což je mimochodem i případ knihovny
Pygments, kde obecně vyžadujeme, aby výstup obsahoval všechny informace získané
ze vstupu (zdrojového kódu).</li>

</ul>



<p><a name="k04"></a></p>
<h2 id="k04">4. Malá odbočka: tokenizace a programovací jazyk BASIC</h2>

<p>Výše zmíněná <i>tokenizace</i> se používala například již
v&nbsp;interpretrech programovacího jazyka BASIC na mnoha osmibitových domácích
počítačích (ovšem i na některých minipočítačích, kde však BASIC nebyl primárním
programovacím jazykem). Ovšem v&nbsp;tomto případě měly tokeny poněkud odlišnou
strukturu, protože všechny příkazy a funkce byly většinou reprezentovány
jednoznačným osmibitovým celým číslem, které tak současně představovalo jak typ
tokenu, tak i jeho hodnotu. Důvod byl jednoduchý &ndash; v&nbsp;operační paměti
(a ostatně i na disketě nebo magnetické pásce) mohl být uložen tokenizovaný kód
a nikoli kód zapsaný uživatelem. Tento kód byl již mnohem jednodušeji
zpracovatelný interpretrem, než původní zdrojový kód (odpadlo neustálé volání
<i>lexeru</i> &ndash; lexikální analýza totiž nebyla na procesorech
s&nbsp;frekvencemi okolo 1MHz tak rychlá operace, jako dnes). Navíc se každý
programový řádek ihned po svém zápisu automaticky normalizoval (odstranily se
bílé znaky, zkratky příkazů se expandovaly atd.). Ostatně množina příkazů a
funkcí byla předem známá a nebyla rozšiřitelná (až na uživatelské funkce
dostupné jen v&nbsp;některých BASICech). Tokenizovaná podoba programu mohla být
i kratší.</p>

<p>Příkladem tokenizace tohoto typu mohou být tokeny použité v&nbsp;interpretru
programovacího jazyka Atari BASIC, které skutečně přímo odpovídají příkazům,
funkcím a operátorům tohoto jazyka. Pro zajímavost:</div></p>

<table>
<tr><th>Příkaz</th><th>Kód tokenu</th><th>Příkaz</th><th>Kód tokenu</th><th>Příkaz</th><th>Kód tokenu</th><th>Příkaz</th><th>Kód tokenu</th></tr>
<tr><td>REM</td><td>00</td><td>NEXT</td><td>09</td><td>CLR</td><td>18</td><td>NOTE</td><td>27</td></tr>
<tr><td>DATA</td><td>01</td><td>GOTO</td><td>10</td><td>DEG</td><td>19</td><td>POINT</td><td>28</td></tr>
<tr><td>INPUT</td><td>02</td><td>GO TO</td><td>11</td><td>DIM</td><td>20</td><td>XIO</td><td>29</td></tr>
<tr><td>COLOR</td><td>03</td><td>GOSUB</td><td>12</td><td>END</td><td>21</td><td>ON</td><td>30</td></tr>
<tr><td>LIST</td><td>04</td><td>TRAP</td><td>13</td><td>NEW</td><td>22</td><td>POKE</td><td>31</td></tr>
<tr><td>ENTER</td><td>05</td><td>BYE</td><td>14</td><td>OPEN</td><td>23</td><td>PRINT</td><td>32</td></tr>
<tr><td>LET</td><td>06</td><td>CONT</td><td>15</td><td>LOAD</td><td>24</td><td>RAD</td><td>33</td></tr>
<tr><td>IF</td><td>07</td><td>COM</td><td>16</td><td>SAVE</td><td>25</td><td>READ</td><td>34</td></tr>
<tr><td>FOR</td><td>08</td><td>CLOSE</td><td>17</td><td>STATUS</td><td>26</td><td>RESTORE</td><td>35</td></tr>
</table>

<p>...atd...</p>

<p>Podívejme se nyní na způsob uložení programového kódu v&nbsp;operační
paměti. Ihned po zápisu každého řádku se provádí již několikrát zmíněná
<i>tokenizace</i>, která nahradí jednotlivé konstrukce jazyka osmibitovými
kódy. Příkladem může být tokenizace tohoto programového řádku:</p>

<pre>
10 LET X=1 : PRINT X
</pre>

<p>V&nbsp;operační paměti tento kód není uložen (pouze v&nbsp;bufferu textového
editoru, odtud je však ihned poté přemazán, jakmile je tokenizace dokončena).
Namísto toho se do paměti uloží sekvence bajtů s&nbsp;následujícím
významem:</p>

<table>
<tr><th>Sekvence bajtů</th><th>Stručný popis</th></tr>
<tr><td>A0 00</td>číslo programového řádku (10)<td></td></tr>
<tr><td>13</td><td>délka celého tokenizovaného řádku (19 bajtů)</td></tr>
<tr><td>0F</td><td>offset konce prvního příkazu</td></tr>
<tr><td>06</td><td>token příkazu <strong>LET</strong></td></tr>
<tr><td>80</td><td>index proměnné <strong>X</strong></td></tr>
<tr><td>2D</td><td>token operátoru <strong>=</strong></td></tr>
<tr><td>0E</td><td>následuje numerická konstanta</td></tr>
<tr><td>40 01 00 00 00 00</td><td>takto vypadá zakódovaná hodnota 1 (na začátku je exponent)</td></tr>
<tr><td>14</td><td>konec (prvního) příkazu</td></tr>
<tr><td>13</td><td>offset konce druhého příkazu</td></tr>
<tr><td>20</td><td>token příkazu <strong>PRINT</strong></td></tr>
<tr><td>80</td><td>index proměnné <strong>X</strong></td></tr>
<tr><td>16</td><td>značka <strong>EOL</strong> pro konec řádku</td></tr>
</table>



<p><a name="k05"></a></p>
<h2 id="k05">5. Příklad tokenizace jednoduchého kódu napsaného v&nbsp;Pythonu</h2>

<p>Podívejme se nyní na příklad tokenizace velmi jednoduchého a krátkého
zdrojového kódu, který je naprogramován v&nbsp;Pythonu:</p>

<pre>
for i in range(1, 11):
    print("Hello world!")
</pre>

<p>Pro tokenizaci v&nbsp;tomto případě použijeme knihovnu Pygments, která
skutečně obsahuje zcela klasickou podobu <i>lexeru</i>. Výsledkem tokenizace je
následující sekvence tokenů, tj.&nbsp;dvojic typ+hodnota (řetězec):</p>

<table>
<tr><th>Typ tokenu</th><th>Řetězec</th></tr>
<tr><td>Token.Keyword</td><td>'for'</td></tr>
<tr><td>Token.Text</td><td>' '</td></tr>
<tr><td>Token.Name</td><td>'i'</td></tr>
<tr><td>Token.Text</td><td>' '</td></tr>
<tr><td>Token.Operator.Word</td><td>'in'</td></tr>
<tr><td>Token.Text</td><td>' '</td></tr>
<tr><td>Token.Name.Builtin</td><td>'range'</td></tr>
<tr><td>Token.Punctuation</td><td>'('</td></tr>
<tr><td>Token.Literal.Number.Integer</td><td>'1'</td></tr>
<tr><td>Token.Punctuation</td><td>','</td></tr>
<tr><td>Token.Text</td><td>' '</td></tr>
<tr><td>Token.Literal.Number.Integer</td><td>'11'</td></tr>
<tr><td>Token.Punctuation</td><td>')'</td></tr>
<tr><td>Token.Punctuation</td><td>':'</td></tr>
<tr><td>Token.Text</td><td>'\n'</td></tr>
<tr><td>Token.Text</td><td>'    '</td></tr>
<tr><td>Token.Keyword</td><td>'print'</td></tr>
<tr><td>Token.Punctuation</td><td>'('</td></tr>
<tr><td>Token.Literal.String.Double</td><td>'"'</td></tr>
<tr><td>Token.Literal.String.Double</td><td>'Hello world!'</td></tr>
<tr><td>Token.Literal.String.Double</td><td>'"'</td></tr>
<tr><td>Token.Punctuation</td><td>')'</td></tr>
<tr><td>Token.Text</td><td>'\n'</td></tr>
</table>

<p><div class="rs-tip-major">Poznámka: <i>lexer</i> se v&nbsp;žádném případě
nesnaží o nalezení syntaktických (a už vůbec ne sémantických) chyb
v&nbsp;programu! Pouze se snaží rozeznat známé vzorky. To například znamená, že
tokenizace proběhne bez problémů i pro tento zdrojový kód, který je sémanticky
naprosto chybný:</div></p>

<pre>
range(1, "FDA") for while with i
except for for i else
    print("Hello world!")
</pre>

<p>Výsledek tokenizace v&nbsp;tomto případě vypadá následovně:</p>

<table>
<tr><th>Typ tokenu</th><th>Řetězec</th></tr>
<tr><td>Token.Name.Builtin</td><td>'range'</td></tr>
<tr><td>Token.Punctuation</td><td>'('</td></tr>
<tr><td>Token.Literal.Number.Integer</td><td>'1'</td></tr>
<tr><td>Token.Punctuation</td><td>','</td></tr>
<tr><td>Token.Text</td><td>' '</td></tr>
<tr><td>Token.Literal.String.Double</td><td>'"'</td></tr>
<tr><td>Token.Literal.String.Double</td><td>'FDA'</td></tr>
<tr><td>Token.Literal.String.Double</td><td>'"'</td></tr>
<tr><td>Token.Punctuation</td><td>')'</td></tr>
<tr><td>Token.Text</td><td>' '</td></tr>
<tr><td>Token.Keyword</td><td>'for'</td></tr>
<tr><td>Token.Text</td><td>' '</td></tr>
<tr><td>Token.Keyword</td><td>'while'</td></tr>
<tr><td>Token.Text</td><td>' '</td></tr>
<tr><td>Token.Keyword</td><td>'with'</td></tr>
<tr><td>Token.Text</td><td>' '</td></tr>
<tr><td>Token.Name</td><td>'i'</td></tr>
<tr><td>Token.Text</td><td>'\n'</td></tr>
<tr><td>Token.Keyword</td><td>'except'</td></tr>
<tr><td>Token.Text</td><td>' '</td></tr>
<tr><td>Token.Keyword</td><td>'for'</td></tr>
<tr><td>Token.Text</td><td>' '</td></tr>
<tr><td>Token.Keyword</td><td>'for'</td></tr>
<tr><td>Token.Text</td><td>' '</td></tr>
<tr><td>Token.Name</td><td>'i'</td></tr>
<tr><td>Token.Text</td><td>' '</td></tr>
<tr><td>Token.Keyword</td><td>'else'</td></tr>
<tr><td>Token.Text</td><td>'\n'</td></tr>
<tr><td>Token.Text</td><td>'    '</td></tr>
<tr><td>Token.Keyword</td><td>'print'</td></tr>
<tr><td>Token.Punctuation</td><td>'('</td></tr>
<tr><td>Token.Literal.String.Double</td><td>'"'</td></tr>
<tr><td>Token.Literal.String.Double</td><td>'Hello world!'</td></tr>
<tr><td>Token.Literal.String.Double</td><td>'"'</td></tr>
<tr><td>Token.Punctuation</td><td>')'</td></tr>
<tr><td>Token.Text</td><td>'\n'</td></tr>
</table>



<p><a name="k06"></a></p>
<h2 id="k06">6. Tokenizace standardním modulem <strong>tokenize</strong></h2>

<p>V&nbsp;navazujících kapitolách se budeme zabývat standardním Pythonovským
modulem nazvaným příznačně <a
href="https://docs.python.org/3.8/library/tokenize.html">tokenize</a>. Tento
modul je určen pro prvotní zpracování vstupních zdrojových kódů s&nbsp;tím, že
výsledkem je sekvence tokenů. Tokeny (tedy jejich typ a příslušný numerický
kód) jsou deklarovány v&nbsp;modulu nazvaném <a
href="https://docs.python.org/3.8/library/token.html">token</a>. Vzhledem
k&nbsp;tomu, že jsou postupně přidávány další typy tokenů
(<strong>ASYNC</strong> apod.), může být zajímavé si nechat vypsat všechny
tokeny pro konkrétní (právě nainstalovanou) verzi Pythonu. Pro tento účel může
sloužit následující skript:</p>

<pre>
import token
&nbsp;
for name, value in vars(token).items():
    if not name.startswith('_'):
        if isinstance(value, int):
            print("{:20s} {}".format(name, value))
</pre>

<p>Výsledkem činnosti tohoto skriptu budou symbolická jména i numerické hodnoty
všech rozeznávaných tokenů.</p>

<p>Konkrétně pro dnes již poněkud obstarožní Python 3.8:</p>

<pre>
ENDMARKER            0
NAME                 1
NUMBER               2
STRING               3
NEWLINE              4
INDENT               5
DEDENT               6
LPAR                 7
RPAR                 8
LSQB                 9
RSQB                 10
COLON                11
COMMA                12
SEMI                 13
PLUS                 14
MINUS                15
STAR                 16
SLASH                17
VBAR                 18
AMPER                19
LESS                 20
GREATER              21
EQUAL                22
DOT                  23
PERCENT              24
LBRACE               25
RBRACE               26
EQEQUAL              27
NOTEQUAL             28
LESSEQUAL            29
GREATEREQUAL         30
TILDE                31
CIRCUMFLEX           32
LEFTSHIFT            33
RIGHTSHIFT           34
DOUBLESTAR           35
PLUSEQUAL            36
MINEQUAL             37
STAREQUAL            38
SLASHEQUAL           39
PERCENTEQUAL         40
AMPEREQUAL           41
VBAREQUAL            42
CIRCUMFLEXEQUAL      43
LEFTSHIFTEQUAL       44
RIGHTSHIFTEQUAL      45
DOUBLESTAREQUAL      46
DOUBLESLASH          47
DOUBLESLASHEQUAL     48
AT                   49
ATEQUAL              50
RARROW               51
ELLIPSIS             52
COLONEQUAL           53
OP                   54
AWAIT                55
ASYNC                56
TYPE_IGNORE          57
TYPE_COMMENT         58
ERRORTOKEN           59
COMMENT              60
NL                   61
ENCODING             62
N_TOKENS             63
NT_OFFSET            256
</pre>

<p><div class="rs-tip-major">Poznámka: nutno podotknout, že sada tokenů ani
numerické hodnoty tokenů nejsou mezi jednotlivými verzemi Pythonu kompatibilní.
Můžeme se o tom snadno přesvědčit porovnáním tokenů z&nbsp;Python 2.7 (poslední
verze Pythonu 2) a již zmíněného Pythonu 3. Výsledek je zobrazen ve formě
&bdquo;dvousloupcového diffu&ldquo; (Python 2.7 je na levé straně):</div></p>

<pre>
AMPER             19                 AMPER                19
AMPEREQUAL        42               | AMPEREQUAL           41
AT                50               | ASYNC                56
BACKQUOTE         25               | AT                   49
CIRCUMFLEX        33               | ATEQUAL              50
CIRCUMFLEXEQUAL   44               | AWAIT                55
                                   &gt;      CIRCUMFLEX           32
                                   &gt;      CIRCUMFLEXEQUAL      43
COLON             11                 COLON                11
                                   &gt;      COLONEQUAL           53
COMMA             12                 COMMA                12
                                   &gt;      COMMENT              60
DEDENT            6                  DEDENT               6
DOT               23                 DOT                  23
DOUBLESLASH       48               | DOUBLESLASH          47
DOUBLESLASHEQUAL  49               | DOUBLESLASHEQUAL     48
DOUBLESTAR        36               | DOUBLESTAR           35
DOUBLESTAREQUAL   47               | DOUBLESTAREQUAL      46
                                   &gt;      ELLIPSIS             52
                                   &gt;      ENCODING             62
ENDMARKER         0                  ENDMARKER            0
EQEQUAL           28               | EQEQUAL              27
EQUAL             22                 EQUAL                22
ERRORTOKEN        52               | ERRORTOKEN           59
GREATER           21                 GREATER              21
GREATEREQUAL      31               | GREATEREQUAL         30
INDENT            5                  INDENT               5
LBRACE            26               | LBRACE               25
LEFTSHIFT         34               | LEFTSHIFT            33
LEFTSHIFTEQUAL    45               | LEFTSHIFTEQUAL       44
LESS              20                 LESS                 20
LESSEQUAL         30               | LESSEQUAL            29
LPAR              7                  LPAR                 7
LSQB              9                  LSQB                 9
MINEQUAL          38               | MINEQUAL             37
MINUS             15                 MINUS                15
NAME              1                  NAME                 1
NEWLINE           4                  NEWLINE              4
NOTEQUAL          29               | NL                   61
                                   &gt;      NOTEQUAL             28
NT_OFFSET         256                NT_OFFSET            256
N_TOKENS          53               | N_TOKENS             63
NUMBER            2                  NUMBER               2
OP                51               | OP                   54
PERCENT           24                 PERCENT              24
PERCENTEQUAL      41               | PERCENTEQUAL         40
PLUS              14                 PLUS                 14
PLUSEQUAL         37               | PLUSEQUAL            36
RBRACE            27               | RARROW               51
RIGHTSHIFT        35               | RBRACE               26
RIGHTSHIFTEQUAL   46               | RIGHTSHIFT           34
                                   &gt;      RIGHTSHIFTEQUAL      45
RPAR              8                  RPAR                 8
RSQB              10                 RSQB                 10
SEMI              13                 SEMI                 13
SLASH             17                 SLASH                17
SLASHEQUAL        40               | SLASHEQUAL           39
STAR              16                 STAR                 16
STAREQUAL         39               | STAREQUAL            38
STRING            3                  STRING               3
TILDE             32               | TILDE                31
                                   &gt;      TYPE_COMMENT         58
                                   &gt;      TYPE_IGNORE          57
VBAR              18                 VBAR                 18
VBAREQUAL         43               | VBAREQUAL            42
</pre>



<p><a name="k07"></a></p>
<h2 id="k07">7. Tokenizace jednoduchého výrazu</h2>

<p>Vyzkoušejme si nyní, jakým způsobem lze získat sekvenci tokenů pro nějaký
jednoduchý výraz zapsaný v&nbsp;syntaxi Pythonu:</p>

<pre>
1+2*3
</pre>

<p>Tento výraz uložíme do souboru nazvaného <strong>expression.py</strong>.
Následně využijeme výše zmíněný modul <strong>tokenize</strong>, a to jeho
přímým zavoláním z&nbsp;příkazové řádky:</p>

<pre>
$ <strong>python3 -m tokenize expression.py</strong>
</pre>

<p>Výsledek by měl vypadat takto:</p>

<pre>
0,0-0,0:            ENCODING       'utf-8'        
1,0-1,1:            NUMBER         '1'            
1,1-1,2:            OP             '+'            
1,2-1,3:            NUMBER         '2'            
1,3-1,4:            OP             '*'            
1,4-1,5:            NUMBER         '3'            
1,5-1,6:            NEWLINE        '\n'           
2,0-2,0:            ENDMARKER      ''             
</pre>

<p>Tokenizer ovšem ve skutečnosti neprovádí žádnou kontrolu syntaxe, takže
dokáže získat sekvenci tokenů i pro následující kód, který je v&nbsp;Pythonu
syntakticky nesprávný:</p>

<pre>
1*+2*3@4
</pre>

<p>S&nbsp;výsledkem:</p>

<pre>
$ <strong>python3 -m tokenize err_expression.py </strong>
0,0-0,0:            ENCODING       'utf-8'        
1,0-1,1:            NUMBER         '1'            
1,1-1,2:            OP             '*'            
1,2-1,3:            OP             '+'            
1,3-1,4:            NUMBER         '2'            
1,4-1,5:            OP             '*'            
1,5-1,6:            NUMBER         '3'            
1,6-1,7:            OP             '@'            
1,7-1,8:            NUMBER         '4'            
1,8-1,9:            NEWLINE        '\n'           
2,0-2,0:            ENDMARKER      ''             
</pre>

<p><div class="rs-tip-major">Poznámka: což mimochodem otevírá různé možnosti
rozšíření syntaxe Pythonu o další programové konstrukce.</div></p>

<p>Tokenizaci lze ovšem vyvolat přímo z&nbsp;Pythonu, a to přímým zavoláním
funkce <strong>tokenize</strong> z&nbsp;modulu se stejným jménem. Výsledkem je
sekvence tokenů, kterou lze procházet:</p>

<pre>
import tokenize
&nbsp;
with open("expression.py", "rb") as fin:
    token_generator = tokenize.tokenize(fin.readline)
    for token in token_generator:
        print(token)
</pre>

<p>S&nbsp;výsledkem:</p>

<pre>
$ <strong>python3 tokenize_expression_1.py</strong>
&nbsp;
TokenInfo(type=62 (ENCODING), string='utf-8', start=(0, 0), end=(0, 0), line='')
TokenInfo(type=2 (NUMBER), string='1', start=(1, 0), end=(1, 1), line='1+2*3\n')
TokenInfo(type=54 (OP), string='+', start=(1, 1), end=(1, 2), line='1+2*3\n')
TokenInfo(type=2 (NUMBER), string='2', start=(1, 2), end=(1, 3), line='1+2*3\n')
TokenInfo(type=54 (OP), string='*', start=(1, 3), end=(1, 4), line='1+2*3\n')
TokenInfo(type=2 (NUMBER), string='3', start=(1, 4), end=(1, 5), line='1+2*3\n')
TokenInfo(type=4 (NEWLINE), string='\n', start=(1, 5), end=(1, 6), line='1+2*3\n')
TokenInfo(type=0 (ENDMARKER), string='', start=(2, 0), end=(2, 0), line='')
</pre>

<p>Alternativní způsob zápisu s&nbsp;využitím funkce
<strong>tokenize.open</strong>:</p>

<pre>
import tokenize
&nbsp;
with tokenize.open("expression.py") as fin:
    token_generator = tokenize.generate_tokens(fin.readline)
    for token in token_generator:
        print(token)
</pre>

<p>Výsledek bude prakticky totožný s&nbsp;předchozím příkladem, až na chybějící
token s&nbsp;informací o kódování:</p>

<pre>
TokenInfo(type=2 (NUMBER), string='1', start=(1, 0), end=(1, 1), line='1+2*3\n')
TokenInfo(type=54 (OP), string='+', start=(1, 1), end=(1, 2), line='1+2*3\n')
TokenInfo(type=2 (NUMBER), string='2', start=(1, 2), end=(1, 3), line='1+2*3\n')
TokenInfo(type=54 (OP), string='*', start=(1, 3), end=(1, 4), line='1+2*3\n')
TokenInfo(type=2 (NUMBER), string='3', start=(1, 4), end=(1, 5), line='1+2*3\n')
TokenInfo(type=4 (NEWLINE), string='\n', start=(1, 5), end=(1, 6), line='1+2*3\n')
TokenInfo(type=0 (ENDMARKER), string='', start=(2, 0), end=(2, 0), line='')
</pre>

<p><div class="rs-tip-major">Poznámka: povšimněte si, kolik informací vlastně
získáme, a to díky tomu, že se nejedná pouze o klasické minimalisticky pojaté
tokeny, ale o tokeny s&nbsp;přidanými metainformacemi &ndash; dostaneme celý
programový řádek, textovou obdobu tokenu tak, jak je zapsán v&nbsp;programovém
kódu, přesné umístění tokenu na programovém řádku atd. Dokonce je tokenizován i
konec řádku. To například umožňuje překladači Pythonu (a Python <i>obsahuje</i>
překladač) zobrazovat přesná chybová hlášení.</div></p>



<p><a name="k08"></a></p>
<h2 id="k08">8. Tokenizace kódu s&nbsp;funkcí, vnořenými smyčkami a podmínkou</h2>

<p>Nyní se podívejme na poněkud složitější programový kód, konkrétně na funkci
určenou pro výpočet prvočísel až do nastaveného limitu. Povšimněte si, že tato
funkce obsahuje vnořené programové smyčky, podmínku, generátorovou notaci
seznamu atd., takže její tokenizovaná podoba bude zajímavější, než tomu bylo u
jednoduchého výrazu:</p>

<pre>
<i># originální kód lze nalézt na adrese:</i>
<i># http://www.rosettacode.org/wiki/Sieve_of_Eratosthenes#Using_array_lookup</i>
def primes2(limit):
    <i>"""Výpočet seznamu prvočísel až do zadaného limitu."""</i>
    is_prime = [False] * 2 + [True] * (limit - 1)
    for n in range(int(limit**0.5 + 1.5)):  # stop at ``sqrt(limit)``
        if is_prime[n]:
            for i in range(n*n, limit+1, n):
                is_prime[i] = False
    return [i for i, prime in enumerate(is_prime) if prime]
&nbsp;
&nbsp;
print(primes2(100))
</pre>

<p>Tokenizaci provedeme tímto skriptem:</p>

<pre>
import tokenize
&nbsp;
with tokenize.open("primes.py") as fin:
    token_generator = tokenize.generate_tokens(fin.readline)
    for token in token_generator:
        print(token)
</pre>

<p>Zkrácený výsledek bude vypadat následovně:</p>

<pre>
TokenInfo(type=60 (COMMENT), string='# originální kód lze nalézt na adrese:', start=(1, 0), end=(1, 38), line='# originální kód lze nalézt na adrese:\n')
TokenInfo(type=61 (NL), string='\n', start=(1, 38), end=(1, 39), line='# originální kód lze nalézt na adrese:\n')
TokenInfo(type=60 (COMMENT), string='# http://www.rosettacode.org/wiki/Sieve_of_Eratosthenes#Using_array_lookup', start=(2, 0), end=(2, 74), line='# http://www.rosettacode.org/wiki/Sieve_of_Eratosthenes#Using_array_lookup\n')
TokenInfo(type=61 (NL), string='\n', start=(2, 74), end=(2, 75), line='# http://www.rosettacode.org/wiki/Sieve_of_Eratosthenes#Using_array_lookup\n')
...
...
...
</pre>

<p>Zajímavá je tokenizace této programové smyčky:</p>

<pre>
    for n in range(int(limit**0.5 + 1.5)):  # stop at ``sqrt(limit)``
</pre>

<p>Do podoby (pro větší přehlednost jsou sloupce zarovnány):</p>

<pre>
TokenInfo(type=5  (INDENT),  string='            ', start=(8, 0),  end=(8, 12), line='            for i in range(n*n, limit+1, n):\n')
TokenInfo(type=1  (NAME),    string='for',          start=(8, 12), end=(8, 15), line='            for i in range(n*n, limit+1, n):\n')
TokenInfo(type=1  (NAME),    string='i',            start=(8, 16), end=(8, 17), line='            for i in range(n*n, limit+1, n):\n')
TokenInfo(type=1  (NAME),    string='in',           start=(8, 18), end=(8, 20), line='            for i in range(n*n, limit+1, n):\n')
TokenInfo(type=1  (NAME),    string='range',        start=(8, 21), end=(8, 26), line='            for i in range(n*n, limit+1, n):\n')
TokenInfo(type=54 (OP),      string='(',            start=(8, 26), end=(8, 27), line='            for i in range(n*n, limit+1, n):\n')
TokenInfo(type=1  (NAME),    string='n',            start=(8, 27), end=(8, 28), line='            for i in range(n*n, limit+1, n):\n')
TokenInfo(type=54 (OP),      string='*',            start=(8, 28), end=(8, 29), line='            for i in range(n*n, limit+1, n):\n')
TokenInfo(type=1  (NAME),    string='n',            start=(8, 29), end=(8, 30), line='            for i in range(n*n, limit+1, n):\n')
TokenInfo(type=54 (OP),      string=',',            start=(8, 30), end=(8, 31), line='            for i in range(n*n, limit+1, n):\n')
TokenInfo(type=1  (NAME),    string='limit',        start=(8, 32), end=(8, 37), line='            for i in range(n*n, limit+1, n):\n')
TokenInfo(type=54 (OP),      string='+',            start=(8, 37), end=(8, 38), line='            for i in range(n*n, limit+1, n):\n')
TokenInfo(type=2  (NUMBER),  string='1',            start=(8, 38), end=(8, 39), line='            for i in range(n*n, limit+1, n):\n')
TokenInfo(type=54 (OP),      string=',',            start=(8, 39), end=(8, 40), line='            for i in range(n*n, limit+1, n):\n')
TokenInfo(type=1  (NAME),    string='n',            start=(8, 41), end=(8, 42), line='            for i in range(n*n, limit+1, n):\n')
TokenInfo(type=54 (OP),      string=')',            start=(8, 42), end=(8, 43), line='            for i in range(n*n, limit+1, n):\n')
TokenInfo(type=54 (OP),      string=':',            start=(8, 43), end=(8, 44), line='            for i in range(n*n, limit+1, n):\n')
TokenInfo(type=4  (NEWLINE), string='\n',           start=(8, 44), end=(8, 45), line='            for i in range(n*n, limit+1, n):\n')
</pre>

<p>Zajímavé je, že v&nbsp;této chvíli vlastně ještě nemáme k&nbsp;dispozici
informaci o tom, že například <strong>for</strong> je klíčové slovo zatímco
<strong>limit</strong> je jméno proměnné. Toto rozlišení je provedeno až na
úrovni AST. Totéž platí pro závorky, které jsou uloženy v&nbsp;tokenu
<strong>OP</strong> a nikoli <strong>LBRACE</strong> a
<strong>RBRACE</strong>.</p>

<p><div class="rs-tip-major">Poznámka: celou tokenizovanou podobu lze najít na
adrese <a
href="https://github.com/tisnik/most-popular-python-libs/blob/master/ast/primes.toks">https://github.com/tisnik/most-popular-python-libs/blob/master/ast/primes.toks</a>.</div></p>



<p><a name="k09"></a></p>
<h2 id="k09">9. Tokenizace kódu s&nbsp;klíčovými slovy <strong>async</strong> a <strong>await</strong></h2>

<p>Ve <a href="#k06">výpisu tokenů</a> nalezneme kromě jiného i tyto
tokeny:</p>

<pre>
AWAIT                55
ASYNC                56
</pre>

<p>Vyzkoušejme si tedy, jestli jsou tyto tokeny skutečně použity, a to při
tokenizaci tohoto zdrojového kódu:</p>

<pre>
import trio
&nbsp;
&nbsp;
async def producer(send_channel):
    for i in range(1, 10):
        message = f"message {i}"
        print(f"Producer: {message}")
        await send_channel.send(message)
&nbsp;
&nbsp;
async def consumer(receive_channel):
    async for value in receive_channel:
        print(f"Consumer: received{value!r}")
        await trio.sleep(1)
&nbsp;
&nbsp;
async def main():
    async with trio.open_nursery() as nursery:
        send_channel, receive_channel = trio.open_memory_channel(0)
        nursery.start_soon(producer, send_channel)
        nursery.start_soon(consumer, receive_channel)
&nbsp;
&nbsp;
trio.run(main)
</pre>

<p>Pro tokenizaci použijeme tento jednoduchý skript:</p>

<pre>
import tokenize
&nbsp;
with tokenize.open("async.py") as fin:
    token_generator = tokenize.generate_tokens(fin.readline)
    for token in token_generator:
        print(token)
</pre>

<p>Výsledek je poměrně dlouhý, takže je zobrazen pouze výběr tokenů z&nbsp;celé
sekvence:</p>

<pre>
$ <strong>python3 tokenize_async.py | grep -E 'string=.async|string=.await' </strong>
&nbsp;
TokenInfo(type=1 (NAME), string='async', start=(4, 0), end=(4, 5), line='async def producer(send_channel):\n')
TokenInfo(type=1 (NAME), string='await', start=(8, 8), end=(8, 13), line='        await send_channel.send(message)\n')
TokenInfo(type=1 (NAME), string='async', start=(11, 0), end=(11, 5), line='async def consumer(receive_channel):\n')
TokenInfo(type=1 (NAME), string='async', start=(12, 4), end=(12, 9), line='    async for value in receive_channel:\n')
TokenInfo(type=1 (NAME), string='await', start=(14, 8), end=(14, 13), line='        await trio.sleep(1)\n')
TokenInfo(type=1 (NAME), string='async', start=(17, 0), end=(17, 5), line='async def main():\n')
TokenInfo(type=1 (NAME), string='async', start=(18, 4), end=(18, 9), line='    async with trio.open_nursery() as nursery:\n')
</pre>

<p>Vidíme, že klíčová slova prozatím nejsou při tokenizaci rozeznávána a jsou
vrácena ve formě tokenu typu <strong>NAME</strong>, tedy obecné jméno či
identifikátor. Rozeznání je opět provedeno až na úrovni AST.</p>

<p><div class="rs-tip-major">Poznámka: celou tokenizovanou podobu lze najít na
adrese <a
href="https://github.com/tisnik/most-popular-python-libs/blob/master/ast/async.toks">https://github.com/tisnik/most-popular-python-libs/blob/master/ast/async.toks</a>.</div></p>



<p><a name="k10"></a></p>
<h2 id="k10">10. Rozlišení operátorů v&nbsp;sekvenci tokenů</h2>

<p>Ještě jednou se vraťme k&nbsp;tokenizaci jednoduchého výrazu:</p>

<pre>
(1+2)*3
</pre>

<p>V&nbsp;případě, že si necháme vypsat pouze sekvenci tokenů skriptem:</p>

<pre>
import tokenize
&nbsp;
with tokenize.open("expression2.py") as fin:
    token_generator = tokenize.generate_tokens(fin.readline)
    for token in token_generator:
        print(token)
</pre>

<p>Získáme výsledek, v&nbsp;němž mají všechny &bdquo;operátorové&ldquo; tokeny,
a to včetně závorek, typ 54 (<strong>OP</strong>):</p>

<pre>
TokenInfo(type=54 (OP),        string='(',  start=(1, 0), end=(1, 1), line='(1+2)*3\n')
TokenInfo(type=2  (NUMBER),    string='1',  start=(1, 1), end=(1, 2), line='(1+2)*3\n')
TokenInfo(type=54 (OP),        string='+',  start=(1, 2), end=(1, 3), line='(1+2)*3\n')
TokenInfo(type=2  (NUMBER),    string='2',  start=(1, 3), end=(1, 4), line='(1+2)*3\n')
TokenInfo(type=54 (OP),        string=')',  start=(1, 4), end=(1, 5), line='(1+2)*3\n')
TokenInfo(type=54 (OP),        string='*',  start=(1, 5), end=(1, 6), line='(1+2)*3\n')
TokenInfo(type=2  (NUMBER),    string='3',  start=(1, 6), end=(1, 7), line='(1+2)*3\n')
TokenInfo(type=4  (NEWLINE),   string='\n', start=(1, 7), end=(1, 8), line='(1+2)*3\n')
TokenInfo(type=0  (ENDMARKER), string='',   start=(2, 0), end=(2, 0), line='')
</pre>

<p>Pro rozlišení konkrétních operátorů a závorek je nutné přečíst atribut
<strong>exact_type</strong>, což vede k&nbsp;následující úpravě skriptu:</p>

<pre>
import tokenize
&nbsp;
with tokenize.open("expression2.py") as fin:
    token_generator = tokenize.generate_tokens(fin.readline)
    for token in token_generator:
        print("{:2}  {:2}  {}".format(token.type, token.exact_type, token))
</pre>

<p>Tento skript vypíše obecný typ tokenu, &bdquo;přesný&ldquo; typ tokenu a
vlastní n-tici s&nbsp;obsahem tokenu. Povšimněte si rozdílů v&nbsp;hodnotách
v&nbsp;prvním a druhém sloupci, ovšem pouze u obecného typu
<strong>OP</strong>:</p>

<pre>
54   7  TokenInfo(type=54 (OP),        string='(',  start=(1, 0), end=(1, 1), line='(1+2)*3\n')
 2   2  TokenInfo(type=2  (NUMBER),    string='1',  start=(1, 1), end=(1, 2), line='(1+2)*3\n')
54  14  TokenInfo(type=54 (OP),        string='+',  start=(1, 2), end=(1, 3), line='(1+2)*3\n')
 2   2  TokenInfo(type=2  (NUMBER),    string='2',  start=(1, 3), end=(1, 4), line='(1+2)*3\n')
54   8  TokenInfo(type=54 (OP),        string=')',  start=(1, 4), end=(1, 5), line='(1+2)*3\n')
54  16  TokenInfo(type=54 (OP),        string='*',  start=(1, 5), end=(1, 6), line='(1+2)*3\n')
 2   2  TokenInfo(type=2  (NUMBER),    string='3',  start=(1, 6), end=(1, 7), line='(1+2)*3\n')
 4   4  TokenInfo(type=4  (NEWLINE),   string='\n', start=(1, 7), end=(1, 8), line='(1+2)*3\n')
 0   0  TokenInfo(type=0  (ENDMARKER), string='',   start=(2, 0), end=(2, 0), line='')
</pre>



<p><a name="k11"></a></p>
<h2 id="k11">11. Zobrazení AST pro část zdrojového kódu Pythonu</h2>

<p>Víme již, že dalším krokem ve zpracování zdrojového kódu je převod sekvence
tokenů na abstraktní syntaktický strom (AST). Interně se jedná o relativně
komplikovaný proces, protože <a
href="https://docs.python.org/3/reference/grammar.html">gramatika Pythonu</a>
je již poměrně složitá. Nicméně z&nbsp;pohledu programátora postačuje znalost
modulu <strong>ast</strong>, který dokáže převést část zdrojového kódu
reprezentovaného řetězcem nebo dokonce i celý soubor se zdrojovým kódem na AST.
AST lze následně zobrazit funkcí <strong>ast.dump</strong>. Podívejme se na
převod jednoduchého výrazu do AST:</p>

<pre>
import ast
&nbsp;
tree = <strong>ast.parse("1+2*3")</strong>
&nbsp;
print(<strong>ast.dump(tree)</strong>)
</pre>

<p>Získaný výsledek je ovšem poměrně nečitelný:</p>

<pre>
Module(body=[Expr(value=BinOp(left=Num(n=1), op=Add(), right=BinOp(left=Num(n=2), op=Mult(), right=Num(n=3))))])
</pre>

<p>Do Pythonu 3.10 byla přidána možnost zobrazení stromu v&nbsp;čitelnější
podobě &ndash; poduzly jsou v&nbsp;tomto případě jednoduše odsazeny o počet
mezer specifikovaných v&nbsp;nepovinném parametru <strong>indent</strong>:</p>

<pre>
import ast
&nbsp;
tree = <strong>ast.parse("1+2*3")</strong>
&nbsp;
print(<strong>ast.dump(tree, indent=4)</strong>)
</pre>

<p>Výsledek je již mnohem čitelnější:</p>

<pre>
Module(
    body=[
        Expr(
            value=BinOp(
                left=Constant(value=1),
                op=Add(),
                right=BinOp(
                    left=Constant(value=2),
                    op=Mult(),
                    right=Constant(value=3))))],
    type_ignores=[])
</pre>



<p><a name="k12"></a></p>
<h2 id="k12">12. Zobrazení AST pro soubor se zdrojovým kódem</h2>

<p>Do AST lze převést i celý soubor se zdrojovým kódem, a to následujícím
způsobem:</p>

<pre>
import ast
&nbsp;
with open("async.py") as fin:
    code = fin.read()
    tree = <strong>ast.parse(code)</strong>
&nbsp;
print(<strong>ast.dump(tree)</strong>)
</pre>

<p>Výsledek je ovšem opět poměrně nečitelný, protože není zdůrazněna samotná
struktura AST, ale pouze hodnoty jeho uzlů:</p>

<p></p>

<p><div class="rs-tip-major">Poznámka: celou podobu AST lze najít na
adrese <a
href="https://github.com/tisnik/most-popular-python-libs/blob/master/ast/async.ast">https://github.com/tisnik/most-popular-python-libs/blob/master/ast/async.ast</a>.</div></p>

<p>Úprava pro Python 3.10 vypadá následovně:</p>

<pre>
import ast
&nbsp;  
with open("async.py") as fin:
    code = fin.read()
    tree = <strong>ast.parse(code)</strong>
&nbsp;
print(<strong>ast.dump(tree, indent=4)</strong>)
</pre>

<p>S&nbsp;mnohem čitelnějším výsledkem:</p>

<pre>
</pre>

<p><div class="rs-tip-major">Poznámka: celou podobu AST lze najít na
adrese <a
href="https://github.com/tisnik/most-popular-python-libs/blob/master/ast/async2.ast">https://github.com/tisnik/most-popular-python-libs/blob/master/ast/async2.ast</a>.</div></p>



<p><a name="k13"></a></p>
<h2 id="k13">13. Obsah navazujícího článku</h2>

<p>Dnes jsme se věnovali pouze naprostému základu manipulace
s&nbsp;abstraktními syntaktickými stromy. Příště si ukážeme některé zajímavější
a v&nbsp;některých případech i praktičtější operace. Zejména se budeme věnovat
tomu, jak lze zajistit průchod všemi uzly AST, a to takovým způsobem, aby bylo
například možné zpracovat jednotlivé výrazy, filtrovat uzly podle různých
kritérií atd. (některé ze skriptů již <a href="#k14">můžete najít
v&nbsp;repositáři</a>. Ovšem vzhledem k&nbsp;tomu, že AST mohou být (a jsou)
velmi rozsáhlé, je vhodné mít nástroj pro jejich vizualizaci nejenom na
terminálu, ale i v&nbsp;grafické podobě. I tímto nástrojem, resp.&nbsp;přesněji
řečeno nástroji, se budeme zabývat příště.</p>



<p><a name="k14"></a></p>
<h2 id="k14">14. Repositář s&nbsp;demonstračními příklady</h2>

<p>Zdrojové kódy všech prozatím popsaných demonstračních příkladů určených pro
programovací jazyk Python 3 byly uloženy do Git repositáře dostupného na adrese
<a
href="https://github.com/tisnik/most-popular-python-libs">https://github.com/tisnik/most-popular-python-libs</a>.
V&nbsp;případě, že nebudete chtít klonovat celý repositář (ten je ovšem stále
velmi malý, dnes má velikost zhruba několik desítek kilobajtů), můžete namísto
toho použít odkazy na jednotlivé příklady, které naleznete v&nbsp;následující
tabulce:</p>

<table>
<tr><th> #</th><th>Demonstrační příklad</th><th>Stručný popis příkladu</th><th>Cesta</th></tr>
<tr><td> 1</td><td>expression.py</td><td>zdrojový kód, který se bude tokenizovat a parsovat, obsahující jednoduchý výraz</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/ast/expression.py">https://github.com/tisnik/most-popular-python-libs/blob/master/ast/expression.py</a></td></tr>
<tr><td> 2</td><td>err_expression.py</td><td>zdrojový kód, který se bude tokenizovat a parsovat, obsahující chybný výraz</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/ast/err_expression.py">https://github.com/tisnik/most-popular-python-libs/blob/master/ast/err_expression.py</a></td></tr>
<tr><td> 3</td><td>async.py</td><td>zdrojový kód, který se bude tokenizovat a parsovat, obsahující async a await</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/ast/async.py">https://github.com/tisnik/most-popular-python-libs/blob/master/ast/async.py</a></td></tr>
<tr><td> 4</td><td>primes.py</td><td>zdrojový kód, který se bude tokenizovat a parsovat, obsahující výpočet celočísel</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/ast/primes.py">https://github.com/tisnik/most-popular-python-libs/blob/master/ast/primes.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td> 5</td><td>print_tokens.py</td><td>výpis všech typů a hodnot tokenů pro aktuální verzi Pythonu</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/ast/print_tokens.py">https://github.com/tisnik/most-popular-python-libs/blob/master/ast/print_tokens.py</a></td></tr>
<tr><td> 6</td><td>tokenize_expression_1.py</td><td>tokenizace výrazu</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/ast/tokenize_expression_1.py">https://github.com/tisnik/most-popular-python-libs/blob/master/ast/tokenize_expression_1.py</a></td></tr>
<tr><td> 7</td><td>tokenize_expression_2.py</td><td>tokenizace výrazu, alternativní způsob otevření zdrojového souboru</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/ast/tokenize_expression_2.py">https://github.com/tisnik/most-popular-python-libs/blob/master/ast/tokenize_expression_2.py</a></td></tr>
<tr><td> 8</td><td>tokenize_expression_3.py</td><td>tokenizace výrazu s&nbsp;více operátory</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/ast/tokenize_expression_3.py">https://github.com/tisnik/most-popular-python-libs/blob/master/ast/tokenize_expression_3.py</a></td></tr>
<tr><td> 9</td><td>tokenize_expression_4.py</td><td>tokenizace výrazu s&nbsp;více operátory, výpis přesného typu tokenu</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/ast/tokenize_expression_4.py">https://github.com/tisnik/most-popular-python-libs/blob/master/ast/tokenize_expression_4.py</a></td></tr>
<tr><td>10</td><td>tokenize_async.py</td><td>tokenizace zdrojového kódu <strong>async.py</strong></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/ast/tokenize_async.py">https://github.com/tisnik/most-popular-python-libs/blob/master/ast/tokenize_async.py</a></td></tr>
<tr><td>11</td><td>tokenize_primes.py</td><td>tokenizace zdrojového kódu <strong>primes.py</strong></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/ast/tokenize_primes.py">https://github.com/tisnik/most-popular-python-libs/blob/master/ast/tokenize_primes.py</a></td></tr>
<tr><td>12</td><td>tokenize_primes_2.py</td><td>tokenizace zdrojového kódu <strong>primes.py</strong>, výpis přesného typu tokenu</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/ast/tokenize_primes_2.py">https://github.com/tisnik/most-popular-python-libs/blob/master/ast/tokenize_primes_2.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>13</td><td>parse_expression.py</td><td>parsing zdrojového kódu s&nbsp;výrazem</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/ast/parse_expression.py">https://github.com/tisnik/most-popular-python-libs/blob/master/ast/parse_expression.py</a></td></tr>
<tr><td>14</td><td>parse_expression_3_10.py</td><td>parsing zdrojového kódu s&nbsp;výrazem, vylepšený výpis v&nbsp;Pythonu 3.10</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/ast/parse_expression_3_10.py">https://github.com/tisnik/most-popular-python-libs/blob/master/ast/parse_expression_3_10.py</a></td></tr>
<tr><td>15</td><td>parse_async.py</td><td>parsing zdrojového kodu <strong>async.py</strong></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/ast/parse_async.py">https://github.com/tisnik/most-popular-python-libs/blob/master/ast/parse_async.py</a></td></tr>
<tr><td>16</td><td>parse_async_3_10.py</td><td>parsing zdrojového kodu <strong>async.py</strong>, vylepšený výpis v&nbsp;Pythonu 3.10</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/ast/parse_async_3_10.py">https://github.com/tisnik/most-popular-python-libs/blob/master/ast/parse_async_3_10.py</a></td></tr>
<tr><td>17</td><td>parse_primes.py</td><td>parsing zdrojového kodu <strong>primes.py</strong></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/ast/parse_primes.py">https://github.com/tisnik/most-popular-python-libs/blob/master/ast/parse_primes.py</a></td></tr>
<tr><td>18</td><td>parse_primes_3_10.py</td><td>parsing zdrojového kodu <strong>primes.py</strong>, vylepšený výpis v&nbsp;Pythonu 3.10</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/ast/parse_primes_3_10.py">https://github.com/tisnik/most-popular-python-libs/blob/master/ast/parse_primes_3_10.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>19</td><td>traverse_expression_1.py</td><td>průchod AST, nejjednodušší varianta</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/ast/traverse_expression_1.py">https://github.com/tisnik/most-popular-python-libs/blob/master/ast/traverse_expression_1.py</a></td></tr>
<tr><td>20</td><td>traverse_expression_2.py</td><td>průchod AST, vzor <i>Visitor</i></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/ast/traverse_expression_2.py">https://github.com/tisnik/most-popular-python-libs/blob/master/ast/traverse_expression_2.py</a></td></tr>
<tr><td>21</td><td>traverse_expression_3.py</td><td>průchod AST, vzor <i>Visitor</i>, odsazení kódu</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/ast/traverse_expression_3.py">https://github.com/tisnik/most-popular-python-libs/blob/master/ast/traverse_expression_3.py</a></td></tr>
</table>



<p><a name="k15"></a></p>
<h2 id="k15">15. Odkazy na Internetu</h2>

<ol>

<li>Abstract syntax tree<br />
<a href="https://en.wikipedia.org/wiki/Abstract_syntax_tree">https://en.wikipedia.org/wiki/Abstract_syntax_tree</a>
</li>

<li>Lexical analysis<br />
<a href="https://en.wikipedia.org/wiki/Lexical_analysis">https://en.wikipedia.org/wiki/Lexical_analysis</a>
</li>

<li>Parser<br />
<a href="https://en.wikipedia.org/wiki/Parsing#Parser">https://en.wikipedia.org/wiki/Parsing#Parser</a>
</li>

<li>Parse tree<br />
<a href="https://en.wikipedia.org/wiki/Parse_tree">https://en.wikipedia.org/wiki/Parse_tree</a>
</li>

<li>Derivační strom<br />
<a href="https://cs.wikipedia.org/wiki/Deriva%C4%8Dn%C3%AD_strom">https://cs.wikipedia.org/wiki/Deriva%C4%8Dn%C3%AD_strom</a>
</li>

<li>Python doc: ast — Abstract Syntax Trees<br />
<a href="https://docs.python.org/3/library/ast.html">https://docs.python.org/3/library/ast.html</a>
</li>

<li>Python doc: tokenize — Tokenizer for Python source<br />
<a href="https://docs.python.org/3/library/tokenize.html">https://docs.python.org/3/library/tokenize.html</a>
</li>

<li>5 Amazing Python AST Module Examples<br />
<a href="https://www.pythonpool.com/python-ast/">https://www.pythonpool.com/python-ast/</a>
</li>

<li>Intro to Python ast Module<br />
<a href="https://medium.com/@wshanshan/intro-to-python-ast-module-bbd22cd505f7">https://medium.com/@wshanshan/intro-to-python-ast-module-bbd22cd505f7</a>
</li>

<li>Golang AST Package<br />
<a href="https://golangdocs.com/golang-ast-package">https://golangdocs.com/golang-ast-package</a>
</li>

<li>AP8, IN8 Regulární jazyky<br />
<a href="http://statnice.dqd.cz/home:inf:ap8">http://statnice.dqd.cz/home:inf:ap8</a>
</li>

<li>AP9, IN9 Konečné automaty<br />
<a href="http://statnice.dqd.cz/home:inf:ap9">http://statnice.dqd.cz/home:inf:ap9</a>
</li>

<li>AP10, IN10 Bezkontextové jazyky<br />
<a href="http://statnice.dqd.cz/home:inf:ap10">http://statnice.dqd.cz/home:inf:ap10</a>
</li>

<li>AP11, IN11 Zásobníkové automaty, Syntaktická analýza<br />
<a href="http://statnice.dqd.cz/home:inf:ap11">http://statnice.dqd.cz/home:inf:ap11</a>
</li>

<li>Introduction to YACC<br />
<a href="https://www.geeksforgeeks.org/introduction-to-yacc/">https://www.geeksforgeeks.org/introduction-to-yacc/</a>
</li>

<li>Introduction of Lexical Analysis<br />
<a href="https://www.geeksforgeeks.org/introduction-of-lexical-analysis/?ref=lbp">https://www.geeksforgeeks.org/introduction-of-lexical-analysis/?ref=lbp</a>
</li>

<li>Využití knihovny Pygments (nejenom) pro obarvení zdrojových kódů<br />
<a href="https://www.root.cz/clanky/vyuziti-knihovny-pygments-nejenom-pro-obarveni-zdrojovych-kodu/">https://www.root.cz/clanky/vyuziti-knihovny-pygments-nejenom-pro-obarveni-zdrojovych-kodu/</a>
</li>

<li>Pygments - Python syntax highlighter<br />
<a href="http://pygments.org/">http://pygments.org/</a>
</li>

<li>Pygments (dokumentace)<br />
<a href="http://pygments.org/docs/">http://pygments.org/docs/</a>
</li>

<li>Write your own filter<br />
<a href="http://pygments.org/docs/filterdevelopment/">http://pygments.org/docs/filterdevelopment/</a>
</li>

<li>Write your own lexer<br />
<a href="http://pygments.org/docs/lexerdevelopment/">http://pygments.org/docs/lexerdevelopment/</a>
</li>

<li>Write your own formatter<br />
<a href="http://pygments.org/docs/formatterdevelopment/">http://pygments.org/docs/formatterdevelopment/</a>
</li>

<li>Jazyky podporované knihovnou Pygments<br />
<a href="http://pygments.org/languages/">http://pygments.org/languages/</a>
</li>

<li>Pygments FAQ<br />
<a href="http://pygments.org/faq/">http://pygments.org/faq/</a>
</li>

<li>Compiler Construction/Lexical analysis<br />
<a href="https://en.wikibooks.org/wiki/Compiler_Construction/Lexical_analysis">https://en.wikibooks.org/wiki/Compiler_Construction/Lexical_analysis</a>
</li>

<li>Compiler Design - Lexical Analysis<br />
<a href="https://www.tutorialspoint.com/compiler_design/compiler_design_lexical_analysis.htm">https://www.tutorialspoint.com/compiler_design/compiler_design_lexical_analysis.htm</a>
</li>

<li>Lexical Analysis - An Intro<br />
<a href="https://www.scribd.com/document/383765692/Lexical-Analysis">https://www.scribd.com/document/383765692/Lexical-Analysis</a>
</li>

<li>Python AST Visualizer<br />
<a href="https://github.com/pombredanne/python-ast-visualizer">https://github.com/pombredanne/python-ast-visualizer</a>
</li>

<li>What is an Abstract Syntax Tree<br />
<a href="https://blog.bitsrc.io/what-is-an-abstract-syntax-tree-7502b71bde27">https://blog.bitsrc.io/what-is-an-abstract-syntax-tree-7502b71bde27</a>
</li>

<li>Why is AST so important<br />
<a href="https://medium.com/@obernardovieira/why-is-ast-so-important-b1e7d6c29260">https://medium.com/@obernardovieira/why-is-ast-so-important-b1e7d6c29260</a>
</li>

<li>Emily Morehouse-Valcarcel - The AST and Me - PyCon 2018<br />
<a href="https://www.youtube.com/watch?v=XhWvz4dK4ng">https://www.youtube.com/watch?v=XhWvz4dK4ng</a>
</li>

<li>Python AST Parsing and Custom Linting<br />
<a href="https://www.youtube.com/watch?v=OjPT15y2EpE">https://www.youtube.com/watch?v=OjPT15y2EpE</a>
</li>

<li>Chase Stevens - Exploring the Python AST Ecosystem<br />
<a href="https://www.youtube.com/watch?v=Yq3wTWkoaYY">https://www.youtube.com/watch?v=Yq3wTWkoaYY</a>
</li>

<li>Full Grammar specification<br />
<a href="https://docs.python.org/3/reference/grammar.html">https://docs.python.org/3/reference/grammar.html</a>
</li>

</ol>



<p></p><p></p>
<p><small>Autor: <a href="http://www.fit.vutbr.cz/~tisnovpa">Pavel Tišnovský</a> &nbsp; 2022</small></p>
</body>
</html>

