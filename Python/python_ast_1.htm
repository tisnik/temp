<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
        "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html>
<head>
<title>Lexikální a syntaktická analýza zdrojových kódů programovacího jazyka Python</title>
<meta name="Author" content="Pavel Tisnovsky" />
<meta name="Generator" content="vim" />
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<style type="text/css">
         body {color:#000000; background:#ffffff;}
         h1  {font-family: arial, helvetica, sans-serif; color:#ffffff; background-color:#c00000; text-align:center; padding-left:1em}
         h2  {font-family: arial, helvetica, sans-serif; color:#ffffff; background-color:#0000c0; padding-left:1em; text-align:left}
         h3  {font-family: arial, helvetica, sans-serif; color:#000000; background-color:#c0c0c0; padding-left:1em; text-align:left}
         h4  {font-family: arial, helvetica, sans-serif; color:#000000; background-color:#e0e0e0; padding-left:1em; text-align:left}
         a   {font-family: arial, helvetica, sans-serif;}
         li  {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify; width:450px;}
         ol  {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify; width:450px;}
         ul  {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify; width:450px;}
         p   {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify;}
         pre {background:#e0e0e0}
</style>
</head>

<body>

<h1>Lexikální a syntaktická analýza zdrojových kódů programovacího jazyka Python</h1>

<h3>Pavel Tišnovský</h3>

<p></p>

<h1>Úvodník</h1>

<p></p>



<h2>Obsah</h2>

<p><a href="#k01">*** 1. Lexikální a syntaktická analýza zdrojových kódů programovacího jazyka Python</a></p>
<p><a href="#k02">*** 2. Od volně zapsaného zdrojového kódu k&nbsp;AST</a></p>
<p><a href="#k03">*** 3. Lexémy a tokeny (tokenizace)</a></p>
<p><a href="#k04">*** 4. Příklad tokenizace jednoduchého kódu napsaného v&nbsp;Pythonu</a></p>
<p><a href="#k05">*** 5. Tokenizace standardním modulem <strong>tokenize</strong></a></p>
<p><a href="#k06">*** 6. Tokenizace jednoduchého výrazu</a></p>
<p><a href="#k07">*** 7. Tokenizace kódu s&nbsp;několika funkcemi</a></p>
<p><a href="#k08">*** 8. Tokenizace kódu s&nbsp;klíčovými slovy <strong>async</strong> a <strong>await</strong></a></p>
<p><a href="#k09">*** 9. </a></p>
<p><a href="#k10">*** 10. </a></p>
<p><a href="#k11">*** 11. </a></p>
<p><a href="#k12">*** 12. Průchod AST</a></p>
<p><a href="#k13">*** 13. Rekurzivní průchod AST s&nbsp;pevně daným pořadím procházení uzly</a></p>
<p><a href="#k14">*** 14. Grafická vizualizace AST</a></p>
<p><a href="#k15">*** 15. Vizualizace AST jednoduchého výrazu</a></p>
<p><a href="#k16">*** 16. Vizualizace AST programu pro výpočet prvočísel</a></p>
<p><a href="#k17">*** 17. Vizualizace AST programu pro provádění asynchronních operací</a></p>
<p><a href="#k18">*** 18. Obsah navazujícího článku</a></p>
<p><a href="#k19">*** 19. Repositář s&nbsp;demonstračními příklady</a></p>
<p><a href="#k20">*** 20. Odkazy na Internetu</a></p>



<p><a name="k01"></a></p>
<h2 id="k01">1. Lexikální a syntaktická analýza zdrojových kódů programovacího jazyka Python</h2>

<p>Na trojici článků, v&nbsp;nichž jsme se zabývali problematikou lexikální a syntaktické analýzy zdrojových kódů jazyka Go [<a href="">1</a>] [<a href="">2</a>] [<a href="">3</a>] dnes do jisté míry navážeme, ovšem přesuneme se <a href="">z&nbsp;jazyka Go</a> k&nbsp;pravděpodobně nejpopulárnějšímu jazyku současnosti &ndash; <a href="">k&nbsp;Pythonu</a>. Ukážeme si, jak lze provést takzvanou <i>tokenizaci</i> a následně <i>parsing</i> zdrojových kódů, jehož výsledkem je AST neboli abstraktní syntaktický strom (<i>Abstract Syntax Tree</i>). A samozřejmě se zmíníme i o tom, jakým způsobem lze abstraktním syntaktickým stromem procházet či jinak manipulovat.</p>



<p><a name="k02"></a></p>
<h2 id="k02">2. Od volně zapsaného zdrojového kódu k&nbsp;AST</h2>

<p>Při zpracování zdrojových kódů se postupně provádí jednotlivé dílčí kroky.  Díky rozdělení celého zpracování do několika konfigurovatelných kroků je zajištěna velká flexibilita a možnost případného relativně snadného rozšiřování o další podporované jazyky, výstupní formáty, speciální filtry atd. (nehledě na to, že každá činnost je založena na odlišné teorii). Celý průběh zpracování vypadá následovně:</p>

<ol>

<li>Na začátku zpracování se nachází takzvaný <i>lexer</i>, který postupně načítá jednotlivé znaky ze vstupního řetězce (resp.&nbsp;souboru) a vytváří z&nbsp;nich lexikální <i>tokeny</i>. Teoreticky se pro každý programovací jazyk používá odlišný lexer a samozřejmě je možné v&nbsp;případě potřeby si napsat lexer vlastní.</li>

<li>Výstup z&nbsp;lexeru může procházet libovolným počtem <i>filtrů</i> sloužících pro odstranění nebo (častěji) modifikaci jednotlivých tokenů; ať již jejich typů či přímo textu, který tvoří hodnotu tokenu. Díky existenci filtrů je například možné nechat si zvýraznit vybrané bílé znaky, slova se speciálním významem v&nbsp;komentářích (TODO:, FIX:) apod.</li>

<li>Sekvence <i>tokenů</i> tvoří základ pro syntaktickou analýzu. Nástroj, který syntaktickou analýzu provádí, se většinou nazývá <i>parser</i> a proto se taktéž někdy setkáme s&nbsp;pojmem <i>parsing</i> (ten je ovšem chybně používán i v&nbsp;těch případech, kdy se provádí &bdquo;pouze&ldquo; lexikální analýza).  Výsledkem parseru je vhodně zvolená datová struktura, typicky abstraktní syntaktický strom (AST); někdy též strom derivační.</li>

</ol>

<p><div class="rs-tip-major">Poznámka: díky tomu, že se prakticky veškeré zpracování zdrojových textů odehrává na úrovni tokenů, není nutné, aby byl celý zpracovávaný zdrojový kód (nebo jeho tokenizovaná podoba) uložen v&nbsp;operační paměti. Je tedy možné zpracovávat i velmi rozsáhlé dokumenty, a to bez větších nároků na operační paměť &ndash; i to je ostatně použito v&nbsp;balíčku <strong>go/scanner</strong>.</div></p>



<p><a name="k03"></a></p>
<h2 id="k03">3. Lexémy a tokeny (tokenizace)</h2>

<p>První část zpracování zdrojových textů je nejzajímavější a implementačně i nejsložitější. <i>Lexer</i> totiž musí v&nbsp;sekvenci znaků tvořících zdrojový text najít takzvané <i>lexémy</i>, tj.&nbsp;skupiny (sousedních) znaků odpovídajících nějakému vzorku (použít lze gramatiku, regulární výraz či ad-hoc testy). Z&nbsp;lexémů se posléze tvoří již zmíněné lexikální <i>tokeny</i>, což je &ndash; poněkud zjednodušeně řečeno &ndash; dvojice obsahující typ tokenu (někdy se namísto &bdquo;typ&ldquo; používá označení &bdquo;jméno&ldquo;) a řetězec ze vstupního zdrojového souboru. Převodu zdrojového textu na sekvenci tokenů se někdy říká <i>tokenizace</i>. Účelem tokenizace může být:</p>

<ul>

<li>Transformace zdrojového textu do podoby, která může být dále zpracovávána dalším modulem překladače (syntaktická analýza). V&nbsp;takovém případě se však některé tokeny mohou zahazovat; příkladem mohou být komentáře, tokeny představující bílé znaky apod. Spojením lexeru a modulu pro syntaktickou analýzu vznikne <i>parser</i> (jeho typickým výsledkem je AST).</li>

<li>Transformace zdrojového kódu pro účely zvýraznění syntaxe v&nbsp;editorech či prohlížečích. V&nbsp;tomto případě se žádné tokeny nezahazují, což je případ knihovny Pygments.</li>

</ul>

<p><div class="rs-tip-major">Poznámka: výše zmíněná <i>tokenizace</i> se používala například již v&nbsp;interpretrech programovacího jazyka BASIC na mnoha osmibitových domácích počítačích. Ovšem v&nbsp;tomto případě měly tokeny poněkud odlišnou strukturu, protože všechny příkazy a funkce byly většinou reprezentovány jednoznačným osmibitovým celým číslem, které tak současně představovalo jak typ tokenu, tak i jeho hodnotu. Důvod byl jednoduchý &ndash; v&nbsp;operační paměti byl uložen tokenizovaný kód a nikoli kód zapsaný uživatelem. Tento kód byl již mnohem jednodušeji zpracovatelný interpretrem, než původní zdrojový kód (odpadlo neustálé volání <i>lexeru</i>). Navíc se každý programový řádek ihned po svém zápisu automaticky normalizoval (odstranily se bílé znaky, zkratky příkazů se expandovaly atd.). Ostatně množina příkazů a funkcí byla předem známá a nebyla rozšiřitelná (až na uživatelské funkce dostupné jen v&nbsp;některých BASICech). Příkladem tokenizace tohoto typu mohou být tokeny použité v&nbsp;interpretru programovacího jazyka Atari BASIC, které skutečně přímo odpovídají příkazům, funkcím a operátorům tohoto jazyka. Pro zajímavost:</div></p>

<table>
<tr><th>Příkaz</th><th>Kód tokenu</th><th>Příkaz</th><th>Kód tokenu</th><th>Příkaz</th><th>Kód tokenu</th><th>Příkaz</th><th>Kód tokenu</th></tr>
<tr><td>REM</td><td>00</td><td>NEXT</td><td>09</td><td>CLR</td><td>18</td><td>NOTE</td><td>27</td></tr>
<tr><td>DATA</td><td>01</td><td>GOTO</td><td>10</td><td>DEG</td><td>19</td><td>POINT</td><td>28</td></tr>
<tr><td>INPUT</td><td>02</td><td>GO TO</td><td>11</td><td>DIM</td><td>20</td><td>XIO</td><td>29</td></tr>
<tr><td>COLOR</td><td>03</td><td>GOSUB</td><td>12</td><td>END</td><td>21</td><td>ON</td><td>30</td></tr>
<tr><td>LIST</td><td>04</td><td>TRAP</td><td>13</td><td>NEW</td><td>22</td><td>POKE</td><td>31</td></tr>
<tr><td>ENTER</td><td>05</td><td>BYE</td><td>14</td><td>OPEN</td><td>23</td><td>PRINT</td><td>32</td></tr>
<tr><td>LET</td><td>06</td><td>CONT</td><td>15</td><td>LOAD</td><td>24</td><td>RAD</td><td>33</td></tr>
<tr><td>IF</td><td>07</td><td>COM</td><td>16</td><td>SAVE</td><td>25</td><td>READ</td><td>34</td></tr>
<tr><td>FOR</td><td>08</td><td>CLOSE</td><td>17</td><td>STATUS</td><td>26</td><td>RESTORE</td><td>35</td></tr>
</table>

<p>...atd...</p>



<p><a name="k04"></a></p>
<h2 id="k04">4. Příklad tokenizace jednoduchého kódu napsaného v&nbsp;Pythonu</h2>

<p>Podívejme se nyní na příklad tokenizace velmi jednoduchého a krátkého kódu, který je naprogramován v&nbsp;Pythonu:</p>

<pre>
for i in range(1, 11):
    print("Hello world!")
</pre>

<p>Výsledkem tokenizace je následující sekvence tokenů, tj.&nbsp;dvojic typ+hodnota (řetězec):</p>

<table>
<tr><th>Typ tokenu</th><th>Řetězec</th></tr>
<tr><td>Token.Keyword</td><td>'for'</td></tr>
<tr><td>Token.Text</td><td>' '</td></tr>
<tr><td>Token.Name</td><td>'i'</td></tr>
<tr><td>Token.Text</td><td>' '</td></tr>
<tr><td>Token.Operator.Word</td><td>'in'</td></tr>
<tr><td>Token.Text</td><td>' '</td></tr>
<tr><td>Token.Name.Builtin</td><td>'range'</td></tr>
<tr><td>Token.Punctuation</td><td>'('</td></tr>
<tr><td>Token.Literal.Number.Integer</td><td>'1'</td></tr>
<tr><td>Token.Punctuation</td><td>','</td></tr>
<tr><td>Token.Text</td><td>' '</td></tr>
<tr><td>Token.Literal.Number.Integer</td><td>'11'</td></tr>
<tr><td>Token.Punctuation</td><td>')'</td></tr>
<tr><td>Token.Punctuation</td><td>':'</td></tr>
<tr><td>Token.Text</td><td>'\n'</td></tr>
<tr><td>Token.Text</td><td>'    '</td></tr>
<tr><td>Token.Keyword</td><td>'print'</td></tr>
<tr><td>Token.Punctuation</td><td>'('</td></tr>
<tr><td>Token.Literal.String.Double</td><td>'"'</td></tr>
<tr><td>Token.Literal.String.Double</td><td>'Hello world!'</td></tr>
<tr><td>Token.Literal.String.Double</td><td>'"'</td></tr>
<tr><td>Token.Punctuation</td><td>')'</td></tr>
<tr><td>Token.Text</td><td>'\n'</td></tr>
</table>

<p><div class="rs-tip-major">Poznámka: lexer se v&nbsp;žádném případě nesnaží o nalezení syntaktických (a už vůbec ne sémantických) chyb v&nbsp;programu! Pouze se snaží rozeznat známé vzorky. To například znamená, že tokenizace proběhne i pro tento zdrojový kód, který je sémanticky naprosto chybný:</div></p>

<pre>
range(1, "FDA") for while with i
except for for i else
    print("Hello world!")
</pre>

<p>Výsledek tokenizace:</p>

<table>
<tr><th>Typ tokenu</th><th>Řetězec</th></tr>
<tr><td>Token.Name.Builtin</td><td>'range'</td></tr>
<tr><td>Token.Punctuation</td><td>'('</td></tr>
<tr><td>Token.Literal.Number.Integer</td><td>'1'</td></tr>
<tr><td>Token.Punctuation</td><td>','</td></tr>
<tr><td>Token.Text</td><td>' '</td></tr>
<tr><td>Token.Literal.String.Double</td><td>'"'</td></tr>
<tr><td>Token.Literal.String.Double</td><td>'FDA'</td></tr>
<tr><td>Token.Literal.String.Double</td><td>'"'</td></tr>
<tr><td>Token.Punctuation</td><td>')'</td></tr>
<tr><td>Token.Text</td><td>' '</td></tr>
<tr><td>Token.Keyword</td><td>'for'</td></tr>
<tr><td>Token.Text</td><td>' '</td></tr>
<tr><td>Token.Keyword</td><td>'while'</td></tr>
<tr><td>Token.Text</td><td>' '</td></tr>
<tr><td>Token.Keyword</td><td>'with'</td></tr>
<tr><td>Token.Text</td><td>' '</td></tr>
<tr><td>Token.Name</td><td>'i'</td></tr>
<tr><td>Token.Text</td><td>'\n'</td></tr>
<tr><td>Token.Keyword</td><td>'except'</td></tr>
<tr><td>Token.Text</td><td>' '</td></tr>
<tr><td>Token.Keyword</td><td>'for'</td></tr>
<tr><td>Token.Text</td><td>' '</td></tr>
<tr><td>Token.Keyword</td><td>'for'</td></tr>
<tr><td>Token.Text</td><td>' '</td></tr>
<tr><td>Token.Name</td><td>'i'</td></tr>
<tr><td>Token.Text</td><td>' '</td></tr>
<tr><td>Token.Keyword</td><td>'else'</td></tr>
<tr><td>Token.Text</td><td>'\n'</td></tr>
<tr><td>Token.Text</td><td>'    '</td></tr>
<tr><td>Token.Keyword</td><td>'print'</td></tr>
<tr><td>Token.Punctuation</td><td>'('</td></tr>
<tr><td>Token.Literal.String.Double</td><td>'"'</td></tr>
<tr><td>Token.Literal.String.Double</td><td>'Hello world!'</td></tr>
<tr><td>Token.Literal.String.Double</td><td>'"'</td></tr>
<tr><td>Token.Punctuation</td><td>')'</td></tr>
<tr><td>Token.Text</td><td>'\n'</td></tr>
</table>



<p><a name="k05"></a></p>
<h2 id="k05">5. Tokenizace standardním modulem <strong>tokenize</strong></h2>

<p></p>

<pre>
import token

for name, value in vars(token).items():
    if not name.startswith('_'):
        if isinstance(value, int):
            print("{:20s} {}".format(name, value))
</pre>

<pre>
ENDMARKER            0
NAME                 1
NUMBER               2
STRING               3
NEWLINE              4
INDENT               5
DEDENT               6
LPAR                 7
RPAR                 8
LSQB                 9
RSQB                 10
COLON                11
COMMA                12
SEMI                 13
PLUS                 14
MINUS                15
STAR                 16
SLASH                17
VBAR                 18
AMPER                19
LESS                 20
GREATER              21
EQUAL                22
DOT                  23
PERCENT              24
LBRACE               25
RBRACE               26
EQEQUAL              27
NOTEQUAL             28
LESSEQUAL            29
GREATEREQUAL         30
TILDE                31
CIRCUMFLEX           32
LEFTSHIFT            33
RIGHTSHIFT           34
DOUBLESTAR           35
PLUSEQUAL            36
MINEQUAL             37
STAREQUAL            38
SLASHEQUAL           39
PERCENTEQUAL         40
AMPEREQUAL           41
VBAREQUAL            42
CIRCUMFLEXEQUAL      43
LEFTSHIFTEQUAL       44
RIGHTSHIFTEQUAL      45
DOUBLESTAREQUAL      46
DOUBLESLASH          47
DOUBLESLASHEQUAL     48
AT                   49
ATEQUAL              50
RARROW               51
ELLIPSIS             52
COLONEQUAL           53
OP                   54
AWAIT                55
ASYNC                56
TYPE_IGNORE          57
TYPE_COMMENT         58
ERRORTOKEN           59
COMMENT              60
NL                   61
ENCODING             62
N_TOKENS             63
NT_OFFSET            256
</pre>



<p><a name="k06"></a></p>
<h2 id="k06">6. Tokenizace jednoduchého výrazu</h2>

<p></p>

<pre>
1+2*3
</pre>

<pre>
</pre>

<pre>
0,0-0,0:            ENCODING       'utf-8'        
1,0-1,1:            NUMBER         '1'            
1,1-1,2:            OP             '+'            
1,2-1,3:            NUMBER         '2'            
1,3-1,4:            OP             '*'            
1,4-1,5:            NUMBER         '3'            
1,5-1,6:            NEWLINE        '\n'           
2,0-2,0:            ENDMARKER      ''             
</pre>

<pre>
import tokenize

with open("expression.py", "rb") as fin:
    token_generator = tokenize.tokenize(fin.readline)
    for token in token_generator:
        print(token)
</pre>

<pre>
import tokenize

with tokenize.open("expression.py") as fin:
    token_generator = tokenize.generate_tokens(fin.readline)
    for token in token_generator:
        print(token)
</pre>

<pre>
TokenInfo(type=62 (ENCODING), string='utf-8', start=(0, 0), end=(0, 0), line='')
TokenInfo(type=2 (NUMBER), string='1', start=(1, 0), end=(1, 1), line='1+2*3\n')
TokenInfo(type=54 (OP), string='+', start=(1, 1), end=(1, 2), line='1+2*3\n')
TokenInfo(type=2 (NUMBER), string='2', start=(1, 2), end=(1, 3), line='1+2*3\n')
TokenInfo(type=54 (OP), string='*', start=(1, 3), end=(1, 4), line='1+2*3\n')
TokenInfo(type=2 (NUMBER), string='3', start=(1, 4), end=(1, 5), line='1+2*3\n')
TokenInfo(type=4 (NEWLINE), string='\n', start=(1, 5), end=(1, 6), line='1+2*3\n')
TokenInfo(type=0 (ENDMARKER), string='', start=(2, 0), end=(2, 0), line='')
</pre>



<p><a name="k07"></a></p>
<h2 id="k07">7. Tokenizace kódu s&nbsp;několika funkcemi</h2>

<p></p>

<pre>
# originální kód lze nalézt na adrese:
# http://www.rosettacode.org/wiki/Sieve_of_Eratosthenes#Using_array_lookup
def primes2(limit):
    """Výpočet seznamu prvočísel až do zadaného limitu."""
    is_prime = [False] * 2 + [True] * (limit - 1)
    for n in range(int(limit**0.5 + 1.5)):  # stop at ``sqrt(limit)``
        if is_prime[n]:
            for i in range(n*n, limit+1, n):
                is_prime[i] = False
    return [i for i, prime in enumerate(is_prime) if prime]


print(primes2(100))
</pre>

<pre>
import tokenize

with tokenize.open("primes.py") as fin:
    token_generator = tokenize.generate_tokens(fin.readline)
    for token in token_generator:
        print(token)
</pre>

<pre>
</pre>



<p><a name="k08"></a></p>
<h2 id="k08">8. Tokenizace kódu s&nbsp;klíčovými slovy <strong>async</strong> a <strong>await</strong></h2>

<p></p>

<pre>
import trio


async def producer(send_channel):
    for i in range(1, 10):
        message = f"message {i}"
        print(f"Producer: {message}")
        await send_channel.send(message)


async def consumer(receive_channel):
    async for value in receive_channel:
        print(f"Consumer: received{value!r}")
        await trio.sleep(1)


async def main():
    async with trio.open_nursery() as nursery:
        send_channel, receive_channel = trio.open_memory_channel(0)
        nursery.start_soon(producer, send_channel)
        nursery.start_soon(consumer, receive_channel)


trio.run(main)
</pre>

<pre>
import tokenize

with tokenize.open("async.py") as fin:
    token_generator = tokenize.generate_tokens(fin.readline)
    for token in token_generator:
        print(token)
</pre>

<pre>
TokenInfo(type=1 (NAME), string='import', start=(1, 0), end=(1, 6), line='import trio\n')
TokenInfo(type=1 (NAME), string='trio', start=(1, 7), end=(1, 11), line='import trio\n')
TokenInfo(type=4 (NEWLINE), string='\n', start=(1, 11), end=(1, 12), line='import trio\n')
TokenInfo(type=61 (NL), string='\n', start=(2, 0), end=(2, 1), line='\n')
TokenInfo(type=61 (NL), string='\n', start=(3, 0), end=(3, 1), line='\n')
TokenInfo(type=1 (NAME), string='async', start=(4, 0), end=(4, 5), line='async def producer(send_channel):\n')
TokenInfo(type=1 (NAME), string='def', start=(4, 6), end=(4, 9), line='async def producer(send_channel):\n')
...
...
...
TokenInfo(type=5 (INDENT), string='    ', start=(5, 0), end=(5, 4), line='    for i in range(1, 10):\n')
TokenInfo(type=1 (NAME), string='for', start=(5, 4), end=(5, 7), line='    for i in range(1, 10):\n')
TokenInfo(type=1 (NAME), string='i', start=(5, 8), end=(5, 9), line='    for i in range(1, 10):\n')
TokenInfo(type=1 (NAME), string='in', start=(5, 10), end=(5, 12), line='    for i in range(1, 10):\n')
TokenInfo(type=1 (NAME), string='range', start=(5, 13), end=(5, 18), line='    for i in range(1, 10):\n')
...
...
...
TokenInfo(type=1 (NAME), string='run', start=(24, 5), end=(24, 8), line='trio.run(main)\n')
TokenInfo(type=54 (OP), string='(', start=(24, 8), end=(24, 9), line='trio.run(main)\n')
TokenInfo(type=1 (NAME), string='main', start=(24, 9), end=(24, 13), line='trio.run(main)\n')
TokenInfo(type=54 (OP), string=')', start=(24, 13), end=(24, 14), line='trio.run(main)\n')
TokenInfo(type=4 (NEWLINE), string='\n', start=(24, 14), end=(24, 15), line='trio.run(main)\n')
TokenInfo(type=0 (ENDMARKER), string='', start=(25, 0), end=(25, 0), line='')
</pre>



<p><a name="k09"></a></p>
<h2 id="k09">9. </h2>

<p></p>



<p><a name="k10"></a></p>
<h2 id="k10">10. </h2>

<p></p>

<pre>
import ast

tree = ast.parse("1+2*3")

print(ast.dump(tree))
</pre>

<pre>
Module(body=[Expr(value=BinOp(left=Num(n=1), op=Add(), right=BinOp(left=Num(n=2), op=Mult(), right=Num(n=3))))])
</pre>

<pre>
import ast

tree = ast.parse("1+2*3")

print(ast.dump(tree, indent=4))
</pre>

<pre>
print(ast.dump(tree, indent=4))
Module(
    body=[
        Expr(
            value=BinOp(
                left=Constant(value=1),
                op=Add(),
                right=BinOp(
                    left=Constant(value=2),
                    op=Mult(),
                    right=Constant(value=3))))],
    type_ignores=[])
</pre>



<p><a name="k11"></a></p>
<h2 id="k11">11. </h2>

<p></p>

<pre>
import ast

with open("async.py") as fin:
    code = fin.read()
    tree = ast.parse(code)

print(ast.dump(tree))
</pre>

<pre>
import ast
  
with open("async.py") as fin:
    code = fin.read()
    tree = ast.parse(code)

print(ast.dump(tree, indent=4))
</pre>



<p><a name="k12"></a></p>
<h2 id="k12">12. Průchod AST</h2>

<p></p>



<p><a name="k13"></a></p>
<h2 id="k13">13. Rekurzivní průchod AST s&nbsp;pevně daným pořadím procházení uzly</h2>

<p></p>



<p><a name="k14"></a></p>
<h2 id="k14">14. Grafická vizualizace AST</h2>

<p></p>



<p><a name="k15"></a></p>
<h2 id="k15">15. Vizualizace AST jednoduchého výrazu</h2>

<p></p>



<p><a name="k16"></a></p>
<h2 id="k16">16. Vizualizace AST programu pro výpočet prvočísel</h2>

<p></p>



<p><a name="k17"></a></p>
<h2 id="k17">17. Vizualizace AST programu pro provádění asynchronních operací</h2>

<p></p>



<p><a name="k18"></a></p>
<h2 id="k18">18. Obsah navazujícího článku</h2>

<p></p>



<p><a name="k19"></a></p>
<h2 id="k19">19. Repositář s&nbsp;demonstračními příklady</h2>

<p>Zdrojové kódy všech prozatím popsaných demonstračních příkladů určených pro
programovací jazyk Python 3 byly uloženy do Git repositáře dostupného na adrese
<a
href="https://github.com/tisnik/most-popular-python-libs">https://github.com/tisnik/most-popular-python-libs</a>.
V&nbsp;případě, že nebudete chtít klonovat celý repositář (ten je ovšem stále
velmi malý, dnes má velikost zhruba několik desítek kilobajtů), můžete namísto
toho použít odkazy na jednotlivé příklady, které naleznete v&nbsp;následující
tabulce:</p>

<table>
<tr><th> #</th><th>Demonstrační příklad</th><th>Stručný popis příkladu</th><th>Cesta</th></tr>
<tr><td> 1</td><td></td><td></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/ast/">https://github.com/tisnik/most-popular-python-libs/blob/master/ast/</a></td></tr>
<tr><td> 2</td><td></td><td></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/ast/">https://github.com/tisnik/most-popular-python-libs/blob/master/ast/</a></td></tr>
<tr><td> 3</td><td></td><td></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/ast/">https://github.com/tisnik/most-popular-python-libs/blob/master/ast/</a></td></tr>
<tr><td> 4</td><td></td><td></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/ast/">https://github.com/tisnik/most-popular-python-libs/blob/master/ast/</a></td></tr>
<tr><td> 5</td><td></td><td></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/ast/">https://github.com/tisnik/most-popular-python-libs/blob/master/ast/</a></td></tr>
<tr><td> 6</td><td></td><td></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/ast/">https://github.com/tisnik/most-popular-python-libs/blob/master/ast/</a></td></tr>
<tr><td> 7</td><td></td><td></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/ast/">https://github.com/tisnik/most-popular-python-libs/blob/master/ast/</a></td></tr>
<tr><td> 8</td><td></td><td></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/ast/">https://github.com/tisnik/most-popular-python-libs/blob/master/ast/</a></td></tr>
<tr><td> 9</td><td></td><td></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/ast/">https://github.com/tisnik/most-popular-python-libs/blob/master/ast/</a></td></tr>
<tr><td>10</td><td></td><td></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/ast/">https://github.com/tisnik/most-popular-python-libs/blob/master/ast/</a></td></tr>
<tr><td>11</td><td></td><td></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/ast/">https://github.com/tisnik/most-popular-python-libs/blob/master/ast/</a></td></tr>
<tr><td>12</td><td></td><td></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/ast/">https://github.com/tisnik/most-popular-python-libs/blob/master/ast/</a></td></tr>
<tr><td>13</td><td></td><td></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/ast/">https://github.com/tisnik/most-popular-python-libs/blob/master/ast/</a></td></tr>
<tr><td>14</td><td></td><td></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/ast/">https://github.com/tisnik/most-popular-python-libs/blob/master/ast/</a></td></tr>
</table>



<p><a name="k20"></a></p>
<h2 id="k20">20. Odkazy na Internetu</h2>

<ol>

<li>Abstract syntax tree<br />
<a href="https://en.wikipedia.org/wiki/Abstract_syntax_tree">https://en.wikipedia.org/wiki/Abstract_syntax_tree</a>
</li>

<li>Lexical analysis<br />
<a href="https://en.wikipedia.org/wiki/Lexical_analysis">https://en.wikipedia.org/wiki/Lexical_analysis</a>
</li>

<li>Parser<br />
<a href="https://en.wikipedia.org/wiki/Parsing#Parser">https://en.wikipedia.org/wiki/Parsing#Parser</a>
</li>

<li>Python doc: ast — Abstract Syntax Trees<br />
<a href="https://docs.python.org/3/library/ast.html">https://docs.python.org/3/library/ast.html</a>
</li>

<li>Python doc: tokenize — Tokenizer for Python source<br />
<a href="https://docs.python.org/3/library/tokenize.html">https://docs.python.org/3/library/tokenize.html</a>
</li>

<li>5 Amazing Python AST Module Examples<br />
<a href="https://www.pythonpool.com/python-ast/">https://www.pythonpool.com/python-ast/</a>
</li>

<li>Intro to Python ast Module<br />
<a href="https://medium.com/@wshanshan/intro-to-python-ast-module-bbd22cd505f7">https://medium.com/@wshanshan/intro-to-python-ast-module-bbd22cd505f7</a>
</li>

<li>Golang AST Package<br />
<a href="https://golangdocs.com/golang-ast-package">https://golangdocs.com/golang-ast-package</a>
</li>

<li>AP8, IN8 Regulární jazyky<br />
<a href="http://statnice.dqd.cz/home:inf:ap8">http://statnice.dqd.cz/home:inf:ap8</a>
</li>

<li>AP9, IN9 Konečné automaty<br />
<a href="http://statnice.dqd.cz/home:inf:ap9">http://statnice.dqd.cz/home:inf:ap9</a>
</li>

<li>AP10, IN10 Bezkontextové jazyky<br />
<a href="http://statnice.dqd.cz/home:inf:ap10">http://statnice.dqd.cz/home:inf:ap10</a>
</li>

<li>AP11, IN11 Zásobníkové automaty, Syntaktická analýza<br />
<a href="http://statnice.dqd.cz/home:inf:ap11">http://statnice.dqd.cz/home:inf:ap11</a>
</li>

<li>Introduction to YACC<br />
<a href="https://www.geeksforgeeks.org/introduction-to-yacc/">https://www.geeksforgeeks.org/introduction-to-yacc/</a>
</li>

<li>Introduction of Lexical Analysis<br />
<a href="https://www.geeksforgeeks.org/introduction-of-lexical-analysis/?ref=lbp">https://www.geeksforgeeks.org/introduction-of-lexical-analysis/?ref=lbp</a>
</li>

<li>Využití knihovny Pygments (nejenom) pro obarvení zdrojových kódů<br />
<a href="https://www.root.cz/clanky/vyuziti-knihovny-pygments-nejenom-pro-obarveni-zdrojovych-kodu/">https://www.root.cz/clanky/vyuziti-knihovny-pygments-nejenom-pro-obarveni-zdrojovych-kodu/</a>
</li>

<li>Pygments - Python syntax highlighter<br />
<a href="http://pygments.org/">http://pygments.org/</a>
</li>

<li>Pygments (dokumentace)<br />
<a href="http://pygments.org/docs/">http://pygments.org/docs/</a>
</li>

<li>Write your own filter<br />
<a href="http://pygments.org/docs/filterdevelopment/">http://pygments.org/docs/filterdevelopment/</a>
</li>

<li>Write your own lexer<br />
<a href="http://pygments.org/docs/lexerdevelopment/">http://pygments.org/docs/lexerdevelopment/</a>
</li>

<li>Write your own formatter<br />
<a href="http://pygments.org/docs/formatterdevelopment/">http://pygments.org/docs/formatterdevelopment/</a>
</li>

<li>Jazyky podporované knihovnou Pygments<br />
<a href="http://pygments.org/languages/">http://pygments.org/languages/</a>
</li>

<li>Pygments FAQ<br />
<a href="http://pygments.org/faq/">http://pygments.org/faq/</a>
</li>

<li>Compiler Construction/Lexical analysis<br />
<a href="https://en.wikibooks.org/wiki/Compiler_Construction/Lexical_analysis">https://en.wikibooks.org/wiki/Compiler_Construction/Lexical_analysis</a>
</li>

<li>Compiler Design - Lexical Analysis<br />
<a href="https://www.tutorialspoint.com/compiler_design/compiler_design_lexical_analysis.htm">https://www.tutorialspoint.com/compiler_design/compiler_design_lexical_analysis.htm</a>
</li>

<li>Lexical Analysis - An Intro<br />
<a href="https://www.scribd.com/document/383765692/Lexical-Analysis">https://www.scribd.com/document/383765692/Lexical-Analysis</a>
</li>

<li>Python AST Visualizer<br />
<a href="https://github.com/pombredanne/python-ast-visualizer">https://github.com/pombredanne/python-ast-visualizer</a>
</li>

</ol>



<p></p><p></p>
<p><small>Autor: <a href="http://www.fit.vutbr.cz/~tisnovpa">Pavel Tišnovský</a> &nbsp; 2022</small></p>
</body>
</html>

