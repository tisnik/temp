<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
        "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html>
<head>
<title>Faust: platforma pro proudovÃ© zpracovÃ¡nÃ­ dat v Pythonu</title>
<meta name="Author" content="Pavel Tisnovsky" />
<meta name="Generator" content="vim" />
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<style type="text/css">
         body {color:#000000; background:#ffffff;}
         h1  {font-family: arial, helvetica, sans-serif; color:#ffffff; background-color:#c00000; text-align:center; padding-left:1em}
         h2  {font-family: arial, helvetica, sans-serif; color:#ffffff; background-color:#0000c0; padding-left:1em; text-align:left}
         h3  {font-family: arial, helvetica, sans-serif; color:#000000; background-color:#c0c0c0; padding-left:1em; text-align:left}
         h4  {font-family: arial, helvetica, sans-serif; color:#000000; background-color:#e0e0e0; padding-left:1em; text-align:left}
         a   {font-family: arial, helvetica, sans-serif;}
         li  {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify; width:450px;}
         ol  {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify; width:450px;}
         ul  {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify; width:450px;}
         p   {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify;}
         pre {background:#e0e0e0}
</style>
</head>

<body>

<h1>Faust: platforma pro proudovÃ© zpracovÃ¡nÃ­ dat v Pythonu</h1>

<h3>Pavel TiÅ¡novskÃ½</h3>

<p></p>

<h1>ÃšvodnÃ­k</h1>

<p>Dnes se seznÃ¡mÃ­me se zajÃ­mavÄ› koncipovanou knihovnou nazvanou Faust. JednÃ¡ se o knihovnu zajiÅ¡Å¥ujÃ­cÃ­ proudovÃ© zpracovÃ¡nÃ­ dat, kterÃ¡ je postavena nad Apache Kafkou. NejednÃ¡ se vÅ¡ak o pouhou realizaci producentÅ¯ a konzumentÅ¯, protoÅ¾e je moÅ¾nÃ© pouÅ¾Ã­vat lokÃ¡lnÃ­ tabulky, asynchronnÃ­ zpracovÃ¡nÃ­ atd.</p>



<h2>Obsah</h2>

<p><a href="#k01">*** 1. Faust: platforma pro proudovÃ© zpracovÃ¡nÃ­ dat v&nbsp;Pythonu</a></p>
<p><a href="#k02">*** 2. ProblematickÃ¡ ÄÃ¡st &ndash; instalace knihovny Faust</a></p>
<p><a href="#k03">3. Instalace, konfigurace a spuÅ¡tÄ›nÃ­ Apache Kafky</a></p>
<p><a href="#k04">4. OtestovÃ¡nÃ­ Äinnosti Apache Kafky</a></p>
<p><a href="#k05">5. KlasickÃ½ producent posÃ­lajÃ­cÃ­ zprÃ¡vy do Kafky</a></p>
<p><a href="#k06">6. KlasickÃ½ konzument, kterÃ½ Äte zprÃ¡vy ze specifikovanÃ©ho tÃ©matu</a></p>
<p><a href="#k07">*** 7. SpuÅ¡tÄ›nÃ­ producenta a konzumenta</a></p>
<p><a href="#k08">8. Konzument realizovanÃ½ formou workera postavenÃ½ na knihovnÄ› Faust</a></p>
<p><a href="#k09">9. SpuÅ¡tÄ›nÃ­ workera</a></p>
<p><a href="#k10">10. Producent posÃ­lajÃ­cÃ­ zprÃ¡vy do vÄ›tÅ¡Ã­ho mnoÅ¾stvÃ­ tÃ©mat</a></p>
<p><a href="#k11">11. Konzumace zprÃ¡v workerem z&nbsp;vÄ›tÅ¡Ã­ho mnoÅ¾stvÃ­ tÃ©mat</a></p>
<p><a href="#k12">*** 12. Produkce zprÃ¡v ve formÃ¡tu JSON</a></p>
<p><a href="#k13">*** 13. Worker pÅ™ijÃ­majÃ­cÃ­ zprÃ¡vy ve formÃ¡tu JSON</a></p>
<p><a href="#k14">*** 14. VyuÅ¾itÃ­ modelÅ¯</a></p>
<p><a href="#k15">*** 15. Producent zaloÅ¾enÃ½ na knihovnÄ› Faust vyuÅ¾Ã­vajÃ­cÃ­ model</a></p>
<p><a href="#k16">*** 16. Kombinace producenta a konzumenta &ndash; reÃ¡lnÃ¡ sÃ­la knihovny Faust</a></p>
<p><a href="#k17">*** 17. SpuÅ¡tÄ›nÃ­ kombinace workerÅ¯ s&nbsp;producenty a konzumenty</a></p>
<p><a href="#k18">*** 18. Obsah navazujÃ­cÃ­ho ÄlÃ¡nku</a></p>
<p><a href="#k19">19. RepositÃ¡Å™ s&nbsp;demonstraÄnÃ­mi pÅ™Ã­klady</a></p>
<p><a href="#k20">20. Odkazy na Internetu</a></p>



<p><a name="k01"></a></p>
<h2 id="k01">1. Faust: platforma pro proudovÃ© zpracovÃ¡nÃ­ dat v&nbsp;Pythonu</h2>



<p><a name="k02"></a></p>
<h2 id="k02">2. ProblematickÃ¡ ÄÃ¡st &ndash; instalace knihovny Faust</h2>

<p>Teoreticky je moÅ¾nÃ© knihovnu Faust nainstalovat naprosto stejnÃ½m zpÅ¯sobem,
jako jakoukoli jinou knihovnu dostupnou pÅ™es Pypi:</p>

<pre>
$ <strong>pip install --user faust</strong>
</pre>

<p>To vÅ¡ak bude bez problÃ©mÅ¯ fungovat pouze pro starÅ¡Ã­ verze Pythonu
(CPythonu).  V&nbsp;pÅ™Ã­padÄ› novÄ›jÅ¡Ã­ch verzÃ­ Pythonu, v&nbsp;nichÅ¾ doÅ¡lo ke
zmÄ›nÄ› balÃ­ÄkÅ¯ pro asynchronnÃ­ bÄ›h aplikacÃ­, nebude moÅ¾nÃ© takto nainstalovanÃ½
Faust pouÅ¾Ã­t. MusÃ­me se tedy uchÃ½lit k&nbsp;jinÃ© formÄ› spuÅ¡tÄ›nÃ­ (coÅ¾ ani
zdaleka nenÃ­ ideÃ¡lnÃ­ situace!).</p>



<p><a name="k03"></a></p>
<h2 id="k03">3. Instalace, konfigurace a spuÅ¡tÄ›nÃ­ Apache Kafky</h2>

<p>DneÅ¡nÃ­ ÄlÃ¡nek je zamÄ›Å™en na ukÃ¡zku nÄ›kterÃ½ch moÅ¾nostÃ­ nabÃ­zenÃ½ch knihovnou
Faust, kterÃ¡ internÄ› vyuÅ¾Ã­vÃ¡ znÃ¡mÃ½ projekt Apache Kafka. To tedy znamenÃ¡, Å¾e
kromÄ› instalace knihovny Faust budeme navÃ­c potÅ™ebovat spustit (lokÃ¡lnÄ›,
vzdÃ¡lenÄ›, Äi v&nbsp;kontejneru) i Apache Kafku. KonkrÃ©tnÄ› nÃ¡m bude postaÄovat
spuÅ¡tÄ›nÃ­ jedinÃ©ho Kafka clusteru, pÅ™iÄemÅ¾ se kaÅ¾dÃ½ Kafka cluster sklÃ¡dÃ¡
z&nbsp;jednoho bÄ›Å¾Ã­cÃ­ho Zookeepera a z&nbsp;jednoho brokera (tedy ze dvou
procesÅ¯, kterÃ© mezi sebou komunikujÃ­ po nakonfigurovanÃ½ch portech).</p>

<p>V&nbsp;praktickÃ© ÄÃ¡sti budeme brokera Apache Kafky i Zookeepera spouÅ¡tÄ›t
lokÃ¡lnÄ› (popÅ™.&nbsp;z&nbsp;Dockeru Äi Podmana), takÅ¾e je nejdÅ™Ã­ve nutnÃ© Apache
Kafku nainstalovat. NenÃ­ to ve skuteÄnosti vÅ¯bec nic sloÅ¾itÃ©ho. V&nbsp;pÅ™Ã­padÄ›,
Å¾e je na poÄÃ­taÄi nainstalovÃ¡no JRE (bÄ›hovÃ© prostÅ™edÃ­ Javy), je instalace
Apache Kafky pro testovacÃ­ ÃºÄely triviÃ¡lnÃ­. V&nbsp;ÄlÃ¡nku si ukÃ¡Å¾eme instalaci
verze 2.13-3.6.1, ovÅ¡em mÅ¯Å¾ete si stÃ¡hnout i prakticky libovolnou novÄ›jÅ¡Ã­ Äi
nÄ›kterÃ© starÅ¡Ã­ verze (3.5.x nebo 3.6.x, ovÅ¡em dÃ¡le popsanÃ½ postup by mÄ›l bÃ½t
platnÃ½ i pro jeÅ¡tÄ› starÅ¡Ã­ verze, v&nbsp;podstatÄ› mÅ¯Å¾eme dojÃ­t aÅ¾ k&nbsp;verzi
2.4.0 a vÅ¡e by mÄ›lo bÃ½t funkÄnÃ­). Tarball s&nbsp;instalacÃ­ Apache Kafky je
moÅ¾nÃ© zÃ­skat z&nbsp;adresy <a
href="https://downloads.apache.org/kafka/3.6.1/kafka_2.13-3.6.1.tgz">https://downloads.apache.org/kafka/3.6.1/kafka_2.13-3.6.1.tgz</a>.</p>

<p>StaÅ¾enÃ­ a rozbalenÃ­ tarballu s&nbsp;Apache Kafkou zajistÃ­ nÃ¡sledujÃ­cÃ­
sekvence pÅ™Ã­kazÅ¯:</p>

<pre>
$ <strong>wget https://downloads.apache.org/kafka/3.6.1/kafka_2.13-3.6.1.tgz</strong>
$ <strong>tar xvfz kafka_2.13-3.6.1.tgz</strong>
$ <strong>cd kafka_2.13-3.6.1/</strong>
</pre>

<p>Po rozbalenÃ­ staÅ¾enÃ©ho tarballu zÃ­skÃ¡me adresÃ¡Å™, v&nbsp;nÄ›mÅ¾ se nachÃ¡zÃ­
vÅ¡echny potÅ™ebnÃ© Java archivy (JAR), konfiguraÄnÃ­ soubory (v&nbsp;podadresÃ¡Å™i
<strong>config</strong>) a nÄ›kolik pomocnÃ½ch skriptÅ¯ (v&nbsp;podadresÃ¡Å™i
<strong>bin</strong> a <strong>bin/windows</strong>). Pro spuÅ¡tÄ›nÃ­ Zookeepera a
brokerÅ¯ je zapotÅ™ebÃ­, jak jsme si jiÅ¾ Å™ekli v&nbsp;pÅ™edchozÃ­m odstavci, mÃ­t
nainstalovÃ¡nu JRE (Java Runtime Environment) a samozÅ™ejmÄ› tÃ©Å¾ nÄ›jakÃ½ shell
(BASH, cmd, ...).</p> 
 
<pre>
.
â”œâ”€â”€ bin
â”‚Â Â  â””â”€â”€ windows
â”œâ”€â”€ config
â”‚Â Â  â””â”€â”€ kraft
â”œâ”€â”€ libs
â”œâ”€â”€ licenses
â””â”€â”€ site-docs
&nbsp;
7 directories
</pre>

<p>Mezi dÅ¯leÅ¾itÃ© soubory, kterÃ© budeme pouÅ¾Ã­vat v&nbsp;rÃ¡mci dalÅ¡Ã­ch kapitol
pro ukÃ¡zku moÅ¾nostÃ­ knihovny Faust, patÅ™Ã­ pÅ™edevÅ¡Ã­m skripty pro spouÅ¡tÄ›nÃ­
jednotlivÃ½ch sluÅ¾eb. Tyto skripty jsou uloÅ¾eny v&nbsp;podadresÃ¡Å™i
<strong>bin</strong> (a pro Windows jeÅ¡tÄ› v&nbsp;dalÅ¡Ã­m podadresÃ¡Å™i
<strong>windows</strong>):</p>

<table>
<tr><th>Skript</th><th>StruÄnÃ½ popis</th></tr>
<tr><td>bin/kafka-server-start.sh</td><td>spuÅ¡tÄ›nÃ­ brokera</td></tr>
<tr><td>bin/zookeeper-server-start.sh</td><td>spuÅ¡tÄ›nÃ­ Zookeepera</td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>bin/kafka-configs.sh</td><td>konfigurace brokerÅ¯</td></tr>
<tr><td>bin/kafka-topics.sh</td><td>konfigurace tÃ©mat, zjiÅ¡tÄ›nÃ­ informace o tÃ©matech atd.</td></tr>
<tr><td>bin/kafka-consumer-groups.sh</td><td>konfigurace popÅ™.&nbsp;zjiÅ¡tÄ›nÃ­ informacÃ­ o skupinÃ¡ch konzumentÅ¯</td></tr>
</table>

<p><div class="rs-tip-major">PoznÃ¡mka: vÄ›tÅ¡ina vÃ½Å¡e uvedenÃ½ch skriptÅ¯ byla
upravena i pro spuÅ¡tÄ›nÃ­ ve Windows. Tyto varianty naleznete v&nbsp;podadresÃ¡Å™i
<strong>bin/windows</strong> a namÃ­sto koncovky <strong>.sh</strong> majÃ­
koncovku <strong>bat</strong> (<i>batch</i>).</div></p>

<p>NynÃ­ jiÅ¾ mÃ¡me vÅ¡echny potÅ™ebnÃ© nÃ¡stroje k&nbsp;dispozici, takÅ¾e Kafka
cluster spustÃ­me. NejdÅ™Ã­ve je vÅ¾dy nutnÃ© spustit Zookeepera a teprve potÃ©
brokera Äi brokery. Pro sledovÃ¡nÃ­ Äinnosti obou procesÅ¯ si mÅ¯Å¾ete Zookeepera i
brokera spustit v&nbsp;samostatnÃ©m terminÃ¡lu, vyuÅ¾Ã­t nÃ¡stroj
<strong>screen</strong> atd.</p>

<p>SpuÅ¡tÄ›nÃ­ Zookeepera:</p>

<pre>
$ <strong>cd ${kafka_dir}</strong>
$ <strong>bin/zookeeper-server-start.sh config/zookeeper.properties</strong>
</pre>

<p>SpuÅ¡tÄ›nÃ­ brokera:</p>

<pre>
$ <strong>cd ${kafka_dir}</strong>
$ <strong>bin/kafka-server-start.sh config/server.properties</strong>
</pre>

<p><div class="rs-tip-major">PoznÃ¡mka: povÅ¡imnÄ›te si, Å¾e pouÅ¾Ã­vÃ¡me konfiguraÄnÃ­
soubory dodÃ¡vanÃ© pÅ™Ã­mo s&nbsp;Apache Kafkou, tedy soubory
<strong>config/zookeeper.properties</strong> a
<strong>config/server.properties</strong>. V&nbsp;pÅ™Ã­padÄ› potÅ™eby si mÅ¯Å¾ete
tyto soubory zkopÃ­rovat, kopie upravit a ty dÃ¡le pouÅ¾Ã­t.</div></p>



<p><a name="k04"></a></p>
<h2 id="k04">4. OtestovÃ¡nÃ­ Äinnosti Apache Kafky</h2>

<p>Pro otestovÃ¡nÃ­, jestli je prÃ¡vÄ› zprovoznÄ›nÃ½ Kafka cluster skuteÄnÄ› funkÄnÃ­,
si vytvoÅ™Ã­me jednoduchÃ© producenty a konzumenty zprÃ¡v. PouÅ¾ijeme pÅ™itom
programovacÃ­ jazyk Python, protoÅ¾e i demonstraÄnÃ­ pÅ™Ã­klady vyuÅ¾Ã­vajÃ­cÃ­ knihovnu
<i>Faust</i> budou naprogramovÃ¡ny v&nbsp;Pythonu. SamozÅ™ejmÄ› by bylo moÅ¾nÃ©
pouÅ¾Ã­t konzolovÃ©ho producenta a konzumenta, kterÃ½ je souÄÃ¡stÃ­ instalace Apache
Kafky, ovÅ¡em moÅ¾nosti tÄ›chto nÃ¡strojÅ¯ jsou ve skuteÄnosti velmi omezenÃ© a
nebudou nÃ¡m postaÄovat v&nbsp;pÅ™Ã­padÄ›, Å¾e budeme chtÃ­t produkovat Äi naopak
konzumovat sloÅ¾itÄ›jÅ¡Ã­ datovÃ© struktury a objekty.</p>

<p>PÅ™ed vytvoÅ™enÃ­m producenta zprÃ¡v v&nbsp;Pythonu je nutnÃ© nainstalovat
knihovnu, kterÃ¡ zabezpeÄuje pÅ™ipojenÃ­ ke clusteru Apache Kafky. TakovÃ½ch
knihoven dnes existuje nÄ›kolik. NejjednoduÅ¡Å¡Ã­ z&nbsp;tÄ›chto knihoven (bez
dalÅ¡Ã­ch zÃ¡vislostÃ­ atd.) je knihovna nazvanÃ¡ <strong>kafka-python</strong>.
Vzhledem k&nbsp;tomu, Å¾e autoÅ™i tÃ©to knihovny nevydÃ¡vajÃ­ ÄastÃ© updaty, vznikl
jejÃ­ fork nazvanÃ½ jednoduÅ¡e <strong>kafka-python-ng</strong>; a prÃ¡vÄ› tento
fork dnes pouÅ¾ijeme. Tuto knihovnu mÅ¯Å¾eme nainstalovat pouze pro aktivnÃ­ho
uÅ¾ivatele (coÅ¾ je nejjednoduÅ¡Å¡Ã­ a nevyÅ¾aduje to rootovskÃ¡ prÃ¡va)
s&nbsp;vyuÅ¾itÃ­m pÅ™Ã­kazu <strong>pip</strong>, protoÅ¾e tato knihovna je
pochopitelnÄ› dostupnÃ¡ na <a
href="https://pypi.org/project/kafka/">PyPi</a>:</p>

<pre>
$ <strong>pip3 install --verbose kafka-python-ng</strong>
&nbsp;
Using pip 22.3.1 from /usr/lib/python3.11/site-packages/pip (python 3.11)
Defaulting to user installation because normal site-packages is not writeable
Collecting kafka-python-ng
  Downloading kafka_python_ng-2.2.2-py2.py3-none-any.whl (232 kB)
     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 232.4/232.4 kB 1.5 MB/s eta 0:00:00
Installing collected packages: kafka-python-ng
Successfully installed kafka-python-ng-2.2.2
</pre>

<p><div class="rs-tip-major">PoznÃ¡mka: povÅ¡imnÄ›te si, Å¾e tato knihovna skuteÄnÄ›
nemÃ¡ Å¾Ã¡dnÃ© tranzitivnÃ­ zÃ¡vislosti.</div></p>



<p><a name="k05"></a></p>
<h2 id="k05">5. KlasickÃ½ producent posÃ­lajÃ­cÃ­ zprÃ¡vy do Kafky</h2>

<p>Producent, tedy aplikace posÃ­lajÃ­cÃ­ zprÃ¡vy do Apache Kafky, je pÅ™edstavovÃ¡n
instancÃ­ tÅ™Ã­dy <strong>KafkaProducer</strong>.  Konstruktoru tÃ©to tÅ™Ã­dy se
pÅ™edÃ¡vÃ¡ pouze seznam brokerÅ¯ (v&nbsp;naÅ¡em pÅ™Ã­padÄ› jedinÃ½ broker, protoÅ¾e nÃ¡Å¡
Kafka cluster obsahuje jedinÃ©ho brokera) a popÅ™.&nbsp;i dalÅ¡Ã­ nepovinnÃ© Ãºdaje.
Mezi nÄ› patÅ™Ã­ i takzvanÃ½ <i>handler</i>, kterÃ½ je zavolÃ¡n pÅ™ed serializacÃ­
kaÅ¾dÃ© zprÃ¡vy.</p>

<p>Prakticky kaÅ¾dou zprÃ¡vu je ve skuteÄnosti nutnÃ© serializovat, protoÅ¾e zprÃ¡vy
v&nbsp;Apache Kafce tvoÅ™Ã­ dÃ¡le nestrukturovanÃ¡ sekvence bajtÅ¯ (jedinou datovou
strukturou, kterou nenÃ­ potÅ™eba serializovat, je typ <strong>bytes</strong>).
TudÃ­Å¾ napÅ™Ã­klad i Å™etÄ›zce (pÅ™esnÄ›ji Å™eÄeno PythonovskÃ© Å™etÄ›zce) se explicitnÄ›
pÅ™evÃ¡dÃ­ na hodnotu typu <strong>bytes</strong>, tj.&nbsp;na nemÄ›nitelnou
(<i>immutable</i>) sekvenci bajtÅ¯. PÅ™evod Å™etÄ›zcÅ¯ do tuto sekvenci je
realizovÃ¡n metodou <strong>encode</strong>, kterÃ© se pÅ™edÃ¡ vyÅ¾adovanÃ© kÃ³dovÃ¡nÃ­,
tj.&nbsp;mapovÃ¡nÃ­ mezi znaky na vstupu a jednÃ­m aÅ¾ vÃ­ce bajty ve vÃ½slednÃ©
sekvenci. V&nbsp;naÅ¡em skriptu pouÅ¾ijeme dnes standardnÃ­ kÃ³dovÃ¡nÃ­ UTF-8:</p>

<pre>
producer = <strong>KafkaProducer</strong>(
    bootstrap_servers=[server],
    value_serializer=lambda x: x.encode("utf-8")
)
</pre>

<p>PoslÃ¡nÃ­ zprÃ¡vy se provÃ¡dÃ­ metodou <strong>KafkaProducer.send</strong>.
ZprÃ¡va je, jak jiÅ¾ vÃ­me z&nbsp;pÅ™edchozÃ­ho textu, uloÅ¾ena do zvolenÃ©ho tÃ©matu
(<i>topic</i>) a sklÃ¡dÃ¡ se z&nbsp;tÄ›la, popÅ™.&nbsp;klÃ­Äe a tÄ›la:</p>

<pre>
producer.send(topic, value=data)
</pre>

<p>V&nbsp;naÅ¡em prvnÃ­m demonstraÄnÃ­m producentovi budou posÃ­lÃ¡ny Å™etÄ›zce, kterÃ©
se budou postupnÄ› mÄ›nit podle hodnoty ÄÃ­taÄe (<i>counter</i>). Mezi jednotlivÃ©
zprÃ¡vy jsou vklÃ¡dÃ¡ny ÄasovÃ© pauzy o dÃ©lce pÅ™ibliÅ¾nÄ› jednÃ© sekundy:</p>

<pre>
for i in range(1000):
    print(i)
    message = f"Greeting #{i}"
    producer.send(topic, value=message)
    sleep(1)
</pre>

<p><div class="rs-tip-major">PoznÃ¡mka: aby byla zprÃ¡va skuteÄnÄ› poslÃ¡na, je
vhodnÃ© zavolat metodu <strong>flush</strong>. V&nbsp;tomto konkrÃ©tnÃ­m skriptu
to nedÄ›lÃ¡me, protoÅ¾e po naplnÄ›nÃ­ bufferu nebo uplynutÃ­ urÄitÃ© doby je zprÃ¡va
poslÃ¡na i bez tohoto volÃ¡nÃ­. PozdÄ›ji ovÅ¡em budeme muset <strong>flush</strong>
explicitnÄ› volat.</div></p>

<p>Takto vypadÃ¡ ÃºplnÃ½ zdrojovÃ½ kÃ³d producenta zprÃ¡v, kterÃ½ naleznete na adrese
<a
href="https://github.com/tisnik/most-popular-python-libs/blob/master/faust/greeting_producer.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faust/greeting_producer.py</a>:</p>

<pre>
#!/usr/bin/env python3
&nbsp;
from kafka import KafkaProducer
from time import sleep
&nbsp;
&nbsp;
server = "localhost:9092"
topic = "greetings"
&nbsp;
print("Connecting to Kafka")
producer = KafkaProducer(
    bootstrap_servers=[server],
    value_serializer=lambda x: x.encode("utf-8")
)
print("Connected to Kafka")
&nbsp;
for i in range(1000):
    print(i)
    message = f"Greeting #{i}"
    producer.send(topic, value=message)
    sleep(1)
</pre>

<p><div class="rs-tip-major">PoznÃ¡mka: povÅ¡imnÄ›te si, Å¾e nÃ¡m pro poslÃ¡nÃ­ zprÃ¡vy
postaÄuje znÃ¡t adresu brokera a jmÃ©no tÃ©matu. Konzument (popsanÃ½
v&nbsp;navazujÃ­cÃ­ kapitole) je nepatrnÄ› sloÅ¾itÄ›jÅ¡Ã­, protoÅ¾e je nutnÃ©
specifikovat i jmÃ©no skupiny (<i>consumer group</i>).</div></p>



<p><a name="k06"></a></p>
<h2 id="k06">6. KlasickÃ½ konzument, kterÃ½ Äte zprÃ¡vy ze specifikovanÃ©ho tÃ©matu</h2>

<p>Implementace jednoduchÃ©ho konzumenta zprÃ¡v v&nbsp;programovacÃ­m jazyku
Python je pomÄ›rnÄ› snadnÃ¡ a vlastnÄ› se ani pÅ™Ã­liÅ¡ neliÅ¡Ã­ od implementace
konzumenta.  NejdÅ™Ã­ve je nutnÃ© vytvoÅ™it instanci tÅ™Ã­dy
<strong>KafkaConsumer</strong>, pÅ™iÄemÅ¾ se specifikuje tÃ©ma (<i>topic</i>),
skupina, seznam brokerÅ¯ a nepovinnÄ› tÃ©Å¾ informace o tom, jakÃ½m zpÅ¯sobem se mÃ¡
nastavit offset prvnÃ­ zpracovanÃ© zprÃ¡vy. VÄ›tÅ¡inou budeme potÅ™ebovat zpracovÃ¡vat
nejnovÄ›jÅ¡Ã­ (<i>earliest</i>) zprÃ¡vy, takÅ¾e nastavenÃ­ konzumenta mÅ¯Å¾e vypadat
napÅ™Ã­klad nÃ¡sledovnÄ›:</p>

<pre>
consumer = <strong>KafkaConsumer</strong>(
    topic, group_id=group_id,
    bootstrap_servers=[server],
    auto_offset_reset="earliest"
)
</pre>

<p>SamotnÃ¡ konzumace (tedy ÄtenÃ­ a zpracovÃ¡nÃ­) zprÃ¡v mÅ¯Å¾e probÃ­hat
v&nbsp;nekoneÄnÃ© programovÃ© smyÄce, pÅ™iÄemÅ¾ kaÅ¾dÃ¡ zprÃ¡va je reprezentovÃ¡na
objektem obsahujÃ­cÃ­m tÃ©ma, ÄÃ­slo oddÃ­lu, offset v&nbsp;rÃ¡mci oddÃ­lu, klÃ­Ä
zprÃ¡vy a samozÅ™ejmÄ› tÃ©Å¾ obsah zprÃ¡vy (klÃ­Ä zprÃ¡vy a tÄ›lo zprÃ¡vy nenÃ­ Å¾Ã¡dnÃ½m
zpÅ¯sobem dekÃ³dovÃ¡no resp.&nbsp;deserializovÃ¡no):</p>

<pre>
for message in consumer:
    print("%s:%d:%d: key=%s value=%s" % (
            message.topic,
            message.partition,
            message.offset,
            message.key,
            message.value,
        )
    )
</pre>

<p><div class="rs-tip-major">PoznÃ¡mka: ve skriptu je navÃ­c pÅ™idÃ¡na reakce na
stisk klÃ¡vesovÃ© zkratky <strong>Ctrl+C</strong> pro ukonÄenÃ­ Äinnosti nekoneÄnÃ©
smyÄky.</div></p>

<p>A takto vypadÃ¡ ÃºplnÃ½ zdrojovÃ½ kÃ³d bÄ›Å¾nÃ©ho konzumenta zprÃ¡v, kterÃ½ naleznete
na adrese <a
href="https://github.com/tisnik/most-popular-python-libs/blob/master/faust/greeting_consumer.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faust/greeting_consumer.py</a>:</p>

<pre>
#!/usr/bin/env python3
&nbsp;
import sys
from kafka import KafkaConsumer
&nbsp;
server = "localhost:9092"
topic = "greetings"
group_id = "group1"
&nbsp;
print("Connecting to Kafka")
consumer = KafkaConsumer(
    topic, group_id=group_id,
    bootstrap_servers=[server],
    auto_offset_reset="earliest"
)
print("Connected to Kafka")
&nbsp;
try:
    for message in consumer:
        print(
            "%s:%d:%d: key=%s value=%s"
            % (
                message.topic,
                message.partition,
                message.offset,
                message.key,
                message.value,
            )
        )
except KeyboardInterrupt:
    sys.exit()
</pre>



<p><a name="k07"></a></p>
<h2 id="k07">7. SpuÅ¡tÄ›nÃ­ producenta a konzumenta</h2>

<p>NynÃ­ si funkce producenta a konzumenta otestujeme. V&nbsp;jednom terminÃ¡lu
spustÃ­me producenta zprÃ¡v:</p>

<pre>
</pre>

<p>Ve druhÃ©m terminÃ¡lu naopak spustÃ­me konzumenta:</p>

<pre>
</pre>

<p>NynÃ­ by mÄ›l konzument zaÄÃ­t pÅ™ijÃ­mat novÃ© zprÃ¡vy posÃ­lanÃ© s&nbsp;periodou
pÅ™ibliÅ¾nÄ› jednÃ© sekundy:</p>

<p><div class="rs-tip-major">PoznÃ¡mka: povÅ¡imnÄ›te si, Å¾e tÄ›lo zprÃ¡v je tvoÅ™eno
hodnotami typu <strong>bytes</strong> a nikoli Å™etÄ›zcem. PÅ™evod na Å™etÄ›zec je
ovÅ¡em triviÃ¡lnÃ­ a lze ho provÃ©st zavolÃ¡nÃ­m jedinÃ© metody.</div></p>



<p><a name="k08"></a></p>
<h2 id="k08">8. Konzument realizovanÃ½ formou workera postavenÃ½ na knihovnÄ› Faust</h2>

<p>PodÃ­vejme se nynÃ­ na to, jak by se konzument realizoval s&nbsp;vyuÅ¾itÃ­m
knihovny Faust. Tato knihovna je postavena na ponÄ›kud odliÅ¡nÃ½ch konceptech, neÅ¾
je klasickÃ½ producent a konzument. NamÃ­sto konzumenta se zde pouÅ¾Ã­vÃ¡ termÃ­n
<i>worker</i>, naÄÃ­tanÃ© zprÃ¡vy mohou bÃ½t popsÃ¡ny <i>modelem</i> a vÅ¡e je
zabaleno v&nbsp;<i>aplikaci</i>, kterÃ¡ Å™Ã­dÃ­ konzumaci Äi produkci zprÃ¡v
(resp.&nbsp;celÃ½ tok dat). ZaÄnÄ›me nejdÅ™Ã­ve poslednÃ­m zmÃ­nÄ›nÃ½m termÃ­nem
<i>aplikace</i>. JednÃ¡ se o koncept umoÅ¾ÅˆujÃ­cÃ­ nejenom spouÅ¡tÄ›nÃ­ workerÅ¯, ale i
dalÅ¡Ã­ Äinnosti, napÅ™Ã­klad prÃ¡ci s&nbsp;tabulkami (coÅ¾ si popÃ­Å¡eme
v&nbsp;navazujÃ­cÃ­m ÄlÃ¡nku), prÃ¡ci s&nbsp;takzvanÃ½mi <i>okny</i> (pohledÅ¯ na
ÄÃ¡st sekvence zprÃ¡v) atd.</p>

<p>Objekt pÅ™edstavujÃ­cÃ­ aplikaci se vytvoÅ™Ã­ konstruktorem <strong>App</strong>,
kterÃ©mu se musÃ­ pÅ™edat minimÃ¡lnÄ› jeden povinnÃ½ parametr &ndash; identifikÃ¡tor
aplikace, coÅ¾ je Å™etÄ›zec. OvÅ¡em pÅ™edat je moÅ¾nÃ© i dalÅ¡Ã­ parametry, zejmÃ©na
adresu Kafka brokeru, zpÅ¯sob (kodek) serializace a deserializace zprÃ¡v atd.
VytvoÅ™enÃ­ aplikace tedy mÅ¯Å¾e vypadat nÃ¡sledovnÄ›:</p>

<pre>
app = faust.App(
    "hello-world",
    broker="kafka://localhost:9092",
    value_serializer="raw",
)
</pre>

<p>V&nbsp;dalÅ¡Ã­m kroku zkonstruujeme jeÅ¡tÄ› jeden objekt, kterÃ½ bude
reprezentovat <i>tÃ©ma</i> (<i>topic</i>). V&nbsp;tomto pÅ™Ã­padÄ› se pÅ™edÃ¡vÃ¡ jen
jmÃ©no tÃ©matu, i kdyÅ¾ opÄ›t platÃ­, Å¾e je moÅ¾nÃ© pÅ™edat i dalÅ¡Ã­ nepovinnÃ© parametry
(schÃ©ma, typ klÃ­ÄÅ¯, typ tÄ›l zprÃ¡v atd. atd.):</p>

<pre>
greetings_topic = app.topic("greetings")
</pre>

<p>NynÃ­ je jiÅ¾ moÅ¾nÃ© nadefinovat asynchronnÃ­ funkci, kterÃ¡ bude konzumovat
zprÃ¡vy pÅ™ichÃ¡zejÃ­cÃ­ do nakonfigurovanÃ©ho tÃ©matu. Tato funkce je obalena
dekorÃ¡torem <strong>@app.agent</strong>, pÅ™iÄemÅ¾ i dekorÃ¡tor akceptuje rÅ¯znÃ©
parametry. My mu prozatÃ­m pÅ™edÃ¡me pouze objekt pÅ™edstavujÃ­cÃ­ tÃ©ma:</p>

<pre>
<strong>@app.agent(greetings_topic)</strong>
async def <strong>greet</strong>(greetings):
    async for greeting in greetings:
        print(greeting)
</pre>

<p>PoslednÃ­m krokem je spuÅ¡tÄ›nÃ­ aplikace, coÅ¾ je jiÅ¾ triviÃ¡lnÃ­ operace:</p>

<pre>
if __name__ == "__main__":
    app.main()
</pre>

<p>ÃšplnÃ½ zdrojovÃ½ kÃ³d workera je moÅ¾nÃ© najÃ­t na adrese <a
href="https://github.com/tisnik/most-popular-python-libs/blob/master/faust/greeting_faust_consumer.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faust/greeting_faust_consumer.py</a>:</p>

<pre>
import faust
&nbsp;
app = faust.App(
    "hello-world",
    broker="kafka://localhost:9092",
    value_serializer="raw",
)
&nbsp;
greetings_topic = app.topic("greetings")
&nbsp;
<u>@app.agent(greetings_topic)</u>
async def <strong>greet</strong>(greetings):
    async for greeting in greetings:
        print(greeting)
&nbsp;
&nbsp;
if __name__ == "__main__":
    app.main()
</pre>



<p><a name="k09"></a></p>
<h2 id="k09">9. SpuÅ¡tÄ›nÃ­ workera</h2>

<p>V&nbsp;pÅ™Ã­padÄ›, Å¾e se pokusÃ­me o spuÅ¡tÄ›nÃ­ workera bÄ›Å¾nÃ½m zpÅ¯sobem,
tj.&nbsp;zadÃ¡nÃ­m:</p>

<pre>
$ <strong>python greeting_faust_consumer.py</strong>
</pre>

<p>ukÃ¡Å¾e se nÃ¡m pouze nÃ¡povÄ›da s&nbsp;informacemi, jakÃ© moÅ¾nosti nÃ¡m worker
umoÅ¾Åˆuje:</p>

<pre>
Usage: greeting_faust_consumer.py [OPTIONS] COMMAND [ARGS]...
&nbsp;
  Welcome, see list of commands and options below.
&nbsp;
  Use --help for help, --version for version information.
&nbsp;
  https://faust-streaming.github.io/faust
&nbsp;
Options:
  --console-port RANGE[1-65535]   when --debug: Port to run debugger console
                                  on.  [default: 50101; 1&lt;=x&lt;=65535]
  --blocking-timeout FLOAT        when --debug: Blocking detector timeout.
  -l, --loglevel [crit|error|warn|info|debug]
                                  Logging level to use.  [default: WARN]
  -f, --logfile FILE              Path to logfile (default is &lt;stderr&gt;).
  -L, --loop [aio|eventlet|uvloop]
                                  Event loop implementation to use.  [default:
                                  aio]
  --json                          Return output in machine-readable JSON
                                  format
  -D, --datadir DIRECTORY         Directory to keep application state.
                                  [default: {conf.name}-data]
  -W, --workdir DIRECTORY         Working directory to change to after start.
  --debug / --no-debug            Enable debugging output, and the blocking
                                  detector.  [default: no-debug]
  -q, --quiet / --no-quiet        Silence output to &lt;stdout&gt;/&lt;stderr&gt;
                                  [default: no-quiet]
  -A, --app TEXT                  Path of Faust application to use, or the
                                  name of a module.
  --version                       Show the version and exit.
  --help                          Show this message and exit.
&nbsp;
Commands:
  agents          List agents.
  clean-versions  Delete old version directories.
  completion      Output shell completion to be evaluated by the shell.
  livecheck       Manage LiveCheck instances.
  model           Show model detail.
  models          List all available models as a tabulated list.
  reset           Delete local table state.
  send            Send message to agent/topic.
  tables          List available tables.
  worker          Start worker instance for given app.
</pre>

<p>VlastnÃ­ spuÅ¡tÄ›nÃ­ se provede pÅ™Ã­kazem:</p>

<pre>
$ <strong>python3 greeting_faust_consumer.py worker</strong>
</pre>

<p>Nejprve se zobrazÃ­ tabulka se zÃ¡kladnÃ­mi informacemi o workerovi:</p>

<pre>
â”ŒÆ’aÂµSâ€  v0.11.1.dev4+ga489db3bâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ id          â”‚ hello-world                                     â”‚
â”‚ transport   â”‚ [URL('kafka://localhost:9092')]                 â”‚
â”‚ store       â”‚ memory:                                         â”‚
â”‚ web         â”‚ http://localhost:6066/                          â”‚
â”‚ log         â”‚ -stderr- (warn)                                 â”‚
â”‚ pid         â”‚ 1296383                                         â”‚
â”‚ hostname    â”‚ ptisnovs.xxx.yyy.zzz                            â”‚
â”‚ platform    â”‚ CPython 3.11.8 (Linux x86_64)                   â”‚
â”‚        +    â”‚ Cython (GCC 13.2.1 20231011 (Red Hat 13.2.1-4)) â”‚
â”‚ drivers     â”‚                                                 â”‚
â”‚   transport â”‚ aiokafka=0.10.0                                 â”‚
â”‚   web       â”‚ aiohttp=3.9.5                                   â”‚
â”‚ datadir     â”‚ /tmp/ramdisk/faust/hello-world-data             â”‚
â”‚ appdir      â”‚ /tmp/ramdisk/faust/hello-world-data/v1          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
startingâ¢ ğŸ˜Š
</pre>

<p>Ihned po vÃ½pisu zprÃ¡vy &bdquo;starting&ldquo; by mÄ›l worker zaÄÃ­t se ÄtenÃ­m
a zpracovÃ¡vÃ¡nÃ­m zprÃ¡v, kterÃ© jsou konzumovÃ¡ny z&nbsp;tÃ©matu
&bdquo;greetings&ldquo;. Informace o kaÅ¾dÃ© zkonzumovanÃ© zprÃ¡vÄ› je vypisovÃ¡na do
terminÃ¡lu:</p>

<pre>
[2024-04-20 18:03:39,583] [1296383] [WARNING] b'Greeting #0' 
[2024-04-20 18:03:39,583] [1296383] [WARNING] b'Greeting #1' 
[2024-04-20 18:03:39,584] [1296383] [WARNING] b'Greeting #2' 
[2024-04-20 18:03:39,584] [1296383] [WARNING] b'Greeting #3' 
[2024-04-20 18:03:39,584] [1296383] [WARNING] b'Greeting #4' 
</pre>

<p><div class="rs-tip-major">PoznÃ¡mka: povÅ¡imnÄ›te si, Å¾e tÄ›la zprÃ¡v jsou
reprezentovÃ¡na typem <i>bytes</i>, tj.&nbsp;nemÄ›nitelnou (<i>immutable</i>)
sekvencÃ­ bajtÅ¯. Tento nedostatek samozÅ™ejmÄ› pozdÄ›ji napravÃ­me.</div></p>



<p><a name="k10"></a></p>
<h2 id="k10">10. Producent posÃ­lajÃ­cÃ­ zprÃ¡vy do vÄ›tÅ¡Ã­ho mnoÅ¾stvÃ­ tÃ©mat</h2>

<p>Mezi pÅ™ednosti knihovny Faust patÅ™Ã­ pÅ™edevÅ¡Ã­m to, Å¾e lze spustit vÄ›tÅ¡Ã­
mnoÅ¾stvÃ­ asynchronnÄ› bÄ›Å¾Ã­cÃ­ch workerÅ¯. Abychom si to ukÃ¡zali v&nbsp;praxi na co
nejjednoduÅ¡Å¡Ã­m pÅ™Ã­kladu, upravÃ­me nepatrnÄ› naÅ¡eho producenta zprÃ¡v takovÃ½m
zpÅ¯sobem, aby posÃ­lal zprÃ¡vy do dvou tÃ©mat nazvanÃ½ch &bdquo;greetings&ldquo; a
&bdquo;real_work&ldquo;. Tato Ãºprava je snadnÃ¡, protoÅ¾e postaÄuje do programovÃ©
smyÄky pÅ™idat dalÅ¡Ã­ volÃ¡nÃ­ metod <strong>producer.send</strong>. VÃ½slednÃ½ kÃ³d
takto upravenÃ©ho producenta zprÃ¡v lze nalÃ©zt na adrese <a
href="https://github.com/tisnik/most-popular-python-libs/blob/master/faust/multi_producer_raw.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faust/multi_producer_raw.py</a>:</p>

<pre>
#!/usr/bin/env python3
&nbsp;
from kafka import KafkaProducer
from time import sleep
from json import dumps
&nbsp;
server = "localhost:9092"
topic1 = "greetings"
topic2 = "real_work"
&nbsp;
print("Connecting to Kafka")
producer = <strong>KafkaProducer</strong>(
    bootstrap_servers=[server],
    value_serializer=lambda x: dumps(x).encode("utf-8")
)
print("Connected to Kafka")
&nbsp;
for i in range(1000):
    print(i)
    message = f"Greeting #{i}"
    producer.send(topic1, value=message)
    sleep(1)
    work = f"Real work #{i*2}"
    producer.send(topic2, value=work)
    work = f"Real work #{i*2+1}"
    producer.send(topic2, value=work)
    sleep(1)
</pre>

<p><div class="rs-tip-major">PoznÃ¡mka: povÅ¡imnÄ›te si, Å¾e se zprÃ¡vy stÃ¡le
serializujÃ­ stejnÃ½m zpÅ¯sobem &ndash; z&nbsp;Å™etÄ›zcÅ¯ na hodnoty typu
<strong>bytes</strong>.</div></p>



<p><a name="k11"></a></p>
<h2 id="k11">11. Konzumace zprÃ¡v workerem z&nbsp;vÄ›tÅ¡Ã­ho mnoÅ¾stvÃ­ tÃ©mat</h2>

<p>Jak bude vypadat konzument zprÃ¡v (resp.&nbsp;konzumenti zprÃ¡v)
v&nbsp;pÅ™Ã­padÄ›, Å¾e pouÅ¾ijeme knihovnu Faust? DÃ­ky tomu, Å¾e kaÅ¾dÃ½ worker je
realizovÃ¡n asynchronnÃ­ funkcÃ­, je Å™eÅ¡enÃ­ aÅ¾ pÅ™ekvapivÄ› snadnÃ© &ndash; kaÅ¾dÃ½
worker bude v&nbsp;takovÃ©m pÅ™Ã­padÄ› definovÃ¡n ve vlastnÃ­ funkci
s&nbsp;dekorÃ¡torem <strong>@app.agent</strong> a kaÅ¾dÃ½ z&nbsp;tÄ›chto workerÅ¯
zpracovÃ¡vÃ¡ zprÃ¡vy z&nbsp;odliÅ¡nÃ©ho tÃ©matu:</p>

<pre>
<u>@app.agent(greetings_topic)</u>
async def <strong>greet</strong>(greetings):
    async for greeting in greetings:
        print(f"Greeter: {greeting}")
&nbsp;
<i>@app.agent(real_work_topic)</i>
async def <strong>worker</strong>(works):
    async for work in works:
        print(f"Worker: {work}")
</pre>

<p>O spouÅ¡tÄ›nÃ­, koordinaci prÃ¡ce atd. se postarÃ¡ knihovna Faust automaticky,
takÅ¾e tento problÃ©m vlastnÄ› vÅ¯bec nemusÃ­me na aplikaÄnÃ­ Ãºrovni Å™eÅ¡it. VÃ½slednÃ¡
aplikace s&nbsp;workery je dostupnÃ¡ na adrese <a
href="https://github.com/tisnik/most-popular-python-libs/blob/master/faust/greeting_worker_consumer_raw.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faust/greeting_worker_consumer_raw.py</a>
a vypadÃ¡ nÃ¡sledovnÄ›:</p>

<pre>
import faust
&nbsp;
app = faust.App(
    "hello-world",
    broker="kafka://localhost:9092",
    value_serializer="raw",
)
&nbsp;
greetings_topic = app.topic("greetings")
real_work_topic = app.topic("real_work")
&nbsp;
<u>@app.agent(greetings_topic)</u>
async def <strong>greet</strong>(greetings):
    async for greeting in greetings:
        print(f"Greeter: {greeting}")
&nbsp;
<u>@app.agent(real_work_topic)</u>
async def <strong>worker</strong>(works):
    async for work in works:
        print(f"Worker: {work}")
&nbsp;
&nbsp;
if __name__ == "__main__":
    app.main()
</pre>



<p><a name="k12"></a></p>
<h2 id="k12">12. Produkce zprÃ¡v ve formÃ¡tu JSON</h2>

<pre>
#!/usr/bin/env python3
&nbsp;
from kafka import KafkaProducer
from time import sleep
&nbsp;
server = "localhost:9092"
topic1 = "greetings"
topic2 = "real_work"
&nbsp;
print("Connecting to Kafka")
producer = <strong>KafkaProducer</strong>(
    bootstrap_servers=[server],
    value_serializer=lambda x: x.encode("utf-8")
)
print("Connected to Kafka")
&nbsp;
for i in range(1000):
    print(i)
    message = {"subject": "Greeting", "value": i}
    producer.send(topic1, value=message)
    sleep(1)
    work = {"subject": "Real work", "value": i*2}
    producer.send(topic2, value=work)
    work = {"subject": "Real work", "value": i*2+1}
    producer.send(topic2, value=work)
    sleep(1)
</pre>



<p><a name="k13"></a></p>
<h2 id="k13">13. Worker pÅ™ijÃ­majÃ­cÃ­ zprÃ¡vy ve formÃ¡tu JSON</h2>

<pre>
import faust
&nbsp;
app = faust.App(
    "hello-world",
    broker="kafka://localhost:9092",
    <u>value_serializer="json"</u>,
)
&nbsp;
greetings_topic = app.topic("greetings")
real_work_topic = app.topic("real_work")
&nbsp;
<u>@app.agent(greetings_topic)</u>
async def <strong>greet</strong>(greetings):
    async for greeting in greetings:
        print(f"Greeter: {greeting}")
&nbsp;
<u>@app.agent(real_work_topic)</u>
async def <strong>worker</strong>(works):
    async for work in works:
        print(f"Worker: {work}")
&nbsp;
&nbsp;
if __name__ == "__main__":
    app.main()
</pre>




<p><a name="k14"></a></p>
<h2 id="k14">14. VyuÅ¾itÃ­ modelÅ¯</h2>

<pre>
class <strong>Registration</strong>(faust.Record):
    name: str
    surname: str
    id: int
</pre>

<pre>
#!/usr/bin/env python3
&nbsp;
from kafka import KafkaProducer
from time import sleep
from json import dumps
&nbsp;
def user(name, surname, id):
    return {
            "name": name,
            "surname": surname,
            "id": id
            }
&nbsp;
&nbsp;
server = "localhost:9092"
topic = "registrations"
&nbsp;
print("Connecting to Kafka")
producer = <strong>KafkaProducer</strong>(
    bootstrap_servers=[server],
    value_serializer=lambda x: dumps(x).encode("utf-8")
)
print("Connected to Kafka")
&nbsp;
producer.send(topic, value=user("EliÅ¡ka", "NajbrtovÃ¡", 4))
producer.send(topic, value=user("Jenny", "Suk", 3))
producer.send(topic, value=user("AniÄka", "Å afÃ¡Å™ovÃ¡", 0))
producer.send(topic, value=user("SvÃ¡Å¥a", "Pulec", 3))
producer.send(topic, value=user("BlaÅ¾ej", "MotyÄka", 8))
producer.send(topic, value=user("Eda", "Wasserfall", 0))
producer.send(topic, value=user("PÅ™emysl", "HÃ¡jek", 10))
producer.flush()
&nbsp;
print("Done")
</pre>

<pre>
import faust
&nbsp;
app = faust.App(
    "registrations",
    broker="kafka://localhost:9092",
    #value_serializer="json",
)
&nbsp;
&nbsp;
class <strong>Registration</strong>(faust.Record):
    name: str
    surname: str
    id: int
&nbsp;
&nbsp;
registrations_topic = app.topic("registrations", key_type=str, value_type=Registration)
&nbsp;
<u>@app.agent(registrations_topic)</u>
async def <strong>register</strong>(registrations):
    async for registration in registrations:
        print(f"Registration: {registration}")
&nbsp;
&nbsp;
if __name__ == "__main__":
    app.main()
</pre>



<p><a name="k15"></a></p>
<h2 id="k15">15. Producent zaloÅ¾enÃ½ na knihovnÄ› Faust vyuÅ¾Ã­vajÃ­cÃ­ model</h2>

<pre>
import faust
&nbsp;
app = faust.App(
    "registrations",
    broker="kafka://localhost:9092",
)
&nbsp;
&nbsp;
class <strong>Registration</strong>(faust.Record):
    name: str
    surname: str
    id: int
&nbsp;
&nbsp;
registrations_topic = app.topic("registrations", key_type=str, value_type=Registration)
&nbsp;
<u>@app.timer(interval=5.0)</u>
async def <strong>example_sender</strong>(app):
    await registrations_topic.send(
        value=Registration("EliÅ¡ka", "NajbrtovÃ¡", 4))
    await registrations_topic.send(
        value=Registration("Jenny", "Suk", 3))
    await registrations_topic.send(
        value=Registration("AniÄka", "Å afÃ¡Å™ovÃ¡", 0))
    await registrations_topic.send(
        value=Registration("SvÃ¡Å¥a", "Pulec", 3))
    await registrations_topic.send(
        value=Registration("BlaÅ¾ej", "MotyÄka", 8))
    await registrations_topic.send(
        value=Registration("Eda", "Wasserfall", 0))
    await registrations_topic.send(
        value=Registration("PÅ™emysl", "HÃ¡jek", 10))
&nbsp;
&nbsp;
if __name__ == "__main__":
    app.main()
</pre>



<p><a name="k16"></a></p>
<h2 id="k16">16. Kombinace producenta a konzumenta &ndash; reÃ¡lnÃ¡ sÃ­la knihovny Faust</h2>

<pre>
import faust
&nbsp;
app = faust.App(
    "registrations",
    broker="kafka://localhost:9092",
)
&nbsp;
&nbsp;
class <strong>Registration</strong>(faust.Record):
    name: str
    surname: str
    id: int
&nbsp;
&nbsp;
registrations_topic = app.topic("registrations", key_type=str, value_type=Registration)
&nbsp;
<u>@app.timer(interval=5.0)</u>
async def <strong>example_sender</strong>(app):
    await registrations_topic.send(
        value=Registration("EliÅ¡ka", "NajbrtovÃ¡", 4))
    await registrations_topic.send(
        value=Registration("Jenny", "Suk", 3))
    await registrations_topic.send(
        value=Registration("AniÄka", "Å afÃ¡Å™ovÃ¡", 0))
    await registrations_topic.send(
        value=Registration("SvÃ¡Å¥a", "Pulec", 3))
    await registrations_topic.send(
        value=Registration("BlaÅ¾ej", "MotyÄka", 8))
    await registrations_topic.send(
        value=Registration("Eda", "Wasserfall", 0))
    await registrations_topic.send(
        value=Registration("PÅ™emysl", "HÃ¡jek", 10))
&nbsp;
&nbsp;
<u>@app.agent(registrations_topic)</u>
async def <strong>register</strong>(registrations):
    async for registration in registrations:
        print(f"Registration: {registration}")
&nbsp;
&nbsp;
if __name__ == "__main__":
    app.main()
</pre>

<p><div class="rs-tip-major">PoznÃ¡mka: </div></p>



<p><a name="k17"></a></p>
<h2 id="k17">17. SpuÅ¡tÄ›nÃ­ kombinace workerÅ¯ s&nbsp;producenty a konzumenty</h2>


<pre>
â”ŒÆ’aÂµSâ€  v0.11.1.dev4+ga489db3bâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ id          â”‚ registrations                                   â”‚
â”‚ transport   â”‚ [URL('kafka://localhost:9092')]                 â”‚
â”‚ store       â”‚ memory:                                         â”‚
â”‚ web         â”‚ http://localhost:6066/                          â”‚
â”‚ log         â”‚ -stderr- (warn)                                 â”‚
â”‚ pid         â”‚ 1508601                                         â”‚
â”‚ hostname    â”‚ ptisnovs.xxx.yyy.zzz                            â”‚
â”‚ platform    â”‚ CPython 3.11.8 (Linux x86_64)                   â”‚
â”‚        +    â”‚ Cython (GCC 13.2.1 20231011 (Red Hat 13.2.1-4)) â”‚
â”‚ drivers     â”‚                                                 â”‚
â”‚   transport â”‚ aiokafka=0.10.0                                 â”‚
â”‚   web       â”‚ aiohttp=3.9.5                                   â”‚
â”‚ datadir     â”‚ /tmp/ramdisk/faust/registrations-data           â”‚
â”‚ appdir      â”‚ /tmp/ramdisk/faust/registrations-data/v1        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
startingâ¢ 
</pre>



<p><a name="k18"></a></p>
<h2 id="k18">18. Obsah navazujÃ­cÃ­ho ÄlÃ¡nku</h2>

<p></p>



<p><a name="k19"></a></p>
<h2 id="k19">19. RepositÃ¡Å™ s&nbsp;demonstraÄnÃ­mi pÅ™Ã­klady</h2>

<p>ZdrojovÃ© kÃ³dy vÅ¡ech prozatÃ­m popsanÃ½ch demonstraÄnÃ­ch pÅ™Ã­kladÅ¯ urÄenÃ½ch pro
programovacÃ­ jazyk Python 3 byly uloÅ¾eny do Git repositÃ¡Å™e dostupnÃ©ho na adrese
<a
href="https://github.com/tisnik/most-popular-python-libs">https://github.com/tisnik/most-popular-python-libs</a>:</p>

<table>
<tr><th> #</th><th>DemonstraÄnÃ­ pÅ™Ã­klad</th><th>StruÄnÃ½ popis pÅ™Ã­kladu</th><th>Cesta</th></tr>
<tr><td> 1</td><td>greeting_producer.py</td><td>klasickÃ½ producent zprÃ¡v vytvoÅ™enÃ½ bez pouÅ¾itÃ­ knihovny Faust (pouÅ¾Ã­vÃ¡ knihovnu Kafka-python-ng)</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faust/greeting_producer.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faust/greeting_producer.py</a></td></tr>
<tr><td> 2</td><td>greeting_consumer.py</td><td>klasickÃ½ konzument zprÃ¡v vytvoÅ™enÃ½ bez pouÅ¾itÃ­ knihovny Faust (pouÅ¾Ã­vÃ¡ knihovnu Kafka-python-ng)</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faust/greeting_consumer.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faust/greeting_consumer.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td> 3</td><td>greeting_faust_consumer.py</td><td>worker definovanÃ½ s&nbsp;vyuÅ¾itÃ­m knihovny Faust</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faust/greeting_faust_consumer.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faust/greeting_faust_consumer.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td> 4</td><td>multi_producer_raw.py</td><td>producent zprÃ¡v do vÄ›tÅ¡Ã­ho mnoÅ¾stvÃ­ tÃ©mat, zprÃ¡vy jsou posÃ­lÃ¡ny jako Å™etÄ›zce</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faust/multi_producer_raw.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faust/multi_producer_raw.py</a></td></tr>
<tr><td> 5</td><td>greeting_worker_consumer_raw.py</td><td>dvojice workerÅ¯ definovanÃ½ch s&nbsp;vyuÅ¾itÃ­m knihovny Faust pro zprÃ¡vy posÃ­lanÃ© jako sekvence bajtÅ¯</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faust/greeting_worker_consumer_raw.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faust/greeting_worker_consumer_raw.py</a></td></tr>
<tr><td> 6</td><td>multi_producer_json.py</td><td>producent zprÃ¡v do vÄ›tÅ¡Ã­ho mnoÅ¾stvÃ­ tÃ©mat, zprÃ¡vy jsou serializovÃ¡ny do formÃ¡tu JSON</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faust/multi_producer_json.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faust/multi_producer_json.py</a></td></tr>
<tr><td> 7</td><td>greeting_worker_consumer_json.py</td><td>dvojice workerÅ¯ definovanÃ½ch s&nbsp;vyuÅ¾itÃ­m knihovny Faust pro zprÃ¡vy ve formÃ¡tu JSON</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faust/greeting_worker_consumer_json.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faust/greeting_worker_consumer_json.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td> 8</td><td>registration_producer.py</td><td>producent zprÃ¡v ve formÃ¡tu JSON obsahujÃ­cÃ­ch atributy objektÅ¯ (pouÅ¾Ã­vÃ¡ knihovnu Kafka-python-ng)</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faust/registration_producer.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faust/registration_producer.py</a></td></tr>
<tr><td> 9</td><td>registration_consumer.py</td><td>konzument zprÃ¡v zaloÅ¾enÃ½ na knihovnÄ› Faust (pracuje s&nbsp;modelem)</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faust/registration_consumer.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faust/registration_consumer.py</a></td></tr>
<tr><td>10</td><td>registration_producer_faust.py</td><td>producent zprÃ¡v zaloÅ¾enÃ½ na knihovnÄ› Faust (pracuje s&nbsp;modelem)</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faust/registration_producer_faust.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faust/registration_producer_faust.py</a></td></tr>
<tr><td>11</td><td>registration_consumer_producer.py</td><td>tok dat od producenta ke konzumentovi, zaloÅ¾eno na knihovnÄ› Faust</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faust/registration_consumer_producer.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faust/registration_consumer_producer.py</a></td></tr>
</table>



<p><a name="k20"></a></p>
<h2 id="k20">20. Odkazy na Internetu</h2>

<ol>

<li>Faust &ndash; Python Stream Processing<br />
<a href="https://faust-streaming.github.io/faust/">https://faust-streaming.github.io/faust/</a>
</li>

<li>Knihovna Faust na GitHubu<br />
<a href="https://github.com/faust-streaming/faust">https://github.com/faust-streaming/faust</a>
</li>

<li>faust 1.10.4 na Pypi<br />
<a href="https://pypi.org/project/faust/">https://pypi.org/project/faust/</a>
</li>

<li>Introduction to Kafka Stream Processing in Python using Faust (video)<br />
<a href="https://www.youtube.com/watch?v=Nt96udaC5Zk">https://www.youtube.com/watch?v=Nt96udaC5Zk</a>
</li>

<li>Windowing in Kafka Streams using Faust Framework in Python | Tumbling Window (video)<br />
<a href="https://www.youtube.com/watch?v=ZlBXg9Kp8vE">https://www.youtube.com/watch?v=ZlBXg9Kp8vE</a>
</li>

<li>Stream Processing with Python, Kafka &amp; Faust<br />
<a href="https://towardsdatascience.com/stream-processing-with-python-kafka-faust-a11740d0910c">https://towardsdatascience.com/stream-processing-with-python-kafka-faust-a11740d0910c</a>
</li>

<li>ETL Batch Processing With Kafka?<br />
<a href="https://medium.com/swlh/etl-batch-processing-with-kafka-7f66f843e20d">https://medium.com/swlh/etl-batch-processing-with-kafka-7f66f843e20d</a>
</li>

<li>ETL with Kafka<br />
<a href="https://blog.codecentric.de/en/2018/03/etl-kafka/">https://blog.codecentric.de/en/2018/03/etl-kafka/</a>
</li>

<li>Building ETL Pipelines with Clojure and Transducers<br />
<a href="https://www.grammarly.com/blog/engineering/building-etl-pipelines-with-clojure-and-transducers/">https://www.grammarly.com/blog/engineering/building-etl-pipelines-with-clojure-and-transducers/</a>
</li>

<li>pipeline (moÅ¾nÃ© pouÅ¾Ã­t pro ETL)<br />
<a href="https://clojuredocs.org/clojure.core.async/pipeline">https://clojuredocs.org/clojure.core.async/pipeline</a>
</li>

<li>On Track with Apache Kafka â€“ Building a Streaming ETL Solution with Rail Data<br />
<a href="https://www.confluent.io/blog/build-streaming-etl-solutions-with-kafka-and-rail-data/">https://www.confluent.io/blog/build-streaming-etl-solutions-with-kafka-and-rail-data/</a>
</li>

<li>Kafka - Understanding Offset Commits<br />
<a href="https://www.logicbig.com/tutorials/misc/kafka/committing-offsets.html">https://www.logicbig.com/tutorials/misc/kafka/committing-offsets.html</a>
</li>

<li>fundingcircle/jackdaw (na Clojars)<br />
<a href="https://clojars.org/fundingcircle/jackdaw/versions/0.7.6">https://clojars.org/fundingcircle/jackdaw/versions/0.7.6</a>
</li>

<li>Dokumentace ke knihovnÄ› jackdaw<br />
<a href="https://cljdoc.org/d/fundingcircle/jackdaw/0.7.6/doc/readme">https://cljdoc.org/d/fundingcircle/jackdaw/0.7.6/doc/readme</a>
</li>

<li>Jackdaw AdminClient API<br />
<a href="https://cljdoc.org/d/fundingcircle/jackdaw/0.7.6/doc/jackdaw-adminclient-api">https://cljdoc.org/d/fundingcircle/jackdaw/0.7.6/doc/jackdaw-adminclient-api</a>
</li>

<li>Jackdaw Client API<br />
<a href="https://cljdoc.org/d/fundingcircle/jackdaw/0.7.6/doc/jackdaw-client-api">https://cljdoc.org/d/fundingcircle/jackdaw/0.7.6/doc/jackdaw-client-api</a>
</li>

<li>Kafka.clj<br />
<a href="https://github.com/helins-io/kafka.clj">https://github.com/helins-io/kafka.clj</a>
</li>

<li>Kafka mirroring (MirrorMaker)<br />
<a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=27846330">https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=27846330</a>
</li>

<li>Mastering Kafka migration with MirrorMaker 2<br />
<a href="https://developers.redhat.com/articles/2024/01/04/mastering-kafka-migration-mirrormaker-2">https://developers.redhat.com/articles/2024/01/04/mastering-kafka-migration-mirrormaker-2</a>
</li>

<li>Apache Kafka MirrorMaker 2 (MM2) Part 1: Theory<br />
<a href="https://www.instaclustr.com/blog/kafka-mirrormaker-2-theory/#h-2-replication-in-kafka">https://www.instaclustr.com/blog/kafka-mirrormaker-2-theory/#h-2-replication-in-kafka</a>
</li>

<li>Apache Kafka MirrorMaker 2 (MM2) Part 2: Practice<br />
<a href="https://www.instaclustr.com/blog/apache-kafka-mirrormaker-2-practice/">https://www.instaclustr.com/blog/apache-kafka-mirrormaker-2-practice/</a>
</li>

<li>Demystifying Kafka MirrorMaker 2: Use cases and architecture <br />
<a href="https://developers.redhat.com/articles/2023/11/13/demystifying-kafka-mirrormaker-2-use-cases-and-architecture#">https://developers.redhat.com/articles/2023/11/13/demystifying-kafka-mirrormaker-2-use-cases-and-architecture#</a>
</li>

<li>How to use Kafka MirrorMaker 2.0 in data migration, replication and the use-cases<br />
<a href="https://learn.microsoft.com/en-us/azure/hdinsight/kafka/kafka-mirrormaker-2-0-guide">https://learn.microsoft.com/en-us/azure/hdinsight/kafka/kafka-mirrormaker-2-0-guide</a>
</li>

<li>Release Notes - Kafka - Version 2.4.0<br />
<a href="https://archive.apache.org/dist/kafka/2.4.0/RELEASE_NOTES.html">https://archive.apache.org/dist/kafka/2.4.0/RELEASE_NOTES.html</a>
</li>

<li>Kafka Mirror Maker Best Practices<br />
<a href="https://community.cloudera.com/t5/Community-Articles/Kafka-Mirror-Maker-Best-Practices/ta-p/249269">https://community.cloudera.com/t5/Community-Articles/Kafka-Mirror-Maker-Best-Practices/ta-p/249269</a>
</li>

<li>Apache Kafka MirrorMaker 2 (MM2) Part 1: Theory<br />
<a href="https://www.instaclustr.com/blog/kafka-mirrormaker-2-theory/">https://www.instaclustr.com/blog/kafka-mirrormaker-2-theory/</a>
</li>

<li>Kcli: is a kafka read only command line browser.<br />
<a href="https://github.com/cswank/kcli">https://github.com/cswank/kcli</a>
</li>

<li>Kcli: a kafka command line browser<br />
<a href="https://go.libhunt.com/kcli-alternatives">https://go.libhunt.com/kcli-alternatives</a>
</li>

<li>Kafka Connect and Schemas<br />
<a href="https://rmoff.net/2020/01/22/kafka-connect-and-schemas/">https://rmoff.net/2020/01/22/kafka-connect-and-schemas/</a>
</li>

<li>JSON and schemas<br />
<a href="https://www.confluent.io/blog/kafka-connect-deep-dive-converters-serialization-explained/#json-schemas">https://www.confluent.io/blog/kafka-connect-deep-dive-converters-serialization-explained/#json-schemas</a>
</li>

<li>What, why, when to use Apache Kafka, with an example<br />
<a href="https://www.startdataengineering.com/post/what-why-and-how-apache-kafka/">https://www.startdataengineering.com/post/what-why-and-how-apache-kafka/</a>
</li>

<li>When NOT to use Apache Kafka?<br />
<a href="https://www.kai-waehner.de/blog/2022/01/04/when-not-to-use-apache-kafka/">https://www.kai-waehner.de/blog/2022/01/04/when-not-to-use-apache-kafka/</a>
</li>

<li>Microservices: The Rise Of Kafka<br />
<a href="https://movio.co/blog/microservices-rise-kafka/">https://movio.co/blog/microservices-rise-kafka/</a>
</li>

<li>Building a Microservices Ecosystem with Kafka Streams and KSQL<br />
<a href="https://www.confluent.io/blog/building-a-microservices-ecosystem-with-kafka-streams-and-ksql/">https://www.confluent.io/blog/building-a-microservices-ecosystem-with-kafka-streams-and-ksql/</a>
</li>

<li>An introduction to Apache Kafka and microservices communication<br />
<a href="https://medium.com/@ulymarins/an-introduction-to-apache-kafka-and-microservices-communication-bf0a0966d63">https://medium.com/@ulymarins/an-introduction-to-apache-kafka-and-microservices-communication-bf0a0966d63</a>
</li>

<li>kappa-architecture.com<br />
<a href="http://milinda.pathirage.org/kappa-architecture.com/">http://milinda.pathirage.org/kappa-architecture.com/</a>
</li>

<li>Questioning the Lambda Architecture<br />
<a href="https://www.oreilly.com/ideas/questioning-the-lambda-architecture">https://www.oreilly.com/ideas/questioning-the-lambda-architecture</a>
</li>

<li>Lambda architecture<br />
<a href="https://en.wikipedia.org/wiki/Lambda_architecture">https://en.wikipedia.org/wiki/Lambda_architecture</a>
</li>

<li>Kafka &ndash; ecosystem (Wiki)<br />
<a href="https://cwiki.apache.org/confluence/display/KAFKA/Ecosystem">https://cwiki.apache.org/confluence/display/KAFKA/Ecosystem</a>
</li>

<li>The Kafka Ecosystem - Kafka Core, Kafka Streams, Kafka Connect, Kafka REST Proxy, and the Schema Registry<br />
<a href="http://cloudurable.com/blog/kafka-ecosystem/index.html">http://cloudurable.com/blog/kafka-ecosystem/index.html</a>
</li>

<li>A Kafka Operator for Kubernetes<br />
<a href="https://github.com/krallistic/kafka-operator">https://github.com/krallistic/kafka-operator</a>
</li>

<li>Kafka Streams<br />
<a href="https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Streams">https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Streams</a>
</li>

<li>Kafka Streams<br />
<a href="http://kafka.apache.org/documentation/streams/">http://kafka.apache.org/documentation/streams/</a>
</li>

<li>Kafka Streams (FAQ)<br />
<a href="https://cwiki.apache.org/confluence/display/KAFKA/FAQ#FAQ-Streams">https://cwiki.apache.org/confluence/display/KAFKA/FAQ#FAQ-Streams</a>
</li>

<li>Event stream processing<br />
<a href="https://en.wikipedia.org/wiki/Event_stream_processing">https://en.wikipedia.org/wiki/Event_stream_processing</a>
</li>

<li>Part 1: Apache Kafka for beginners - What is Apache Kafka?<br />
<a href="https://www.cloudkarafka.com/blog/2016-11-30-part1-kafka-for-beginners-what-is-apache-kafka.html">https://www.cloudkarafka.com/blog/2016-11-30-part1-kafka-for-beginners-what-is-apache-kafka.html</a>
</li>

<li>What are some alternatives to Apache Kafka?<br />
<a href="https://www.quora.com/What-are-some-alternatives-to-Apache-Kafka">https://www.quora.com/What-are-some-alternatives-to-Apache-Kafka</a>
</li>

<li>What is the best alternative to Kafka?<br />
<a href="https://www.slant.co/options/961/alternatives/~kafka-alternatives">https://www.slant.co/options/961/alternatives/~kafka-alternatives</a>
</li>

<li>A super quick comparison between Kafka and Message Queues<br />
<a href="https://hackernoon.com/a-super-quick-comparison-between-kafka-and-message-queues-e69742d855a8?gi=e965191e72d0">https://hackernoon.com/a-super-quick-comparison-between-kafka-and-message-queues-e69742d855a8?gi=e965191e72d0</a>
</li>

<li>Kafka Queuing: Kafka as a Messaging System<br />
<a href="https://dzone.com/articles/kafka-queuing-kafka-as-a-messaging-system">https://dzone.com/articles/kafka-queuing-kafka-as-a-messaging-system</a>
</li>

<li>Apache Kafka Logs: A Comprehensive Guide<br />
<a href="https://hevodata.com/learn/apache-kafka-logs-a-comprehensive-guide/">https://hevodata.com/learn/apache-kafka-logs-a-comprehensive-guide/</a>
</li>

<li>Microservices â€“ Not a free lunch!<br />
<a href="http://highscalability.com/blog/2014/4/8/microservices-not-a-free-lunch.html">http://highscalability.com/blog/2014/4/8/microservices-not-a-free-lunch.html</a>
</li>

<li>Microservices, Monoliths, and NoOps<br />
<a href="http://blog.arungupta.me/microservices-monoliths-noops/">http://blog.arungupta.me/microservices-monoliths-noops/</a>
</li>

<li>Microservice Design Patterns<br />
<a href="http://blog.arungupta.me/microservice-design-patterns/">http://blog.arungupta.me/microservice-design-patterns/</a>
</li>

<li>REST vs Messaging for Microservices â€“ Which One is Best?<br />
<a href="https://solace.com/blog/experience-awesomeness-event-driven-microservices/">https://solace.com/blog/experience-awesomeness-event-driven-microservices/</a>
</li>

<li>Kappa Architecture Our Experience<br />
<a href="https://events.static.linuxÂ­found.org/sites/events/fiÂ­les/slides/ASPgems%20-%20Kappa%20Architecture.pdf">https://events.static.linuxÂ­found.org/sites/events/fiÂ­les/slides/ASPgems%20-%20Kappa%20Architecture.pdf</a>
</li>

<li>Apache Kafka Streams and Tables, the stream-table duality<br />
<a href="https://towardsdatascience.com/apache-kafka-streams-and-tables-the-stream-table-duality-ee904251a7e?gi=f22a29cd1854">https://towardsdatascience.com/apache-kafka-streams-and-tables-the-stream-table-duality-ee904251a7e?gi=f22a29cd1854</a>
</li>

<li>Configure Self-Managed Connectors<br />
<a href="https://docs.confluent.io/kafka-connectors/self-managed/configuring.html#configure-self-managed-connectors">https://docs.confluent.io/kafka-connectors/self-managed/configuring.html#configure-self-managed-connectors</a>
</li>

<li>Schema Evolution and Compatibility<br />
<a href="https://docs.confluent.io/platform/current/schema-registry/avro.html#schema-evolution-and-compatibility">https://docs.confluent.io/platform/current/schema-registry/avro.html#schema-evolution-and-compatibility</a>
</li>

<li>Configuring Key and Value Converters<br />
<a href="https://docs.confluent.io/kafka-connectors/self-managed/userguide.html#configuring-key-and-value-converters">https://docs.confluent.io/kafka-connectors/self-managed/userguide.html#configuring-key-and-value-converters</a>
</li>

<li>Introduction to Kafka Connectors<br />
<a href="https://www.baeldung.com/kafka-connectors-guide">https://www.baeldung.com/kafka-connectors-guide</a>
</li>

<li>Kafka CLI: command to list all consumer groups for a topic?<br />
<a href="https://stackoverflow.com/questions/63883999/kafka-cli-command-to-list-all-consumer-groups-for-a-topic">https://stackoverflow.com/questions/63883999/kafka-cli-command-to-list-all-consumer-groups-for-a-topic</a>
</li>

<li>Java Property File Processing<br />
<a href="https://www.w3resource.com/java-tutorial/java-propertyfile-processing.php">https://www.w3resource.com/java-tutorial/java-propertyfile-processing.php</a>
</li>

<li>Skipping bad records with the Kafka Connect JDBC sink connector<br />
<a href="https://rmoff.net/2019/10/15/skipping-bad-records-with-the-kafka-connect-jdbc-sink-connector/">https://rmoff.net/2019/10/15/skipping-bad-records-with-the-kafka-connect-jdbc-sink-connector/</a>
</li>

<li>Kafka Connect Deep Dive â€“ Error Handling and Dead Letter Queues<br />
<a href="https://www.confluent.io/blog/kafka-connect-deep-dive-error-handling-dead-letter-queues/">https://www.confluent.io/blog/kafka-connect-deep-dive-error-handling-dead-letter-queues/</a>
</li>

<li>Errors and Dead Letter Queues<br />
<a href="https://developer.confluent.io/learn-kafka/kafka-connect/error-handling-and-dead-letter-queues/">https://developer.confluent.io/learn-kafka/kafka-connect/error-handling-and-dead-letter-queues/</a>
</li>

<li>Confluent Cloud Dead Letter Queue<br />
<a href="https://docs.confluent.io/cloud/current/connectors/dead-letter-queue.html">https://docs.confluent.io/cloud/current/connectors/dead-letter-queue.html</a>
</li>

<li>Dead Letter Queues (DLQs) in Kafka<br />
<a href="https://medium.com/@sannidhi.s.t/dead-letter-queues-dlqs-in-kafka-afb4b6835309">https://medium.com/@sannidhi.s.t/dead-letter-queues-dlqs-in-kafka-afb4b6835309</a>
</li>

<li>Deserializer<br />
<a href="https://docs.confluent.io/platform/current/schema-registry/serdes-develop/serdes-json.html#json-schema-serializer-and-deserializer">https://docs.confluent.io/platform/current/schema-registry/serdes-develop/serdes-json.html#json-schema-serializer-and-deserializer</a>
</li>

<li>JSON, Kafka, and the need for schema<br />
<a href="https://mikemybytes.com/2022/07/11/json-kafka-and-the-need-for-schema/">https://mikemybytes.com/2022/07/11/json-kafka-and-the-need-for-schema/</a>
</li>

<li>Using Kafka Connect with Schema Registry<br />
<a href="https://docs.confluent.io/platform/current/schema-registry/connect.html">https://docs.confluent.io/platform/current/schema-registry/connect.html</a>
</li>

<li>ZpracovÃ¡nÃ­ dat reprezentovanÃ½ch ve formÃ¡tu JSON nÃ¡strojem jq<br />
<a href="https://www.root.cz/clanky/zpracovani-dat-reprezentovanych-ve-formatu-json-nastrojem-jq/">https://www.root.cz/clanky/zpracovani-dat-reprezentovanych-ve-formatu-json-nastrojem-jq/</a>
</li>

<li>RepositÃ¡Å™ projektu jq (GitHub)<br />
<a href="https://github.com/stedolan/jq">https://github.com/stedolan/jq</a>
</li>

<li>GitHub strÃ¡nky projektu jq<br />
<a href="https://stedolan.github.io/jq/">https://stedolan.github.io/jq/</a>
</li>

<li>5Â modern alternatives to essential Linux command-line tools<br />
<a href="https://opensource.com/arÂ­ticle/20/6/modern-linux-command-line-tools">https://opensource.com/arÂ­ticle/20/6/modern-linux-command-line-tools</a>
</li>

<li>NÃ¡vod kÂ nÃ¡stroji jq<br />
<a href="https://stedolan.github.iÂ­o/jq/tutorial/">https://stedolan.github.iÂ­o/jq/tutorial/</a>
</li>

<li>jq Manual (development version)<br />
<a href="https://stedolan.github.io/jq/manual/">https://stedolan.github.io/jq/manual/</a>
</li>

<li>Introducing JSON<br />
<a href="https://www.json.org/json-en.html">https://www.json.org/json-en.html</a>
</li>

<li>Understanding JSON schema<br />
<a href="https://json-schema.org/understanding-json-schema/index.html">https://json-schema.org/understanding-json-schema/index.html</a>
</li>

<li>JDBC Sink Connector for Confluent Platform<br />
<a href="https://docs.confluent.io/kafka-connectors/jdbc/current/sink-connector/overview.html#jdbc-sink-connector-for-cp">https://docs.confluent.io/kafka-connectors/jdbc/current/sink-connector/overview.html#jdbc-sink-connector-for-cp</a>
</li>

<li>JDBC Connector (Source and Sink)<br />
<a href="https://www.confluent.io/hub/confluentinc/kafka-connect-jdbc">https://www.confluent.io/hub/confluentinc/kafka-connect-jdbc</a>
</li>

<li>Introduction to Schema Registry in Kafka<br />
<a href="https://medium.com/slalom-technology/introduction-to-schema-registry-in-kafka-915ccf06b902">https://medium.com/slalom-technology/introduction-to-schema-registry-in-kafka-915ccf06b902</a>
</li>

<li>Understanding JSON Schema Compatibility<br />
<a href="https://yokota.blog/2021/03/29/understanding-json-schema-compatibility/">https://yokota.blog/2021/03/29/understanding-json-schema-compatibility/</a>
</li>

</ol>



<p></p><p></p>
<p><small>Autor: <a href="http://www.fit.vutbr.cz/~tisnovpa">Pavel TiÅ¡novskÃ½</a> &nbsp; 2024</small></p>
</body>
</html>

