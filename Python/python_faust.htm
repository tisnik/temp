<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
        "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html>
<head>
<title>Faust: platforma pro proudové zpracování dat v Pythonu</title>
<meta name="Author" content="Pavel Tisnovsky" />
<meta name="Generator" content="vim" />
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<style type="text/css">
         body {color:#000000; background:#ffffff;}
         h1  {font-family: arial, helvetica, sans-serif; color:#ffffff; background-color:#c00000; text-align:center; padding-left:1em}
         h2  {font-family: arial, helvetica, sans-serif; color:#ffffff; background-color:#0000c0; padding-left:1em; text-align:left}
         h3  {font-family: arial, helvetica, sans-serif; color:#000000; background-color:#c0c0c0; padding-left:1em; text-align:left}
         h4  {font-family: arial, helvetica, sans-serif; color:#000000; background-color:#e0e0e0; padding-left:1em; text-align:left}
         a   {font-family: arial, helvetica, sans-serif;}
         li  {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify; width:450px;}
         ol  {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify; width:450px;}
         ul  {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify; width:450px;}
         p   {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify;}
         pre {background:#e0e0e0}
</style>
</head>

<body>

<h1>Faust: platforma pro proudové zpracování dat v Pythonu</h1>

<h3>Pavel Tišnovský</h3>

<p></p>

<h1>Úvodník</h1>

<p>Dnes se seznámíme se zajímavě koncipovanou knihovnou nazvanou Faust. Jedná se o knihovnu zajišťující proudové zpracování dat, která je postavena nad Apache Kafkou. Nejedná se však o pouhou realizaci producentů a konzumentů, protože je možné používat lokální tabulky, asynchronní zpracování atd.</p>



<h2>Obsah</h2>

<p><a href="#k01">*** 1. Faust: platforma pro proudové zpracování dat v&nbsp;Pythonu</a></p>
<p><a href="#k02">*** 2. Problematická část &ndash; instalace knihovny Faust</a></p>
<p><a href="#k03">*** 3. Instalace, konfigurace a spuštění Apache Kafky</a></p>
<p><a href="#k04">*** 4. Otestování činnosti Apache Kafky</a></p>
<p><a href="#k05">*** 5. Klasický producent a konzument</a></p>
<p><a href="#k06">*** 6. Konzument realizovaný formou workera postavený na knihovně Faust</a></p>
<p><a href="#k07">*** 7. Spuštění workera</a></p>
<p><a href="#k08">*** 8. Producent posílající zprávy do většího množství témat</a></p>
<p><a href="#k09">*** 9. Konzumace zpráv workerem z&nbsp;většího množství témat</a></p>
<p><a href="#k10">*** 10. </a></p>
<p><a href="#k11">*** 11. </a></p>
<p><a href="#k12">*** 12. </a></p>
<p><a href="#k13">*** 13. </a></p>
<p><a href="#k14">*** 14. </a></p>
<p><a href="#k15">*** 15. </a></p>
<p><a href="#k16">*** 16. </a></p>
<p><a href="#k17">*** 17. </a></p>
<p><a href="#k18">*** 18. </a></p>
<p><a href="#k19">*** 19. Repositář s&nbsp;demonstračními příklady</a></p>
<p><a href="#k20">*** 20. Odkazy na Internetu</a></p>



<p><a name="k01"></a></p>
<h2 id="k01">1. Faust: platforma pro proudové zpracování dat v&nbsp;Pythonu</h2>



<p><a name="k02"></a></p>
<h2 id="k02">2. Problematická část &ndash; instalace knihovny Faust</h2>



<p><a name="k03"></a></p>
<h2 id="k03">3. Instalace, konfigurace a spuštění Apache Kafky</h2>

<p>Dnešní článek je zaměřen na ukázku některých možností nabízených knihovnou
Faust. Kromě instalace knihovny Faust tedy budeme potřebovat spustit (lokálně,
vzdáleně, či v&nbsp;kontejneru) Apache Kafku. Konkrétně nám bude postačovat
spuštění jediného Kafka clusteru, přičemž každý Kafka cluster se skládá
z&nbsp;jednoho běžícího Zookeepera a z&nbsp;jednoho brokera (tedy ze dvou
procesů, které mezi sebou komunikují po nakonfigurovaných portech).</p>

<p>V&nbsp;praktické části budeme brokera Apache Kafky i Zookeepera spouštět
lokálně (popř.&nbsp;z&nbsp;Dockeru či Podmana), takže je nejdříve nutné Apache
Kafku nainstalovat. Není to ve skutečnosti vůbec nic složitého. V&nbsp;případě,
že je na počítači nainstalováno JRE (běhové prostředí Javy), je instalace
Apache Kafky pro testovací účely triviální. V&nbsp;článku si ukážeme instalaci
verze 2.13-3.6.1, ovšem můžete si stáhnout i prakticky libovolnou novější či
některé starší verze (3.5.x nebo 3.6.x, ovšem dále popsaný postup by měl být
platný i pro ještě starší verze, v&nbsp;podstatě můžeme dojít až k&nbsp;verzi
2.4.0).  Tarball s&nbsp;instalací Apache Kafky lze získat z&nbsp;adresy <a
href="https://downloads.apache.org/kafka/3.6.1/kafka_2.13-3.6.1.tgz">https://downloads.apache.org/kafka/3.6.1/kafka_2.13-3.6.1.tgz</a>.</p>

<p>Stažení a rozbalení tarballu zajistí následující sekvence příkazů:</p>

<pre>
$ <strong>wget https://downloads.apache.org/kafka/3.6.1/kafka_2.13-3.6.1.tgz</strong>
$ <strong>tar xvfz kafka_2.13-3.6.1.tgz</strong>
$ <strong>cd kafka_2.13-3.6.1/</strong>
</pre>

<p>Po rozbalení staženého tarballu získáme adresář, v&nbsp;němž se nachází
všechny potřebné Java archivy (JAR), konfigurační soubory (v&nbsp;podadresáři
<strong>config</strong>) a několik pomocných skriptů (v&nbsp;podadresáři
<strong>bin</strong> a <strong>bin/windows</strong>). Pro spuštění Zookeepera a
brokerů je zapotřebí, jak jsme si již řekli v&nbsp;předchozím odstavci, mít
nainstalovánu JRE (Java Runtime Environment) a samozřejmě též nějaký shell
(BASH, cmd, ...).</p> 
 
<pre>
.
├── bin
│   └── windows
├── config
│   └── kraft
├── libs
├── licenses
└── site-docs
&nbsp;
7 directories
</pre>

<p>Mezi důležité soubory, které budeme používat v&nbsp;rámci dalších kapitol,
patří především skripty pro spouštění jednotlivých služeb. Tyto skripty jsou
uloženy v&nbsp;podadresáři <strong>bin</strong> (a pro Windows ještě
v&nbsp;dalším podadresáři <strong>windows</strong>):</p>

<table>
<tr><th>Skript</th><th>Stručný popis</th></tr>
<tr><td>bin/kafka-server-start.sh</td><td>spuštění brokera</td></tr>
<tr><td>bin/zookeeper-server-start.sh</td><td>spuštění Zookeepera</td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>bin/kafka-configs.sh</td><td>konfigurace brokerů</td></tr>
<tr><td>bin/kafka-topics.sh</td><td>konfigurace témat, zjištění informace o tématech atd.</td></tr>
<tr><td>bin/kafka-consumer-groups.sh</td><td>konfigurace popř.&nbsp;zjištění informací o skupinách konzumentů</td></tr>
</table>

<p><div class="rs-tip-major">Poznámka: většina výše uvedených skriptů byla
upravena i pro spuštění ve Windows. Tyto varianty naleznete v&nbsp;podadresáři
<strong>bin/windows</strong>.</div></p>

<p>Nyní si již Kafka cluster spustíme. Nejdříve je vždy nutné spustit
Zookeepera a teprve poté brokera či brokery. Pro sledování činnosti obou
procesů si můžete Zookeepera i brokera spustit v&nbsp;samostatném terminálu,
využít nástroj <strong>screen</strong> atd.</p>

<p>Spuštění Zookeepera:</p>

<pre>
$ <strong>cd ${kafka_dir}</strong>
$ <strong>bin/zookeeper-server-start.sh zookeeper.properties</strong>
</pre>

<p>Spuštění brokera:</p>

<pre>
$ <strong>cd ${kafka_dir}</strong>
$ <strong>bin/kafka-server-start.sh server.properties</strong>
</pre>



<p><a name="k04"></a></p>
<h2 id="k04">4. Otestování činnosti Apache Kafky</h2>

<p>Pro otestování, jestli je právě zprovozněný Kafka cluster skutečně funkční, si vytvoříme jednoduché producenty a konzumenty zpráv. Samozřejmě by bylo možné použít konzolového producenta a konzumenta, který je součástí instalace Apache Kafky, ovšem možnosti těchto nástrojů jsou ve skutečnosti velmi omezené a nebudou nám postačovat.</p>

<p>Před vytvořením producenta zpráv v&nbsp;Pythonu je nutné nainstalovat
knihovnu, která zabezpečuje připojení ke Kafce. Tuto knihovnu můžeme
nainstalovat pouze pro aktivního uživatele (což je nejjednodušší a nevyžaduje
to rootovská práva) s&nbsp;využitím příkazu <strong>pip</strong>, protože tato
knihovna je pochopitelně dostupná na <a
href="https://pypi.org/project/kafka/">PyPi</a>:</p>

<pre>
$ <strong>pip3 install --verbose kafka-python-ng</strong>
&nbsp;
Using pip 22.3.1 from /usr/lib/python3.11/site-packages/pip (python 3.11)
Defaulting to user installation because normal site-packages is not writeable
Collecting kafka-python-ng
  Downloading kafka_python_ng-2.2.2-py2.py3-none-any.whl (232 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 232.4/232.4 kB 1.5 MB/s eta 0:00:00
Installing collected packages: kafka-python-ng
Successfully installed kafka-python-ng-2.2.2
</pre>



<p><a name="k05"></a></p>
<h2 id="k05">5. Klasický producent a konzument</h2>

<p>Producent je představován instancí třídy <strong>KafkaProducer</strong>.
Konstruktoru této třídy se předává pouze seznam brokerů (v&nbsp;našem případě
jediný broker) a popř.&nbsp;i další nepovinné údaje. Mezi ně patří i handler,
který je zavolán před serializací každé zprávy. Prakticky každou zprávu je nutné serializovat, protože zprávy v&nbsp;Apache Kafce tvoří dále nestrukturovaná sekvence bajtů. Tudíž například i řetězce (Pythonovské řetězce) se explicitně převádí na hodnotu typu <strong>bytes</strong>, tj.&nbsp;na neměnitelnou (<i>immutable</i>) sekvenci bajtů. Převod řetězců do tuto sekvenci je realizován metodou <strong>encode</strong>, které se předá vyžadované kódování (tj.&nbsp;mapování mezi znaky na vstupu a jedním až více bajty ve výsledné sekvenci):</p>

<pre>
producer = KafkaProducer(
    bootstrap_servers=[server],
    value_serializer=lambda x: x.encode("utf-8")
)
</pre>

<p>Poslání zprávy se provádí metodou <strong>KafkaProducer.send</strong>.
Zpráva je, jak již víme z&nbsp;předchozího textu, uložena do zvoleného tématu
(<i>topic</i>) a skládá se z&nbsp;těla, popř.&nbsp;klíče a těla:</p>

<pre>
producer.send(topic, value=data)
</pre>

<p>V&nbsp;našem prvním demonstračním producentovi budou posílány řetězce, které se budou postupně měnit podle hodnoty čítače (<i>counter</i>). Mezi jednotlivé zprávy jsou vkládány časové pauzy o délce přibližně jedné sekundy:</p>

<pre>
for i in range(1000):
    print(i)
    message = f"Greeting #{i}"
    producer.send(topic, value=message)
    sleep(1)
</pre>

<p>Takto vypadá úplný zdrojový kód producenta zpráv:</p>

<pre>
#!/usr/bin/env python3
&nbsp;
from kafka import KafkaProducer
from time import sleep
&nbsp;
&nbsp;
server = "localhost:9092"
topic = "greetings"
&nbsp;
print("Connecting to Kafka")
producer = KafkaProducer(
    bootstrap_servers=[server],
    value_serializer=lambda x: x.encode("utf-8")
)
print("Connected to Kafka")
&nbsp;
for i in range(1000):
    print(i)
    message = f"Greeting #{i}"
    producer.send(topic, value=message)
    sleep(1)
</pre>

<p><div class="rs-tip-major">Poznámka: povšimněte si, že nám pro poslání zprávy
postačuje znát adresu brokera a jméno tématu. Konzument (popsaný níže) je
nepatrně složitější, protože je nutné specifikovat i jméno skupiny (<i>consumer
group</i>).</div></p>

<p>Implementace jednoduchého konzumenta zpráv v&nbsp;programovacím jazyku
Python je poměrně snadná a příliš se neliší od implementace konzumenta.
Nejdříve je nutné vytvořit instanci třídy <strong>KafkaConsumer</strong>,
přičemž se specifikuje téma (<i>topic</i>), skupina, seznam brokerů a nepovinně
též informace o tom, jakým způsobem se má nastavit offset první zpracované
zprávy. Většinou budeme potřebovat zpracovávat nejnovější (<i>earliest</i>)
zprávy, takže nastavení konzumenta může vypadat například následovně:</p>

<pre>
consumer = KafkaConsumer(
    topic, group_id=group_id,
    bootstrap_servers=[server],
    auto_offset_reset="earliest"
)
</pre>

<p>Samotná konzumace (čtení a zpracování) zpráv může probíhat v&nbsp;nekonečné
programové smyčce, přičemž každá zpráva je reprezentována objektem obsahujícím
téma, číslo oddílu, offset v&nbsp;rámci oddílu, klíč zprávy a samozřejmě též
obsah zprávy (klíč zprávy a tělo zprávy není žádný způsobem dekódováno
resp.&nbsp;deserializováno):</p>

<pre>
for message in consumer:
    print("%s:%d:%d: key=%s value=%s" % (
            message.topic,
            message.partition,
            message.offset,
            message.key,
            message.value,
        )
    )
</pre>

<p>A takto vypadá úplný zdrojový kód běžného konzumenta zpráv:</p>

<pre>
#!/usr/bin/env python3
&nbsp;
import sys
from kafka import KafkaConsumer
&nbsp;
server = "localhost:9092"
topic = "greetings"
group_id = "group1"
&nbsp;
print("Connecting to Kafka")
consumer = KafkaConsumer(
    topic, group_id=group_id,
    bootstrap_servers=[server],
    auto_offset_reset="earliest"
)
print("Connected to Kafka")
&nbsp;
try:
    for message in consumer:
        print(
            "%s:%d:%d: key=%s value=%s"
            % (
                message.topic,
                message.partition,
                message.offset,
                message.key,
                message.value,
            )
        )
except KeyboardInterrupt:
    sys.exit()
</pre>



<p><a name="k06"></a></p>
<h2 id="k06">6. Konzument realizovaný formou workera postavený na knihovně Faust</h2>

<pre>
import faust
&nbsp;
app = faust.App(
    "hello-world",
    broker="kafka://localhost:9092",
    value_serializer="raw",
)
&nbsp;
greetings_topic = app.topic("greetings")
&nbsp;
@app.agent(greetings_topic)
async def greet(greetings):
    async for greeting in greetings:
        print(greeting)
&nbsp;
&nbsp;
if __name__ == "__main__":
    app.main()
</pre>



<p><a name="k07"></a></p>
<h2 id="k07">7. Spuštění workera</h2>

<pre>
┌ƒaµS† v0.11.1.dev4+ga489db3b───────────────────────────────────┐
│ id          │ hello-world                                     │
│ transport   │ [URL('kafka://localhost:9092')]                 │
│ store       │ memory:                                         │
│ web         │ http://localhost:6066/                          │
│ log         │ -stderr- (warn)                                 │
│ pid         │ 1296383                                         │
│ hostname    │ ptisnovs.xxx.yyy.zzz                            │
│ platform    │ CPython 3.11.8 (Linux x86_64)                   │
│        +    │ Cython (GCC 13.2.1 20231011 (Red Hat 13.2.1-4)) │
│ drivers     │                                                 │
│   transport │ aiokafka=0.10.0                                 │
│   web       │ aiohttp=3.9.5                                   │
│ datadir     │ /tmp/ramdisk/faust/hello-world-data             │
│ appdir      │ /tmp/ramdisk/faust/hello-world-data/v1          │
└─────────────┴─────────────────────────────────────────────────┘
starting➢ 😊
</pre>

<p>Ihned po výpisu zprávy &bdquo;starting&ldquo; by měl worker začít se čtením
a zpracováváním zpráv, které jsou konzumovány z&nbsp;tématu
&bdquo;greetings&ldquo;. Informace o každé zkonzumované zprávě je vypisována do
terminálu:</p>

<pre>
[2024-04-20 18:03:39,583] [1296383] [WARNING] b'Greeting #0' 
[2024-04-20 18:03:39,583] [1296383] [WARNING] b'Greeting #1' 
[2024-04-20 18:03:39,584] [1296383] [WARNING] b'Greeting #2' 
[2024-04-20 18:03:39,584] [1296383] [WARNING] b'Greeting #3' 
[2024-04-20 18:03:39,584] [1296383] [WARNING] b'Greeting #4' 
</pre>

<p><div class="rs-tip-major">Poznámka: povšimněte si, že těla zpráv jsou reprezentována typem <i>bytes</i>, tj.&nbsp;neměnitelnou (<i>immutable</i>) sekvencí bajtů.</div></p>



<p><a name="k08"></a></p>
<h2 id="k08">8. Producent posílající zprávy do většího množství témat</h2>

<pre>
#!/usr/bin/env python3
&nbsp;
from kafka import KafkaProducer
from time import sleep
from json import dumps
&nbsp;
server = "localhost:9092"
topic1 = "greetings"
topic2 = "real_work"
&nbsp;
print("Connecting to Kafka")
producer = KafkaProducer(
    bootstrap_servers=[server],
    value_serializer=lambda x: dumps(x).encode("utf-8")
)
print("Connected to Kafka")
&nbsp;
for i in range(1000):
    print(i)
    message = f"Greeting #{i}"
    producer.send(topic1, value=message)
    sleep(1)
    work = f"Real work #{i*2}"
    producer.send(topic2, value=work)
    work = f"Real work #{i*2+1}"
    producer.send(topic2, value=work)
    sleep(1)
</pre>



<p><a name="k09"></a></p>
<h2 id="k09">9. Konzumace zpráv workerem z&nbsp;většího množství témat</h2>

<pre>
import faust
&nbsp;
app = faust.App(
    "hello-world",
    broker="kafka://localhost:9092",
    value_serializer="raw",
)
&nbsp;
greetings_topic = app.topic("greetings")
real_work_topic = app.topic("real_work")
&nbsp;
@app.agent(greetings_topic)
async def greet(greetings):
    async for greeting in greetings:
        print(f"Greeter: {greeting}")
&nbsp;
@app.agent(real_work_topic)
async def worker(works):
    async for work in works:
        print(f"Worker: {work}")
&nbsp;
&nbsp;
if __name__ == "__main__":
    app.main()
</pre>



<p><a name="k10"></a></p>
<h2 id="k10">10. Produkce a konzumace zpráv ve formátu JSON</h2>

<pre>
#!/usr/bin/env python3
&nbsp;
from kafka import KafkaProducer
from time import sleep
&nbsp;
server = "localhost:9092"
topic1 = "greetings"
topic2 = "real_work"
&nbsp;
print("Connecting to Kafka")
producer = KafkaProducer(
    bootstrap_servers=[server],
    value_serializer=lambda x: x.encode("utf-8")
)
print("Connected to Kafka")
&nbsp;
for i in range(1000):
    print(i)
    message = {"subject": "Greeting", "value": i}
    producer.send(topic1, value=message)
    sleep(1)
    work = {"subject": "Real work", "value": i*2}
    producer.send(topic2, value=work)
    work = {"subject": "Real work", "value": i*2+1}
    producer.send(topic2, value=work)
    sleep(1)
</pre>

<pre>
import faust
&nbsp;
app = faust.App(
    "hello-world",
    broker="kafka://localhost:9092",
    value_serializer="json",
)
&nbsp;
greetings_topic = app.topic("greetings")
real_work_topic = app.topic("real_work")
&nbsp;
@app.agent(greetings_topic)
async def greet(greetings):
    async for greeting in greetings:
        print(f"Greeter: {greeting}")
&nbsp;
@app.agent(real_work_topic)
async def worker(works):
    async for work in works:
        print(f"Worker: {work}")
&nbsp;
&nbsp;
if __name__ == "__main__":
    app.main()
</pre>



<p><a name="k11"></a></p>
<h2 id="k11">11. </h2>



<p><a name="k12"></a></p>
<h2 id="k12">12. </h2>



<p><a name="k13"></a></p>
<h2 id="k13">13. </h2>



<p><a name="k14"></a></p>
<h2 id="k14">14. </h2>



<p><a name="k15"></a></p>
<h2 id="k15">15. </h2>



<p><a name="k16"></a></p>
<h2 id="k16">16. </h2>



<p><a name="k17"></a></p>
<h2 id="k17">17. </h2>



<p><a name="k18"></a></p>
<h2 id="k18">18. </h2>



<p><a name="k19"></a></p>
<h2 id="k19">19. Repositář s&nbsp;demonstračními příklady</h2>

<p>Zdrojové kódy všech prozatím popsaných demonstračních příkladů určených pro
programovací jazyk Python 3 byly uloženy do Git repositáře dostupného na adrese
<a
href="https://github.com/tisnik/most-popular-python-libs">https://github.com/tisnik/most-popular-python-libs</a>:</p>

<table>
<tr><th> #</th><th>Demonstrační příklad</th><th>Stručný popis příkladu</th><th>Cesta</th></tr>
<tr><td> 1</td><td>greeting_producer.py</td><td>klasický producent zpráv vytvořený bez použití knihovny Faust</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faust/greeting_producer.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faust/greeting_producer.py</a></td></tr>
<tr><td> 2</td><td>greeting_consumer.py</td><td>klasický konzument zpráv vytvořený bez použití knihovny Faust</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faust/greeting_consumer.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faust/greeting_consumer.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td> 3</td><td>greeting_faust_consumer.py</td><td>worker definovaný s&nbsp;využitím knihovny Faust</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faust/greeting_faust_consumer.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faust/greeting_faust_consumer.py</a></td></tr>
<tr><td> 4</td><td>multi_producer_raw.py</td><td>producent zpráv do většího množství témat, zprávy jsou posílány jako sekvence bajtů</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faust/multi_producer_raw.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faust/multi_producer_raw.py</a></td></tr>
<tr><td> 5</td><td>greeting_worker_consumer_raw.py</td><td>dvojice workerů definovaných s&nbsp;využitím knihovny Faust pro zprávy posílané jako sekvence bajtů</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faust/greeting_worker_consumer_raw.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faust/greeting_worker_consumer_raw.py</a></td></tr>
<tr><td> 6</td><td>multi_producer_json.py</td><td>producent zpráv do většího množství témat, zprávy jsou serializovány do formátu JSON</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faust/multi_producer_json.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faust/multi_producer_json.py</a></td></tr>
<tr><td> 7</td><td>greeting_worker_consumer_json.py</td><td>dvojice workerů definovaných s&nbsp;využitím knihovny Faust pro zprávy ve formátu JSON</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faust/greeting_worker_consumer_json.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faust/greeting_worker_consumer_json.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td> 8</td><td></td><td></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faust/">https://github.com/tisnik/most-popular-python-libs/blob/master/faust/</a></td></tr>
<tr><td> 9</td><td></td><td></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faust/">https://github.com/tisnik/most-popular-python-libs/blob/master/faust/</a></td></tr>
<tr><td>10</td><td></td><td></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faust/">https://github.com/tisnik/most-popular-python-libs/blob/master/faust/</a></td></tr>
</table>



<p><a name="k20"></a></p>
<h2 id="k20">20. Odkazy na Internetu</h2>

<ol>

<li>ETL Batch Processing With Kafka?<br />
<a href="https://medium.com/swlh/etl-batch-processing-with-kafka-7f66f843e20d">https://medium.com/swlh/etl-batch-processing-with-kafka-7f66f843e20d</a>
</li>

<li>ETL with Kafka<br />
<a href="https://blog.codecentric.de/en/2018/03/etl-kafka/">https://blog.codecentric.de/en/2018/03/etl-kafka/</a>
</li>

<li>Building ETL Pipelines with Clojure and Transducers<br />
<a href="https://www.grammarly.com/blog/engineering/building-etl-pipelines-with-clojure-and-transducers/">https://www.grammarly.com/blog/engineering/building-etl-pipelines-with-clojure-and-transducers/</a>
</li>

<li>pipeline (možné použít pro ETL)<br />
<a href="https://clojuredocs.org/clojure.core.async/pipeline">https://clojuredocs.org/clojure.core.async/pipeline</a>
</li>

<li>On Track with Apache Kafka – Building a Streaming ETL Solution with Rail Data<br />
<a href="https://www.confluent.io/blog/build-streaming-etl-solutions-with-kafka-and-rail-data/">https://www.confluent.io/blog/build-streaming-etl-solutions-with-kafka-and-rail-data/</a>
</li>

<li>Kafka - Understanding Offset Commits<br />
<a href="https://www.logicbig.com/tutorials/misc/kafka/committing-offsets.html">https://www.logicbig.com/tutorials/misc/kafka/committing-offsets.html</a>
</li>

<li>fundingcircle/jackdaw (na Clojars)<br />
<a href="https://clojars.org/fundingcircle/jackdaw/versions/0.7.6">https://clojars.org/fundingcircle/jackdaw/versions/0.7.6</a>
</li>

<li>Dokumentace ke knihovně jackdaw<br />
<a href="https://cljdoc.org/d/fundingcircle/jackdaw/0.7.6/doc/readme">https://cljdoc.org/d/fundingcircle/jackdaw/0.7.6/doc/readme</a>
</li>

<li>Jackdaw AdminClient API<br />
<a href="https://cljdoc.org/d/fundingcircle/jackdaw/0.7.6/doc/jackdaw-adminclient-api">https://cljdoc.org/d/fundingcircle/jackdaw/0.7.6/doc/jackdaw-adminclient-api</a>
</li>

<li>Jackdaw Client API<br />
<a href="https://cljdoc.org/d/fundingcircle/jackdaw/0.7.6/doc/jackdaw-client-api">https://cljdoc.org/d/fundingcircle/jackdaw/0.7.6/doc/jackdaw-client-api</a>
</li>

<li>Kafka.clj<br />
<a href="https://github.com/helins-io/kafka.clj">https://github.com/helins-io/kafka.clj</a>
</li>

<li>Kafka mirroring (MirrorMaker)<br />
<a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=27846330">https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=27846330</a>
</li>

<li>Mastering Kafka migration with MirrorMaker 2<br />
<a href="https://developers.redhat.com/articles/2024/01/04/mastering-kafka-migration-mirrormaker-2">https://developers.redhat.com/articles/2024/01/04/mastering-kafka-migration-mirrormaker-2</a>
</li>

<li>Apache Kafka MirrorMaker 2 (MM2) Part 1: Theory<br />
<a href="https://www.instaclustr.com/blog/kafka-mirrormaker-2-theory/#h-2-replication-in-kafka">https://www.instaclustr.com/blog/kafka-mirrormaker-2-theory/#h-2-replication-in-kafka</a>
</li>

<li>Apache Kafka MirrorMaker 2 (MM2) Part 2: Practice<br />
<a href="https://www.instaclustr.com/blog/apache-kafka-mirrormaker-2-practice/">https://www.instaclustr.com/blog/apache-kafka-mirrormaker-2-practice/</a>
</li>

<li>Demystifying Kafka MirrorMaker 2: Use cases and architecture <br />
<a href="https://developers.redhat.com/articles/2023/11/13/demystifying-kafka-mirrormaker-2-use-cases-and-architecture#">https://developers.redhat.com/articles/2023/11/13/demystifying-kafka-mirrormaker-2-use-cases-and-architecture#</a>
</li>

<li>How to use Kafka MirrorMaker 2.0 in data migration, replication and the use-cases<br />
<a href="https://learn.microsoft.com/en-us/azure/hdinsight/kafka/kafka-mirrormaker-2-0-guide">https://learn.microsoft.com/en-us/azure/hdinsight/kafka/kafka-mirrormaker-2-0-guide</a>
</li>

<li>Release Notes - Kafka - Version 2.4.0<br />
<a href="https://archive.apache.org/dist/kafka/2.4.0/RELEASE_NOTES.html">https://archive.apache.org/dist/kafka/2.4.0/RELEASE_NOTES.html</a>
</li>

<li>Kafka Mirror Maker Best Practices<br />
<a href="https://community.cloudera.com/t5/Community-Articles/Kafka-Mirror-Maker-Best-Practices/ta-p/249269">https://community.cloudera.com/t5/Community-Articles/Kafka-Mirror-Maker-Best-Practices/ta-p/249269</a>
</li>

<li>Apache Kafka MirrorMaker 2 (MM2) Part 1: Theory<br />
<a href="https://www.instaclustr.com/blog/kafka-mirrormaker-2-theory/">https://www.instaclustr.com/blog/kafka-mirrormaker-2-theory/</a>
</li>

<li>Kcli: is a kafka read only command line browser.<br />
<a href="https://github.com/cswank/kcli">https://github.com/cswank/kcli</a>
</li>

<li>Kcli: a kafka command line browser<br />
<a href="https://go.libhunt.com/kcli-alternatives">https://go.libhunt.com/kcli-alternatives</a>
</li>

<li>Kafka Connect and Schemas<br />
<a href="https://rmoff.net/2020/01/22/kafka-connect-and-schemas/">https://rmoff.net/2020/01/22/kafka-connect-and-schemas/</a>
</li>

<li>JSON and schemas<br />
<a href="https://www.confluent.io/blog/kafka-connect-deep-dive-converters-serialization-explained/#json-schemas">https://www.confluent.io/blog/kafka-connect-deep-dive-converters-serialization-explained/#json-schemas</a>
</li>

<li>What, why, when to use Apache Kafka, with an example<br />
<a href="https://www.startdataengineering.com/post/what-why-and-how-apache-kafka/">https://www.startdataengineering.com/post/what-why-and-how-apache-kafka/</a>
</li>

<li>When NOT to use Apache Kafka?<br />
<a href="https://www.kai-waehner.de/blog/2022/01/04/when-not-to-use-apache-kafka/">https://www.kai-waehner.de/blog/2022/01/04/when-not-to-use-apache-kafka/</a>
</li>

<li>Microservices: The Rise Of Kafka<br />
<a href="https://movio.co/blog/microservices-rise-kafka/">https://movio.co/blog/microservices-rise-kafka/</a>
</li>

<li>Building a Microservices Ecosystem with Kafka Streams and KSQL<br />
<a href="https://www.confluent.io/blog/building-a-microservices-ecosystem-with-kafka-streams-and-ksql/">https://www.confluent.io/blog/building-a-microservices-ecosystem-with-kafka-streams-and-ksql/</a>
</li>

<li>An introduction to Apache Kafka and microservices communication<br />
<a href="https://medium.com/@ulymarins/an-introduction-to-apache-kafka-and-microservices-communication-bf0a0966d63">https://medium.com/@ulymarins/an-introduction-to-apache-kafka-and-microservices-communication-bf0a0966d63</a>
</li>

<li>kappa-architecture.com<br />
<a href="http://milinda.pathirage.org/kappa-architecture.com/">http://milinda.pathirage.org/kappa-architecture.com/</a>
</li>

<li>Questioning the Lambda Architecture<br />
<a href="https://www.oreilly.com/ideas/questioning-the-lambda-architecture">https://www.oreilly.com/ideas/questioning-the-lambda-architecture</a>
</li>

<li>Lambda architecture<br />
<a href="https://en.wikipedia.org/wiki/Lambda_architecture">https://en.wikipedia.org/wiki/Lambda_architecture</a>
</li>

<li>Kafka &ndash; ecosystem (Wiki)<br />
<a href="https://cwiki.apache.org/confluence/display/KAFKA/Ecosystem">https://cwiki.apache.org/confluence/display/KAFKA/Ecosystem</a>
</li>

<li>The Kafka Ecosystem - Kafka Core, Kafka Streams, Kafka Connect, Kafka REST Proxy, and the Schema Registry<br />
<a href="http://cloudurable.com/blog/kafka-ecosystem/index.html">http://cloudurable.com/blog/kafka-ecosystem/index.html</a>
</li>

<li>A Kafka Operator for Kubernetes<br />
<a href="https://github.com/krallistic/kafka-operator">https://github.com/krallistic/kafka-operator</a>
</li>

<li>Kafka Streams<br />
<a href="https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Streams">https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Streams</a>
</li>

<li>Kafka Streams<br />
<a href="http://kafka.apache.org/documentation/streams/">http://kafka.apache.org/documentation/streams/</a>
</li>

<li>Kafka Streams (FAQ)<br />
<a href="https://cwiki.apache.org/confluence/display/KAFKA/FAQ#FAQ-Streams">https://cwiki.apache.org/confluence/display/KAFKA/FAQ#FAQ-Streams</a>
</li>

<li>Event stream processing<br />
<a href="https://en.wikipedia.org/wiki/Event_stream_processing">https://en.wikipedia.org/wiki/Event_stream_processing</a>
</li>

<li>Part 1: Apache Kafka for beginners - What is Apache Kafka?<br />
<a href="https://www.cloudkarafka.com/blog/2016-11-30-part1-kafka-for-beginners-what-is-apache-kafka.html">https://www.cloudkarafka.com/blog/2016-11-30-part1-kafka-for-beginners-what-is-apache-kafka.html</a>
</li>

<li>What are some alternatives to Apache Kafka?<br />
<a href="https://www.quora.com/What-are-some-alternatives-to-Apache-Kafka">https://www.quora.com/What-are-some-alternatives-to-Apache-Kafka</a>
</li>

<li>What is the best alternative to Kafka?<br />
<a href="https://www.slant.co/options/961/alternatives/~kafka-alternatives">https://www.slant.co/options/961/alternatives/~kafka-alternatives</a>
</li>

<li>A super quick comparison between Kafka and Message Queues<br />
<a href="https://hackernoon.com/a-super-quick-comparison-between-kafka-and-message-queues-e69742d855a8?gi=e965191e72d0">https://hackernoon.com/a-super-quick-comparison-between-kafka-and-message-queues-e69742d855a8?gi=e965191e72d0</a>
</li>

<li>Kafka Queuing: Kafka as a Messaging System<br />
<a href="https://dzone.com/articles/kafka-queuing-kafka-as-a-messaging-system">https://dzone.com/articles/kafka-queuing-kafka-as-a-messaging-system</a>
</li>

<li>Apache Kafka Logs: A Comprehensive Guide<br />
<a href="https://hevodata.com/learn/apache-kafka-logs-a-comprehensive-guide/">https://hevodata.com/learn/apache-kafka-logs-a-comprehensive-guide/</a>
</li>

<li>Microservices – Not a free lunch!<br />
<a href="http://highscalability.com/blog/2014/4/8/microservices-not-a-free-lunch.html">http://highscalability.com/blog/2014/4/8/microservices-not-a-free-lunch.html</a>
</li>

<li>Microservices, Monoliths, and NoOps<br />
<a href="http://blog.arungupta.me/microservices-monoliths-noops/">http://blog.arungupta.me/microservices-monoliths-noops/</a>
</li>

<li>Microservice Design Patterns<br />
<a href="http://blog.arungupta.me/microservice-design-patterns/">http://blog.arungupta.me/microservice-design-patterns/</a>
</li>

<li>REST vs Messaging for Microservices – Which One is Best?<br />
<a href="https://solace.com/blog/experience-awesomeness-event-driven-microservices/">https://solace.com/blog/experience-awesomeness-event-driven-microservices/</a>
</li>

<li>Kappa Architecture Our Experience<br />
<a href="https://events.static.linux­found.org/sites/events/fi­les/slides/ASPgems%20-%20Kappa%20Architecture.pdf">https://events.static.linux­found.org/sites/events/fi­les/slides/ASPgems%20-%20Kappa%20Architecture.pdf</a>
</li>

<li>Apache Kafka Streams and Tables, the stream-table duality<br />
<a href="https://towardsdatascience.com/apache-kafka-streams-and-tables-the-stream-table-duality-ee904251a7e?gi=f22a29cd1854">https://towardsdatascience.com/apache-kafka-streams-and-tables-the-stream-table-duality-ee904251a7e?gi=f22a29cd1854</a>
</li>

<li>Configure Self-Managed Connectors<br />
<a href="https://docs.confluent.io/kafka-connectors/self-managed/configuring.html#configure-self-managed-connectors">https://docs.confluent.io/kafka-connectors/self-managed/configuring.html#configure-self-managed-connectors</a>
</li>

<li>Schema Evolution and Compatibility<br />
<a href="https://docs.confluent.io/platform/current/schema-registry/avro.html#schema-evolution-and-compatibility">https://docs.confluent.io/platform/current/schema-registry/avro.html#schema-evolution-and-compatibility</a>
</li>

<li>Configuring Key and Value Converters<br />
<a href="https://docs.confluent.io/kafka-connectors/self-managed/userguide.html#configuring-key-and-value-converters">https://docs.confluent.io/kafka-connectors/self-managed/userguide.html#configuring-key-and-value-converters</a>
</li>

<li>Introduction to Kafka Connectors<br />
<a href="https://www.baeldung.com/kafka-connectors-guide">https://www.baeldung.com/kafka-connectors-guide</a>
</li>

<li>Kafka CLI: command to list all consumer groups for a topic?<br />
<a href="https://stackoverflow.com/questions/63883999/kafka-cli-command-to-list-all-consumer-groups-for-a-topic">https://stackoverflow.com/questions/63883999/kafka-cli-command-to-list-all-consumer-groups-for-a-topic</a>
</li>

<li>Java Property File Processing<br />
<a href="https://www.w3resource.com/java-tutorial/java-propertyfile-processing.php">https://www.w3resource.com/java-tutorial/java-propertyfile-processing.php</a>
</li>

<li>Skipping bad records with the Kafka Connect JDBC sink connector<br />
<a href="https://rmoff.net/2019/10/15/skipping-bad-records-with-the-kafka-connect-jdbc-sink-connector/">https://rmoff.net/2019/10/15/skipping-bad-records-with-the-kafka-connect-jdbc-sink-connector/</a>
</li>

<li>Kafka Connect Deep Dive – Error Handling and Dead Letter Queues<br />
<a href="https://www.confluent.io/blog/kafka-connect-deep-dive-error-handling-dead-letter-queues/">https://www.confluent.io/blog/kafka-connect-deep-dive-error-handling-dead-letter-queues/</a>
</li>

<li>Errors and Dead Letter Queues<br />
<a href="https://developer.confluent.io/learn-kafka/kafka-connect/error-handling-and-dead-letter-queues/">https://developer.confluent.io/learn-kafka/kafka-connect/error-handling-and-dead-letter-queues/</a>
</li>

<li>Confluent Cloud Dead Letter Queue<br />
<a href="https://docs.confluent.io/cloud/current/connectors/dead-letter-queue.html">https://docs.confluent.io/cloud/current/connectors/dead-letter-queue.html</a>
</li>

<li>Dead Letter Queues (DLQs) in Kafka<br />
<a href="https://medium.com/@sannidhi.s.t/dead-letter-queues-dlqs-in-kafka-afb4b6835309">https://medium.com/@sannidhi.s.t/dead-letter-queues-dlqs-in-kafka-afb4b6835309</a>
</li>

<li>Deserializer<br />
<a href="https://docs.confluent.io/platform/current/schema-registry/serdes-develop/serdes-json.html#json-schema-serializer-and-deserializer">https://docs.confluent.io/platform/current/schema-registry/serdes-develop/serdes-json.html#json-schema-serializer-and-deserializer</a>
</li>

<li>JSON, Kafka, and the need for schema<br />
<a href="https://mikemybytes.com/2022/07/11/json-kafka-and-the-need-for-schema/">https://mikemybytes.com/2022/07/11/json-kafka-and-the-need-for-schema/</a>
</li>

<li>Using Kafka Connect with Schema Registry<br />
<a href="https://docs.confluent.io/platform/current/schema-registry/connect.html">https://docs.confluent.io/platform/current/schema-registry/connect.html</a>
</li>

<li>Zpracování dat reprezentovaných ve formátu JSON nástrojem jq<br />
<a href="https://www.root.cz/clanky/zpracovani-dat-reprezentovanych-ve-formatu-json-nastrojem-jq/">https://www.root.cz/clanky/zpracovani-dat-reprezentovanych-ve-formatu-json-nastrojem-jq/</a>
</li>

<li>Repositář projektu jq (GitHub)<br />
<a href="https://github.com/stedolan/jq">https://github.com/stedolan/jq</a>
</li>

<li>GitHub stránky projektu jq<br />
<a href="https://stedolan.github.io/jq/">https://stedolan.github.io/jq/</a>
</li>

<li>5 modern alternatives to essential Linux command-line tools<br />
<a href="https://opensource.com/ar­ticle/20/6/modern-linux-command-line-tools">https://opensource.com/ar­ticle/20/6/modern-linux-command-line-tools</a>
</li>

<li>Návod k nástroji jq<br />
<a href="https://stedolan.github.i­o/jq/tutorial/">https://stedolan.github.i­o/jq/tutorial/</a>
</li>

<li>jq Manual (development version)<br />
<a href="https://stedolan.github.io/jq/manual/">https://stedolan.github.io/jq/manual/</a>
</li>

<li>Introducing JSON<br />
<a href="https://www.json.org/json-en.html">https://www.json.org/json-en.html</a>
</li>

<li>Understanding JSON schema<br />
<a href="https://json-schema.org/understanding-json-schema/index.html">https://json-schema.org/understanding-json-schema/index.html</a>
</li>

<li>JDBC Sink Connector for Confluent Platform<br />
<a href="https://docs.confluent.io/kafka-connectors/jdbc/current/sink-connector/overview.html#jdbc-sink-connector-for-cp">https://docs.confluent.io/kafka-connectors/jdbc/current/sink-connector/overview.html#jdbc-sink-connector-for-cp</a>
</li>

<li>JDBC Connector (Source and Sink)<br />
<a href="https://www.confluent.io/hub/confluentinc/kafka-connect-jdbc">https://www.confluent.io/hub/confluentinc/kafka-connect-jdbc</a>
</li>

<li>Introduction to Schema Registry in Kafka<br />
<a href="https://medium.com/slalom-technology/introduction-to-schema-registry-in-kafka-915ccf06b902">https://medium.com/slalom-technology/introduction-to-schema-registry-in-kafka-915ccf06b902</a>
</li>

<li>Understanding JSON Schema Compatibility<br />
<a href="https://yokota.blog/2021/03/29/understanding-json-schema-compatibility/">https://yokota.blog/2021/03/29/understanding-json-schema-compatibility/</a>
</li>

</ol>



<p></p><p></p>
<p><small>Autor: <a href="http://www.fit.vutbr.cz/~tisnovpa">Pavel Tišnovský</a> &nbsp; 2024</small></p>
</body>
</html>

