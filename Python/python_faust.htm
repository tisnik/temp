<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
        "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html>
<head>
<title>Faust: platforma pro proudové zpracování dat v Pythonu</title>
<meta name="Author" content="Pavel Tisnovsky" />
<meta name="Generator" content="vim" />
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<style type="text/css">
         body {color:#000000; background:#ffffff;}
         h1  {font-family: arial, helvetica, sans-serif; color:#ffffff; background-color:#c00000; text-align:center; padding-left:1em}
         h2  {font-family: arial, helvetica, sans-serif; color:#ffffff; background-color:#0000c0; padding-left:1em; text-align:left}
         h3  {font-family: arial, helvetica, sans-serif; color:#000000; background-color:#c0c0c0; padding-left:1em; text-align:left}
         h4  {font-family: arial, helvetica, sans-serif; color:#000000; background-color:#e0e0e0; padding-left:1em; text-align:left}
         a   {font-family: arial, helvetica, sans-serif;}
         li  {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify; width:450px;}
         ol  {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify; width:450px;}
         ul  {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify; width:450px;}
         p   {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify;}
         pre {background:#e0e0e0}
</style>
</head>

<body>

<h1>Faust: platforma pro proudové zpracování dat v Pythonu</h1>

<h3>Pavel Tišnovský</h3>

<p></p>

<h1>Úvodník</h1>

<p>Dnes se seznámíme se zajímavě koncipovanou knihovnou nazvanou Faust. Jedná se o knihovnu zajišťující proudové zpracování dat, která je postavena nad Apache Kafkou. Nejedná se však o pouhou realizaci producentů a konzumentů, protože je možné používat lokální tabulky, asynchronní zpracování atd.</p>



<h2>Obsah</h2>

<p><a href="#k01">*** 1. Faust: platforma pro proudové zpracování dat v&nbsp;Pythonu</a></p>
<p><a href="#k02">*** 2. Problematická část &ndash; instalace knihovny Faust</a></p>
<p><a href="#k03">3. Instalace, konfigurace a spuštění Apache Kafky</a></p>
<p><a href="#k04">4. Otestování činnosti Apache Kafky</a></p>
<p><a href="#k05">5. Klasický producent posílající zprávy do Kafky</a></p>
<p><a href="#k06">6. Klasický konzument, který čte zprávy ze specifikovaného tématu</a></p>
<p><a href="#k07">*** 7. Spuštění producenta a konzumenta</a></p>
<p><a href="#k08">8. Konzument realizovaný formou workera postavený na knihovně Faust</a></p>
<p><a href="#k09">9. Spuštění workera</a></p>
<p><a href="#k10">10. Producent posílající zprávy do většího množství témat</a></p>
<p><a href="#k11">11. Konzumace zpráv workerem z&nbsp;většího množství témat</a></p>
<p><a href="#k12">*** 12. Produkce zpráv ve formátu JSON</a></p>
<p><a href="#k13">*** 13. Worker přijímající zprávy ve formátu JSON</a></p>
<p><a href="#k14">*** 14. Využití modelů</a></p>
<p><a href="#k15">*** 15. Producent založený na knihovně Faust využívající model</a></p>
<p><a href="#k16">*** 16. Kombinace producenta a konzumenta &ndash; reálná síla knihovny Faust</a></p>
<p><a href="#k17">*** 17. Spuštění kombinace workerů s&nbsp;producenty a konzumenty</a></p>
<p><a href="#k18">*** 18. Obsah navazujícího článku</a></p>
<p><a href="#k19">19. Repositář s&nbsp;demonstračními příklady</a></p>
<p><a href="#k20">20. Odkazy na Internetu</a></p>



<p><a name="k01"></a></p>
<h2 id="k01">1. Faust: platforma pro proudové zpracování dat v&nbsp;Pythonu</h2>



<p><a name="k02"></a></p>
<h2 id="k02">2. Problematická část &ndash; instalace knihovny Faust</h2>

<p>Teoreticky je možné knihovnu Faust nainstalovat naprosto stejným způsobem,
jako jakoukoli jinou knihovnu dostupnou přes Pypi:</p>

<pre>
$ <strong>pip install --user faust</strong>
</pre>

<p>To však bude bez problémů fungovat pouze pro starší verze Pythonu
(CPythonu).  V&nbsp;případě novějších verzí Pythonu, v&nbsp;nichž došlo ke
změně balíčků pro asynchronní běh aplikací, nebude možné takto nainstalovaný
Faust použít. Musíme se tedy uchýlit k&nbsp;jiné formě spuštění (což ani
zdaleka není ideální situace!).</p>



<p><a name="k03"></a></p>
<h2 id="k03">3. Instalace, konfigurace a spuštění Apache Kafky</h2>

<p>Dnešní článek je zaměřen na ukázku některých možností nabízených knihovnou
Faust, která interně využívá známý projekt Apache Kafka. To tedy znamená, že
kromě instalace knihovny Faust budeme navíc potřebovat spustit (lokálně,
vzdáleně, či v&nbsp;kontejneru) i Apache Kafku. Konkrétně nám bude postačovat
spuštění jediného Kafka clusteru, přičemž se každý Kafka cluster skládá
z&nbsp;jednoho běžícího Zookeepera a z&nbsp;jednoho brokera (tedy ze dvou
procesů, které mezi sebou komunikují po nakonfigurovaných portech).</p>

<p>V&nbsp;praktické části budeme brokera Apache Kafky i Zookeepera spouštět
lokálně (popř.&nbsp;z&nbsp;Dockeru či Podmana), takže je nejdříve nutné Apache
Kafku nainstalovat. Není to ve skutečnosti vůbec nic složitého. V&nbsp;případě,
že je na počítači nainstalováno JRE (běhové prostředí Javy), je instalace
Apache Kafky pro testovací účely triviální. V&nbsp;článku si ukážeme instalaci
verze 2.13-3.6.1, ovšem můžete si stáhnout i prakticky libovolnou novější či
některé starší verze (3.5.x nebo 3.6.x, ovšem dále popsaný postup by měl být
platný i pro ještě starší verze, v&nbsp;podstatě můžeme dojít až k&nbsp;verzi
2.4.0 a vše by mělo být funkční). Tarball s&nbsp;instalací Apache Kafky je
možné získat z&nbsp;adresy <a
href="https://downloads.apache.org/kafka/3.6.1/kafka_2.13-3.6.1.tgz">https://downloads.apache.org/kafka/3.6.1/kafka_2.13-3.6.1.tgz</a>.</p>

<p>Stažení a rozbalení tarballu s&nbsp;Apache Kafkou zajistí následující
sekvence příkazů:</p>

<pre>
$ <strong>wget https://downloads.apache.org/kafka/3.6.1/kafka_2.13-3.6.1.tgz</strong>
$ <strong>tar xvfz kafka_2.13-3.6.1.tgz</strong>
$ <strong>cd kafka_2.13-3.6.1/</strong>
</pre>

<p>Po rozbalení staženého tarballu získáme adresář, v&nbsp;němž se nachází
všechny potřebné Java archivy (JAR), konfigurační soubory (v&nbsp;podadresáři
<strong>config</strong>) a několik pomocných skriptů (v&nbsp;podadresáři
<strong>bin</strong> a <strong>bin/windows</strong>). Pro spuštění Zookeepera a
brokerů je zapotřebí, jak jsme si již řekli v&nbsp;předchozím odstavci, mít
nainstalovánu JRE (Java Runtime Environment) a samozřejmě též nějaký shell
(BASH, cmd, ...).</p> 
 
<pre>
.
├── bin
│   └── windows
├── config
│   └── kraft
├── libs
├── licenses
└── site-docs
&nbsp;
7 directories
</pre>

<p>Mezi důležité soubory, které budeme používat v&nbsp;rámci dalších kapitol
pro ukázku možností knihovny Faust, patří především skripty pro spouštění
jednotlivých služeb. Tyto skripty jsou uloženy v&nbsp;podadresáři
<strong>bin</strong> (a pro Windows ještě v&nbsp;dalším podadresáři
<strong>windows</strong>):</p>

<table>
<tr><th>Skript</th><th>Stručný popis</th></tr>
<tr><td>bin/kafka-server-start.sh</td><td>spuštění brokera</td></tr>
<tr><td>bin/zookeeper-server-start.sh</td><td>spuštění Zookeepera</td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>bin/kafka-configs.sh</td><td>konfigurace brokerů</td></tr>
<tr><td>bin/kafka-topics.sh</td><td>konfigurace témat, zjištění informace o tématech atd.</td></tr>
<tr><td>bin/kafka-consumer-groups.sh</td><td>konfigurace popř.&nbsp;zjištění informací o skupinách konzumentů</td></tr>
</table>

<p><div class="rs-tip-major">Poznámka: většina výše uvedených skriptů byla
upravena i pro spuštění ve Windows. Tyto varianty naleznete v&nbsp;podadresáři
<strong>bin/windows</strong> a namísto koncovky <strong>.sh</strong> mají
koncovku <strong>bat</strong> (<i>batch</i>).</div></p>

<p>Nyní již máme všechny potřebné nástroje k&nbsp;dispozici, takže Kafka
cluster spustíme. Nejdříve je vždy nutné spustit Zookeepera a teprve poté
brokera či brokery. Pro sledování činnosti obou procesů si můžete Zookeepera i
brokera spustit v&nbsp;samostatném terminálu, využít nástroj
<strong>screen</strong> atd.</p>

<p>Spuštění Zookeepera:</p>

<pre>
$ <strong>cd ${kafka_dir}</strong>
$ <strong>bin/zookeeper-server-start.sh config/zookeeper.properties</strong>
</pre>

<p>Spuštění brokera:</p>

<pre>
$ <strong>cd ${kafka_dir}</strong>
$ <strong>bin/kafka-server-start.sh config/server.properties</strong>
</pre>

<p><div class="rs-tip-major">Poznámka: povšimněte si, že používáme konfigurační
soubory dodávané přímo s&nbsp;Apache Kafkou, tedy soubory
<strong>config/zookeeper.properties</strong> a
<strong>config/server.properties</strong>. V&nbsp;případě potřeby si můžete
tyto soubory zkopírovat, kopie upravit a ty dále použít.</div></p>



<p><a name="k04"></a></p>
<h2 id="k04">4. Otestování činnosti Apache Kafky</h2>

<p>Pro otestování, jestli je právě zprovozněný Kafka cluster skutečně funkční,
si vytvoříme jednoduché producenty a konzumenty zpráv. Použijeme přitom
programovací jazyk Python, protože i demonstrační příklady využívající knihovnu
<i>Faust</i> budou naprogramovány v&nbsp;Pythonu. Samozřejmě by bylo možné
použít konzolového producenta a konzumenta, který je součástí instalace Apache
Kafky, ovšem možnosti těchto nástrojů jsou ve skutečnosti velmi omezené a
nebudou nám postačovat v&nbsp;případě, že budeme chtít produkovat či naopak
konzumovat složitější datové struktury a objekty.</p>

<p>Před vytvořením producenta zpráv v&nbsp;Pythonu je nutné nainstalovat
knihovnu, která zabezpečuje připojení ke clusteru Apache Kafky. Takových
knihoven dnes existuje několik. Nejjednodušší z&nbsp;těchto knihoven (bez
dalších závislostí atd.) je knihovna nazvaná <strong>kafka-python</strong>.
Vzhledem k&nbsp;tomu, že autoři této knihovny nevydávají časté updaty, vznikl
její fork nazvaný jednoduše <strong>kafka-python-ng</strong>; a právě tento
fork dnes použijeme. Tuto knihovnu můžeme nainstalovat pouze pro aktivního
uživatele (což je nejjednodušší a nevyžaduje to rootovská práva)
s&nbsp;využitím příkazu <strong>pip</strong>, protože tato knihovna je
pochopitelně dostupná na <a
href="https://pypi.org/project/kafka/">PyPi</a>:</p>

<pre>
$ <strong>pip3 install --verbose kafka-python-ng</strong>
&nbsp;
Using pip 22.3.1 from /usr/lib/python3.11/site-packages/pip (python 3.11)
Defaulting to user installation because normal site-packages is not writeable
Collecting kafka-python-ng
  Downloading kafka_python_ng-2.2.2-py2.py3-none-any.whl (232 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 232.4/232.4 kB 1.5 MB/s eta 0:00:00
Installing collected packages: kafka-python-ng
Successfully installed kafka-python-ng-2.2.2
</pre>

<p><div class="rs-tip-major">Poznámka: povšimněte si, že tato knihovna skutečně
nemá žádné tranzitivní závislosti.</div></p>



<p><a name="k05"></a></p>
<h2 id="k05">5. Klasický producent posílající zprávy do Kafky</h2>

<p>Producent, tedy aplikace posílající zprávy do Apache Kafky, je představován
instancí třídy <strong>KafkaProducer</strong>.  Konstruktoru této třídy se
předává pouze seznam brokerů (v&nbsp;našem případě jediný broker, protože náš
Kafka cluster obsahuje jediného brokera) a popř.&nbsp;i další nepovinné údaje.
Mezi ně patří i takzvaný <i>handler</i>, který je zavolán před serializací
každé zprávy.</p>

<p>Prakticky každou zprávu je ve skutečnosti nutné serializovat, protože zprávy
v&nbsp;Apache Kafce tvoří dále nestrukturovaná sekvence bajtů (jedinou datovou
strukturou, kterou není potřeba serializovat, je typ <strong>bytes</strong>).
Tudíž například i řetězce (přesněji řečeno Pythonovské řetězce) se explicitně
převádí na hodnotu typu <strong>bytes</strong>, tj.&nbsp;na neměnitelnou
(<i>immutable</i>) sekvenci bajtů. Převod řetězců do tuto sekvenci je
realizován metodou <strong>encode</strong>, které se předá vyžadované kódování,
tj.&nbsp;mapování mezi znaky na vstupu a jedním až více bajty ve výsledné
sekvenci. V&nbsp;našem skriptu použijeme dnes standardní kódování UTF-8:</p>

<pre>
producer = <strong>KafkaProducer</strong>(
    bootstrap_servers=[server],
    value_serializer=lambda x: x.encode("utf-8")
)
</pre>

<p>Poslání zprávy se provádí metodou <strong>KafkaProducer.send</strong>.
Zpráva je, jak již víme z&nbsp;předchozího textu, uložena do zvoleného tématu
(<i>topic</i>) a skládá se z&nbsp;těla, popř.&nbsp;klíče a těla:</p>

<pre>
producer.send(topic, value=data)
</pre>

<p>V&nbsp;našem prvním demonstračním producentovi budou posílány řetězce, které
se budou postupně měnit podle hodnoty čítače (<i>counter</i>). Mezi jednotlivé
zprávy jsou vkládány časové pauzy o délce přibližně jedné sekundy:</p>

<pre>
for i in range(1000):
    print(i)
    message = f"Greeting #{i}"
    producer.send(topic, value=message)
    sleep(1)
</pre>

<p><div class="rs-tip-major">Poznámka: aby byla zpráva skutečně poslána, je
vhodné zavolat metodu <strong>flush</strong>. V&nbsp;tomto konkrétním skriptu
to neděláme, protože po naplnění bufferu nebo uplynutí určité doby je zpráva
poslána i bez tohoto volání. Později ovšem budeme muset <strong>flush</strong>
explicitně volat.</div></p>

<p>Takto vypadá úplný zdrojový kód producenta zpráv, který naleznete na adrese
<a
href="https://github.com/tisnik/most-popular-python-libs/blob/master/faust/greeting_producer.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faust/greeting_producer.py</a>:</p>

<pre>
#!/usr/bin/env python3
&nbsp;
from kafka import KafkaProducer
from time import sleep
&nbsp;
&nbsp;
server = "localhost:9092"
topic = "greetings"
&nbsp;
print("Connecting to Kafka")
producer = KafkaProducer(
    bootstrap_servers=[server],
    value_serializer=lambda x: x.encode("utf-8")
)
print("Connected to Kafka")
&nbsp;
for i in range(1000):
    print(i)
    message = f"Greeting #{i}"
    producer.send(topic, value=message)
    sleep(1)
</pre>

<p><div class="rs-tip-major">Poznámka: povšimněte si, že nám pro poslání zprávy
postačuje znát adresu brokera a jméno tématu. Konzument (popsaný
v&nbsp;navazující kapitole) je nepatrně složitější, protože je nutné
specifikovat i jméno skupiny (<i>consumer group</i>).</div></p>



<p><a name="k06"></a></p>
<h2 id="k06">6. Klasický konzument, který čte zprávy ze specifikovaného tématu</h2>

<p>Implementace jednoduchého konzumenta zpráv v&nbsp;programovacím jazyku
Python je poměrně snadná a vlastně se ani příliš neliší od implementace
konzumenta.  Nejdříve je nutné vytvořit instanci třídy
<strong>KafkaConsumer</strong>, přičemž se specifikuje téma (<i>topic</i>),
skupina, seznam brokerů a nepovinně též informace o tom, jakým způsobem se má
nastavit offset první zpracované zprávy. Většinou budeme potřebovat zpracovávat
nejnovější (<i>earliest</i>) zprávy, takže nastavení konzumenta může vypadat
například následovně:</p>

<pre>
consumer = <strong>KafkaConsumer</strong>(
    topic, group_id=group_id,
    bootstrap_servers=[server],
    auto_offset_reset="earliest"
)
</pre>

<p>Samotná konzumace (tedy čtení a zpracování) zpráv může probíhat
v&nbsp;nekonečné programové smyčce, přičemž každá zpráva je reprezentována
objektem obsahujícím téma, číslo oddílu, offset v&nbsp;rámci oddílu, klíč
zprávy a samozřejmě též obsah zprávy (klíč zprávy a tělo zprávy není žádným
způsobem dekódováno resp.&nbsp;deserializováno):</p>

<pre>
for message in consumer:
    print("%s:%d:%d: key=%s value=%s" % (
            message.topic,
            message.partition,
            message.offset,
            message.key,
            message.value,
        )
    )
</pre>

<p><div class="rs-tip-major">Poznámka: ve skriptu je navíc přidána reakce na
stisk klávesové zkratky <strong>Ctrl+C</strong> pro ukončení činnosti nekonečné
smyčky.</div></p>

<p>A takto vypadá úplný zdrojový kód běžného konzumenta zpráv, který naleznete
na adrese <a
href="https://github.com/tisnik/most-popular-python-libs/blob/master/faust/greeting_consumer.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faust/greeting_consumer.py</a>:</p>

<pre>
#!/usr/bin/env python3
&nbsp;
import sys
from kafka import KafkaConsumer
&nbsp;
server = "localhost:9092"
topic = "greetings"
group_id = "group1"
&nbsp;
print("Connecting to Kafka")
consumer = KafkaConsumer(
    topic, group_id=group_id,
    bootstrap_servers=[server],
    auto_offset_reset="earliest"
)
print("Connected to Kafka")
&nbsp;
try:
    for message in consumer:
        print(
            "%s:%d:%d: key=%s value=%s"
            % (
                message.topic,
                message.partition,
                message.offset,
                message.key,
                message.value,
            )
        )
except KeyboardInterrupt:
    sys.exit()
</pre>



<p><a name="k07"></a></p>
<h2 id="k07">7. Spuštění producenta a konzumenta</h2>

<p>Nyní si funkce producenta a konzumenta otestujeme. V&nbsp;jednom terminálu
spustíme producenta zpráv:</p>

<pre>
</pre>

<p>Ve druhém terminálu naopak spustíme konzumenta:</p>

<pre>
</pre>

<p>Nyní by měl konzument začít přijímat nové zprávy posílané s&nbsp;periodou
přibližně jedné sekundy:</p>

<p><div class="rs-tip-major">Poznámka: povšimněte si, že tělo zpráv je tvořeno
hodnotami typu <strong>bytes</strong> a nikoli řetězcem. Převod na řetězec je
ovšem triviální a lze ho provést zavoláním jediné metody.</div></p>



<p><a name="k08"></a></p>
<h2 id="k08">8. Konzument realizovaný formou workera postavený na knihovně Faust</h2>

<p>Podívejme se nyní na to, jak by se konzument realizoval s&nbsp;využitím
knihovny Faust. Tato knihovna je postavena na poněkud odlišných konceptech, než
je klasický producent a konzument. Namísto konzumenta se zde používá termín
<i>worker</i>, načítané zprávy mohou být popsány <i>modelem</i> a vše je
zabaleno v&nbsp;<i>aplikaci</i>, která řídí konzumaci či produkci zpráv
(resp.&nbsp;celý tok dat). Začněme nejdříve posledním zmíněným termínem
<i>aplikace</i>. Jedná se o koncept umožňující nejenom spouštění workerů, ale i
další činnosti, například práci s&nbsp;tabulkami (což si popíšeme
v&nbsp;navazujícím článku), práci s&nbsp;takzvanými <i>okny</i> (pohledů na
část sekvence zpráv) atd.</p>

<p>Objekt představující aplikaci se vytvoří konstruktorem <strong>App</strong>,
kterému se musí předat minimálně jeden povinný parametr &ndash; identifikátor
aplikace, což je řetězec. Ovšem předat je možné i další parametry, zejména
adresu Kafka brokeru, způsob (kodek) serializace a deserializace zpráv atd.
Vytvoření aplikace tedy může vypadat následovně:</p>

<pre>
app = faust.App(
    "hello-world",
    broker="kafka://localhost:9092",
    value_serializer="raw",
)
</pre>

<p>V&nbsp;dalším kroku zkonstruujeme ještě jeden objekt, který bude
reprezentovat <i>téma</i> (<i>topic</i>). V&nbsp;tomto případě se předává jen
jméno tématu, i když opět platí, že je možné předat i další nepovinné parametry
(schéma, typ klíčů, typ těl zpráv atd. atd.):</p>

<pre>
greetings_topic = app.topic("greetings")
</pre>

<p>Nyní je již možné nadefinovat asynchronní funkci, která bude konzumovat
zprávy přicházející do nakonfigurovaného tématu. Tato funkce je obalena
dekorátorem <strong>@app.agent</strong>, přičemž i dekorátor akceptuje různé
parametry. My mu prozatím předáme pouze objekt představující téma:</p>

<pre>
<strong>@app.agent(greetings_topic)</strong>
async def <strong>greet</strong>(greetings):
    async for greeting in greetings:
        print(greeting)
</pre>

<p>Posledním krokem je spuštění aplikace, což je již triviální operace:</p>

<pre>
if __name__ == "__main__":
    app.main()
</pre>

<p>Úplný zdrojový kód workera je možné najít na adrese <a
href="https://github.com/tisnik/most-popular-python-libs/blob/master/faust/greeting_faust_consumer.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faust/greeting_faust_consumer.py</a>:</p>

<pre>
import faust
&nbsp;
app = faust.App(
    "hello-world",
    broker="kafka://localhost:9092",
    value_serializer="raw",
)
&nbsp;
greetings_topic = app.topic("greetings")
&nbsp;
<u>@app.agent(greetings_topic)</u>
async def <strong>greet</strong>(greetings):
    async for greeting in greetings:
        print(greeting)
&nbsp;
&nbsp;
if __name__ == "__main__":
    app.main()
</pre>



<p><a name="k09"></a></p>
<h2 id="k09">9. Spuštění workera</h2>

<p>V&nbsp;případě, že se pokusíme o spuštění workera běžným způsobem,
tj.&nbsp;zadáním:</p>

<pre>
$ <strong>python greeting_faust_consumer.py</strong>
</pre>

<p>ukáže se nám pouze nápověda s&nbsp;informacemi, jaké možnosti nám worker
umožňuje:</p>

<pre>
Usage: greeting_faust_consumer.py [OPTIONS] COMMAND [ARGS]...
&nbsp;
  Welcome, see list of commands and options below.
&nbsp;
  Use --help for help, --version for version information.
&nbsp;
  https://faust-streaming.github.io/faust
&nbsp;
Options:
  --console-port RANGE[1-65535]   when --debug: Port to run debugger console
                                  on.  [default: 50101; 1&lt;=x&lt;=65535]
  --blocking-timeout FLOAT        when --debug: Blocking detector timeout.
  -l, --loglevel [crit|error|warn|info|debug]
                                  Logging level to use.  [default: WARN]
  -f, --logfile FILE              Path to logfile (default is &lt;stderr&gt;).
  -L, --loop [aio|eventlet|uvloop]
                                  Event loop implementation to use.  [default:
                                  aio]
  --json                          Return output in machine-readable JSON
                                  format
  -D, --datadir DIRECTORY         Directory to keep application state.
                                  [default: {conf.name}-data]
  -W, --workdir DIRECTORY         Working directory to change to after start.
  --debug / --no-debug            Enable debugging output, and the blocking
                                  detector.  [default: no-debug]
  -q, --quiet / --no-quiet        Silence output to &lt;stdout&gt;/&lt;stderr&gt;
                                  [default: no-quiet]
  -A, --app TEXT                  Path of Faust application to use, or the
                                  name of a module.
  --version                       Show the version and exit.
  --help                          Show this message and exit.
&nbsp;
Commands:
  agents          List agents.
  clean-versions  Delete old version directories.
  completion      Output shell completion to be evaluated by the shell.
  livecheck       Manage LiveCheck instances.
  model           Show model detail.
  models          List all available models as a tabulated list.
  reset           Delete local table state.
  send            Send message to agent/topic.
  tables          List available tables.
  worker          Start worker instance for given app.
</pre>

<p>Vlastní spuštění se provede příkazem:</p>

<pre>
$ <strong>python3 greeting_faust_consumer.py worker</strong>
</pre>

<p>Nejprve se zobrazí tabulka se základními informacemi o workerovi:</p>

<pre>
┌ƒaµS† v0.11.1.dev4+ga489db3b───────────────────────────────────┐
│ id          │ hello-world                                     │
│ transport   │ [URL('kafka://localhost:9092')]                 │
│ store       │ memory:                                         │
│ web         │ http://localhost:6066/                          │
│ log         │ -stderr- (warn)                                 │
│ pid         │ 1296383                                         │
│ hostname    │ ptisnovs.xxx.yyy.zzz                            │
│ platform    │ CPython 3.11.8 (Linux x86_64)                   │
│        +    │ Cython (GCC 13.2.1 20231011 (Red Hat 13.2.1-4)) │
│ drivers     │                                                 │
│   transport │ aiokafka=0.10.0                                 │
│   web       │ aiohttp=3.9.5                                   │
│ datadir     │ /tmp/ramdisk/faust/hello-world-data             │
│ appdir      │ /tmp/ramdisk/faust/hello-world-data/v1          │
└─────────────┴─────────────────────────────────────────────────┘
starting➢ 😊
</pre>

<p>Ihned po výpisu zprávy &bdquo;starting&ldquo; by měl worker začít se čtením
a zpracováváním zpráv, které jsou konzumovány z&nbsp;tématu
&bdquo;greetings&ldquo;. Informace o každé zkonzumované zprávě je vypisována do
terminálu:</p>

<pre>
[2024-04-20 18:03:39,583] [1296383] [WARNING] b'Greeting #0' 
[2024-04-20 18:03:39,583] [1296383] [WARNING] b'Greeting #1' 
[2024-04-20 18:03:39,584] [1296383] [WARNING] b'Greeting #2' 
[2024-04-20 18:03:39,584] [1296383] [WARNING] b'Greeting #3' 
[2024-04-20 18:03:39,584] [1296383] [WARNING] b'Greeting #4' 
</pre>

<p><div class="rs-tip-major">Poznámka: povšimněte si, že těla zpráv jsou
reprezentována typem <i>bytes</i>, tj.&nbsp;neměnitelnou (<i>immutable</i>)
sekvencí bajtů. Tento nedostatek samozřejmě později napravíme.</div></p>



<p><a name="k10"></a></p>
<h2 id="k10">10. Producent posílající zprávy do většího množství témat</h2>

<p>Mezi přednosti knihovny Faust patří především to, že lze spustit větší
množství asynchronně běžících workerů. Abychom si to ukázali v&nbsp;praxi na co
nejjednodušším příkladu, upravíme nepatrně našeho producenta zpráv takovým
způsobem, aby posílal zprávy do dvou témat nazvaných &bdquo;greetings&ldquo; a
&bdquo;real_work&ldquo;. Tato úprava je snadná, protože postačuje do programové
smyčky přidat další volání metod <strong>producer.send</strong>. Výsledný kód
takto upraveného producenta zpráv lze nalézt na adrese <a
href="https://github.com/tisnik/most-popular-python-libs/blob/master/faust/multi_producer_raw.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faust/multi_producer_raw.py</a>:</p>

<pre>
#!/usr/bin/env python3
&nbsp;
from kafka import KafkaProducer
from time import sleep
from json import dumps
&nbsp;
server = "localhost:9092"
topic1 = "greetings"
topic2 = "real_work"
&nbsp;
print("Connecting to Kafka")
producer = <strong>KafkaProducer</strong>(
    bootstrap_servers=[server],
    value_serializer=lambda x: dumps(x).encode("utf-8")
)
print("Connected to Kafka")
&nbsp;
for i in range(1000):
    print(i)
    message = f"Greeting #{i}"
    producer.send(topic1, value=message)
    sleep(1)
    work = f"Real work #{i*2}"
    producer.send(topic2, value=work)
    work = f"Real work #{i*2+1}"
    producer.send(topic2, value=work)
    sleep(1)
</pre>

<p><div class="rs-tip-major">Poznámka: povšimněte si, že se zprávy stále
serializují stejným způsobem &ndash; z&nbsp;řetězců na hodnoty typu
<strong>bytes</strong>.</div></p>



<p><a name="k11"></a></p>
<h2 id="k11">11. Konzumace zpráv workerem z&nbsp;většího množství témat</h2>

<p>Jak bude vypadat konzument zpráv (resp.&nbsp;konzumenti zpráv)
v&nbsp;případě, že použijeme knihovnu Faust? Díky tomu, že každý worker je
realizován asynchronní funkcí, je řešení až překvapivě snadné &ndash; každý
worker bude v&nbsp;takovém případě definován ve vlastní funkci
s&nbsp;dekorátorem <strong>@app.agent</strong> a každý z&nbsp;těchto workerů
zpracovává zprávy z&nbsp;odlišného tématu:</p>

<pre>
<u>@app.agent(greetings_topic)</u>
async def <strong>greet</strong>(greetings):
    async for greeting in greetings:
        print(f"Greeter: {greeting}")
&nbsp;
<i>@app.agent(real_work_topic)</i>
async def <strong>worker</strong>(works):
    async for work in works:
        print(f"Worker: {work}")
</pre>

<p>O spouštění, koordinaci práce atd. se postará knihovna Faust automaticky,
takže tento problém vlastně vůbec nemusíme na aplikační úrovni řešit. Výsledná
aplikace s&nbsp;workery je dostupná na adrese <a
href="https://github.com/tisnik/most-popular-python-libs/blob/master/faust/greeting_worker_consumer_raw.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faust/greeting_worker_consumer_raw.py</a>
a vypadá následovně:</p>

<pre>
import faust
&nbsp;
app = faust.App(
    "hello-world",
    broker="kafka://localhost:9092",
    value_serializer="raw",
)
&nbsp;
greetings_topic = app.topic("greetings")
real_work_topic = app.topic("real_work")
&nbsp;
<u>@app.agent(greetings_topic)</u>
async def <strong>greet</strong>(greetings):
    async for greeting in greetings:
        print(f"Greeter: {greeting}")
&nbsp;
<u>@app.agent(real_work_topic)</u>
async def <strong>worker</strong>(works):
    async for work in works:
        print(f"Worker: {work}")
&nbsp;
&nbsp;
if __name__ == "__main__":
    app.main()
</pre>



<p><a name="k12"></a></p>
<h2 id="k12">12. Produkce zpráv ve formátu JSON</h2>

<pre>
#!/usr/bin/env python3
&nbsp;
from kafka import KafkaProducer
from time import sleep
&nbsp;
server = "localhost:9092"
topic1 = "greetings"
topic2 = "real_work"
&nbsp;
print("Connecting to Kafka")
producer = <strong>KafkaProducer</strong>(
    bootstrap_servers=[server],
    value_serializer=lambda x: x.encode("utf-8")
)
print("Connected to Kafka")
&nbsp;
for i in range(1000):
    print(i)
    message = {"subject": "Greeting", "value": i}
    producer.send(topic1, value=message)
    sleep(1)
    work = {"subject": "Real work", "value": i*2}
    producer.send(topic2, value=work)
    work = {"subject": "Real work", "value": i*2+1}
    producer.send(topic2, value=work)
    sleep(1)
</pre>



<p><a name="k13"></a></p>
<h2 id="k13">13. Worker přijímající zprávy ve formátu JSON</h2>

<pre>
import faust
&nbsp;
app = faust.App(
    "hello-world",
    broker="kafka://localhost:9092",
    <u>value_serializer="json"</u>,
)
&nbsp;
greetings_topic = app.topic("greetings")
real_work_topic = app.topic("real_work")
&nbsp;
<u>@app.agent(greetings_topic)</u>
async def <strong>greet</strong>(greetings):
    async for greeting in greetings:
        print(f"Greeter: {greeting}")
&nbsp;
<u>@app.agent(real_work_topic)</u>
async def <strong>worker</strong>(works):
    async for work in works:
        print(f"Worker: {work}")
&nbsp;
&nbsp;
if __name__ == "__main__":
    app.main()
</pre>




<p><a name="k14"></a></p>
<h2 id="k14">14. Využití modelů</h2>

<pre>
class <strong>Registration</strong>(faust.Record):
    name: str
    surname: str
    id: int
</pre>

<pre>
#!/usr/bin/env python3
&nbsp;
from kafka import KafkaProducer
from time import sleep
from json import dumps
&nbsp;
def user(name, surname, id):
    return {
            "name": name,
            "surname": surname,
            "id": id
            }
&nbsp;
&nbsp;
server = "localhost:9092"
topic = "registrations"
&nbsp;
print("Connecting to Kafka")
producer = <strong>KafkaProducer</strong>(
    bootstrap_servers=[server],
    value_serializer=lambda x: dumps(x).encode("utf-8")
)
print("Connected to Kafka")
&nbsp;
producer.send(topic, value=user("Eliška", "Najbrtová", 4))
producer.send(topic, value=user("Jenny", "Suk", 3))
producer.send(topic, value=user("Anička", "Šafářová", 0))
producer.send(topic, value=user("Sváťa", "Pulec", 3))
producer.send(topic, value=user("Blažej", "Motyčka", 8))
producer.send(topic, value=user("Eda", "Wasserfall", 0))
producer.send(topic, value=user("Přemysl", "Hájek", 10))
producer.flush()
&nbsp;
print("Done")
</pre>

<pre>
import faust
&nbsp;
app = faust.App(
    "registrations",
    broker="kafka://localhost:9092",
    #value_serializer="json",
)
&nbsp;
&nbsp;
class <strong>Registration</strong>(faust.Record):
    name: str
    surname: str
    id: int
&nbsp;
&nbsp;
registrations_topic = app.topic("registrations", key_type=str, value_type=Registration)
&nbsp;
<u>@app.agent(registrations_topic)</u>
async def <strong>register</strong>(registrations):
    async for registration in registrations:
        print(f"Registration: {registration}")
&nbsp;
&nbsp;
if __name__ == "__main__":
    app.main()
</pre>



<p><a name="k15"></a></p>
<h2 id="k15">15. Producent založený na knihovně Faust využívající model</h2>

<pre>
import faust
&nbsp;
app = faust.App(
    "registrations",
    broker="kafka://localhost:9092",
)
&nbsp;
&nbsp;
class <strong>Registration</strong>(faust.Record):
    name: str
    surname: str
    id: int
&nbsp;
&nbsp;
registrations_topic = app.topic("registrations", key_type=str, value_type=Registration)
&nbsp;
<u>@app.timer(interval=5.0)</u>
async def <strong>example_sender</strong>(app):
    await registrations_topic.send(
        value=Registration("Eliška", "Najbrtová", 4))
    await registrations_topic.send(
        value=Registration("Jenny", "Suk", 3))
    await registrations_topic.send(
        value=Registration("Anička", "Šafářová", 0))
    await registrations_topic.send(
        value=Registration("Sváťa", "Pulec", 3))
    await registrations_topic.send(
        value=Registration("Blažej", "Motyčka", 8))
    await registrations_topic.send(
        value=Registration("Eda", "Wasserfall", 0))
    await registrations_topic.send(
        value=Registration("Přemysl", "Hájek", 10))
&nbsp;
&nbsp;
if __name__ == "__main__":
    app.main()
</pre>



<p><a name="k16"></a></p>
<h2 id="k16">16. Kombinace producenta a konzumenta &ndash; reálná síla knihovny Faust</h2>

<pre>
import faust
&nbsp;
app = faust.App(
    "registrations",
    broker="kafka://localhost:9092",
)
&nbsp;
&nbsp;
class <strong>Registration</strong>(faust.Record):
    name: str
    surname: str
    id: int
&nbsp;
&nbsp;
registrations_topic = app.topic("registrations", key_type=str, value_type=Registration)
&nbsp;
<u>@app.timer(interval=5.0)</u>
async def <strong>example_sender</strong>(app):
    await registrations_topic.send(
        value=Registration("Eliška", "Najbrtová", 4))
    await registrations_topic.send(
        value=Registration("Jenny", "Suk", 3))
    await registrations_topic.send(
        value=Registration("Anička", "Šafářová", 0))
    await registrations_topic.send(
        value=Registration("Sváťa", "Pulec", 3))
    await registrations_topic.send(
        value=Registration("Blažej", "Motyčka", 8))
    await registrations_topic.send(
        value=Registration("Eda", "Wasserfall", 0))
    await registrations_topic.send(
        value=Registration("Přemysl", "Hájek", 10))
&nbsp;
&nbsp;
<u>@app.agent(registrations_topic)</u>
async def <strong>register</strong>(registrations):
    async for registration in registrations:
        print(f"Registration: {registration}")
&nbsp;
&nbsp;
if __name__ == "__main__":
    app.main()
</pre>

<p><div class="rs-tip-major">Poznámka: </div></p>



<p><a name="k17"></a></p>
<h2 id="k17">17. Spuštění kombinace workerů s&nbsp;producenty a konzumenty</h2>


<pre>
┌ƒaµS† v0.11.1.dev4+ga489db3b───────────────────────────────────┐
│ id          │ registrations                                   │
│ transport   │ [URL('kafka://localhost:9092')]                 │
│ store       │ memory:                                         │
│ web         │ http://localhost:6066/                          │
│ log         │ -stderr- (warn)                                 │
│ pid         │ 1508601                                         │
│ hostname    │ ptisnovs.xxx.yyy.zzz                            │
│ platform    │ CPython 3.11.8 (Linux x86_64)                   │
│        +    │ Cython (GCC 13.2.1 20231011 (Red Hat 13.2.1-4)) │
│ drivers     │                                                 │
│   transport │ aiokafka=0.10.0                                 │
│   web       │ aiohttp=3.9.5                                   │
│ datadir     │ /tmp/ramdisk/faust/registrations-data           │
│ appdir      │ /tmp/ramdisk/faust/registrations-data/v1        │
└─────────────┴─────────────────────────────────────────────────┘
starting➢ 
</pre>



<p><a name="k18"></a></p>
<h2 id="k18">18. Obsah navazujícího článku</h2>

<p></p>



<p><a name="k19"></a></p>
<h2 id="k19">19. Repositář s&nbsp;demonstračními příklady</h2>

<p>Zdrojové kódy všech prozatím popsaných demonstračních příkladů určených pro
programovací jazyk Python 3 byly uloženy do Git repositáře dostupného na adrese
<a
href="https://github.com/tisnik/most-popular-python-libs">https://github.com/tisnik/most-popular-python-libs</a>:</p>

<table>
<tr><th> #</th><th>Demonstrační příklad</th><th>Stručný popis příkladu</th><th>Cesta</th></tr>
<tr><td> 1</td><td>greeting_producer.py</td><td>klasický producent zpráv vytvořený bez použití knihovny Faust (používá knihovnu Kafka-python-ng)</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faust/greeting_producer.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faust/greeting_producer.py</a></td></tr>
<tr><td> 2</td><td>greeting_consumer.py</td><td>klasický konzument zpráv vytvořený bez použití knihovny Faust (používá knihovnu Kafka-python-ng)</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faust/greeting_consumer.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faust/greeting_consumer.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td> 3</td><td>greeting_faust_consumer.py</td><td>worker definovaný s&nbsp;využitím knihovny Faust</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faust/greeting_faust_consumer.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faust/greeting_faust_consumer.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td> 4</td><td>multi_producer_raw.py</td><td>producent zpráv do většího množství témat, zprávy jsou posílány jako řetězce</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faust/multi_producer_raw.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faust/multi_producer_raw.py</a></td></tr>
<tr><td> 5</td><td>greeting_worker_consumer_raw.py</td><td>dvojice workerů definovaných s&nbsp;využitím knihovny Faust pro zprávy posílané jako sekvence bajtů</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faust/greeting_worker_consumer_raw.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faust/greeting_worker_consumer_raw.py</a></td></tr>
<tr><td> 6</td><td>multi_producer_json.py</td><td>producent zpráv do většího množství témat, zprávy jsou serializovány do formátu JSON</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faust/multi_producer_json.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faust/multi_producer_json.py</a></td></tr>
<tr><td> 7</td><td>greeting_worker_consumer_json.py</td><td>dvojice workerů definovaných s&nbsp;využitím knihovny Faust pro zprávy ve formátu JSON</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faust/greeting_worker_consumer_json.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faust/greeting_worker_consumer_json.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td> 8</td><td>registration_producer.py</td><td>producent zpráv ve formátu JSON obsahujících atributy objektů (používá knihovnu Kafka-python-ng)</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faust/registration_producer.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faust/registration_producer.py</a></td></tr>
<tr><td> 9</td><td>registration_consumer.py</td><td>konzument zpráv založený na knihovně Faust (pracuje s&nbsp;modelem)</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faust/registration_consumer.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faust/registration_consumer.py</a></td></tr>
<tr><td>10</td><td>registration_producer_faust.py</td><td>producent zpráv založený na knihovně Faust (pracuje s&nbsp;modelem)</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faust/registration_producer_faust.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faust/registration_producer_faust.py</a></td></tr>
<tr><td>11</td><td>registration_consumer_producer.py</td><td>tok dat od producenta ke konzumentovi, založeno na knihovně Faust</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faust/registration_consumer_producer.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faust/registration_consumer_producer.py</a></td></tr>
</table>



<p><a name="k20"></a></p>
<h2 id="k20">20. Odkazy na Internetu</h2>

<ol>

<li>Faust &ndash; Python Stream Processing<br />
<a href="https://faust-streaming.github.io/faust/">https://faust-streaming.github.io/faust/</a>
</li>

<li>Knihovna Faust na GitHubu<br />
<a href="https://github.com/faust-streaming/faust">https://github.com/faust-streaming/faust</a>
</li>

<li>faust 1.10.4 na Pypi<br />
<a href="https://pypi.org/project/faust/">https://pypi.org/project/faust/</a>
</li>

<li>Introduction to Kafka Stream Processing in Python using Faust (video)<br />
<a href="https://www.youtube.com/watch?v=Nt96udaC5Zk">https://www.youtube.com/watch?v=Nt96udaC5Zk</a>
</li>

<li>Windowing in Kafka Streams using Faust Framework in Python | Tumbling Window (video)<br />
<a href="https://www.youtube.com/watch?v=ZlBXg9Kp8vE">https://www.youtube.com/watch?v=ZlBXg9Kp8vE</a>
</li>

<li>Stream Processing with Python, Kafka &amp; Faust<br />
<a href="https://towardsdatascience.com/stream-processing-with-python-kafka-faust-a11740d0910c">https://towardsdatascience.com/stream-processing-with-python-kafka-faust-a11740d0910c</a>
</li>

<li>ETL Batch Processing With Kafka?<br />
<a href="https://medium.com/swlh/etl-batch-processing-with-kafka-7f66f843e20d">https://medium.com/swlh/etl-batch-processing-with-kafka-7f66f843e20d</a>
</li>

<li>ETL with Kafka<br />
<a href="https://blog.codecentric.de/en/2018/03/etl-kafka/">https://blog.codecentric.de/en/2018/03/etl-kafka/</a>
</li>

<li>Building ETL Pipelines with Clojure and Transducers<br />
<a href="https://www.grammarly.com/blog/engineering/building-etl-pipelines-with-clojure-and-transducers/">https://www.grammarly.com/blog/engineering/building-etl-pipelines-with-clojure-and-transducers/</a>
</li>

<li>pipeline (možné použít pro ETL)<br />
<a href="https://clojuredocs.org/clojure.core.async/pipeline">https://clojuredocs.org/clojure.core.async/pipeline</a>
</li>

<li>On Track with Apache Kafka – Building a Streaming ETL Solution with Rail Data<br />
<a href="https://www.confluent.io/blog/build-streaming-etl-solutions-with-kafka-and-rail-data/">https://www.confluent.io/blog/build-streaming-etl-solutions-with-kafka-and-rail-data/</a>
</li>

<li>Kafka - Understanding Offset Commits<br />
<a href="https://www.logicbig.com/tutorials/misc/kafka/committing-offsets.html">https://www.logicbig.com/tutorials/misc/kafka/committing-offsets.html</a>
</li>

<li>fundingcircle/jackdaw (na Clojars)<br />
<a href="https://clojars.org/fundingcircle/jackdaw/versions/0.7.6">https://clojars.org/fundingcircle/jackdaw/versions/0.7.6</a>
</li>

<li>Dokumentace ke knihovně jackdaw<br />
<a href="https://cljdoc.org/d/fundingcircle/jackdaw/0.7.6/doc/readme">https://cljdoc.org/d/fundingcircle/jackdaw/0.7.6/doc/readme</a>
</li>

<li>Jackdaw AdminClient API<br />
<a href="https://cljdoc.org/d/fundingcircle/jackdaw/0.7.6/doc/jackdaw-adminclient-api">https://cljdoc.org/d/fundingcircle/jackdaw/0.7.6/doc/jackdaw-adminclient-api</a>
</li>

<li>Jackdaw Client API<br />
<a href="https://cljdoc.org/d/fundingcircle/jackdaw/0.7.6/doc/jackdaw-client-api">https://cljdoc.org/d/fundingcircle/jackdaw/0.7.6/doc/jackdaw-client-api</a>
</li>

<li>Kafka.clj<br />
<a href="https://github.com/helins-io/kafka.clj">https://github.com/helins-io/kafka.clj</a>
</li>

<li>Kafka mirroring (MirrorMaker)<br />
<a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=27846330">https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=27846330</a>
</li>

<li>Mastering Kafka migration with MirrorMaker 2<br />
<a href="https://developers.redhat.com/articles/2024/01/04/mastering-kafka-migration-mirrormaker-2">https://developers.redhat.com/articles/2024/01/04/mastering-kafka-migration-mirrormaker-2</a>
</li>

<li>Apache Kafka MirrorMaker 2 (MM2) Part 1: Theory<br />
<a href="https://www.instaclustr.com/blog/kafka-mirrormaker-2-theory/#h-2-replication-in-kafka">https://www.instaclustr.com/blog/kafka-mirrormaker-2-theory/#h-2-replication-in-kafka</a>
</li>

<li>Apache Kafka MirrorMaker 2 (MM2) Part 2: Practice<br />
<a href="https://www.instaclustr.com/blog/apache-kafka-mirrormaker-2-practice/">https://www.instaclustr.com/blog/apache-kafka-mirrormaker-2-practice/</a>
</li>

<li>Demystifying Kafka MirrorMaker 2: Use cases and architecture <br />
<a href="https://developers.redhat.com/articles/2023/11/13/demystifying-kafka-mirrormaker-2-use-cases-and-architecture#">https://developers.redhat.com/articles/2023/11/13/demystifying-kafka-mirrormaker-2-use-cases-and-architecture#</a>
</li>

<li>How to use Kafka MirrorMaker 2.0 in data migration, replication and the use-cases<br />
<a href="https://learn.microsoft.com/en-us/azure/hdinsight/kafka/kafka-mirrormaker-2-0-guide">https://learn.microsoft.com/en-us/azure/hdinsight/kafka/kafka-mirrormaker-2-0-guide</a>
</li>

<li>Release Notes - Kafka - Version 2.4.0<br />
<a href="https://archive.apache.org/dist/kafka/2.4.0/RELEASE_NOTES.html">https://archive.apache.org/dist/kafka/2.4.0/RELEASE_NOTES.html</a>
</li>

<li>Kafka Mirror Maker Best Practices<br />
<a href="https://community.cloudera.com/t5/Community-Articles/Kafka-Mirror-Maker-Best-Practices/ta-p/249269">https://community.cloudera.com/t5/Community-Articles/Kafka-Mirror-Maker-Best-Practices/ta-p/249269</a>
</li>

<li>Apache Kafka MirrorMaker 2 (MM2) Part 1: Theory<br />
<a href="https://www.instaclustr.com/blog/kafka-mirrormaker-2-theory/">https://www.instaclustr.com/blog/kafka-mirrormaker-2-theory/</a>
</li>

<li>Kcli: is a kafka read only command line browser.<br />
<a href="https://github.com/cswank/kcli">https://github.com/cswank/kcli</a>
</li>

<li>Kcli: a kafka command line browser<br />
<a href="https://go.libhunt.com/kcli-alternatives">https://go.libhunt.com/kcli-alternatives</a>
</li>

<li>Kafka Connect and Schemas<br />
<a href="https://rmoff.net/2020/01/22/kafka-connect-and-schemas/">https://rmoff.net/2020/01/22/kafka-connect-and-schemas/</a>
</li>

<li>JSON and schemas<br />
<a href="https://www.confluent.io/blog/kafka-connect-deep-dive-converters-serialization-explained/#json-schemas">https://www.confluent.io/blog/kafka-connect-deep-dive-converters-serialization-explained/#json-schemas</a>
</li>

<li>What, why, when to use Apache Kafka, with an example<br />
<a href="https://www.startdataengineering.com/post/what-why-and-how-apache-kafka/">https://www.startdataengineering.com/post/what-why-and-how-apache-kafka/</a>
</li>

<li>When NOT to use Apache Kafka?<br />
<a href="https://www.kai-waehner.de/blog/2022/01/04/when-not-to-use-apache-kafka/">https://www.kai-waehner.de/blog/2022/01/04/when-not-to-use-apache-kafka/</a>
</li>

<li>Microservices: The Rise Of Kafka<br />
<a href="https://movio.co/blog/microservices-rise-kafka/">https://movio.co/blog/microservices-rise-kafka/</a>
</li>

<li>Building a Microservices Ecosystem with Kafka Streams and KSQL<br />
<a href="https://www.confluent.io/blog/building-a-microservices-ecosystem-with-kafka-streams-and-ksql/">https://www.confluent.io/blog/building-a-microservices-ecosystem-with-kafka-streams-and-ksql/</a>
</li>

<li>An introduction to Apache Kafka and microservices communication<br />
<a href="https://medium.com/@ulymarins/an-introduction-to-apache-kafka-and-microservices-communication-bf0a0966d63">https://medium.com/@ulymarins/an-introduction-to-apache-kafka-and-microservices-communication-bf0a0966d63</a>
</li>

<li>kappa-architecture.com<br />
<a href="http://milinda.pathirage.org/kappa-architecture.com/">http://milinda.pathirage.org/kappa-architecture.com/</a>
</li>

<li>Questioning the Lambda Architecture<br />
<a href="https://www.oreilly.com/ideas/questioning-the-lambda-architecture">https://www.oreilly.com/ideas/questioning-the-lambda-architecture</a>
</li>

<li>Lambda architecture<br />
<a href="https://en.wikipedia.org/wiki/Lambda_architecture">https://en.wikipedia.org/wiki/Lambda_architecture</a>
</li>

<li>Kafka &ndash; ecosystem (Wiki)<br />
<a href="https://cwiki.apache.org/confluence/display/KAFKA/Ecosystem">https://cwiki.apache.org/confluence/display/KAFKA/Ecosystem</a>
</li>

<li>The Kafka Ecosystem - Kafka Core, Kafka Streams, Kafka Connect, Kafka REST Proxy, and the Schema Registry<br />
<a href="http://cloudurable.com/blog/kafka-ecosystem/index.html">http://cloudurable.com/blog/kafka-ecosystem/index.html</a>
</li>

<li>A Kafka Operator for Kubernetes<br />
<a href="https://github.com/krallistic/kafka-operator">https://github.com/krallistic/kafka-operator</a>
</li>

<li>Kafka Streams<br />
<a href="https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Streams">https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Streams</a>
</li>

<li>Kafka Streams<br />
<a href="http://kafka.apache.org/documentation/streams/">http://kafka.apache.org/documentation/streams/</a>
</li>

<li>Kafka Streams (FAQ)<br />
<a href="https://cwiki.apache.org/confluence/display/KAFKA/FAQ#FAQ-Streams">https://cwiki.apache.org/confluence/display/KAFKA/FAQ#FAQ-Streams</a>
</li>

<li>Event stream processing<br />
<a href="https://en.wikipedia.org/wiki/Event_stream_processing">https://en.wikipedia.org/wiki/Event_stream_processing</a>
</li>

<li>Part 1: Apache Kafka for beginners - What is Apache Kafka?<br />
<a href="https://www.cloudkarafka.com/blog/2016-11-30-part1-kafka-for-beginners-what-is-apache-kafka.html">https://www.cloudkarafka.com/blog/2016-11-30-part1-kafka-for-beginners-what-is-apache-kafka.html</a>
</li>

<li>What are some alternatives to Apache Kafka?<br />
<a href="https://www.quora.com/What-are-some-alternatives-to-Apache-Kafka">https://www.quora.com/What-are-some-alternatives-to-Apache-Kafka</a>
</li>

<li>What is the best alternative to Kafka?<br />
<a href="https://www.slant.co/options/961/alternatives/~kafka-alternatives">https://www.slant.co/options/961/alternatives/~kafka-alternatives</a>
</li>

<li>A super quick comparison between Kafka and Message Queues<br />
<a href="https://hackernoon.com/a-super-quick-comparison-between-kafka-and-message-queues-e69742d855a8?gi=e965191e72d0">https://hackernoon.com/a-super-quick-comparison-between-kafka-and-message-queues-e69742d855a8?gi=e965191e72d0</a>
</li>

<li>Kafka Queuing: Kafka as a Messaging System<br />
<a href="https://dzone.com/articles/kafka-queuing-kafka-as-a-messaging-system">https://dzone.com/articles/kafka-queuing-kafka-as-a-messaging-system</a>
</li>

<li>Apache Kafka Logs: A Comprehensive Guide<br />
<a href="https://hevodata.com/learn/apache-kafka-logs-a-comprehensive-guide/">https://hevodata.com/learn/apache-kafka-logs-a-comprehensive-guide/</a>
</li>

<li>Microservices – Not a free lunch!<br />
<a href="http://highscalability.com/blog/2014/4/8/microservices-not-a-free-lunch.html">http://highscalability.com/blog/2014/4/8/microservices-not-a-free-lunch.html</a>
</li>

<li>Microservices, Monoliths, and NoOps<br />
<a href="http://blog.arungupta.me/microservices-monoliths-noops/">http://blog.arungupta.me/microservices-monoliths-noops/</a>
</li>

<li>Microservice Design Patterns<br />
<a href="http://blog.arungupta.me/microservice-design-patterns/">http://blog.arungupta.me/microservice-design-patterns/</a>
</li>

<li>REST vs Messaging for Microservices – Which One is Best?<br />
<a href="https://solace.com/blog/experience-awesomeness-event-driven-microservices/">https://solace.com/blog/experience-awesomeness-event-driven-microservices/</a>
</li>

<li>Kappa Architecture Our Experience<br />
<a href="https://events.static.linux­found.org/sites/events/fi­les/slides/ASPgems%20-%20Kappa%20Architecture.pdf">https://events.static.linux­found.org/sites/events/fi­les/slides/ASPgems%20-%20Kappa%20Architecture.pdf</a>
</li>

<li>Apache Kafka Streams and Tables, the stream-table duality<br />
<a href="https://towardsdatascience.com/apache-kafka-streams-and-tables-the-stream-table-duality-ee904251a7e?gi=f22a29cd1854">https://towardsdatascience.com/apache-kafka-streams-and-tables-the-stream-table-duality-ee904251a7e?gi=f22a29cd1854</a>
</li>

<li>Configure Self-Managed Connectors<br />
<a href="https://docs.confluent.io/kafka-connectors/self-managed/configuring.html#configure-self-managed-connectors">https://docs.confluent.io/kafka-connectors/self-managed/configuring.html#configure-self-managed-connectors</a>
</li>

<li>Schema Evolution and Compatibility<br />
<a href="https://docs.confluent.io/platform/current/schema-registry/avro.html#schema-evolution-and-compatibility">https://docs.confluent.io/platform/current/schema-registry/avro.html#schema-evolution-and-compatibility</a>
</li>

<li>Configuring Key and Value Converters<br />
<a href="https://docs.confluent.io/kafka-connectors/self-managed/userguide.html#configuring-key-and-value-converters">https://docs.confluent.io/kafka-connectors/self-managed/userguide.html#configuring-key-and-value-converters</a>
</li>

<li>Introduction to Kafka Connectors<br />
<a href="https://www.baeldung.com/kafka-connectors-guide">https://www.baeldung.com/kafka-connectors-guide</a>
</li>

<li>Kafka CLI: command to list all consumer groups for a topic?<br />
<a href="https://stackoverflow.com/questions/63883999/kafka-cli-command-to-list-all-consumer-groups-for-a-topic">https://stackoverflow.com/questions/63883999/kafka-cli-command-to-list-all-consumer-groups-for-a-topic</a>
</li>

<li>Java Property File Processing<br />
<a href="https://www.w3resource.com/java-tutorial/java-propertyfile-processing.php">https://www.w3resource.com/java-tutorial/java-propertyfile-processing.php</a>
</li>

<li>Skipping bad records with the Kafka Connect JDBC sink connector<br />
<a href="https://rmoff.net/2019/10/15/skipping-bad-records-with-the-kafka-connect-jdbc-sink-connector/">https://rmoff.net/2019/10/15/skipping-bad-records-with-the-kafka-connect-jdbc-sink-connector/</a>
</li>

<li>Kafka Connect Deep Dive – Error Handling and Dead Letter Queues<br />
<a href="https://www.confluent.io/blog/kafka-connect-deep-dive-error-handling-dead-letter-queues/">https://www.confluent.io/blog/kafka-connect-deep-dive-error-handling-dead-letter-queues/</a>
</li>

<li>Errors and Dead Letter Queues<br />
<a href="https://developer.confluent.io/learn-kafka/kafka-connect/error-handling-and-dead-letter-queues/">https://developer.confluent.io/learn-kafka/kafka-connect/error-handling-and-dead-letter-queues/</a>
</li>

<li>Confluent Cloud Dead Letter Queue<br />
<a href="https://docs.confluent.io/cloud/current/connectors/dead-letter-queue.html">https://docs.confluent.io/cloud/current/connectors/dead-letter-queue.html</a>
</li>

<li>Dead Letter Queues (DLQs) in Kafka<br />
<a href="https://medium.com/@sannidhi.s.t/dead-letter-queues-dlqs-in-kafka-afb4b6835309">https://medium.com/@sannidhi.s.t/dead-letter-queues-dlqs-in-kafka-afb4b6835309</a>
</li>

<li>Deserializer<br />
<a href="https://docs.confluent.io/platform/current/schema-registry/serdes-develop/serdes-json.html#json-schema-serializer-and-deserializer">https://docs.confluent.io/platform/current/schema-registry/serdes-develop/serdes-json.html#json-schema-serializer-and-deserializer</a>
</li>

<li>JSON, Kafka, and the need for schema<br />
<a href="https://mikemybytes.com/2022/07/11/json-kafka-and-the-need-for-schema/">https://mikemybytes.com/2022/07/11/json-kafka-and-the-need-for-schema/</a>
</li>

<li>Using Kafka Connect with Schema Registry<br />
<a href="https://docs.confluent.io/platform/current/schema-registry/connect.html">https://docs.confluent.io/platform/current/schema-registry/connect.html</a>
</li>

<li>Zpracování dat reprezentovaných ve formátu JSON nástrojem jq<br />
<a href="https://www.root.cz/clanky/zpracovani-dat-reprezentovanych-ve-formatu-json-nastrojem-jq/">https://www.root.cz/clanky/zpracovani-dat-reprezentovanych-ve-formatu-json-nastrojem-jq/</a>
</li>

<li>Repositář projektu jq (GitHub)<br />
<a href="https://github.com/stedolan/jq">https://github.com/stedolan/jq</a>
</li>

<li>GitHub stránky projektu jq<br />
<a href="https://stedolan.github.io/jq/">https://stedolan.github.io/jq/</a>
</li>

<li>5 modern alternatives to essential Linux command-line tools<br />
<a href="https://opensource.com/ar­ticle/20/6/modern-linux-command-line-tools">https://opensource.com/ar­ticle/20/6/modern-linux-command-line-tools</a>
</li>

<li>Návod k nástroji jq<br />
<a href="https://stedolan.github.i­o/jq/tutorial/">https://stedolan.github.i­o/jq/tutorial/</a>
</li>

<li>jq Manual (development version)<br />
<a href="https://stedolan.github.io/jq/manual/">https://stedolan.github.io/jq/manual/</a>
</li>

<li>Introducing JSON<br />
<a href="https://www.json.org/json-en.html">https://www.json.org/json-en.html</a>
</li>

<li>Understanding JSON schema<br />
<a href="https://json-schema.org/understanding-json-schema/index.html">https://json-schema.org/understanding-json-schema/index.html</a>
</li>

<li>JDBC Sink Connector for Confluent Platform<br />
<a href="https://docs.confluent.io/kafka-connectors/jdbc/current/sink-connector/overview.html#jdbc-sink-connector-for-cp">https://docs.confluent.io/kafka-connectors/jdbc/current/sink-connector/overview.html#jdbc-sink-connector-for-cp</a>
</li>

<li>JDBC Connector (Source and Sink)<br />
<a href="https://www.confluent.io/hub/confluentinc/kafka-connect-jdbc">https://www.confluent.io/hub/confluentinc/kafka-connect-jdbc</a>
</li>

<li>Introduction to Schema Registry in Kafka<br />
<a href="https://medium.com/slalom-technology/introduction-to-schema-registry-in-kafka-915ccf06b902">https://medium.com/slalom-technology/introduction-to-schema-registry-in-kafka-915ccf06b902</a>
</li>

<li>Understanding JSON Schema Compatibility<br />
<a href="https://yokota.blog/2021/03/29/understanding-json-schema-compatibility/">https://yokota.blog/2021/03/29/understanding-json-schema-compatibility/</a>
</li>

</ol>



<p></p><p></p>
<p><small>Autor: <a href="http://www.fit.vutbr.cz/~tisnovpa">Pavel Tišnovský</a> &nbsp; 2024</small></p>
</body>
</html>

