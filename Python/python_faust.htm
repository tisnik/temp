<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
        "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html>
<head>
<title>Faust: platforma pro proudovÃ© zpracovÃ¡nÃ­ dat v Pythonu</title>
<meta name="Author" content="Pavel Tisnovsky" />
<meta name="Generator" content="vim" />
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<style type="text/css">
         body {color:#000000; background:#ffffff;}
         h1  {font-family: arial, helvetica, sans-serif; color:#ffffff; background-color:#c00000; text-align:center; padding-left:1em}
         h2  {font-family: arial, helvetica, sans-serif; color:#ffffff; background-color:#0000c0; padding-left:1em; text-align:left}
         h3  {font-family: arial, helvetica, sans-serif; color:#000000; background-color:#c0c0c0; padding-left:1em; text-align:left}
         h4  {font-family: arial, helvetica, sans-serif; color:#000000; background-color:#e0e0e0; padding-left:1em; text-align:left}
         a   {font-family: arial, helvetica, sans-serif;}
         li  {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify; width:450px;}
         ol  {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify; width:450px;}
         ul  {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify; width:450px;}
         p   {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify;}
         pre {background:#e0e0e0}
</style>
</head>

<body>

<h1>Faust: platforma pro proudovÃ© zpracovÃ¡nÃ­ dat v Pythonu</h1>

<h3>Pavel TiÅ¡novskÃ½</h3>

<p></p>

<h1>ÃšvodnÃ­k</h1>

<p>Dnes se seznÃ¡mÃ­me se zajÃ­mavÄ› koncipovanou knihovnou nazvanou Faust. JednÃ¡ se o knihovnu zajiÅ¡Å¥ujÃ­cÃ­ proudovÃ© zpracovÃ¡nÃ­ dat, kterÃ¡ je postavena nad Apache Kafkou. NejednÃ¡ se vÅ¡ak o pouhou realizaci producentÅ¯ a konzumentÅ¯, protoÅ¾e je moÅ¾nÃ© pouÅ¾Ã­vat lokÃ¡lnÃ­ tabulky, asynchronnÃ­ zpracovÃ¡nÃ­ atd.</p>



<h2>Obsah</h2>

<p><a href="#k01">*** 1. Faust: platforma pro proudovÃ© zpracovÃ¡nÃ­ dat v&nbsp;Pythonu</a></p>
<p><a href="#k02">*** 2. ProblematickÃ¡ ÄÃ¡st &ndash; instalace knihovny Faust</a></p>
<p><a href="#k03">*** 3. Instalace, konfigurace a spuÅ¡tÄ›nÃ­ Apache Kafky</a></p>
<p><a href="#k04">*** 4. OtestovÃ¡nÃ­ Äinnosti Apache Kafky</a></p>
<p><a href="#k05">*** 5. KlasickÃ½ producent a konzument</a></p>
<p><a href="#k06">*** 6. Konzument realizovanÃ½ formou workera postavenÃ½ na knihovnÄ› Faust</a></p>
<p><a href="#k07">*** 7. SpuÅ¡tÄ›nÃ­ workera</a></p>
<p><a href="#k08">*** 8. Producent posÃ­lajÃ­cÃ­ zprÃ¡vy do vÄ›tÅ¡Ã­ho mnoÅ¾stvÃ­ tÃ©mat</a></p>
<p><a href="#k09">*** 9. Konzumace zprÃ¡v workerem z&nbsp;vÄ›tÅ¡Ã­ho mnoÅ¾stvÃ­ tÃ©mat</a></p>
<p><a href="#k10">*** 10. </a></p>
<p><a href="#k11">*** 11. </a></p>
<p><a href="#k12">*** 12. </a></p>
<p><a href="#k13">*** 13. </a></p>
<p><a href="#k14">*** 14. </a></p>
<p><a href="#k15">*** 15. </a></p>
<p><a href="#k16">*** 16. </a></p>
<p><a href="#k17">*** 17. </a></p>
<p><a href="#k18">*** 18. </a></p>
<p><a href="#k19">*** 19. RepositÃ¡Å™ s&nbsp;demonstraÄnÃ­mi pÅ™Ã­klady</a></p>
<p><a href="#k20">*** 20. Odkazy na Internetu</a></p>



<p><a name="k01"></a></p>
<h2 id="k01">1. Faust: platforma pro proudovÃ© zpracovÃ¡nÃ­ dat v&nbsp;Pythonu</h2>



<p><a name="k02"></a></p>
<h2 id="k02">2. ProblematickÃ¡ ÄÃ¡st &ndash; instalace knihovny Faust</h2>



<p><a name="k03"></a></p>
<h2 id="k03">3. Instalace, konfigurace a spuÅ¡tÄ›nÃ­ Apache Kafky</h2>

<p>DneÅ¡nÃ­ ÄlÃ¡nek je zamÄ›Å™en na ukÃ¡zku nÄ›kterÃ½ch moÅ¾nostÃ­ nabÃ­zenÃ½ch knihovnou
Faust. KromÄ› instalace knihovny Faust tedy budeme potÅ™ebovat spustit (lokÃ¡lnÄ›,
vzdÃ¡lenÄ›, Äi v&nbsp;kontejneru) Apache Kafku. KonkrÃ©tnÄ› nÃ¡m bude postaÄovat
spuÅ¡tÄ›nÃ­ jedinÃ©ho Kafka clusteru, pÅ™iÄemÅ¾ kaÅ¾dÃ½ Kafka cluster se sklÃ¡dÃ¡
z&nbsp;jednoho bÄ›Å¾Ã­cÃ­ho Zookeepera a z&nbsp;jednoho brokera (tedy ze dvou
procesÅ¯, kterÃ© mezi sebou komunikujÃ­ po nakonfigurovanÃ½ch portech).</p>

<p>V&nbsp;praktickÃ© ÄÃ¡sti budeme brokera Apache Kafky i Zookeepera spouÅ¡tÄ›t
lokÃ¡lnÄ› (popÅ™.&nbsp;z&nbsp;Dockeru Äi Podmana), takÅ¾e je nejdÅ™Ã­ve nutnÃ© Apache
Kafku nainstalovat. NenÃ­ to ve skuteÄnosti vÅ¯bec nic sloÅ¾itÃ©ho. V&nbsp;pÅ™Ã­padÄ›,
Å¾e je na poÄÃ­taÄi nainstalovÃ¡no JRE (bÄ›hovÃ© prostÅ™edÃ­ Javy), je instalace
Apache Kafky pro testovacÃ­ ÃºÄely triviÃ¡lnÃ­. V&nbsp;ÄlÃ¡nku si ukÃ¡Å¾eme instalaci
verze 2.13-3.6.1, ovÅ¡em mÅ¯Å¾ete si stÃ¡hnout i prakticky libovolnou novÄ›jÅ¡Ã­ Äi
nÄ›kterÃ© starÅ¡Ã­ verze (3.5.x nebo 3.6.x, ovÅ¡em dÃ¡le popsanÃ½ postup by mÄ›l bÃ½t
platnÃ½ i pro jeÅ¡tÄ› starÅ¡Ã­ verze, v&nbsp;podstatÄ› mÅ¯Å¾eme dojÃ­t aÅ¾ k&nbsp;verzi
2.4.0).  Tarball s&nbsp;instalacÃ­ Apache Kafky lze zÃ­skat z&nbsp;adresy <a
href="https://downloads.apache.org/kafka/3.6.1/kafka_2.13-3.6.1.tgz">https://downloads.apache.org/kafka/3.6.1/kafka_2.13-3.6.1.tgz</a>.</p>

<p>StaÅ¾enÃ­ a rozbalenÃ­ tarballu zajistÃ­ nÃ¡sledujÃ­cÃ­ sekvence pÅ™Ã­kazÅ¯:</p>

<pre>
$ <strong>wget https://downloads.apache.org/kafka/3.6.1/kafka_2.13-3.6.1.tgz</strong>
$ <strong>tar xvfz kafka_2.13-3.6.1.tgz</strong>
$ <strong>cd kafka_2.13-3.6.1/</strong>
</pre>

<p>Po rozbalenÃ­ staÅ¾enÃ©ho tarballu zÃ­skÃ¡me adresÃ¡Å™, v&nbsp;nÄ›mÅ¾ se nachÃ¡zÃ­
vÅ¡echny potÅ™ebnÃ© Java archivy (JAR), konfiguraÄnÃ­ soubory (v&nbsp;podadresÃ¡Å™i
<strong>config</strong>) a nÄ›kolik pomocnÃ½ch skriptÅ¯ (v&nbsp;podadresÃ¡Å™i
<strong>bin</strong> a <strong>bin/windows</strong>). Pro spuÅ¡tÄ›nÃ­ Zookeepera a
brokerÅ¯ je zapotÅ™ebÃ­, jak jsme si jiÅ¾ Å™ekli v&nbsp;pÅ™edchozÃ­m odstavci, mÃ­t
nainstalovÃ¡nu JRE (Java Runtime Environment) a samozÅ™ejmÄ› tÃ©Å¾ nÄ›jakÃ½ shell
(BASH, cmd, ...).</p> 
 
<pre>
.
â”œâ”€â”€ bin
â”‚Â Â  â””â”€â”€ windows
â”œâ”€â”€ config
â”‚Â Â  â””â”€â”€ kraft
â”œâ”€â”€ libs
â”œâ”€â”€ licenses
â””â”€â”€ site-docs
&nbsp;
7 directories
</pre>

<p>Mezi dÅ¯leÅ¾itÃ© soubory, kterÃ© budeme pouÅ¾Ã­vat v&nbsp;rÃ¡mci dalÅ¡Ã­ch kapitol,
patÅ™Ã­ pÅ™edevÅ¡Ã­m skripty pro spouÅ¡tÄ›nÃ­ jednotlivÃ½ch sluÅ¾eb. Tyto skripty jsou
uloÅ¾eny v&nbsp;podadresÃ¡Å™i <strong>bin</strong> (a pro Windows jeÅ¡tÄ›
v&nbsp;dalÅ¡Ã­m podadresÃ¡Å™i <strong>windows</strong>):</p>

<table>
<tr><th>Skript</th><th>StruÄnÃ½ popis</th></tr>
<tr><td>bin/kafka-server-start.sh</td><td>spuÅ¡tÄ›nÃ­ brokera</td></tr>
<tr><td>bin/zookeeper-server-start.sh</td><td>spuÅ¡tÄ›nÃ­ Zookeepera</td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>bin/kafka-configs.sh</td><td>konfigurace brokerÅ¯</td></tr>
<tr><td>bin/kafka-topics.sh</td><td>konfigurace tÃ©mat, zjiÅ¡tÄ›nÃ­ informace o tÃ©matech atd.</td></tr>
<tr><td>bin/kafka-consumer-groups.sh</td><td>konfigurace popÅ™.&nbsp;zjiÅ¡tÄ›nÃ­ informacÃ­ o skupinÃ¡ch konzumentÅ¯</td></tr>
</table>

<p><div class="rs-tip-major">PoznÃ¡mka: vÄ›tÅ¡ina vÃ½Å¡e uvedenÃ½ch skriptÅ¯ byla
upravena i pro spuÅ¡tÄ›nÃ­ ve Windows. Tyto varianty naleznete v&nbsp;podadresÃ¡Å™i
<strong>bin/windows</strong>.</div></p>

<p>NynÃ­ si jiÅ¾ Kafka cluster spustÃ­me. NejdÅ™Ã­ve je vÅ¾dy nutnÃ© spustit
Zookeepera a teprve potÃ© brokera Äi brokery. Pro sledovÃ¡nÃ­ Äinnosti obou
procesÅ¯ si mÅ¯Å¾ete Zookeepera i brokera spustit v&nbsp;samostatnÃ©m terminÃ¡lu,
vyuÅ¾Ã­t nÃ¡stroj <strong>screen</strong> atd.</p>

<p>SpuÅ¡tÄ›nÃ­ Zookeepera:</p>

<pre>
$ <strong>cd ${kafka_dir}</strong>
$ <strong>bin/zookeeper-server-start.sh zookeeper.properties</strong>
</pre>

<p>SpuÅ¡tÄ›nÃ­ brokera:</p>

<pre>
$ <strong>cd ${kafka_dir}</strong>
$ <strong>bin/kafka-server-start.sh server.properties</strong>
</pre>



<p><a name="k04"></a></p>
<h2 id="k04">4. OtestovÃ¡nÃ­ Äinnosti Apache Kafky</h2>

<p>Pro otestovÃ¡nÃ­, jestli je prÃ¡vÄ› zprovoznÄ›nÃ½ Kafka cluster skuteÄnÄ› funkÄnÃ­, si vytvoÅ™Ã­me jednoduchÃ© producenty a konzumenty zprÃ¡v. SamozÅ™ejmÄ› by bylo moÅ¾nÃ© pouÅ¾Ã­t konzolovÃ©ho producenta a konzumenta, kterÃ½ je souÄÃ¡stÃ­ instalace Apache Kafky, ovÅ¡em moÅ¾nosti tÄ›chto nÃ¡strojÅ¯ jsou ve skuteÄnosti velmi omezenÃ© a nebudou nÃ¡m postaÄovat.</p>

<p>PÅ™ed vytvoÅ™enÃ­m producenta zprÃ¡v v&nbsp;Pythonu je nutnÃ© nainstalovat
knihovnu, kterÃ¡ zabezpeÄuje pÅ™ipojenÃ­ ke Kafce. Tuto knihovnu mÅ¯Å¾eme
nainstalovat pouze pro aktivnÃ­ho uÅ¾ivatele (coÅ¾ je nejjednoduÅ¡Å¡Ã­ a nevyÅ¾aduje
to rootovskÃ¡ prÃ¡va) s&nbsp;vyuÅ¾itÃ­m pÅ™Ã­kazu <strong>pip</strong>, protoÅ¾e tato
knihovna je pochopitelnÄ› dostupnÃ¡ na <a
href="https://pypi.org/project/kafka/">PyPi</a>:</p>

<pre>
$ <strong>pip3 install --verbose kafka-python-ng</strong>
&nbsp;
Using pip 22.3.1 from /usr/lib/python3.11/site-packages/pip (python 3.11)
Defaulting to user installation because normal site-packages is not writeable
Collecting kafka-python-ng
  Downloading kafka_python_ng-2.2.2-py2.py3-none-any.whl (232 kB)
     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 232.4/232.4 kB 1.5 MB/s eta 0:00:00
Installing collected packages: kafka-python-ng
Successfully installed kafka-python-ng-2.2.2
</pre>



<p><a name="k05"></a></p>
<h2 id="k05">5. KlasickÃ½ producent a konzument</h2>

<p>Producent je pÅ™edstavovÃ¡n instancÃ­ tÅ™Ã­dy <strong>KafkaProducer</strong>.
Konstruktoru tÃ©to tÅ™Ã­dy se pÅ™edÃ¡vÃ¡ pouze seznam brokerÅ¯ (v&nbsp;naÅ¡em pÅ™Ã­padÄ›
jedinÃ½ broker) a popÅ™.&nbsp;i dalÅ¡Ã­ nepovinnÃ© Ãºdaje. Mezi nÄ› patÅ™Ã­ i handler,
kterÃ½ je zavolÃ¡n pÅ™ed serializacÃ­ kaÅ¾dÃ© zprÃ¡vy. Prakticky kaÅ¾dou zprÃ¡vu je nutnÃ© serializovat, protoÅ¾e zprÃ¡vy v&nbsp;Apache Kafce tvoÅ™Ã­ dÃ¡le nestrukturovanÃ¡ sekvence bajtÅ¯. TudÃ­Å¾ napÅ™Ã­klad i Å™etÄ›zce (PythonovskÃ© Å™etÄ›zce) se explicitnÄ› pÅ™evÃ¡dÃ­ na hodnotu typu <strong>bytes</strong>, tj.&nbsp;na nemÄ›nitelnou (<i>immutable</i>) sekvenci bajtÅ¯. PÅ™evod Å™etÄ›zcÅ¯ do tuto sekvenci je realizovÃ¡n metodou <strong>encode</strong>, kterÃ© se pÅ™edÃ¡ vyÅ¾adovanÃ© kÃ³dovÃ¡nÃ­ (tj.&nbsp;mapovÃ¡nÃ­ mezi znaky na vstupu a jednÃ­m aÅ¾ vÃ­ce bajty ve vÃ½slednÃ© sekvenci):</p>

<pre>
producer = KafkaProducer(
    bootstrap_servers=[server],
    value_serializer=lambda x: x.encode("utf-8")
)
</pre>

<p>PoslÃ¡nÃ­ zprÃ¡vy se provÃ¡dÃ­ metodou <strong>KafkaProducer.send</strong>.
ZprÃ¡va je, jak jiÅ¾ vÃ­me z&nbsp;pÅ™edchozÃ­ho textu, uloÅ¾ena do zvolenÃ©ho tÃ©matu
(<i>topic</i>) a sklÃ¡dÃ¡ se z&nbsp;tÄ›la, popÅ™.&nbsp;klÃ­Äe a tÄ›la:</p>

<pre>
producer.send(topic, value=data)
</pre>

<p>V&nbsp;naÅ¡em prvnÃ­m demonstraÄnÃ­m producentovi budou posÃ­lÃ¡ny Å™etÄ›zce, kterÃ© se budou postupnÄ› mÄ›nit podle hodnoty ÄÃ­taÄe (<i>counter</i>). Mezi jednotlivÃ© zprÃ¡vy jsou vklÃ¡dÃ¡ny ÄasovÃ© pauzy o dÃ©lce pÅ™ibliÅ¾nÄ› jednÃ© sekundy:</p>

<pre>
for i in range(1000):
    print(i)
    message = f"Greeting #{i}"
    producer.send(topic, value=message)
    sleep(1)
</pre>

<p>Takto vypadÃ¡ ÃºplnÃ½ zdrojovÃ½ kÃ³d producenta zprÃ¡v:</p>

<pre>
#!/usr/bin/env python3
&nbsp;
from kafka import KafkaProducer
from time import sleep
&nbsp;
&nbsp;
server = "localhost:9092"
topic = "greetings"
&nbsp;
print("Connecting to Kafka")
producer = KafkaProducer(
    bootstrap_servers=[server],
    value_serializer=lambda x: x.encode("utf-8")
)
print("Connected to Kafka")
&nbsp;
for i in range(1000):
    print(i)
    message = f"Greeting #{i}"
    producer.send(topic, value=message)
    sleep(1)
</pre>

<p><div class="rs-tip-major">PoznÃ¡mka: povÅ¡imnÄ›te si, Å¾e nÃ¡m pro poslÃ¡nÃ­ zprÃ¡vy
postaÄuje znÃ¡t adresu brokera a jmÃ©no tÃ©matu. Konzument (popsanÃ½ nÃ­Å¾e) je
nepatrnÄ› sloÅ¾itÄ›jÅ¡Ã­, protoÅ¾e je nutnÃ© specifikovat i jmÃ©no skupiny (<i>consumer
group</i>).</div></p>

<p>Implementace jednoduchÃ©ho konzumenta zprÃ¡v v&nbsp;programovacÃ­m jazyku
Python je pomÄ›rnÄ› snadnÃ¡ a pÅ™Ã­liÅ¡ se neliÅ¡Ã­ od implementace konzumenta.
NejdÅ™Ã­ve je nutnÃ© vytvoÅ™it instanci tÅ™Ã­dy <strong>KafkaConsumer</strong>,
pÅ™iÄemÅ¾ se specifikuje tÃ©ma (<i>topic</i>), skupina, seznam brokerÅ¯ a nepovinnÄ›
tÃ©Å¾ informace o tom, jakÃ½m zpÅ¯sobem se mÃ¡ nastavit offset prvnÃ­ zpracovanÃ©
zprÃ¡vy. VÄ›tÅ¡inou budeme potÅ™ebovat zpracovÃ¡vat nejnovÄ›jÅ¡Ã­ (<i>earliest</i>)
zprÃ¡vy, takÅ¾e nastavenÃ­ konzumenta mÅ¯Å¾e vypadat napÅ™Ã­klad nÃ¡sledovnÄ›:</p>

<pre>
consumer = KafkaConsumer(
    topic, group_id=group_id,
    bootstrap_servers=[server],
    auto_offset_reset="earliest"
)
</pre>

<p>SamotnÃ¡ konzumace (ÄtenÃ­ a zpracovÃ¡nÃ­) zprÃ¡v mÅ¯Å¾e probÃ­hat v&nbsp;nekoneÄnÃ©
programovÃ© smyÄce, pÅ™iÄemÅ¾ kaÅ¾dÃ¡ zprÃ¡va je reprezentovÃ¡na objektem obsahujÃ­cÃ­m
tÃ©ma, ÄÃ­slo oddÃ­lu, offset v&nbsp;rÃ¡mci oddÃ­lu, klÃ­Ä zprÃ¡vy a samozÅ™ejmÄ› tÃ©Å¾
obsah zprÃ¡vy (klÃ­Ä zprÃ¡vy a tÄ›lo zprÃ¡vy nenÃ­ Å¾Ã¡dnÃ½ zpÅ¯sobem dekÃ³dovÃ¡no
resp.&nbsp;deserializovÃ¡no):</p>

<pre>
for message in consumer:
    print("%s:%d:%d: key=%s value=%s" % (
            message.topic,
            message.partition,
            message.offset,
            message.key,
            message.value,
        )
    )
</pre>

<p>A takto vypadÃ¡ ÃºplnÃ½ zdrojovÃ½ kÃ³d bÄ›Å¾nÃ©ho konzumenta zprÃ¡v:</p>

<pre>
#!/usr/bin/env python3
&nbsp;
import sys
from kafka import KafkaConsumer
&nbsp;
server = "localhost:9092"
topic = "greetings"
group_id = "group1"
&nbsp;
print("Connecting to Kafka")
consumer = KafkaConsumer(
    topic, group_id=group_id,
    bootstrap_servers=[server],
    auto_offset_reset="earliest"
)
print("Connected to Kafka")
&nbsp;
try:
    for message in consumer:
        print(
            "%s:%d:%d: key=%s value=%s"
            % (
                message.topic,
                message.partition,
                message.offset,
                message.key,
                message.value,
            )
        )
except KeyboardInterrupt:
    sys.exit()
</pre>



<p><a name="k06"></a></p>
<h2 id="k06">6. Konzument realizovanÃ½ formou workera postavenÃ½ na knihovnÄ› Faust</h2>

<pre>
import faust
&nbsp;
app = faust.App(
    "hello-world",
    broker="kafka://localhost:9092",
    value_serializer="raw",
)
&nbsp;
greetings_topic = app.topic("greetings")
&nbsp;
@app.agent(greetings_topic)
async def greet(greetings):
    async for greeting in greetings:
        print(greeting)
&nbsp;
&nbsp;
if __name__ == "__main__":
    app.main()
</pre>



<p><a name="k07"></a></p>
<h2 id="k07">7. SpuÅ¡tÄ›nÃ­ workera</h2>

<pre>
â”ŒÆ’aÂµSâ€  v0.11.1.dev4+ga489db3bâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ id          â”‚ hello-world                                     â”‚
â”‚ transport   â”‚ [URL('kafka://localhost:9092')]                 â”‚
â”‚ store       â”‚ memory:                                         â”‚
â”‚ web         â”‚ http://localhost:6066/                          â”‚
â”‚ log         â”‚ -stderr- (warn)                                 â”‚
â”‚ pid         â”‚ 1296383                                         â”‚
â”‚ hostname    â”‚ ptisnovs.xxx.yyy.zzz                            â”‚
â”‚ platform    â”‚ CPython 3.11.8 (Linux x86_64)                   â”‚
â”‚        +    â”‚ Cython (GCC 13.2.1 20231011 (Red Hat 13.2.1-4)) â”‚
â”‚ drivers     â”‚                                                 â”‚
â”‚   transport â”‚ aiokafka=0.10.0                                 â”‚
â”‚   web       â”‚ aiohttp=3.9.5                                   â”‚
â”‚ datadir     â”‚ /tmp/ramdisk/faust/hello-world-data             â”‚
â”‚ appdir      â”‚ /tmp/ramdisk/faust/hello-world-data/v1          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
startingâ¢ ğŸ˜Š
</pre>

<p>Ihned po vÃ½pisu zprÃ¡vy &bdquo;starting&ldquo; by mÄ›l worker zaÄÃ­t se ÄtenÃ­m
a zpracovÃ¡vÃ¡nÃ­m zprÃ¡v, kterÃ© jsou konzumovÃ¡ny z&nbsp;tÃ©matu
&bdquo;greetings&ldquo;. Informace o kaÅ¾dÃ© zkonzumovanÃ© zprÃ¡vÄ› je vypisovÃ¡na do
terminÃ¡lu:</p>

<pre>
[2024-04-20 18:03:39,583] [1296383] [WARNING] b'Greeting #0' 
[2024-04-20 18:03:39,583] [1296383] [WARNING] b'Greeting #1' 
[2024-04-20 18:03:39,584] [1296383] [WARNING] b'Greeting #2' 
[2024-04-20 18:03:39,584] [1296383] [WARNING] b'Greeting #3' 
[2024-04-20 18:03:39,584] [1296383] [WARNING] b'Greeting #4' 
</pre>

<p><div class="rs-tip-major">PoznÃ¡mka: povÅ¡imnÄ›te si, Å¾e tÄ›la zprÃ¡v jsou reprezentovÃ¡na typem <i>bytes</i>, tj.&nbsp;nemÄ›nitelnou (<i>immutable</i>) sekvencÃ­ bajtÅ¯.</div></p>



<p><a name="k08"></a></p>
<h2 id="k08">8. Producent posÃ­lajÃ­cÃ­ zprÃ¡vy do vÄ›tÅ¡Ã­ho mnoÅ¾stvÃ­ tÃ©mat</h2>

<pre>
#!/usr/bin/env python3
&nbsp;
from kafka import KafkaProducer
from time import sleep
from json import dumps
&nbsp;
server = "localhost:9092"
topic1 = "greetings"
topic2 = "real_work"
&nbsp;
print("Connecting to Kafka")
producer = KafkaProducer(
    bootstrap_servers=[server],
    value_serializer=lambda x: dumps(x).encode("utf-8")
)
print("Connected to Kafka")
&nbsp;
for i in range(1000):
    print(i)
    message = f"Greeting #{i}"
    producer.send(topic1, value=message)
    sleep(1)
    work = f"Real work #{i*2}"
    producer.send(topic2, value=work)
    work = f"Real work #{i*2+1}"
    producer.send(topic2, value=work)
    sleep(1)
</pre>



<p><a name="k09"></a></p>
<h2 id="k09">9. Konzumace zprÃ¡v workerem z&nbsp;vÄ›tÅ¡Ã­ho mnoÅ¾stvÃ­ tÃ©mat</h2>

<pre>
import faust
&nbsp;
app = faust.App(
    "hello-world",
    broker="kafka://localhost:9092",
    value_serializer="raw",
)
&nbsp;
greetings_topic = app.topic("greetings")
real_work_topic = app.topic("real_work")
&nbsp;
@app.agent(greetings_topic)
async def greet(greetings):
    async for greeting in greetings:
        print(f"Greeter: {greeting}")
&nbsp;
@app.agent(real_work_topic)
async def worker(works):
    async for work in works:
        print(f"Worker: {work}")
&nbsp;
&nbsp;
if __name__ == "__main__":
    app.main()
</pre>



<p><a name="k10"></a></p>
<h2 id="k10">10. Produkce a konzumace zprÃ¡v ve formÃ¡tu JSON</h2>

<pre>
#!/usr/bin/env python3
&nbsp;
from kafka import KafkaProducer
from time import sleep
&nbsp;
server = "localhost:9092"
topic1 = "greetings"
topic2 = "real_work"
&nbsp;
print("Connecting to Kafka")
producer = KafkaProducer(
    bootstrap_servers=[server],
    value_serializer=lambda x: x.encode("utf-8")
)
print("Connected to Kafka")
&nbsp;
for i in range(1000):
    print(i)
    message = {"subject": "Greeting", "value": i}
    producer.send(topic1, value=message)
    sleep(1)
    work = {"subject": "Real work", "value": i*2}
    producer.send(topic2, value=work)
    work = {"subject": "Real work", "value": i*2+1}
    producer.send(topic2, value=work)
    sleep(1)
</pre>

<pre>
import faust
&nbsp;
app = faust.App(
    "hello-world",
    broker="kafka://localhost:9092",
    value_serializer="json",
)
&nbsp;
greetings_topic = app.topic("greetings")
real_work_topic = app.topic("real_work")
&nbsp;
@app.agent(greetings_topic)
async def greet(greetings):
    async for greeting in greetings:
        print(f"Greeter: {greeting}")
&nbsp;
@app.agent(real_work_topic)
async def worker(works):
    async for work in works:
        print(f"Worker: {work}")
&nbsp;
&nbsp;
if __name__ == "__main__":
    app.main()
</pre>



<p><a name="k11"></a></p>
<h2 id="k11">11. </h2>



<p><a name="k12"></a></p>
<h2 id="k12">12. </h2>



<p><a name="k13"></a></p>
<h2 id="k13">13. </h2>



<p><a name="k14"></a></p>
<h2 id="k14">14. </h2>



<p><a name="k15"></a></p>
<h2 id="k15">15. </h2>



<p><a name="k16"></a></p>
<h2 id="k16">16. </h2>



<p><a name="k17"></a></p>
<h2 id="k17">17. </h2>



<p><a name="k18"></a></p>
<h2 id="k18">18. </h2>



<p><a name="k19"></a></p>
<h2 id="k19">19. RepositÃ¡Å™ s&nbsp;demonstraÄnÃ­mi pÅ™Ã­klady</h2>

<p>ZdrojovÃ© kÃ³dy vÅ¡ech prozatÃ­m popsanÃ½ch demonstraÄnÃ­ch pÅ™Ã­kladÅ¯ urÄenÃ½ch pro
programovacÃ­ jazyk Python 3 byly uloÅ¾eny do Git repositÃ¡Å™e dostupnÃ©ho na adrese
<a
href="https://github.com/tisnik/most-popular-python-libs">https://github.com/tisnik/most-popular-python-libs</a>:</p>

<table>
<tr><th> #</th><th>DemonstraÄnÃ­ pÅ™Ã­klad</th><th>StruÄnÃ½ popis pÅ™Ã­kladu</th><th>Cesta</th></tr>
<tr><td> 1</td><td>greeting_producer.py</td><td>klasickÃ½ producent zprÃ¡v vytvoÅ™enÃ½ bez pouÅ¾itÃ­ knihovny Faust</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faust/greeting_producer.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faust/greeting_producer.py</a></td></tr>
<tr><td> 2</td><td>greeting_consumer.py</td><td>klasickÃ½ konzument zprÃ¡v vytvoÅ™enÃ½ bez pouÅ¾itÃ­ knihovny Faust</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faust/greeting_consumer.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faust/greeting_consumer.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td> 3</td><td>greeting_faust_consumer.py</td><td>worker definovanÃ½ s&nbsp;vyuÅ¾itÃ­m knihovny Faust</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faust/greeting_faust_consumer.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faust/greeting_faust_consumer.py</a></td></tr>
<tr><td> 4</td><td>multi_producer_raw.py</td><td>producent zprÃ¡v do vÄ›tÅ¡Ã­ho mnoÅ¾stvÃ­ tÃ©mat, zprÃ¡vy jsou posÃ­lÃ¡ny jako sekvence bajtÅ¯</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faust/multi_producer_raw.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faust/multi_producer_raw.py</a></td></tr>
<tr><td> 5</td><td>greeting_worker_consumer_raw.py</td><td>dvojice workerÅ¯ definovanÃ½ch s&nbsp;vyuÅ¾itÃ­m knihovny Faust pro zprÃ¡vy posÃ­lanÃ© jako sekvence bajtÅ¯</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faust/greeting_worker_consumer_raw.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faust/greeting_worker_consumer_raw.py</a></td></tr>
<tr><td> 6</td><td>multi_producer_json.py</td><td>producent zprÃ¡v do vÄ›tÅ¡Ã­ho mnoÅ¾stvÃ­ tÃ©mat, zprÃ¡vy jsou serializovÃ¡ny do formÃ¡tu JSON</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faust/multi_producer_json.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faust/multi_producer_json.py</a></td></tr>
<tr><td> 7</td><td>greeting_worker_consumer_json.py</td><td>dvojice workerÅ¯ definovanÃ½ch s&nbsp;vyuÅ¾itÃ­m knihovny Faust pro zprÃ¡vy ve formÃ¡tu JSON</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faust/greeting_worker_consumer_json.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faust/greeting_worker_consumer_json.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td> 8</td><td></td><td></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faust/">https://github.com/tisnik/most-popular-python-libs/blob/master/faust/</a></td></tr>
<tr><td> 9</td><td></td><td></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faust/">https://github.com/tisnik/most-popular-python-libs/blob/master/faust/</a></td></tr>
<tr><td>10</td><td></td><td></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faust/">https://github.com/tisnik/most-popular-python-libs/blob/master/faust/</a></td></tr>
</table>



<p><a name="k20"></a></p>
<h2 id="k20">20. Odkazy na Internetu</h2>

<ol>

<li>ETL Batch Processing With Kafka?<br />
<a href="https://medium.com/swlh/etl-batch-processing-with-kafka-7f66f843e20d">https://medium.com/swlh/etl-batch-processing-with-kafka-7f66f843e20d</a>
</li>

<li>ETL with Kafka<br />
<a href="https://blog.codecentric.de/en/2018/03/etl-kafka/">https://blog.codecentric.de/en/2018/03/etl-kafka/</a>
</li>

<li>Building ETL Pipelines with Clojure and Transducers<br />
<a href="https://www.grammarly.com/blog/engineering/building-etl-pipelines-with-clojure-and-transducers/">https://www.grammarly.com/blog/engineering/building-etl-pipelines-with-clojure-and-transducers/</a>
</li>

<li>pipeline (moÅ¾nÃ© pouÅ¾Ã­t pro ETL)<br />
<a href="https://clojuredocs.org/clojure.core.async/pipeline">https://clojuredocs.org/clojure.core.async/pipeline</a>
</li>

<li>On Track with Apache Kafka â€“ Building a Streaming ETL Solution with Rail Data<br />
<a href="https://www.confluent.io/blog/build-streaming-etl-solutions-with-kafka-and-rail-data/">https://www.confluent.io/blog/build-streaming-etl-solutions-with-kafka-and-rail-data/</a>
</li>

<li>Kafka - Understanding Offset Commits<br />
<a href="https://www.logicbig.com/tutorials/misc/kafka/committing-offsets.html">https://www.logicbig.com/tutorials/misc/kafka/committing-offsets.html</a>
</li>

<li>fundingcircle/jackdaw (na Clojars)<br />
<a href="https://clojars.org/fundingcircle/jackdaw/versions/0.7.6">https://clojars.org/fundingcircle/jackdaw/versions/0.7.6</a>
</li>

<li>Dokumentace ke knihovnÄ› jackdaw<br />
<a href="https://cljdoc.org/d/fundingcircle/jackdaw/0.7.6/doc/readme">https://cljdoc.org/d/fundingcircle/jackdaw/0.7.6/doc/readme</a>
</li>

<li>Jackdaw AdminClient API<br />
<a href="https://cljdoc.org/d/fundingcircle/jackdaw/0.7.6/doc/jackdaw-adminclient-api">https://cljdoc.org/d/fundingcircle/jackdaw/0.7.6/doc/jackdaw-adminclient-api</a>
</li>

<li>Jackdaw Client API<br />
<a href="https://cljdoc.org/d/fundingcircle/jackdaw/0.7.6/doc/jackdaw-client-api">https://cljdoc.org/d/fundingcircle/jackdaw/0.7.6/doc/jackdaw-client-api</a>
</li>

<li>Kafka.clj<br />
<a href="https://github.com/helins-io/kafka.clj">https://github.com/helins-io/kafka.clj</a>
</li>

<li>Kafka mirroring (MirrorMaker)<br />
<a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=27846330">https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=27846330</a>
</li>

<li>Mastering Kafka migration with MirrorMaker 2<br />
<a href="https://developers.redhat.com/articles/2024/01/04/mastering-kafka-migration-mirrormaker-2">https://developers.redhat.com/articles/2024/01/04/mastering-kafka-migration-mirrormaker-2</a>
</li>

<li>Apache Kafka MirrorMaker 2 (MM2) Part 1: Theory<br />
<a href="https://www.instaclustr.com/blog/kafka-mirrormaker-2-theory/#h-2-replication-in-kafka">https://www.instaclustr.com/blog/kafka-mirrormaker-2-theory/#h-2-replication-in-kafka</a>
</li>

<li>Apache Kafka MirrorMaker 2 (MM2) Part 2: Practice<br />
<a href="https://www.instaclustr.com/blog/apache-kafka-mirrormaker-2-practice/">https://www.instaclustr.com/blog/apache-kafka-mirrormaker-2-practice/</a>
</li>

<li>Demystifying Kafka MirrorMaker 2: Use cases and architecture <br />
<a href="https://developers.redhat.com/articles/2023/11/13/demystifying-kafka-mirrormaker-2-use-cases-and-architecture#">https://developers.redhat.com/articles/2023/11/13/demystifying-kafka-mirrormaker-2-use-cases-and-architecture#</a>
</li>

<li>How to use Kafka MirrorMaker 2.0 in data migration, replication and the use-cases<br />
<a href="https://learn.microsoft.com/en-us/azure/hdinsight/kafka/kafka-mirrormaker-2-0-guide">https://learn.microsoft.com/en-us/azure/hdinsight/kafka/kafka-mirrormaker-2-0-guide</a>
</li>

<li>Release Notes - Kafka - Version 2.4.0<br />
<a href="https://archive.apache.org/dist/kafka/2.4.0/RELEASE_NOTES.html">https://archive.apache.org/dist/kafka/2.4.0/RELEASE_NOTES.html</a>
</li>

<li>Kafka Mirror Maker Best Practices<br />
<a href="https://community.cloudera.com/t5/Community-Articles/Kafka-Mirror-Maker-Best-Practices/ta-p/249269">https://community.cloudera.com/t5/Community-Articles/Kafka-Mirror-Maker-Best-Practices/ta-p/249269</a>
</li>

<li>Apache Kafka MirrorMaker 2 (MM2) Part 1: Theory<br />
<a href="https://www.instaclustr.com/blog/kafka-mirrormaker-2-theory/">https://www.instaclustr.com/blog/kafka-mirrormaker-2-theory/</a>
</li>

<li>Kcli: is a kafka read only command line browser.<br />
<a href="https://github.com/cswank/kcli">https://github.com/cswank/kcli</a>
</li>

<li>Kcli: a kafka command line browser<br />
<a href="https://go.libhunt.com/kcli-alternatives">https://go.libhunt.com/kcli-alternatives</a>
</li>

<li>Kafka Connect and Schemas<br />
<a href="https://rmoff.net/2020/01/22/kafka-connect-and-schemas/">https://rmoff.net/2020/01/22/kafka-connect-and-schemas/</a>
</li>

<li>JSON and schemas<br />
<a href="https://www.confluent.io/blog/kafka-connect-deep-dive-converters-serialization-explained/#json-schemas">https://www.confluent.io/blog/kafka-connect-deep-dive-converters-serialization-explained/#json-schemas</a>
</li>

<li>What, why, when to use Apache Kafka, with an example<br />
<a href="https://www.startdataengineering.com/post/what-why-and-how-apache-kafka/">https://www.startdataengineering.com/post/what-why-and-how-apache-kafka/</a>
</li>

<li>When NOT to use Apache Kafka?<br />
<a href="https://www.kai-waehner.de/blog/2022/01/04/when-not-to-use-apache-kafka/">https://www.kai-waehner.de/blog/2022/01/04/when-not-to-use-apache-kafka/</a>
</li>

<li>Microservices: The Rise Of Kafka<br />
<a href="https://movio.co/blog/microservices-rise-kafka/">https://movio.co/blog/microservices-rise-kafka/</a>
</li>

<li>Building a Microservices Ecosystem with Kafka Streams and KSQL<br />
<a href="https://www.confluent.io/blog/building-a-microservices-ecosystem-with-kafka-streams-and-ksql/">https://www.confluent.io/blog/building-a-microservices-ecosystem-with-kafka-streams-and-ksql/</a>
</li>

<li>An introduction to Apache Kafka and microservices communication<br />
<a href="https://medium.com/@ulymarins/an-introduction-to-apache-kafka-and-microservices-communication-bf0a0966d63">https://medium.com/@ulymarins/an-introduction-to-apache-kafka-and-microservices-communication-bf0a0966d63</a>
</li>

<li>kappa-architecture.com<br />
<a href="http://milinda.pathirage.org/kappa-architecture.com/">http://milinda.pathirage.org/kappa-architecture.com/</a>
</li>

<li>Questioning the Lambda Architecture<br />
<a href="https://www.oreilly.com/ideas/questioning-the-lambda-architecture">https://www.oreilly.com/ideas/questioning-the-lambda-architecture</a>
</li>

<li>Lambda architecture<br />
<a href="https://en.wikipedia.org/wiki/Lambda_architecture">https://en.wikipedia.org/wiki/Lambda_architecture</a>
</li>

<li>Kafka &ndash; ecosystem (Wiki)<br />
<a href="https://cwiki.apache.org/confluence/display/KAFKA/Ecosystem">https://cwiki.apache.org/confluence/display/KAFKA/Ecosystem</a>
</li>

<li>The Kafka Ecosystem - Kafka Core, Kafka Streams, Kafka Connect, Kafka REST Proxy, and the Schema Registry<br />
<a href="http://cloudurable.com/blog/kafka-ecosystem/index.html">http://cloudurable.com/blog/kafka-ecosystem/index.html</a>
</li>

<li>A Kafka Operator for Kubernetes<br />
<a href="https://github.com/krallistic/kafka-operator">https://github.com/krallistic/kafka-operator</a>
</li>

<li>Kafka Streams<br />
<a href="https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Streams">https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Streams</a>
</li>

<li>Kafka Streams<br />
<a href="http://kafka.apache.org/documentation/streams/">http://kafka.apache.org/documentation/streams/</a>
</li>

<li>Kafka Streams (FAQ)<br />
<a href="https://cwiki.apache.org/confluence/display/KAFKA/FAQ#FAQ-Streams">https://cwiki.apache.org/confluence/display/KAFKA/FAQ#FAQ-Streams</a>
</li>

<li>Event stream processing<br />
<a href="https://en.wikipedia.org/wiki/Event_stream_processing">https://en.wikipedia.org/wiki/Event_stream_processing</a>
</li>

<li>Part 1: Apache Kafka for beginners - What is Apache Kafka?<br />
<a href="https://www.cloudkarafka.com/blog/2016-11-30-part1-kafka-for-beginners-what-is-apache-kafka.html">https://www.cloudkarafka.com/blog/2016-11-30-part1-kafka-for-beginners-what-is-apache-kafka.html</a>
</li>

<li>What are some alternatives to Apache Kafka?<br />
<a href="https://www.quora.com/What-are-some-alternatives-to-Apache-Kafka">https://www.quora.com/What-are-some-alternatives-to-Apache-Kafka</a>
</li>

<li>What is the best alternative to Kafka?<br />
<a href="https://www.slant.co/options/961/alternatives/~kafka-alternatives">https://www.slant.co/options/961/alternatives/~kafka-alternatives</a>
</li>

<li>A super quick comparison between Kafka and Message Queues<br />
<a href="https://hackernoon.com/a-super-quick-comparison-between-kafka-and-message-queues-e69742d855a8?gi=e965191e72d0">https://hackernoon.com/a-super-quick-comparison-between-kafka-and-message-queues-e69742d855a8?gi=e965191e72d0</a>
</li>

<li>Kafka Queuing: Kafka as a Messaging System<br />
<a href="https://dzone.com/articles/kafka-queuing-kafka-as-a-messaging-system">https://dzone.com/articles/kafka-queuing-kafka-as-a-messaging-system</a>
</li>

<li>Apache Kafka Logs: A Comprehensive Guide<br />
<a href="https://hevodata.com/learn/apache-kafka-logs-a-comprehensive-guide/">https://hevodata.com/learn/apache-kafka-logs-a-comprehensive-guide/</a>
</li>

<li>Microservices â€“ Not a free lunch!<br />
<a href="http://highscalability.com/blog/2014/4/8/microservices-not-a-free-lunch.html">http://highscalability.com/blog/2014/4/8/microservices-not-a-free-lunch.html</a>
</li>

<li>Microservices, Monoliths, and NoOps<br />
<a href="http://blog.arungupta.me/microservices-monoliths-noops/">http://blog.arungupta.me/microservices-monoliths-noops/</a>
</li>

<li>Microservice Design Patterns<br />
<a href="http://blog.arungupta.me/microservice-design-patterns/">http://blog.arungupta.me/microservice-design-patterns/</a>
</li>

<li>REST vs Messaging for Microservices â€“ Which One is Best?<br />
<a href="https://solace.com/blog/experience-awesomeness-event-driven-microservices/">https://solace.com/blog/experience-awesomeness-event-driven-microservices/</a>
</li>

<li>Kappa Architecture Our Experience<br />
<a href="https://events.static.linuxÂ­found.org/sites/events/fiÂ­les/slides/ASPgems%20-%20Kappa%20Architecture.pdf">https://events.static.linuxÂ­found.org/sites/events/fiÂ­les/slides/ASPgems%20-%20Kappa%20Architecture.pdf</a>
</li>

<li>Apache Kafka Streams and Tables, the stream-table duality<br />
<a href="https://towardsdatascience.com/apache-kafka-streams-and-tables-the-stream-table-duality-ee904251a7e?gi=f22a29cd1854">https://towardsdatascience.com/apache-kafka-streams-and-tables-the-stream-table-duality-ee904251a7e?gi=f22a29cd1854</a>
</li>

<li>Configure Self-Managed Connectors<br />
<a href="https://docs.confluent.io/kafka-connectors/self-managed/configuring.html#configure-self-managed-connectors">https://docs.confluent.io/kafka-connectors/self-managed/configuring.html#configure-self-managed-connectors</a>
</li>

<li>Schema Evolution and Compatibility<br />
<a href="https://docs.confluent.io/platform/current/schema-registry/avro.html#schema-evolution-and-compatibility">https://docs.confluent.io/platform/current/schema-registry/avro.html#schema-evolution-and-compatibility</a>
</li>

<li>Configuring Key and Value Converters<br />
<a href="https://docs.confluent.io/kafka-connectors/self-managed/userguide.html#configuring-key-and-value-converters">https://docs.confluent.io/kafka-connectors/self-managed/userguide.html#configuring-key-and-value-converters</a>
</li>

<li>Introduction to Kafka Connectors<br />
<a href="https://www.baeldung.com/kafka-connectors-guide">https://www.baeldung.com/kafka-connectors-guide</a>
</li>

<li>Kafka CLI: command to list all consumer groups for a topic?<br />
<a href="https://stackoverflow.com/questions/63883999/kafka-cli-command-to-list-all-consumer-groups-for-a-topic">https://stackoverflow.com/questions/63883999/kafka-cli-command-to-list-all-consumer-groups-for-a-topic</a>
</li>

<li>Java Property File Processing<br />
<a href="https://www.w3resource.com/java-tutorial/java-propertyfile-processing.php">https://www.w3resource.com/java-tutorial/java-propertyfile-processing.php</a>
</li>

<li>Skipping bad records with the Kafka Connect JDBC sink connector<br />
<a href="https://rmoff.net/2019/10/15/skipping-bad-records-with-the-kafka-connect-jdbc-sink-connector/">https://rmoff.net/2019/10/15/skipping-bad-records-with-the-kafka-connect-jdbc-sink-connector/</a>
</li>

<li>Kafka Connect Deep Dive â€“ Error Handling and Dead Letter Queues<br />
<a href="https://www.confluent.io/blog/kafka-connect-deep-dive-error-handling-dead-letter-queues/">https://www.confluent.io/blog/kafka-connect-deep-dive-error-handling-dead-letter-queues/</a>
</li>

<li>Errors and Dead Letter Queues<br />
<a href="https://developer.confluent.io/learn-kafka/kafka-connect/error-handling-and-dead-letter-queues/">https://developer.confluent.io/learn-kafka/kafka-connect/error-handling-and-dead-letter-queues/</a>
</li>

<li>Confluent Cloud Dead Letter Queue<br />
<a href="https://docs.confluent.io/cloud/current/connectors/dead-letter-queue.html">https://docs.confluent.io/cloud/current/connectors/dead-letter-queue.html</a>
</li>

<li>Dead Letter Queues (DLQs) in Kafka<br />
<a href="https://medium.com/@sannidhi.s.t/dead-letter-queues-dlqs-in-kafka-afb4b6835309">https://medium.com/@sannidhi.s.t/dead-letter-queues-dlqs-in-kafka-afb4b6835309</a>
</li>

<li>Deserializer<br />
<a href="https://docs.confluent.io/platform/current/schema-registry/serdes-develop/serdes-json.html#json-schema-serializer-and-deserializer">https://docs.confluent.io/platform/current/schema-registry/serdes-develop/serdes-json.html#json-schema-serializer-and-deserializer</a>
</li>

<li>JSON, Kafka, and the need for schema<br />
<a href="https://mikemybytes.com/2022/07/11/json-kafka-and-the-need-for-schema/">https://mikemybytes.com/2022/07/11/json-kafka-and-the-need-for-schema/</a>
</li>

<li>Using Kafka Connect with Schema Registry<br />
<a href="https://docs.confluent.io/platform/current/schema-registry/connect.html">https://docs.confluent.io/platform/current/schema-registry/connect.html</a>
</li>

<li>ZpracovÃ¡nÃ­ dat reprezentovanÃ½ch ve formÃ¡tu JSON nÃ¡strojem jq<br />
<a href="https://www.root.cz/clanky/zpracovani-dat-reprezentovanych-ve-formatu-json-nastrojem-jq/">https://www.root.cz/clanky/zpracovani-dat-reprezentovanych-ve-formatu-json-nastrojem-jq/</a>
</li>

<li>RepositÃ¡Å™ projektu jq (GitHub)<br />
<a href="https://github.com/stedolan/jq">https://github.com/stedolan/jq</a>
</li>

<li>GitHub strÃ¡nky projektu jq<br />
<a href="https://stedolan.github.io/jq/">https://stedolan.github.io/jq/</a>
</li>

<li>5Â modern alternatives to essential Linux command-line tools<br />
<a href="https://opensource.com/arÂ­ticle/20/6/modern-linux-command-line-tools">https://opensource.com/arÂ­ticle/20/6/modern-linux-command-line-tools</a>
</li>

<li>NÃ¡vod kÂ nÃ¡stroji jq<br />
<a href="https://stedolan.github.iÂ­o/jq/tutorial/">https://stedolan.github.iÂ­o/jq/tutorial/</a>
</li>

<li>jq Manual (development version)<br />
<a href="https://stedolan.github.io/jq/manual/">https://stedolan.github.io/jq/manual/</a>
</li>

<li>Introducing JSON<br />
<a href="https://www.json.org/json-en.html">https://www.json.org/json-en.html</a>
</li>

<li>Understanding JSON schema<br />
<a href="https://json-schema.org/understanding-json-schema/index.html">https://json-schema.org/understanding-json-schema/index.html</a>
</li>

<li>JDBC Sink Connector for Confluent Platform<br />
<a href="https://docs.confluent.io/kafka-connectors/jdbc/current/sink-connector/overview.html#jdbc-sink-connector-for-cp">https://docs.confluent.io/kafka-connectors/jdbc/current/sink-connector/overview.html#jdbc-sink-connector-for-cp</a>
</li>

<li>JDBC Connector (Source and Sink)<br />
<a href="https://www.confluent.io/hub/confluentinc/kafka-connect-jdbc">https://www.confluent.io/hub/confluentinc/kafka-connect-jdbc</a>
</li>

<li>Introduction to Schema Registry in Kafka<br />
<a href="https://medium.com/slalom-technology/introduction-to-schema-registry-in-kafka-915ccf06b902">https://medium.com/slalom-technology/introduction-to-schema-registry-in-kafka-915ccf06b902</a>
</li>

<li>Understanding JSON Schema Compatibility<br />
<a href="https://yokota.blog/2021/03/29/understanding-json-schema-compatibility/">https://yokota.blog/2021/03/29/understanding-json-schema-compatibility/</a>
</li>

</ol>



<p></p><p></p>
<p><small>Autor: <a href="http://www.fit.vutbr.cz/~tisnovpa">Pavel TiÅ¡novskÃ½</a> &nbsp; 2024</small></p>
</body>
</html>

