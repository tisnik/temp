<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
        "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html>
<head>
<title>Neuronové sítě v knihovně scikit-learn</title>
<meta name="Author" content="Pavel Tisnovsky" />
<meta name="Generator" content="vim" />
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<style type="text/css">
         body {color:#000000; background:#ffffff;}
         h1  {font-family: arial, helvetica, sans-serif; color:#ffffff; background-color:#c00000; text-align:center; padding-left:1em}
         h2  {font-family: arial, helvetica, sans-serif; color:#ffffff; background-color:#0000c0; padding-left:1em; text-align:left}
         h3  {font-family: arial, helvetica, sans-serif; color:#000000; background-color:#c0c0c0; padding-left:1em; text-align:left}
         h4  {font-family: arial, helvetica, sans-serif; color:#000000; background-color:#e0e0e0; padding-left:1em; text-align:left}
         a   {font-family: arial, helvetica, sans-serif;}
         li  {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify; width:450px;}
         ol  {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify; width:450px;}
         ul  {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify; width:450px;}
         p   {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify;}
         pre {background:#e0e0e0}
</style>
</head>

<body>

<h1></h1>

<h3>Pavel Tišnovský</h3>

<p></p>

<h1>Úvodník</h1>

<p>V dnešní části seriálu o balíčku scikit-learn si ukážeme, jakým způsobem je možné vytvořit neuronovou síť s volitelným počtem skrytých vrstev, naučit tuto síť řešit zvolený problém s využitím sady trénovacích dat a následně tuto síť použít nad další sadou (validačních) dat.</p>



<h2>Obsah</h2>

<p><a href="#k01">1. Neuronové sítě v&nbsp;knihovně <i>scikit-learn</i></a></p>
<p><a href="#k02">2. Idealizovaný model neuronu používaný v&nbsp;umělých neuronových sítích</a></p>
<p><a href="#k03">3. Role biasu</a></p>
<p><a href="#k04">4. Aktivační funkce</a></p>
<p><a href="#k05">5. Vytvoření feed-forward sítě z&nbsp;jednotlivých neuronů</a></p>
<p><a href="#k06">6. Vstupní vrstva, výstupní vrstva a skryté vrstvy neuronů</a></p>
<p><a href="#k07">7. Trénink (učení) sítě s&nbsp;využitím trénovacích dat</a></p>
<p><a href="#k08">*** 8. Neuronová síť jako další typ modelu ve <i>scikit-learn</i></a></p>
<p><a href="#k09">*** 9. Konstrukce neuronové sítě, její trénink a následné použití</a></p>
<p><a href="#k10">*** 10. </a></p>
<p><a href="#k11">*** 11. </a></p>
<p><a href="#k12">*** 12. </a></p>
<p><a href="#k13">*** 13. </a></p>
<p><a href="#k14">*** 14. </a></p>
<p><a href="#k15">*** 15. </a></p>
<p><a href="#k16">*** 16. </a></p>
<p><a href="#k17">*** 17. </a></p>
<p><a href="#k18">*** 18. </a></p>
<p><a href="#k19">*** 19. Repositář s&nbsp;demonstračními příklady</a></p>
<p><a href="#k20">20. Odkazy na Internetu</a></p>



<p><a name="k01"></a></p>
<h2 id="k01">1. Neuronové sítě v&nbsp;knihovně <i>scikit-learn</i></h2>

<p>V&nbsp;dnešním článku o knihovně <i>scikit-learn</i> se budeme zabývat
problematikou vytvoření jednoduchých umělých neuronových sítí (zkráceně jen
neuronových sítí neboli <i>neural network</i> popř.&nbsp;<i>nn</i>) a taktéž
způsobem tréninku (učení) těchto sítí s&nbsp;jejich následnou křížovou
validací. Již na začátku je nutné říct, že se sice jedná o dosti rozsáhlou
problematiku, pro jejíž pochopení se navíc očekává alespoň základní znalost
teorie neuronových sítí, na druhou stranu je však použití těchto sítí (ve
funkci blackboxu) ve <i>scikit-learn</i> až triviálně jednoduché, vlastně
stejně jednoduché, jako použití jiných modelů.</p>

<p>Zpočátku se zaměříme na neuronové sítě typu <i>feed-forward</i>
s&nbsp;volitelným počtem vrstev neuronů, přičemž neurony na sousedních vrstvách
budou propojeny (synapsemi) systémem &bdquo;každý s&nbsp;každým&ldquo; a
informace mezi neurony potečou pouze jedním směrem (<i>forward</i>).
V&nbsp;dalších částech tohoto seriálu si popíšeme i další typy sítí,
v&nbsp;nichž bude použito více vrstev neuronů, neurony budou propojeny odlišným
způsobem, budou použity jiné <i>aktivační funkce</i> atd. (což již ovšem
částečně přesahuje možnosti <i>scikit-learn</i> a sáhneme tedy například po
<i>pytorchi</i> atd.</p>

<p>Neuronové sítě se používají zejména v&nbsp;těch projektech, v&nbsp;nichž je
zapotřebí vytvořit funkční systém už ve chvíli, kdy ještě neznáme všechny možné
kombinace vstupů a výstupů, popř.&nbsp;když je chování systému, který se
implementuje, tak složité, že klasický návrh a implementace algoritmů by byl
příliš zdlouhavý nebo s&nbsp;danými prostředky (čas, počet vývojářů a testerů)
neefektivní. Jednou z&nbsp;nevýhod neuronových sítí může být to, že je někdy
velmi obtížné zjistit, jaký problém jsme vlastně neuronovou síť naučili řešit.
Výsledná síť se totiž (pokud se nebudeme hodně snažit zjisti více) chová jako
<i>blackbox</i>, o němž není snadné říct, jaké konkrétní rozhodování ten který
neuron provádí (v&nbsp;některých případech to ovšem možné je). Kritickou úlohu
zde sehrává výběr vhodné množiny trénovacích dat. K&nbsp;tomu se však dostaneme
až v&nbsp;dalších částech seriálu, v&nbsp;nichž se zmíníme o již existujících
trénovacích datech.</p>

<p><div class="rs-tip-major">Poznámka: již na úvod si však řekněme, že
neuronové sítě jsou pro mnoho problémů příliš složitými modely, takže bývá
snazší (a rychlejší) nejprve použít modely jednodušší.</div></p>



<p><a name="k02"></a></p>
<h2 id="k02">2. Idealizovaný model neuronu používaný v&nbsp;umělých neuronových sítích</h2>

<p>Při práci s&nbsp;umělými neuronovými sítěmi je vhodné vědět, jak je vlastně
taková síť zkonstruována a z&nbsp;jakých prvků se skládá. Základním stavebním
prvkem je umělý neuron, resp.&nbsp;velmi zjednodušený a idealizovaný model
skutečného neuronu. Původní model neuronu byl navržen Warrenem McCullochem a
Walterem Pittsem (MCP) již ve čtyřicátých letech minulého století, z&nbsp;čehož
plyne, že neuronové sítě nejsou jen módním výstřelkem poslední doby (naopak,
moderní GPU umožňují jejich nasazení i tam, kde to dříve nebylo možné, to je
však téma na samostatný článek.). Na dalším obrázku jsou naznačeny prvky modelu
neuronu:</p>

<img src="https://i.iinfo.cz/images/329/torch-nn1-1.png" class="image-312261" alt="&#160;" width="459" height="140" />
<p><i>Obrázek 1: Idealizovaný model neuronu.</i></p>

<p>Vidíme, že neuron může mít libovolný počet vstupů (na obrázku jsou tři
vstupy <strong>x<sub>1</sub></strong>, <strong>x<sub>2</sub></strong> a
<strong>x<sub>3</sub></strong>, ovšem může to být jen jeden vstup nebo i sto
vstupů) a má pouze jeden výstup <strong>y</strong>. Vstupem a výstupem jsou
reálná čísla; typicky bývá výstup upraven aktivační funkcí tak, že leží
v&nbsp;rozsahu &lt;-1..1&gt; nebo &lt;0..1&gt;.</p>

<p>Dále na schématu vidíme váhy <strong>w<sub>1</sub></strong>,
<strong>w<sub>2</sub></strong> a <strong>w<sub>3</sub></strong>. Těmito váhami
jsou vynásobeny vstupní hodnoty. Váhy vlastně představují stav neuronu,
tj.&nbsp;o funkci, na kterou byl neuron natrénován (naučen). Vstupní hodnoty
<strong>x<sub>1</sub></strong> až <strong>x<sub>n</sub></strong> jsou tedy
postupně vynásobeny váhami <strong>w<sub>1</sub></strong> až
<strong>w<sub>n</sub></strong> a výsledky součinu jsou v&nbsp;neuronu sečteny,
takže získáme jediné reálné číslo. Toto číslo je zpracováno <i>aktivační
funkcí</i> (ta již většinou žádný stav nemá, ostatně stejně jako funkce pro
výpočet sumy) výsledek této funkce je poslán na výstup neuronu.</p>

<p>Neuron tedy provádí tento výpočet:</p>

<p>
y = f(w<sub>1</sub>x<sub>1</sub> + w<sub>2</sub>x<sub>2</sub> + ... + w<sub>n</sub>x<sub>n</sub>)
</p>



<p><a name="k03"></a></p>
<h2 id="k03">3. Role biasu</h2>

<p>Ve skutečnosti není stav neuronu pro <i>n</i> vstupů
<strong>x<sub>1</sub></strong> až <strong>x<sub>n</sub></strong> určen pouze
<i>n</i> vahami <strong>w<sub>1</sub></strong> až
<strong>w<sub>n</sub></strong>. Musíme přidat ještě váhu
<strong>w<sub>0</sub></strong>, na kterou je připojena konstanta 1 (někdy se
proto můžeme setkat s&nbsp;nákresem neuronové sítě, v&nbsp;níž se nachází
speciální neurony bez vstupů a s&nbsp;jedničkou na výstupu). Model neuronu se
přidáním nového vstupu nepatrně zkomplikuje:</p>

<img src="https://i.iinfo.cz/images/329/torch-nn1-2.png" class="image-312262" alt="&#160;" width="465" height="145" />
<p><i>Obrázek 2: Idealizovaný model neuronu s&nbsp;biasem.</i></p>

<p>I výpočet bude vypadat (nepatrně) odlišně, neboť do něho přidáme nový
člen:</p>

<p>
y = f(<strong>w<sub>0</sub></strong> + w<sub>1</sub>x<sub>1</sub> + w<sub>2</sub>x<sub>2</sub> + ... + w<sub>n</sub>x<sub>n</sub>)
</p>

<p>Tato přidaná váha se někdy nazývá <i>bias</i>, protože vlastně umožňuje
posouvat <a
href="https://stackoverflow.com/questions/2480650/role-of-bias-in-neural-networks">průběh
aktivační funkce nalevo a napravo</a>, v&nbsp;závislosti na jeho hodnotě.</p>



<p><a name="k04"></a></p>
<h2 id="k04">4. Aktivační funkce</h2>

<p>Bez aktivační funkce by se neuron choval jednoduše &ndash; spočítal by
vážený součet vstupů a výsledek by poslal na výstup. Aktivační funkce, kterou
jsme v&nbsp;předchozích dvou kapitolách označovali symbolem <i>f</i>, do celého
výpočtu vnáší nelinearitu (ta je velmi důležitá). Nejjednodušší aktivační
funkce může pro vstupní hodnoty &lt;0 vracet -1 a pro hodnoty &ge;0 vracet 1,
což vlastně říká, že je nutné dosáhnout určité hraniční hodnoty váženého součtu
vstupů, aby byl neuron aktivován (tj.&nbsp;na výstup vyslal jedničku a nikoli
-1). Ostatně právě zde znovu vidíme význam biasu, který onu hraniční hodnotu
posunuje.</p>

<img src="https://i.iinfo.cz/images/329/torch-nn1-3.png" class="image-312263" alt="&#160;" width="640" height="480" />
<p><i>Obrázek 3: Aktivační funkce ReLU.</i></p>

<p>V&nbsp;praxi je však aktivační funkce složitější, než zmíněný jednotkový
skok. Často se používá <i>ReLU</i>, <i>sigmoid</i> nebo <i>hyperbolický
tangent</i>. Pro specializovanější účely se však používají i další funkce,
které dokonce nemusí mít monotonní průběh. S&nbsp;dalšími podporovanými
funkcemi se seznámíme příště.</p>

<img src="https://i.iinfo.cz/images/329/torch-nn1-4.png" class="image-312264" alt="&#160;" width="640" height="480" />
<p><i>Obrázek 4: Aktivační funkce Tanh.</i></p>



<p><a name="k05"></a></p>
<h2 id="k05">5. Vytvoření feed-forward sítě z&nbsp;jednotlivých neuronů</h2>

<p>Samostatné neurony i s&nbsp;aktivační funkcí stále provádí velmi jednoduchou
činnost, ovšem aby se mohly stát součástí složitějšího systému (řekněme
automatického řízení auta), musíme z&nbsp;nich vytvořit síť. Jedna
z&nbsp;nejjednodušších forem umělé neuronové sítě se nazývá
<i>feed-forward</i>, a to z&nbsp;toho důvodu, že informace (tedy vstupní
hodnoty, mezihodnoty i hodnoty výstupní) touto sítí tečou jen jedním směrem
(při učení je tomu jinak). Neurony jsou uspořádány pravidelně do vrstev:</p>

<img src="https://i.iinfo.cz/images/329/torch-nn1-5.png" class="image-312265" alt="&#160;" width="600" height="400" />
<p><i>Obrázek 5: Uspořádání neuronů do vrstev ve feed-forward síti.</i></p>

<p>Kolečka na obrázku představují jednotlivé neurony, přičemž žlutě jsou
označeny neurony na vstupu, zeleně &bdquo;interní&ldquo; (skryté) neurony a
červeně neurony, které produkují kýžený výstup neuronové sítě.</p>

<p>Zcela nalevo jsou šipkami naznačeny vstupy. Jejich počet je prakticky zcela
závislý na řešeném problému. Může se jednat jen o několik vstupů (viz naše
testovací síť popsaná níže), ovšem pokud například budeme tvořit síť určenou
pro rozpoznání objektů v&nbsp;rastrovém obrázku, může být počet vstupů roven
počtu pixelů (což ovšem v&nbsp;praxi realizujeme odlišně &ndash; konvolučními
sítěmi).</p>



<p><a name="k06"></a></p>
<h2 id="k06">6. Vstupní vrstva, výstupní vrstva a skryté vrstvy neuronů</h2>

<p>Vraťme se ještě jednou k&nbsp;obrázku číslo 5.</p>

<p>Povšimněte si, že vstupní neurony mají vlastně zjednodušenou funkci, protože
mají jen jeden vstup. V&nbsp;mnoha typech sítí tyto neurony jen rozesílají
vstup na další neurony a neprovádí žádný složitější výpočet, například u nich
není použita aktivační funkce, ovšem to již záleží na konkrétní konfiguraci
sítě. Dále stojí za povšimnutí, že neurony posílají svůj výstup neuronům na
nejbližší další vrstvě; nejsou zde tedy žádné zkratky, žádné zpětné vazby
atd.</p>

<p>Existují samozřejmě složitější typy sítí, těmi se teď ale nebudeme zabývat.
Dále tato síť propojuje neurony na sousedních vrstvách systémem &bdquo;každý
s&nbsp;každým&ldquo;. V&nbsp;našem konkrétním příkladu mají neurony na
prostřední vrstvě dva vstupy, protože předchozí vrstva má jen dva neurony.
Ovšem neurony na poslední vrstvě již musí mít tři vstupy.</p>

<p><div class="rs-tip-major">Poznámka: může se stát, že síť bude po naučení
obsahovat neurony, jejichž váhy na vstupu budou nulové. To vlastně značí, že ze
sítě některé spoje (šipky) zmizí, protože vynásobením jakéhokoli vstupu nulou
dostaneme zase jen nulu.</div></p>

<p>První vrstva s&nbsp;jednoduchými (&bdquo;hloupými&ldquo;) neurony se nazývá
<i>vstupní vrstva</i>, poslední vrstva je <i>vrstva výstupní</i>. Vrstvy mezi
vrstvou vstupní a výstupní, kterých může být teoreticky libovolné množství, se
nazývají <i>skryté vrstvy</i>.</p>

<p>&bdquo;Paměť&ldquo; neuronové sítě je tvořena vahami na vstupech
neuronů (včetně biasu):</p>

<table>
<tr><th>Vrstva</th><th>Neuronů</th><th>Počet vstupů/neuron</th><th>Počet vah/neuron</th><th>Celkem</th></tr>
<tr><td>1</td><td>2</td><td>1</td><td>2</td><td>4</td></tr>
<tr><td>2</td><td>3</td><td>2</td><td>3</td><td>9</td></tr>
<tr><td>3</td><td>2</td><td>3</td><td>4</td><td>8</td></tr>
<tr><td>&sum;</td><td>7</td><td>&nbsp;</td><td>&nbsp;</td><td>21</td></tr>
</table>

<p>V&nbsp;praxi se používají sítě s&nbsp;více vrstvami a především
s&nbsp;větším počtem neuronů v&nbsp;každé vrstvě. Stavový prostor a tím i
schopnosti sítě se tak prudce rozšiřují (viz již zmíněná problematika
rozpoznávání objektů v&nbsp;rastrových obrázcích).</p>

<p><div class="rs-tip-major">Poznámka: někdy se počet neuronů v&nbsp;umělých
sítích porovnává s&nbsp;počtem neuronů v&nbsp;mozku, ale to je ve skutečnosti
dost zavádějící, neboť záleží spíše na uspořádání sítě, složitosti neuronů
(více výstupů) atd. A pravděpodobně mnoho věcí o mozku ještě neznáme.</div></p>



<p><a name="k07"></a></p>
<h2 id="k07">7. Trénink (učení) sítě s&nbsp;využitím trénovacích dat</h2>

<p>Nejzajímavější je proces tréninku (učení) sítě. Ten může probíhat několika
způsoby, ovšem nejčastější je učení založené na tom, že na vstup sítě přivedeme
data, u nichž dopředu známe očekávaný výsledek. Síť pro tato vstupní data
provede svůj odhad a na základě rozdílů mezi odhadem sítě a očekávaným
výsledkem se více či méně sofistikovanými algoritmy nepatrně pozmění váhy
<strong>w<sub>i</sub></strong> na vstupech do neuronů (včetně biasu, tedy
<strong>w<sub>0</sub></strong>).</p>

<p>Konkrétní míra změn váhy na vstupech neuronů je globálně řízena dalším
parametrem či parametry, z&nbsp;nichž ten nejdůležitější ovlivňuje rychlost
učení. Ta by neměla být příliš nízká (to vyžaduje objemná trénovací data nebo
jejich opakování), ale ani příliš vysoká. Základní algoritmus učení sítě se
jmenuje <i>backpropagation</i>, protože se váhy skutečně mění v&nbsp;opačném
směru &ndash; od výstupů (na něž se přivede vypočtená chyba) ke vstupům. Asi
nejlépe je tento koncept popsán v&nbsp;článku dostupném na adrese <a
href="https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/">https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/</a>,
tuto část za nás však vykoná knihovna <i>scikit-learn</i> zcela
automaticky.</p>



<p><a name="k08"></a></p>
<h2 id="k08">8. Neuronová síť jako další typ modelu ve <i>scikit-learn</i></h2>

<p></p>


<pre>
from sklearn.datasets import load_iris
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
import numpy as np
&nbsp;
&nbsp;
<i># nacteni datove sady</i>
iris = load_iris()
&nbsp;
<i># konstrukce klasifikatoru</i>
<i># (s hyperparametrem)</i>
classifier = KNeighborsClassifier(n_neighbors=1)
&nbsp;
<i># X je matice (feature matrix)</i>
X = iris.data
&nbsp;
<i># y je vektor (response vector)</i>
y = iris.target
&nbsp;
<i># rozdělení na trénovací a testovací data</i>
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
&nbsp;
<i># trening modelu</i>
classifier.fit(X_train, y_train)
&nbsp;
<i># očekávané výsledky</i>
expexted_labels = y_test
&nbsp;
<i># výsledky modelu (predikované výsledky)</i>
predicted_labels = classifier.predict(X_test)
&nbsp;
<i># jak je náš model úspěšný?</i>
total = 0
same = 0
&nbsp;
<i># porovnání predikce s očekáváním</i>
for (expected, predicted) in zip(expexted_labels, predicted_labels):
    if expected==predicted:
        same+=1
    total+=1
&nbsp;
print(f"total:    {total}")
print(f"same:     {same}")
print(f"accuracy: {100.0*same/total:4.1f}%")
</pre>

<pre>
total:    30
same:     29
accuracy: 96.7%
</pre>



<p><a name="k04"></a></p>
<h2 id="k04">4. </h2>

<pre>
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
&nbsp;
<i># model zalozeny na neuronove siti</i>
from sklearn.neural_network import MLPClassifier
&nbsp;
&nbsp;
<i># nacteni datove sady</i>
iris = load_iris()
&nbsp;
<i># konstrukce klasifikatoru</i>
<i># (s hyperparametrem)</i>
classifier = MLPClassifier(max_iter=5000)
&nbsp;
<i># X je matice (feature matrix)</i>
X = iris.data
&nbsp;
<i># y je vektor (response vector)</i>
y = iris.target
&nbsp;
<i># rozdělení na trénovací a testovací data</i>
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
&nbsp;
<i># trening modelu</i>
classifier.fit(X_train, y_train)
&nbsp;
<i># očekávané výsledky</i>
expexted_labels = y_test
&nbsp;
<i># výsledky modelu (predikované výsledky)</i>
predicted_labels = classifier.predict(X_test)
&nbsp;
<i># jak je náš model úspěšný?</i>
total = 0
same = 0
&nbsp;
<i># porovnání predikce s očekáváním</i>
for (expected, predicted) in zip(expexted_labels, predicted_labels):
    if expected==predicted:
        same+=1
    total+=1
&nbsp;
print(f"total:    {total}")
print(f"same:     {same}")
print(f"accuracy: {100.0*same/total:4.1f}%")
&nbsp;
print(f"Features: {classifier.n_features_in_}")
print(f"Layers:   {classifier.n_layers_}")
print(f"Outputs:  {classifier.n_outputs_}")
print("Weights:")
&nbsp;
for layer, weights in enumerate(classifier.coefs_):
    print("\t", layer, weights.shape)
</pre>

<pre>
total:    30
same:     30
accuracy: 100.0%
</pre>

<pre>
total:    30
same:     28
accuracy: 93.3%
</pre>

<pre>
Features: 4
Layers:   3
Outputs:  3
Weights:
         0 (4, 100)
         1 (100, 3)
</pre>


<p><a name="k05"></a></p>
<h2 id="k05">5. </h2>

<pre>
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
&nbsp;
from sklearn.metrics import accuracy_score
&nbsp;
<i># model zalozeny na neuronove siti</i>
from sklearn.neural_network import MLPClassifier
&nbsp;
&nbsp;
<i># nacteni datove sady</i>
iris = load_iris()
&nbsp;
<i># konstrukce klasifikatoru</i>
<i># (s hyperparametrem)</i>
classifier = MLPClassifier(max_iter=5000)
&nbsp;
<i># X je matice (feature matrix)</i>
X = iris.data
&nbsp;
<i># y je vektor (response vector)</i>
y = iris.target
&nbsp;
<i># rozdělení na trénovací a testovací data</i>
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
&nbsp;
<i># trening modelu</i>
classifier.fit(X_train, y_train)
&nbsp;
&nbsp;
<i># výsledky modelu (predikované výsledky)</i>
y_pred = classifier.predict(X_test)
&nbsp;
<i># vypoctena presnost modelu</i>
print(accuracy_score(y_test, y_pred))
&nbsp;
print(f"Features: {classifier.n_features_in_}")
print(f"Layers:   {classifier.n_layers_}")
print(f"Outputs:  {classifier.n_outputs_}")
print("Weights:")
&nbsp;
for layer, weights in enumerate(classifier.coefs_):
    print("\t", layer, weights.shape)
</pre>

<pre>
0.9666666666666667
</pre>

<pre>
1.0
</pre>

<pre>
Features: 4
Layers:   3
Outputs:  3
Weights:
         0 (4, 100)
         1 (100, 3)
</pre>



<p><a name="k06"></a></p>
<h2 id="k06">6. </h2>

<pre>
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
&nbsp;
from sklearn.metrics import accuracy_score
&nbsp;
<i># model zalozeny na neuronove siti</i>
from sklearn.neural_network import MLPClassifier
&nbsp;
&nbsp;
<i># nacteni datove sady</i>
iris = load_iris()
&nbsp;
<i># konstrukce klasifikatoru</i>
<i># (s hyperparametrem)</i>
classifier = MLPClassifier(max_iter=5000, hidden_layer_sizes = (10, 10, 10))
&nbsp;
<i># X je matice (feature matrix)</i>
X = iris.data
&nbsp;
<i># y je vektor (response vector)</i>
y = iris.target
&nbsp;
<i># rozdělení na trénovací a testovací data</i>
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
&nbsp;
<i># trening modelu</i>
classifier.fit(X_train, y_train)
&nbsp;
&nbsp;
<i># výsledky modelu (predikované výsledky)</i>
y_pred = classifier.predict(X_test)
&nbsp;
<i># vypoctena presnost modelu</i>
print(accuracy_score(y_test, y_pred))
&nbsp;
print(f"Features: {classifier.n_features_in_}")
print(f"Layers:   {classifier.n_layers_}")
print(f"Outputs:  {classifier.n_outputs_}")
print("Weights:")
&nbsp;
for layer, weights in enumerate(classifier.coefs_):
    print("\t", layer, weights.shape)
</pre>

<pre>
0.9666666666666667
0.9333333333333333
1.0
</pre>

<pre>
Features: 4
Layers:   5
Outputs:  3
Weights:
         0 (4, 10)
         1 (10, 10)
         2 (10, 10)
         3 (10, 3)
</pre>



<p><a name="k07"></a></p>
<h2 id="k07">7. </h2>

<pre>
import matplotlib.pyplot as plt
&nbsp;
from sklearn.datasets import load_iris
from sklearn.model_selection import cross_val_score
&nbsp;
<i># model zalozeny na neuronove siti</i>
from sklearn.neural_network import MLPClassifier
&nbsp;
&nbsp;
<i># nacteni datove sady</i>
iris = load_iris()
&nbsp;
<i># X je matice (feature matrix)</i>
X = iris.data
&nbsp;
<i># y je vektor (response vector)</i>
y = iris.target
&nbsp;
avg_scores = []
&nbsp;
r = range(1, 20)
&nbsp;
<i># hledani optimalniho poctu neuronu ve vrstvach</i>
for neurons in r:
    <i># konstrukce klasifikatoru</i>
    <i># (s hyperparametrem)</i>
    classifier = MLPClassifier(max_iter=5000, hidden_layer_sizes = (neurons, neurons, neurons))
&nbsp;
    <i># vypocet skore</i>
    scores = cross_val_score(classifier, X, y, cv=10, scoring='accuracy')
&nbsp;
    avg_score = scores.mean()
&nbsp;
    <i># vypsani prumerneho skore do tabulky</i>
    print(neurons, avg_score)
&nbsp;
    avg_scores.append(avg_score)
&nbsp;
plt.plot(r, avg_scores)
plt.xlabel("Změna počtu neuronů ve třech vrstvách")
plt.ylabel("Přesnost modelu")
&nbsp;
<i># ulozeni grafu do souboru</i>
plt.savefig("139.png")
&nbsp;
<i># vykresleni grafu na obrazovku</i>
plt.show()
</pre>

<pre>
1 0.33333333333333337
2 0.6533333333333333
3 0.7533333333333332
4 0.76
5 0.9666666666666668
6 0.9066666666666666
7 0.9466666666666667
8 0.9866666666666667
9 0.9866666666666667
10 0.9866666666666667
11 0.9666666666666668
12 0.9733333333333334
13 0.9666666666666668
14 0.9733333333333334
15 0.9733333333333334
16 0.9733333333333334
17 0.9733333333333334
18 0.9733333333333334
19 0.9733333333333334
</pre>



<p><a name="k08"></a></p>
<h2 id="k08">8. </h2>

<pre>
import matplotlib.pyplot as plt
&nbsp;
from sklearn.datasets import load_iris
from sklearn.model_selection import cross_val_score
&nbsp;
<i># model zalozeny na neuronove siti</i>
from sklearn.neural_network import MLPClassifier
&nbsp;
&nbsp;
<i># nacteni datove sady</i>
iris = load_iris()
&nbsp;
<i># X je matice (feature matrix)</i>
X = iris.data
&nbsp;
<i># y je vektor (response vector)</i>
y = iris.target
&nbsp;
avg_scores = []
&nbsp;
r = range(1, 20)
&nbsp;
<i># hledani optimalniho poctu neuronu ve vrstvach</i>
for neurons in r:
    <i># konstrukce klasifikatoru</i>
    <i># (s hyperparametrem)</i>
    classifier = MLPClassifier(max_iter=5000, hidden_layer_sizes = (neurons, neurons, neurons, neurons, neurons))
&nbsp;
    <i># vypocet skore</i>
    scores = cross_val_score(classifier, X, y, cv=10, scoring='accuracy')
&nbsp;
    avg_score = scores.mean()
&nbsp;
    <i># vypsani prumerneho skore do tabulky</i>
    print(neurons, avg_score)
&nbsp;
    avg_scores.append(avg_score)
&nbsp;
plt.plot(r, avg_scores)
plt.xlabel("Změna počtu neuronů v pěti vrstvách")
plt.ylabel("Přesnost modelu")
&nbsp;
<i># ulozeni grafu do souboru</i>
plt.savefig("140.png")
&nbsp;
<i># vykresleni grafu na obrazovku</i>
plt.show()
</pre>

<pre>
1 0.33333333333333337
2 0.4666666666666666
3 0.7133333333333333
4 0.7
5 0.9066666666666666
6 0.9733333333333334
7 0.9866666666666667
8 0.9133333333333333
9 0.9133333333333333
10 0.9733333333333334
11 0.9866666666666667
12 0.9866666666666667
13 0.9733333333333334
14 0.9733333333333334
15 0.9800000000000001
16 0.9733333333333334
17 0.9800000000000001
18 0.9800000000000001
19 0.9733333333333334
</pre>



<p><a name="k09"></a></p>
<h2 id="k09">9. </h2>

<pre>
import matplotlib.pyplot as plt
&nbsp;
from sklearn.datasets import load_iris
from sklearn.model_selection import cross_val_score
&nbsp;
<i># model zalozeny na neuronove siti</i>
from sklearn.neural_network import MLPClassifier
&nbsp;
&nbsp;
<i># nacteni datove sady</i>
iris = load_iris()
&nbsp;
<i># X je matice (feature matrix)</i>
X = iris.data
&nbsp;
<i># y je vektor (response vector)</i>
y = iris.target
&nbsp;
avg_scores = []
&nbsp;
NEURONS = 5
r = range(1, 40)
&nbsp;
<i># hledani optimalniho poctu neuronu ve vrstvach</i>
for layers in r:
    <i># konstrukce klasifikatoru</i>
    <i># (s hyperparametrem)</i>
    layer_sizes = (NEURONS, ) * layers
    classifier = MLPClassifier(max_iter=5000, hidden_layer_sizes = layer_sizes)
&nbsp;
    <i># vypocet skore</i>
    scores = cross_val_score(classifier, X, y, cv=10, scoring='accuracy')
&nbsp;
    avg_score = scores.mean()
&nbsp;
    <i># vypsani prumerneho skore do tabulky</i>
    print(layers, avg_score)
&nbsp;
    avg_scores.append(avg_score)
&nbsp;
plt.plot(r, avg_scores)
plt.xlabel("Změna počtu vrstev")
plt.ylabel("Přesnost modelu")
&nbsp;
<i># ulozeni grafu do souboru</i>
plt.savefig("141.png")
&nbsp;
<i># vykresleni grafu na obrazovku</i>
plt.show()

</pre>

<pre>
1 0.9666666666666668
2 0.9800000000000001
3 0.8466666666666667
4 0.9733333333333334
5 0.9733333333333334
6 0.7133333333333333
7 0.8733333333333334
8 0.8866666666666667
9 0.5999999999999999
10 0.7466666666666667
11 0.7333333333333333
12 0.6799999999999999
13 0.5866666666666667
14 0.8066666666666666
15 0.4666666666666666
16 0.4666666666666666
17 0.4533333333333333
18 0.4333333333333333
19 0.39333333333333337
20 0.4
21 0.33333333333333337
22 0.33333333333333337
23 0.44666666666666666
24 0.33333333333333337
25 0.42000000000000004
26 0.4
27 0.33333333333333337
28 0.33333333333333337
29 0.33333333333333337
30 0.33333333333333337
31 0.33333333333333337
32 0.33333333333333337
33 0.33333333333333337
34 0.33333333333333337
35 0.33333333333333337
36 0.33333333333333337
37 0.33333333333333337
38 0.33333333333333337
39 0.33333333333333337
</pre>



<p><a name="k10"></a></p>
<h2 id="k10">10. </h2>

<pre>
from sklearn import linear_model
from sklearn.datasets import fetch_california_housing
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
&nbsp;
<i># nacteni datove sady</i>
housings = fetch_california_housing()
&nbsp;
<i># precteni dat z datove sady</i>
<i># urcenych pro trenink, validaci atd.</i>
data = housings["data"]
&nbsp;
<i># ceny bloku</i>
targets = housings["target"]
&nbsp;
<i># X je matice, y je vektor</i>
X = data
y = targets
&nbsp;
<i># rozdeleni dat na treninkovou a testovaci mnozinu</i>
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.6)
&nbsp;
<i># konstrukce modelu</i>
lr = linear_model.LinearRegression()
&nbsp;
<i># trénink modelu</i>
lr.fit(X_train, y_train)
&nbsp;
<i># predikce modelu</i>
y_pred = lr.predict(X_test)
&nbsp;
<i># výpis vypočtených koeficientů modelu</i>
print("Coefficients: \n", lr.coef_)
print("Intercept: \n", lr.intercept_)
&nbsp;
<i># chyba predikce</i>
print("Mean squared error: %.2f" % mean_squared_error(y_test, y_pred))
&nbsp;
<i># 1 = nejlepší predikce modelu</i>
print("Coefficient of determination: %.2f" % r2_score(y_test, y_pred))
</pre>

<pre>
Coefficients:
 [ 4.45786809e-01  9.72953693e-03 -1.35720128e-01  8.20514635e-01
 -1.33231372e-06 -2.84786314e-03 -4.16735165e-01 -4.31811104e-01]
Intercept:
 -36.869534449989914
Mean squared error: 0.53
Coefficient of determination: 0.61
</pre>



<p><a name="k11"></a></p>
<h2 id="k11">11. </h2>



<p><a name="k12"></a></p>
<h2 id="k12">12. </h2>

<pre>
<i># model zalozeny na neuronove siti</i>
from sklearn.neural_network import MLPRegressor

from sklearn.datasets import fetch_california_housing
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split

<i># nacteni datove sady</i>
housings = fetch_california_housing()

<i># precteni dat z datove sady</i>
<i># urcenych pro trenink, validaci atd.</i>
data = housings["data"]

<i># ceny bloku</i>
targets = housings["target"]

<i># X je matice, y je vektor</i>
X = data
y = targets

<i># rozdeleni dat na treninkovou a testovaci mnozinu</i>
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)

<i># konstrukce modelu</i>
nn = MLPRegressor(max_iter=5000)

<i># trénink modelu</i>
nn.fit(X_train, y_train)

<i># predikce modelu</i>
y_pred = nn.predict(X_test)

<i># chyba predikce</i>
print("Mean squared error: %.2f" % mean_squared_error(y_test, y_pred))

<i># 1 = nejlepší predikce modelu</i>
print("Coefficient of determination: %.2f" % r2_score(y_test, y_pred))

print(f"Features: {nn.n_features_in_}")
print(f"Layers:   {nn.n_layers_}")
print(f"Outputs:  {nn.n_outputs_}")
print("Weights:")

for layer, weights in enumerate(nn.coefs_):
    print("\t", layer, weights.shape)
</pre>

<pre>
Mean squared error: 0.61
Coefficient of determination: 0.55
Features: 8
Layers:   3
Outputs:  1
Weights:
         0 (8, 100)
         1 (100, 1)
</pre>



<p><a name="k13"></a></p>
<h2 id="k13">13. </h2>

<pre>
<i># model zalozeny na neuronove siti</i>
from sklearn.neural_network import MLPRegressor

from sklearn.datasets import fetch_california_housing
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split

<i># nacteni datove sady</i>
housings = fetch_california_housing()

<i># precteni dat z datove sady</i>
<i># urcenych pro trenink, validaci atd.</i>
data = housings["data"]

<i># ceny bloku</i>
targets = housings["target"]

<i># X je matice, y je vektor</i>
X = data
y = targets

<i># rozdeleni dat na treninkovou a testovaci mnozinu</i>
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)

neurons = 1000

<i># konstrukce modelu</i>
nn = MLPRegressor(max_iter=5000, hidden_layer_sizes = (neurons, neurons, ))

<i># trénink modelu</i>
nn.fit(X_train, y_train)

<i># predikce modelu</i>
y_pred = nn.predict(X_test)

<i># chyba predikce</i>
print("Mean squared error: %.2f" % mean_squared_error(y_test, y_pred))

<i># 1 = nejlepší predikce modelu</i>
print("Coefficient of determination: %.2f" % r2_score(y_test, y_pred))

print(f"Features: {nn.n_features_in_}")
print(f"Layers:   {nn.n_layers_}")
print(f"Outputs:  {nn.n_outputs_}")
print("Weights:")

for layer, weights in enumerate(nn.coefs_):
    print("\t", layer, weights.shape)
</pre>

<pre>
Mean squared error: 1.07
Coefficient of determination: 0.19
Features: 8
Layers:   4
Outputs:  1
Weights:
         0 (8, 1000)
         1 (1000, 1000)
         2 (1000, 1)
</pre>



<p><a name="k14"></a></p>
<h2 id="k14">14. </h2>


<p><a name="k15"></a></p>
<h2 id="k15">15. </h2>



<p><a name="k16"></a></p>
<h2 id="k16">16. </h2>


<p><a name="k17"></a></p>
<h2 id="k17">17. </h2>



<p><a name="k18"></a></p>
<h2 id="k18">18. </h2>



<p><a name="k19"></a></p>
<h2 id="k19">19. Repositář s&nbsp;demonstračními příklady</h2>

<p>Všechny demonstrační příklady využívající knihovnu Scikit-learn lze nalézt
v&nbsp;repositáři <a
href="https://github.com/tisnik/most-popular-python-libs">https://github.com/tisnik/most-popular-python-libs</a>.
Následují odkazy na jednotlivé příklady i na (Jupyter) diáře s&nbsp;postupem
výpočtů a analýz:</p>

<table>
<tr><th>#<th>Příklad</th><th>Stručný popis</th><th>Adresa příkladu</th></tr></i>
<tr><td> 1</td><td>01_show_matrix.py</td><td>kooperace mezi knihovnami Matplotlib a NumPy: vizualizace obsahu 2D matice</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/01_show_matrix.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/01_show_matrix.py</a></td></tr>
<tr><td> 2</td><td>02_get_digits.py</td><td>datová množina obsahující naskenované ručně napsané číslice</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/02_get_digits.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/02_get_digits.py</a></td></tr>
<tr><td> 3</td><td>03_get_features.py</td><td>další atributy datové množiny, které použijeme při trénování</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/03_get_features.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/03_get_features.py</a></td></tr>
<tr><td> 4</td><td>04_get_images.py</td><td>přečtení a následné vykreslení jednotlivých ručně nakreslených číslic</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/04_get_images.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/04_get_images.py</a></td></tr>
<tr><td> 5</td><td>05_show_grayscale_matrix.py</td><td>odstranění umělé aplikované barvové palety (obrázky ve stupních šedi)</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/05_show_grayscale_matrix.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/05_show_grayscale_matrix.py</a></td></tr>
<tr><td> 6</td><td>06_grayscale_images.py</td><td>vykreslení ručně nakreslených číslic ve formě obrázků ve stupních šedi</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/06_grayscale_images.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/06_grayscale_images.py</a></td></tr>
<tr><td> 7</td><td>07_multiplot.py</td><td>rozdělení plochy grafu do oblastí; vykreslení více obrázků do jediného grafu</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/07_multiplot.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/07_multiplot.py</a></td></tr>
<tr><td> 8</td><td>08_model_preperation_1.py</td><td>obrázky s&nbsp;jejich ohodnocením</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/08_model_preperation_1.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/08_model_preperation_1.py</a></td></tr>
<tr><td> 9</td><td>09_training_set.py</td><td>příprava dat pro trénink</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/09_training_set.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/09_training_set.py</a></td></tr>
<tr><td>10</td><td>10_classification.py</td><td>klasifikace obrázků</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/10_classification.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/10_classification.py</a></td></tr>
<tr><td>11</td><td>11_results.py</td><td>vykreslení obrázků společně s&nbsp;jejich klasifikací</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/11_results.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/11_results.py</a></td></tr>
<tr><td>12</td><td>12_change_training_set.py</td><td>změna poměru rozdělení dat na tréninkovou a testovací množinu</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/12_change_training_set.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/12_change_training_set.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>13</td><td>13_blobs.py</td><td>použití funkce <strong>make_blobs</strong> pro vygenerování sady bodů v&nbsp;rovině sdružených do oblastí</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/13_blobs.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/13_blobs.py</a></td></tr>
<tr><td>14</td><td>14_swap_coords.py</td><td>úprava předchozího příkladu: prohození souřadnic na osách</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/14_swap_coords.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/14_swap_coords.py</a></td></tr>
<tr><td>15</td><td>15_blobs_scatter_plot.py</td><td>základní podoba bodového diagramu (<i>scatter plot</i>)</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/15_blobs_scatter_plot.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/15_blobs_scatter_plot.py</a></td></tr>
<tr><td>16</td><td>16_blobs_scatter_plot.py</td><td>úprava bodového diagramu při zobrazení většího množství bodů</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/16_blobs_scatter_plot.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/16_blobs_scatter_plot.py</a></td></tr>
<tr><td>17</td><td>17_colorized_blobs.py</td><td>obarvení bodů podle oblastí</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/17_colorized_blobs.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/17_colorized_blobs.py</a></td></tr>
<tr><td>18</td><td>18_k-means.py</td><td>základní použití algoritmu K-means pro clustering</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/18_k-means.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/18_k-means.py</a></td></tr>
<tr><td>19</td><td>19_combination.py</td><td>zobrazení centroidů společně s&nbsp;původními body</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/19_combination.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/19_combination.py</a></td></tr>
<tr><td>20</td><td>20_combinations.py</td><td>vizualizace clusteringu původní množiny bodů</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/20_combinations.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/20_combinations.py</a></td></tr>
<tr><td>21</td><td>21_other_settings.py</td><td>vizualizace clusteringu původní množiny bodů pro odlišnou množinu</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/21_other_settings.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/21_other_settings.py</a></td></tr>
<tr><td>22</td><td>22_random_points.py</td><td>clustering pro náhodná data</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/22_random_points.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/22_random_points.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>23</td><td>23_circles.py</td><td>pseudonáhodné rozmístění bodů do kružnic, menší náhodnost výsledku</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/23_circles.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/23_circles.py</a></td></tr>
<tr><td>24</td><td>24_more_noise_circles.py</td><td>pseudonáhodné rozmístění bodů do kružnic, větší náhodnost výsledku</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/24_more_noise_circles.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/24_more_noise_circles.py</a></td></tr>
<tr><td>25</td><td>25_moons.py</td><td>pseudonáhodné rozmístění bodů do tvaru dvou půlměsíců, menší náhodnost</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/25_moons.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/25_moons.py</a></td></tr>
<tr><td>26</td><td>26_more_noisy_moons.py</td><td>pseudonáhodné rozmístění bodů do tvaru dvou půlměsíců, větší náhodnost</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/26_more_noisy_moons.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/26_more_noisy_moons.py</a></td></tr>
</table>

<p>V&nbsp;repositáři nalezneme taktéž projektový soubor a Jupyter Notebook
s&nbsp;vysvětlením, jak lze modely využít pro rozpoznávání obsahu rastrových
obrázků:</p>

<table>
<tr><th>#<th>Příklad</th><th>Stručný popis</th><th>Adresa příkladu</th></tr></i>
<tr><td>1</td><td>pyproject.toml</td><td>projektový soubor (pro PDM) se všemi závislostmi</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/pyproject.toml">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/pyproject.toml</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>2</td><td>pdm.lock</td><td>lock soubor s&nbsp;konkrétními verzemi všech přímých i tranzitivních závislostí</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/pdm.lock">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/pdm.lock</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>3</td><td>Rozpoznání_obrazu_scikit-learn.ipynb</td><td>Jupyter notebook s&nbsp;celým postupem</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/Rozpoznání_obrazu_scikit-learn.ipynb">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/Rozpoznání_obrazu_scikit-learn.ipynb</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>4</td><td>particle_life.py</td><td>emergence: příklad vzniku struktury</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/particles/particle_life.py">https://github.com/tisnik/most-popular-python-libs/blob/master/particles/particle_life.py</a></td></tr>
</table>



<p><a name="k20"></a></p>
<h2 id="k20">20. Odkazy na Internetu</h2>

<ol>

<li>Shluková analýza (clustering) a knihovna Scikit-learn<br />
<a href="https://www.root.cz/clanky/shlukova-analyza-clustering-a-knihovna-scikit-learn/">https://www.root.cz/clanky/shlukova-analyza-clustering-a-knihovna-scikit-learn/</a>
</li>

<li>Shluková analýza (clustering) a knihovna Scikit-learn (2)<br />
<a href="https://www.root.cz/clanky/shlukova-analyza-clustering-a-knihovna-scikit-learn-2/">https://www.root.cz/clanky/shlukova-analyza-clustering-a-knihovna-scikit-learn-2/</a>
</li>

<li>Shluková analýza (clustering) a knihovna Scikit-learn (z plochy do 3D prostoru)<br />
<a href="https://www.root.cz/clanky/shlukova-analyza-clustering-a-knihovna-scikit-learn-z-plochy-do-3d-prostoru/">https://www.root.cz/clanky/shlukova-analyza-clustering-a-knihovna-scikit-learn-z-plochy-do-3d-prostoru/</a>
</li>

<li>Rozpoznávání obrázků knihovnou Scikit-learn: první kroky<br />
<a href="https://www.root.cz/clanky/rozpoznavani-obrazku-knihovnou-scikit-learn-prvni-kroky/">https://www.root.cz/clanky/rozpoznavani-obrazku-knihovnou-scikit-learn-prvni-kroky/</a>
</li>

<li>scikit-learn: Machine Learning in Python<br />
<a href="https://scikit-learn.org/stable/index.html">https://scikit-learn.org/stable/index.html</a>
</li>

<li>Sklearn-pandas<br />
<a href="https://github.com/scikit-learn-contrib/sklearn-pandas">https://github.com/scikit-learn-contrib/sklearn-pandas</a>
</li>

<li>sklearn-xarray<br />
<a href="https://github.com/phausamann/sklearn-xarray/">https://github.com/phausamann/sklearn-xarray/</a>
</li>

<li>Clustering<br />
<a href="https://scikit-learn.org/stable/modules/clustering.html">https://scikit-learn.org/stable/modules/clustering.html</a>
</li>

<li>Cluster analysis (Wikipedia)<br />
<a href="https://en.wikipedia.org/wiki/Cluster_analysis">https://en.wikipedia.org/wiki/Cluster_analysis</a>
</li>

<li>Shluková analýza (Wikipedia)<br />
<a href="https://cs.wikipedia.org/wiki/Shlukov%C3%A1_anal%C3%BDza">https://cs.wikipedia.org/wiki/Shlukov%C3%A1_anal%C3%BDza</a>
</li>

<li>K-means<br />
<a href="https://cs.wikipedia.org/wiki/K-means">https://cs.wikipedia.org/wiki/K-means</a>
</li>

<li>k-means clustering<br />
<a href="https://en.wikipedia.org/wiki/K-means_clustering">https://en.wikipedia.org/wiki/K-means_clustering</a>
</li>

<li>Spectral clustering<br />
<a href="https://en.wikipedia.org/wiki/Spectral_clustering">https://en.wikipedia.org/wiki/Spectral_clustering</a>
</li>

<li>Emergence<br />
<a href="https://cs.wikipedia.org/wiki/Emergence">https://cs.wikipedia.org/wiki/Emergence</a>
</li>

<li>Particle Life: Vivid structures from rudimentary rules<br />
<a href="https://particle-life.com/">https://particle-life.com/</a>
</li>

<li>Hertzsprungův–Russellův diagram<br />
<a href="https://cs.wikipedia.org/wiki/Hertzsprung%C5%AFv%E2%80%93Russell%C5%AFv_diagram">https://cs.wikipedia.org/wiki/Hertzsprung%C5%AFv%E2%80%93Russell%C5%AFv_diagram</a>
</li>

<li>Using Machine Learning in an HR Diagram<br />
<a href="https://cocalc.com/share/public_paths/08b6e03583cbdef3cdb9813a54ec68ff773c747f">https://cocalc.com/share/public_paths/08b6e03583cbdef3cdb9813a54ec68ff773c747f</a>
</li>

<li>Gaia H-R diagrams: Querying Gaia data for one million nearby stars<br />
<a href="https://vlas.dev/post/gaia-dr2-hrd/">https://vlas.dev/post/gaia-dr2-hrd/</a>
</li>

<li>The Hertzsprung–Russell diagram<br />
<a href="https://scipython.com/book2/chapter-9-data-analysis-with-pandas/problems/p92/the-hertzsprung-russell-diagram/">https://scipython.com/book2/chapter-9-data-analysis-with-pandas/problems/p92/the-hertzsprung-russell-diagram/</a>
</li>

<li>Animated Hertzsprung-Russell Diagram with 119,614 datapoints<br />
<a href="https://github.com/zonination/h-r-diagram">https://github.com/zonination/h-r-diagram</a>
</li>

<li>Neuraxle Pipelines<br />
<a href="https://github.com/Neuraxio/Neuraxle">https://github.com/Neuraxio/Neuraxle</a>
</li>

<li>scikit-learn: Getting Started<br />
<a href="https://scikit-learn.org/stable/getting_started.html">https://scikit-learn.org/stable/getting_started.html</a>
</li>

<li>Support Vector Machines<br />
<a href="https://scikit-learn.org/stable/modules/svm.html">https://scikit-learn.org/stable/modules/svm.html</a>
</li>

<li>Use Deep Learning to Detect Programming Languages<br />
<a href="http://searene.me/2017/11/26/use-neural-networks-to-detect-programming-languages/">http://searene.me/2017/11/26/use-neural-networks-to-detect-programming-languages/</a>
</li>

<li>Natural-language processing<br />
<a href="https://en.wikipedia.org/wiki/Natural-language_processing">https://en.wikipedia.org/wiki/Natural-language_processing</a>
</li>

<li>THE MNIST DATABASE of handwritten digits<br />
<a href="http://yann.lecun.com/exdb/mnist/">http://yann.lecun.com/exdb/mnist/</a>
</li>

<li>MNIST database (Wikipedia)<br />
<a href="https://en.wikipedia.org/wiki/MNIST_database">https://en.wikipedia.org/wiki/MNIST_database</a>
</li>

<li>MNIST For ML Beginners<br />
<a href="https://www.tensorflow.org/get_started/mnist/beginners">https://www.tensorflow.org/get_started/mnist/beginners</a>
</li>

<li>Stránka projektu Torch<br />
<a href="http://torch.ch/">http://torch.ch/</a>
</li>

<li>Torch: Serialization<br />
<a href="https://github.com/torch/torch7/blob/master/doc/serialization.md">https://github.com/torch/torch7/blob/master/doc/serialization.md</a>
</li>

<li>Torch: modul image<br />
<a href="https://github.com/torch/image/blob/master/README.md">https://github.com/torch/image/blob/master/README.md</a>
</li>

<li>Data pro neuronové sítě<br />
<a href="http://archive.ics.uci.edu/ml/index.php">http://archive.ics.uci.edu/ml/index.php</a>
</li>

<li>Torch na GitHubu (několik repositářů)<br />
<a href="https://github.com/torch">https://github.com/torch</a>
</li>

<li>Torch (machine learning), Wikipedia<br />
<a href="https://en.wikipedia.org/wiki/Torch_%28machine_learning%29">https://en.wikipedia.org/wiki/Torch_%28machine_learning%29</a>
</li>

<li>Torch Package Reference Manual<br />
<a href="https://github.com/torch/torch7/blob/master/README.md">https://github.com/torch/torch7/blob/master/README.md</a>
</li>

<li>Torch Cheatsheet<br />
<a href="https://github.com/torch/torch7/wiki/Cheatsheet">https://github.com/torch/torch7/wiki/Cheatsheet</a>
</li>

<li>Neural network containres (Torch)<br />
<a href="https://github.com/torch/nn/blob/master/doc/containers.md">https://github.com/torch/nn/blob/master/doc/containers.md</a>
</li>

<li>Simple layers<br />
<a href="https://github.com/torch/nn/blob/master/doc/simple.md#nn.Linear">https://github.com/torch/nn/blob/master/doc/simple.md#nn.Linear</a>
</li>

<li>Transfer Function Layers<br />
<a href="https://github.com/torch/nn/blob/master/doc/transfer.md#nn.transfer.dok">https://github.com/torch/nn/blob/master/doc/transfer.md#nn.transfer.dok</a>
</li>

<li>Feedforward neural network<br />
<a href="https://en.wikipedia.org/wiki/Feedforward_neural_network">https://en.wikipedia.org/wiki/Feedforward_neural_network</a>
</li>

<li>Biologické algoritmy (4) - Neuronové sítě<br />
<a href="https://www.root.cz/clanky/biologicke-algoritmy-4-neuronove-site/">https://www.root.cz/clanky/biologicke-algoritmy-4-neuronove-site/</a>
</li>

<li>Biologické algoritmy (5) - Neuronové sítě<br />
<a href="https://www.root.cz/clanky/biologicke-algoritmy-5-neuronove-site/">https://www.root.cz/clanky/biologicke-algoritmy-5-neuronove-site/</a>
</li>

<li>Umělá neuronová síť (Wikipedia)<br />
<a href="https://cs.wikipedia.org/wiki/Um%C4%9Bl%C3%A1_neuronov%C3%A1_s%C3%AD%C5%A5">https://cs.wikipedia.org/wiki/Um%C4%9Bl%C3%A1_neuronov%C3%A1_s%C3%AD%C5%A5</a>
</li>

<li>PyTorch<br />
<a href="http://pytorch.org/">http://pytorch.org/</a>
</li>

<li>JupyterLite na PyPi<br />
<a href="https://pypi.org/project/jupyterlite/">https://pypi.org/project/jupyterlite/</a>
</li>

<li>JupyterLite na GitHubu<br />
<a href="https://github.com/jupyterlite/jupyterlite">https://github.com/jupyterlite/jupyterlite</a>
</li>

<li>Dokumentace k&nbsp;projektu JupyterLite<br />
<a href="https://github.com/jupyterlite/jupyterlite">https://github.com/jupyterlite/jupyterlite</a>
</li>

<li>Matplotlib Home Page<br />
<a href="http://matplotlib.org/">http://matplotlib.org/</a>
</li>

<li>Matplotlib (Wikipedia)<br />
<a href="https://en.wikipedia.org/wiki/Matplotlib">https://en.wikipedia.org/wiki/Matplotlib</a>
</li>

<li>Popis barvových map modulu matplotlib.cm<br />
<a href="https://gist.github.com/endolith/2719900#id7">https://gist.github.com/endolith/2719900#id7</a>
</li>

<li>Ukázky (palety) barvových map modulu matplotlib.cm<br />
<a href="http://matplotlib.org/examples/color/colormaps_reference.html">http://matplotlib.org/examples/color/colormaps_reference.html</a>
</li>

<li>Galerie grafů vytvořených v&nbsp;Matplotlibu<br />
<a href="https://matplotlib.org/3.2.1/gallery/">https://matplotlib.org/3.2.1/gallery/</a>
</li>

<li>3D rendering<br />
<a href="https://en.wikipedia.org/wiki/3D_rendering">https://en.wikipedia.org/wiki/3D_rendering</a>
</li>

<li>3D computer graphics<br />
<a href="https://en.wikipedia.org/wiki/3D_computer_graphics">https://en.wikipedia.org/wiki/3D_computer_graphics</a>
</li>

<li>Primary 3D view planes<br />
<a href="https://matplotlib.org/stable/gallery/mplot3d/view_planes_3d.html">https://matplotlib.org/stable/gallery/mplot3d/view_planes_3d.html</a>
</li>

<li>Getting started in scikit-learn with the famous iris dataset<br />
<a href="https://www.youtube.com/watch?v=hd1W4CyPX58">https://www.youtube.com/watch?v=hd1W4CyPX58</a>
</li>

<li>Training a machine learning model with scikit-learn<br />
<a href="https://www.youtube.com/watch?v=RlQuVL6-qe8">https://www.youtube.com/watch?v=RlQuVL6-qe8</a>
</li>

<li>Iris (plant)<br />
<a href="https://en.wikipedia.org/wiki/Iris_(plant)">https://en.wikipedia.org/wiki/Iris_(plant)</a>
</li>

<li>Kosatec<br />
<a href="https://cs.wikipedia.org/wiki/Kosatec">https://cs.wikipedia.org/wiki/Kosatec</a>
</li>

<li>Iris setosa<br />
<a href="https://en.wikipedia.org/wiki/Iris_setosa">https://en.wikipedia.org/wiki/Iris_setosa</a>
</li>

<li>Iris versicolor<br />
<a href="https://en.wikipedia.org/wiki/Iris_versicolor">https://en.wikipedia.org/wiki/Iris_versicolor</a>
</li>

<li>Iris virginica<br />
<a href="https://en.wikipedia.org/wiki/Iris_virginica">https://en.wikipedia.org/wiki/Iris_virginica</a>
</li>

<li>Druh<br />
<a href="https://cs.wikipedia.org/wiki/Druh">https://cs.wikipedia.org/wiki/Druh</a>
</li>

<li>Iris subg. Limniris<br />
<a href="https://en.wikipedia.org/wiki/Iris_subg._Limniris">https://en.wikipedia.org/wiki/Iris_subg._Limniris</a>
</li>

<li>Iris Dataset Classification with Python: A Tutorial<br />
<a href="https://www.pycodemates.com/2022/05/iris-dataset-classification-with-python.html">https://www.pycodemates.com/2022/05/iris-dataset-classification-with-python.html</a>
</li>

<li>Iris flower data set<br />
<a href="https://en.wikipedia.org/wiki/Iris_flower_data_set">https://en.wikipedia.org/wiki/Iris_flower_data_set</a>
</li>

<li>List of datasets for machine-learning research<br />
<a href="https://en.wikipedia.org/wiki/List_of_datasets_for_machine-learning_research">https://en.wikipedia.org/wiki/List_of_datasets_for_machine-learning_research</a>
</li>

<li>Analýza hlavních komponent<br />
<a href="https://cs.wikipedia.org/wiki/Anal%C3%BDza_hlavn%C3%ADch_komponent">https://cs.wikipedia.org/wiki/Anal%C3%BDza_hlavn%C3%ADch_komponent</a>
</li>

<li>Principal component analysis<br />
<a href="https://en.wikipedia.org/wiki/Principal_component_analysis">https://en.wikipedia.org/wiki/Principal_component_analysis</a>
</li>

<li>Scikit-learn Crash Course - Machine Learning Library for Python<br />
<a href="https://www.youtube.com/watch?v=0B5eIE_1vpU">https://www.youtube.com/watch?v=0B5eIE_1vpU</a>
</li>

<li>calm-notebooks<br />
<a href="https://github.com/koaning/calm-notebooks">https://github.com/koaning/calm-notebooks</a>
</li>

<li>Should you teach Python or R for data science?<br />
<a href="https://www.dataschool.io/python-or-r-for-data-science/">https://www.dataschool.io/python-or-r-for-data-science/</a>
</li>

<li>nbviewer: A simple way to share Jupyter Notebooks<br />
<a href="https://nbviewer.org/">https://nbviewer.org/</a>
</li>

<li>AI vs Machine Learning (Youtube)<br />
<a href="https://www.youtube.com/watch?v=4RixMPF4xis">https://www.youtube.com/watch?v=4RixMPF4xis</a>
</li>

<li>Machine Learning | What Is Machine Learning? | Introduction To Machine Learning | 2024 | Simplilearn (Youtube)<br />
<a href="https://www.youtube.com/watch?v=ukzFI9rgwfU">https://www.youtube.com/watch?v=ukzFI9rgwfU</a>
</li>

<li>A Gentle Introduction to Machine Learning (Youtube)<br />
<a href="https://www.youtube.com/watch?v=Gv9_4yMHFhI">https://www.youtube.com/watch?v=Gv9_4yMHFhI</a>
</li>

<li>Machine Learning vs Deep Learning<br />
<a href="https://www.youtube.com/watch?v=q6kJ71tEYqM">https://www.youtube.com/watch?v=q6kJ71tEYqM</a>
</li>

<li>Umělá inteligence (slajdy)<br />
<a href="https://slideplayer.cz/slide/12119218/">https://slideplayer.cz/slide/12119218/</a>
</li>

<li>Úvod do umělé inteligence<br />
<a href="https://slideplayer.cz/slide/2505525/">https://slideplayer.cz/slide/2505525/</a>
</li>

<li>Umělá inteligence I / Artificial Intelligence I<br />
<a href="https://ktiml.mff.cuni.cz/~bartak/ui/">https://ktiml.mff.cuni.cz/~bartak/ui/</a>
</li>

<li>Matplotlib vs. seaborn vs. Plotly vs. MATLAB vs. ggplot2 vs. pandas<br />
<a href="https://ritza.co/articles/matplotlib-vs-seaborn-vs-plotly-vs-MATLAB-vs-ggplot2-vs-pandas/">https://ritza.co/articles/matplotlib-vs-seaborn-vs-plotly-vs-MATLAB-vs-ggplot2-vs-pandas/</a>
</li>

<li>Matplotlib, Seaborn or Plotnine?<br />
<a href="https://www.reddit.com/r/datascience/comments/jvrqxt/matplotlib_seaborn_or_plotnine/">https://www.reddit.com/r/datascience/comments/jvrqxt/matplotlib_seaborn_or_plotnine/</a>
</li>

<li>@Rabeez: Rabeez/plotting_comparison.ipynb<br />
<a href="https://gist.github.com/Rabeez/ffc0b59d4a41e20fa8d944c44a96adbc">https://gist.github.com/Rabeez/ffc0b59d4a41e20fa8d944c44a96adbc</a>
</li>

<li>Matplotlib, Seaborn, Plotly and Plotnine Comparison<br />
<a href="https://python.plainenglish.io/matplotlib-seaborn-plotly-and-plotnine-comparison-baf2db5a9c40">https://python.plainenglish.io/matplotlib-seaborn-plotly-and-plotnine-comparison-baf2db5a9c40</a>
</li>

<li>Data Visualization 101: How to Choose a Python Plotting Library<br />
<a href="https://towardsdatascience.com/data-visualization-101-how-to-choose-a-python-plotting-library-853460a08a8a">https://towardsdatascience.com/data-visualization-101-how-to-choose-a-python-plotting-library-853460a08a8a</a>
</li>

<li>Data science in Python: pandas, seaborn, scikit-learn<br />
<a href="https://www.youtube.com/watch?v=3ZWuPVWq7p4">https://www.youtube.com/watch?v=3ZWuPVWq7p4</a>
</li>

<li>7.2. Real world datasets<br />
<a href="https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset">https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset</a>
</li>

<li>7.2.7. California Housing dataset<br />
<a href="https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset">https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset</a>
</li>

<li>Comprehensive Guide to Classification Models in Scikit-Learn<br />
<a href="https://www.geeksforgeeks.org/comprehensive-guide-to-classification-models-in-scikit-learn/">https://www.geeksforgeeks.org/comprehensive-guide-to-classification-models-in-scikit-learn/</a>
</li>

<li>Tidy Data Visualization: ggplot2 vs seaborn<br />
<a href="https://blog.tidy-intelligence.com/posts/ggplot2-vs-seaborn/">https://blog.tidy-intelligence.com/posts/ggplot2-vs-seaborn/</a>
</li>

<li>seaborn: statistical data visualization<br />
<a href="https://seaborn.pydata.org/">https://seaborn.pydata.org/</a>
</li>

<li>Linear regression (Wikipedia)<br />
<a href="https://en.wikipedia.org/wiki/Linear_regression">https://en.wikipedia.org/wiki/Linear_regression</a>
</li>

<li>Lineární regrese (Wikipedia)<br />
<a href="https://cs.wikipedia.org/wiki/Line%C3%A1rn%C3%AD_regrese">https://cs.wikipedia.org/wiki/Line%C3%A1rn%C3%AD_regrese</a>
</li>

<li>Iris Flower Classification with MLP Classifier<br />
<a href="https://www.metriccoders.com/post/iris-flower-classification-with-mlp-classifier">https://www.metriccoders.com/post/iris-flower-classification-with-mlp-classifier</a>
</li>

</ol>



<p></p><p></p>
<p><small>Autor: <a href="http://www.fit.vutbr.cz/~tisnovpa">Pavel Tišnovský</a> &nbsp; 2024</small></p>
</body>
</html>

