<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
        "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html>
<head>
<title>Llama Stack: framework pro tvorbu aplikací s generativní AI</title>
<meta name="Author" content="Pavel Tisnovsky" />
<meta name="Generator" content="vim" />
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<style type="text/css">
         body {color:#000000; background:#ffffff;}
         h1  {font-family: arial, helvetica, sans-serif; color:#ffffff; background-color:#c00000; text-align:center; padding-left:1em}
         h2  {font-family: arial, helvetica, sans-serif; color:#ffffff; background-color:#0000c0; padding-left:1em; text-align:left}
         h3  {font-family: arial, helvetica, sans-serif; color:#000000; background-color:#c0c0c0; padding-left:1em; text-align:left}
         h4  {font-family: arial, helvetica, sans-serif; color:#000000; background-color:#e0e0e0; padding-left:1em; text-align:left}
         a   {font-family: arial, helvetica, sans-serif;}
         li  {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify; width:450px;}
         ol  {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify; width:450px;}
         ul  {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify; width:450px;}
         p   {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify;}
         pre {background:#e0e0e0}
</style>
</head>

<body>

<h1>Llama Stack: framework pro tvorbu aplikací s generativní AI</h1>

<h3>Pavel Tišnovský</h3>

<p></p>

<h1>Úvodník</h1>

<p>V dnešním článku se seznámíme se základy použití frameworku Llama Stack, který je určen pro tvorbu aplikací a nástrojů vybavených generativní AI. Do této oblasti spadají i velké jazykové modely zpopularizované službami jako ChatGPT, Gemini nebo Cursor.</p>



<h2>Obsah</h2>

<p><a href="#k01">1. Llama Stack: framework pro tvorbu aplikací s&nbsp;generativní AI</a></p>
<p><a href="#k02">2. Framework Llama Stack se představuje</a></p>
<p><a href="#k03">3. Příprava projektu pro spuštění a otestování Llama Stacku</a></p>
<p><a href="#k04">4. Přidání všech potřebných závislostí do projektu</a></p>
<p><a href="#k05">5. Kontrola, zda instalace Llama Stacku dopadla úspěšně</a></p>
<p><a href="#k06">6. Nejjednodušší funkční konfigurace Llama Stacku</a></p>
<p><a href="#k07">7. První spuštění Llama Stacku</a></p>
<p><a href="#k08">8. REST API Llama Stacku</a></p>
<p><a href="#k09">9. Přístup ke <i>stacku</i> z&nbsp;aplikací psaných v&nbsp;Pythonu</a></p>
<p><a href="#k10">10. Jednoduchý klient komunikující se stackem</a></p>
<p><a href="#k11">11. Úplný zdrojový kód klienta</a></p>
<p><a href="#k12">12. Spuštění klienta</a></p>
<p><a href="#k13">13. Klient zjišťující, které LLM modely je možné využít</a></p>
<p><a href="#k14">14. Použití Llama Stacku v&nbsp;roli Pythonní knihovny (bez serveru)</a></p>
<p><a href="#k15">15. Úplný zdrojový kód třetí verze klienta</a></p>
<p><a href="#k16">16. Spuštění třetí verze klienta</a></p>
<p><a href="#k17">17. Další možnosti konfigurace <i>stacku</i></a></p>
<p><a href="#k18">18. Příloha: seznam poskytovatelů (<i>providers</i>)</a></p>
<p><a href="#k19">19. Repositář s&nbsp;demonstračními příklady</a></p>
<p><a href="#k20">20. Odkazy na Internetu</a></p>



<p><a name="k01"></a></p>
<h2 id="k01">1. Llama Stack: framework pro tvorbu aplikací s&nbsp;generativní AI</h2>

<p>Pravděpodobně naprostá většina lidí, kteří se pohybují v&nbsp;oblasti
informatiky, zaznamenali revoluci, kterou v&nbsp;posledních dvou či tří letech
přinesla generativní umělá inteligence (<i>generative AI</i>).
V&nbsp;současnosti se zdá, že prakticky každá nová verze většiny aplikací je
vybavena více či méně sofistikovanou umělou inteligencí, což pochopitelně
přináší jak klady, tak i zápory (které mnohdy převažují nad klady, což je však
téma na samostatný článek). A pochopitelně nastala situace běžná v&nbsp;celém
IT: společně s&nbsp;rozvojem generativní AI (nejvíce pak velkých jazykových
modelů neboli LLM) vzniklo i velké množství knihoven a frameworků určených pro
vývoj aplikací vybavených nějakou formou AI, přičemž mnohdy nejsou tyto
knihovny rozšiřitelné, mají mnoho chyb, nepodporují všechny potřebné vlastnosti
(z&nbsp;poslední doby agent-to-agent neboli A2A) apod.</p>

<p>Abychom si ukázali, jak je situace okolo generativní AI (a LLM) rozsáhlá a
chaotická, podívejme se pouze na oblast velkých jazykových modelů (což je
podmnožina generativní AI) a navíc se omezme jen na ekosystém programovacího
jazyka <a
href="https://www.itbiz.cz/clanky/generativni-ai-a-llm-revoluce-v-umele-inteligenci/">Python</a>.
Zde se můžeme se setkat s&nbsp;velkou řadou knihoven a frameworků, zejména
pak:</p>

<ol>
<li>LangChain</li>
<li>langgraph</li>
<li>autogen</li>
<li>metaGPT</li>
<li>phidata</li>
<li>CrewAI</li>
<li>pydanticAI</li>
<li>controlflow</li>
<li>langflow</li>
<li>LiteLLM</li>
<li>Llama Stack</li>
</ol>

<p><div class="rs-tip-major">Poznámka: vybral jsem jen ty nástroje,
s&nbsp;nimiž mám alespoň nějakou zkušenost a které jsme otestovali.</div></p>



<p><a name="k02"></a></p>
<h2 id="k02">2. Framework Llama Stack se představuje</h2>

<p>V&nbsp;seznamu, který byl uveden <a href="#k01">v&nbsp;předchozí
kapitole</a>, nalezneme jak knihovny (příkladem může být <i>LangChain</i>), tak
i více či méně sofistikované frameworky. A právě jedním z&nbsp;frameworků
určených pro tvorbu aplikací, které v&nbsp;nějaké míře využívají generativní
AI, je i framework nazvaný <i>Llama Stack</i>, za jehož vývojem stojí firma
Meta společně s&nbsp;dalšími přispěvateli (což jsou jak firmy, tak i
jednotlivci). Tento framework je možné využít několika způsoby. Buď je možné
celý <i>stack</i> (tj.&nbsp;různé moduly) spustit jako samostatný server,
k&nbsp;němuž je možné se připojit vhodně nakonfigurovaným klientem, nebo se
<i>Llama Stack</i> použije jako běžná knihovna a výsledkem tak bude aplikace,
která bude celý stack obsahovat (ve skutečnosti může být celá konfigurace ještě
složitější, k&nbsp;tomuto tématu se ještě později vrátíme).</p>

<p>Samotný <i>Llama Stack</i> je skutečně jen &bdquo;pouhým&ldquo; frameworkem,
který je však poměrně jednoduše rozšiřitelný o další pluginy, které se nazývají
<i>poskytovatelé</i> neboli anglicky <i>providers</i>. V&nbsp;navazujících
kapitolách si ukážeme realizaci jednoduchého chatu (podobného ChatGPT atd.),
který vyžaduje konfiguraci minimálně dvou poskytovatelů. V&nbsp;první řadě se
jedná o poskytovatele vlastního inference serveru, v&nbsp;našem případě
konkrétně OpenAI. A druhým poskytovatelem, kterého nakonfigurujeme, je
poskytovatel pro telemetrii, logování a trasování (ten je nutné nakonfigurovat
minimálně proto, aby se vypisovaly logovací informace). To znamená, že náš
<i>stack</i> může vypadat takto:</p>

<pre>
providers:
  inference:
    - konfigurace poskytovatele
  telemetry:
    - konfigurace poskytovatele
</pre>

<p>Konkrétně pro OpenAI a logování:</p>

<pre>
providers:
  inference:
    - provider_id: openai
      provider_type: remote::openai
      config:
        api_key: <u>${env.OPENAI_API_KEY}</u>
  telemetry:
    - provider_id: meta-reference
      provider_type: inline::meta-reference
      config:
        sinks: ['console']
</pre>

<p><div class="rs-tip-major">Poznámka: jak je z&nbsp;těchto výpisů patrné, je
celý <i>stack</i> popsán v&nbsp;konfiguračním souboru typu YAML.</div></p>



<p><a name="k03"></a></p>
<h2 id="k03">3. Příprava projektu pro spuštění a otestování Llama Stacku</h2>

<p>Dnešní článek je zaměřen spíše prakticky, což znamená, že se v&nbsp;něm
seznámíme se základním způsobem využití Llama Stacku. V&nbsp;prvním kroku si
musíme nechat vytvořit kostru projektu, který postupně upravíme do finální
podoby. Pro správu projektů využijeme nástroj <i>PDM</i>, který byl popsán
v&nbsp;článku <a
href="https://www.root.cz/clanky/pdm-moderni-spravce-balicku-a-virtualnich-prostredi-pythonu/">PDM:
moderní správce balíčků a virtuálních prostředí Pythonu</a> (ovšem stejně dobře
můžete použít i klasický <strong>pip</strong> zkombinovaný
s&nbsp;<strong>venv</strong> nebo <strong>virtualenv</strong>; velmi dobrou
alternativou je pak <a href="https://docs.astral.sh/uv/">uv</a>).</p>

<p>Nejprve si nainstalujte samotné PDM:</p>

<pre>
$ <strong>pip install --user pdm</strong>
</pre>

<p>Otestujeme, že je příkaz <strong>pdm</strong> dostupný:</p>

<pre>
$ <strong>pdm --version</strong>
&nbsp;
PDM, version 2.22.2
</pre>

<p>Kostra nového projektu se vytvoří příkazem:</p>

<pre>
$ <strong>pdm init</strong>
</pre>

<p>Po zadání tohoto příkazu se systém PDM zeptá na několik otázek. Odpovědi,
které musí poskytnou programátor, jsou zvýrazněny tučným písmem:</p>

<pre>
Creating a pyproject.toml for PDM...
Please enter the Python interpreter to use
 0. cpython@3.12 (/usr/bin/python)
 1. cpython@3.13 (/usr/bin/python3.13)
 2. cpython@3.12 (/usr/bin/python3.12)
 3. cpython@3.11 (/usr/bin/python3.11)
Please select (0): <strong>0</strong>
Virtualenv is created successfully at /tmp/ramdisk/llama-stack-demo/.venv
Project name (llama-stack-demo):  <strong>[Enter]</strong>
Project version (0.1.0):  <strong>[Enter]</strong>
Do you want to build this project for distribution(such as wheel)?
If yes, it will be installed by default when running `pdm install`. [y/n] (n):  <strong>[Enter]</strong>
License(SPDX name) (MIT):  <strong>[Enter]</strong>
Author name (Pavel Tisnovsky):  <strong>[Enter]</strong>
Author email (tisnik@nowhere.com):  <strong>[Enter]</strong>
Python requires('*' to allow any) (==3.12.*):  <strong>[Enter]</strong>
Project is initialized successfully
</pre>

<p><div class="rs-tip-major">Poznámka: nabídka interpretrů jazyka Python a
nabízené jméno a e-mail se pochopitelně budou lišit.</div></p>

<p>Výsledkem výše uvedeného kroku by měl být projektový soubor nazvaný
<strong>pyproject.toml</strong> s&nbsp;následujícím obsahem:</p>

<pre>
[project]
name = "llama-stack-demo"
version = "0.1.0"
description = "Default template for PDM package"
authors = [
    {name = "Pavel Tisnovsky", email = "tisnik@nowhere.com"},
]
dependencies = []
requires-python = "==3.12.*"
readme = "README.md"
license = {text = "MIT"}
&nbsp;
&nbsp;
[tool.pdm]
distribution = false
</pre>

<p><div class="rs-tip-major">Poznámka: povšimněte si řádku, na kterém je
specifikována verze Pythonu. Tento řádek můžete změnit například do této
podoby:</div></p>

<pre>
requires-python = "&gt;=3.11.1,&lt;=3.12.10"
</pre>



<p><a name="k04"></a></p>
<h2 id="k04">4. Přidání všech potřebných závislostí do projektu</h2>

<p>V&nbsp;dalším kroku je nutné do projektu přidat další závislosti, mezi které
patří samotný Llama stack a taktéž balíčky používané při volání LLM atd.
Prozatím použijeme &bdquo;uzamčenou&ldquo; verzi Llama stacku 0.2.8 a teprve
později tuto verzi zvýšíme. Llama stack se totiž poměrně bouřlivě vyvíjí a i
přechod (například) mezi 0.2.8 a 0.2.10 způsobuje nekompatibility (vítejte ve
světě Pythonu+AI).</p>

<pre>
$ <strong>pdm add llama-stack==0.2.8 fastapi opentelemetry-sdk opentelemetry-exporter-otlp opentelemetry-instrumentation aiosqlite litellm uvicorn blobfile</strong>
</pre>

<p>Průběh instalace do virtuálního prostředí Pythonu:</p>

<pre>
Adding packages to default dependencies: llama-stack==0.2.8, fastapi, opentelemetry-sdk, opentelemetry-exporter-otlp,
opentelemetry-instrumentation, aiosqlite, litellm, uvicorn
  0:01:51 🔒 Lock successful.
Changes are written to pyproject.toml.
Synchronizing working set with resolved packages: 87 to add, 0 to update, 0 to remove
&nbsp;
  ✔ Install setuptools 80.9.0 successful
  ✔ Install annotated-types 0.7.0 successful
  ✔ Install distro 1.9.0 successful
  ✔ Install click 8.2.1 successful
  ✔ Install h11 0.16.0 successful
  ...
  ...
  ...
  ✔ Install multidict 6.4.4 successful
  ✔ Install tokenizers 0.21.1 successful
  ✔ Install pillow 11.2.1 successful
  ✔ Install numpy 2.2.6 successful
  ✔ Install rpds-py 0.25.1 successful
  ✔ Install yarl 1.20.0 successful
  ✔ Install regex 2024.11.6 successful
  ✔ Install grpcio 1.72.1 successful
  ✔ Install pydantic-core 2.33.2 successful
  ✔ Install aiohttp 3.12.9 successful
&nbsp;
  0:01:29 🎉 All complete! 87/87
</pre>

<p>Nyní by měl projektový soubor <strong>pyproject.toml</strong> vypadat
následovně:</p>

<pre>
[project]
name = "llama-stack-demo"
version = "0.1.0"
description = "Default template for PDM package"
authors = [
    {name = "Pavel Tisnovsky", email = "tisnik@nowhere.com"},
]
dependencies = [
    "llama-stack==0.2.8",
    "fastapi&gt;=0.115.12",
    "opentelemetry-sdk&gt;=1.34.0",
    "opentelemetry-exporter-otlp&gt;=1.34.0",
    "opentelemetry-instrumentation&gt;=0.55b0",
    "aiosqlite&gt;=0.21.0",
    "litellm&gt;=1.72.1",
    "uvicorn&gt;=0.34.3",
    "blobfile&gt;=3.0.0"]
requires-python = "==3.12.*"
readme = "README.md"
license = {text = "MIT"}
&nbsp;
&nbsp;
[tool.pdm]
distribution = false
</pre>

<p><div class="rs-tip-major">Poznámka: některé verze nástroje PDM všechny
závislosti uloží na jediný řádek, to však nemá na výslednou funkcionalitu
projektu žádný vliv.</div></p>



<p><a name="k05"></a></p>
<h2 id="k05">5. Kontrola, zda instalace Llama Stacku dopadla úspěšně</h2>

<p>V&nbsp;dalším kroku si ověříme, že je možné spustit nástroj nazvaný
<strong>llama</strong>. Ten byl nainstalován do virtuálního prostředí Pythonu a
proto ho musíme spouštět přes <strong>pdm run</strong>:</p>

<pre>
$ <strong>pdm run llama</strong>
</pre>

<p>Pokud instalace proběhla úspěšně, měly by se na terminál vypsat následující
zprávy:</p>

<pre>
usage: llama [-h] {model,stack,download,verify-download} ...
&nbsp;
Welcome to the Llama CLI
&nbsp;
options:
  -h, --help            show this help message and exit
&nbsp;
subcommands:
  {model,stack,download,verify-download}
&nbsp;
  model                 Work with llama models
  stack                 Operations for the Llama Stack / Distributions
  download              Download a model from llama.meta.com or Hugging Face Hub
  verify-download       Verify integrity of downloaded model files
</pre>

<p><div class="rs-tip-major">Poznámka: později využijeme možnost manipulace
s&nbsp;celým &bdquo;stackem&ldquo;, ovšem prozatím nám bude postačovat jeho
spuštění. Ovšem nejdříve je nutné celý &bdquo;stack&ldquo; vhodným způsobem
nakonfigurovat, což je téma navazující kapitoly.</div></p>



<p><a name="k06"></a></p>
<h2 id="k06">6. Nejjednodušší funkční konfigurace Llama Stacku</h2>

<p>V&nbsp;případě, že se pokusíme spustit Llama Stack bez jeho konfigurace,
vypíše se pouze informace o výjimce (což tedy není příliš uživatelsky
příjemné):</p>

<pre>
$ <strong>pdm run llama stack run</strong>
&nbsp;
INFO     2025-06-27 14:36:57,618 llama_stack.cli.stack.run:146 server: No image type or image name provided. Assuming   
         environment packages.                                                                                          
Traceback (most recent call last):
  File "/tmp/ramdisk/llama-stack-demo/demo1/.venv/bin/llama", line 10, in &lt;module&gt;
    sys.exit(main())
             ^^^^^^
  File "/tmp/ramdisk/llama-stack-demo/demo1/.venv/lib/python3.12/site-packages/llama_stack/cli/llama.py", line 53, in main
    parser.run(args)
  File "/tmp/ramdisk/llama-stack-demo/demo1/.venv/lib/python3.12/site-packages/llama_stack/cli/llama.py", line 47, in run
    args.func(args)
  File "/tmp/ramdisk/llama-stack-demo/demo1/.venv/lib/python3.12/site-packages/llama_stack/cli/stack/run.py", line 160, in _run_stack_run_cmd
    server_main(server_args)
  File "/tmp/ramdisk/llama-stack-demo/demo1/.venv/lib/python3.12/site-packages/llama_stack/distribution/server/server.py", line 399, in main
    elif args.template:
         ^^^^^^^^^^^^^
AttributeError: 'Namespace' object has no attribute 'template'
</pre>

<p>Aby bylo možné Llama Stack skutečně spustit a provozovat, musíme si náš
&bdquo;stack&ldquo; vhodným způsobem nakonfigurovat. Prozatím si vystačíme
s&nbsp;velmi jednoduchou konfigurací, která bude mít specifikované pouze dva
poskytovatele (<i>providers</i>), konkrétně poskytovatele, který bude na
konzoli vypisovat telemetrické informace a dále poskytovatele s&nbsp;LLM,
konkrétně s&nbsp;rozhraním pro OpenAI (ale můžeme použít i další poskytovatele
LLM, například Azure Open AI, Gemini, WatsonX, lokálně běžící LLM atd. atd.).
Povšimněte si, že je v&nbsp;konfiguračním souboru možné specifikovat i to, že
některá volba (zde konkrétně přístupový klíč k&nbsp;OpenAI) bude načtena
z&nbsp;proměnné prostředí:</p>

<pre>
version: '2'
image_name: simplest-llama-stack-configuration
container_image: null
&nbsp;
distribution_spec:
  local:
    services:
      - inference
      - telemetry
&nbsp;
apis:
  - inference
  - telemetry
&nbsp;
providers:
  inference:
    - provider_id: openai
      provider_type: remote::openai
      config:
        api_key: <u>${env.OPENAI_API_KEY}</u>
  telemetry:
    - provider_id: meta-reference
      provider_type: inline::meta-reference
      config:
        sinks: ['console']
&nbsp;
models:
  - model_id: gpt-4-turbo
    provider_id: openai
    model_type: llm
    provider_model_id: gpt-4-turbo
&nbsp;
server:
  port: 8321
</pre>

<p>Tato konfigurace předpokládá, že ještě před vlastním spuštěním serveru dojde
k&nbsp;nastavení klíče pro OpenAI do proměnné prostředí nazvané
<strong>OPENAI_API_KEY</strong>. Toto nastavení bude vypadat následovně:</p>

<pre>
$ export OPENAI_API_KEY="sk-xyzzy1234567890xyzzy1234567890xyzzy1234567890..."
</pre>

<p><div class="rs-tip-major">Poznámka: v&nbsp;sekci <strong>server</strong> je
uvedeno, že Llama Stack po svém spuštění bude dostupný právě na portu číslo
8321.</div></p>



<p><a name="k07"></a></p>
<h2 id="k07">7. První spuštění Llama Stacku</h2>

<p>Nyní, když již máme připraven konfigurační soubor pro náš <i>stack</i>, je
možné spustit server s&nbsp;tímto stackem. Pro tento účel použijeme příkaz:</p>

<pre>
$ <strong>pdm run llama stack run run.yaml</strong>
</pre>

<p>Konfigurace je korektní (nebo by alespoň měla být korektní), takže by se
nejprve měly vypsat informace o načítání konfiguračního souboru a následně i
aktuální konfigurace celého serveru:</p>

<pre>
INFO     2025-06-06 10:51:57,634 llama_stack.cli.stack.run:125 server: Using run configuration: run.yaml                
INFO     2025-06-06 10:51:57,640 llama_stack.cli.stack.run:146 server: No image type or image name provided. Assuming   
         environment packages.                                                                                          
INFO     2025-06-06 10:51:58,282 llama_stack.distribution.server.server:422 server: Using config file: run.yaml         
INFO     2025-06-06 10:51:58,285 llama_stack.distribution.server.server:424 server: Run configuration:                  
INFO     2025-06-06 10:51:58,290 llama_stack.distribution.server.server:426 server: apis:                               
         - inference                                                                                                    
         - telemetry                                                                                                    
         benchmarks: []                                                                                                 
         container_image: null                                                                                          
         datasets: []                                                                                                   
         external_providers_dir: null                                                                                   
         image_name: simplest-llama-stack-configuration                                                                 
         inference_store: null                                                                                          
         logging: null                                                                                                  
         metadata_store: null                                                                                           
         models:                                                                                                        
         - metadata: {}                                                                                                 
           model_id: gpt-4-turbo                                                                                        
           model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType                                   
           - llm                                                                                                        
           provider_id: openai                                                                                          
           provider_model_id: gpt-4-turbo                                                                               
         providers:                                                                                                     
           inference:                                                                                                   
           - config:                                                                                                    
               api_key: '********'                                                                                      
             provider_id: openai                                                                                        
             provider_type: remote::openai                                                                              
           telemetry:                                                                                                   
           - config:                                                                                                    
               sinks:                                                                                                   
               - console                                                                                                
             provider_id: meta-reference                                                                                
             provider_type: inline::meta-reference                                                                      
         scoring_fns: []                                                                                                
         server:                                                                                                        
           auth: null                                                                                                   
           host: null                                                                                                   
           port: 8321                                                                                                   
           quota: null                                                                                                  
           tls_cafile: null                                                                                             
           tls_certfile: null                                                                                           
           tls_keyfile: null                                                                                            
         shields: []                                                                                                    
         tool_groups: []                                                                                                
         vector_dbs: []                                                                                                 
         version: '2'                                                                                                   
&nbsp;                                                                                                                        
INFO     2025-06-06 10:52:00,166 llama_stack.distribution.server.server:564 server: Listening on ['::', '0.0.0.0']:8321 
INFO     2025-06-06 10:52:00,176 llama_stack.distribution.server.server:156 server: Starting up                         
INFO:     ::1:57018 - "GET /v1/models HTTP/1.1" 200 OK
</pre>



<p><a name="k08"></a></p>
<h2 id="k08">8. REST API Llama Stacku</h2>

<p>Server naslouchá na portu 8321. K&nbsp;dispozici je popis REST API dostupný
ve formě OpenAPI (endpoint <strong>/openapi.json</strong>), ovšem využít lze i
další endpointy. Například endpoint <strong>/v1/models</strong> vrací seznam AI
modelů, včetně námi nakonfigurovaného LLM modelu OpenAI:</p>

<pre>
$ <strong>curl localhost:8321/v1/models</strong>
&nbsp;
{"data":[{"identifier":"gpt-4-turbo","provider_resource_id":"gpt-4-turbo","provider_id":"openai","type":"model","metadata":{},"model_type":"llm"}]}
</pre>

<p><div class="rs-tip-major">Poznámka: v&nbsp;dalších příkladech budeme kromě
nástroje <strong>curl</strong> využívat i nástroj <strong>jq</strong>, kterým
si necháme naformátovat výsledný JSON, který je serverem posílán jako odpověď.
Tímto nástrojem jsme se již zabývali v&nbsp;článku <a
href="https://www.root.cz/clanky/zpracovani-dat-reprezentovanych-ve-formatu-json-nastrojem-jq/">Zpracování
dat reprezentovaných ve formátu JSON nástrojem jq </a>.</div></p>

<p>Podrobnější informace modelech, ale v&nbsp;čitelné podobně:</p>

<pre>
$ <strong>curl localhost:8321/v1/models | jq .</strong>
&nbsp;
{
  "data": [
    {
      "identifier": "gpt-4-turbo",
      "provider_resource_id": "gpt-4-turbo",
      "provider_id": "openai",
      "type": "model",
      "metadata": {},
      "model_type": "llm"
    }
  ]
}
</pre>

<p>Informace o OpenAI modelech. My máme nakonfigurován pouze model
<i>gpt-4-turbo</i>, takže se vrátí pole s&nbsp;jediným prvkem:</p>

<pre>
$ <strong>curl localhost:8321/v1/openai/v1/models | jq .</strong>
&nbsp;
{
  "data": [
    {
      "id": "gpt-4-turbo",
      "object": "model",
      "created": 1751115324,
      "owned_by": "llama_stack"
    }
  ]
}
</pre>

<p>Získání informace o verzi samotného Llama Stacku:</p>

<pre>
$ <strong>curl localhost:8321/v1/version | jq .</strong>
&nbsp;
{
  "version": "0.2.8"
}
</pre>

<p>Popř.&nbsp;se selektorem:</p>

<pre>
$ <strong>curl localhost:8321/v1/version | jq .version</strong>
&nbsp;
"0.2.8"
</pre>

<p>V&nbsp;posledním příkladu zjistíme, zda je vše v&nbsp;pořádku:</p>

<pre>
$ <strong>curl localhost:8321/v1/health | jq .</strong>
&nbsp;
{
  "status": "OK"
}
</pre>



<p><a name="k09"></a></p>
<h2 id="k09">9. Přístup ke <i>stacku</i> z&nbsp;aplikací psaných v&nbsp;Pythonu</h2>

<p>Teoreticky je možné spustit <i>stack</i> způsobem naznačeným výše a
přistupovat k&nbsp;němu přes REST API. Ovšem namísto přímé komunikace přes REST
API (a formát JSON) existuje ještě lepší alternativa. Ta je založena na
takzvaném Llama Stack Clientovi. Jedná se o knihovnu dostupnou pro <a
href="https://github.com/meta-llama/llama-stack-client-python">Python</a>, <a
href="https://github.com/meta-llama/llama-stack-client-swift/tree/latest-release">Swift</a>,
<a href="https://github.com/meta-llama/llama-stack-client-node">Node.js</a>
nebo <a
href="https://github.com/meta-llama/llama-stack-client-kotlin/tree/latest-release">Kotlin</a>
(mimochodem: zajímavý výběr jazyků), která REST API stacku vhodným způsobem
&bdquo;obaluje&ldquo;, což je pro mnoho aplikací jednodušší.
V&nbsp;navazujících kapitolách si ukážeme, jak může takový klient vypadat.
Uvidíme, že se nejedná o nijak komplikovanou technologii, spíš naopak.</p>

<p>Všechny ukázky budou naprogramovány v&nbsp;Pythonu, ovšem jejich přepis do
ostatních zmíněných jazyků není příliš komplikovaný, protože jména tříd a
jejich metod jsou podobná či dokonce stejná.</p>



<p><a name="k10"></a></p>
<h2 id="k10">10. Jednoduchý klient komunikující se stackem</h2>

<p>Podívejme se nyní na způsob realizace velmi jednoduchého klienta
naprogramovaného v&nbsp;Pythonu. Nejprve musíme pochopitelně naimportovat
modul, který zprostředkovává spojení se stackem. Tento modul se jmenuje
<strong>LlamaStackClient</strong>:</p>

<pre>
from llama_stack_client import LlamaStackClient
</pre>

<p>Dále se pokusíme o připojení k&nbsp;běžícímu <i>stacku</i>. Víme, že je
dostupný na portu 8321, takže spojení realizujeme snadno:</p>

<pre>
client = LlamaStackClient(base_url="http://localhost:8321")
</pre>

<p>V&nbsp;této chvíli je již možné se <i>stackem</i> komunikovat a například
zjistit verzi frameworku <i>Llama Stack</i>:</p>

<pre>
print(f"Using Llama Stack version {client._version}")
</pre>

<p>Ovšem užitečnější je navázání (nepřímé) komunikace se zvoleným LLM modelem,
poslání otázky do modelu a přečtení jeho odpovědi. V&nbsp;následujícím kódu se
do LLM pošle tento text:</p>

<pre>
PROMPT = "Say Hello"
</pre>

<p>V&nbsp;kódu vybereme první (a jediný nakonfigurovaný) model a pošleme
dotaz:</p>

<pre>
response = client.inference.chat_completion(
    messages=[{"role": "user", "content": PROMPT}],
    model_id=client.models.list()[0].identifier,
)
</pre>

<p>Odpověď je uložena v&nbsp;proměnné <strong>response</strong>. Kromě metadat
zde nalezneme i vlastní odpověď LLM modelu, kterou můžeme vypsat či jinak
zpracovat:</p>

<pre>
text = response.completion_message.content
print(f"LLM response: {text}")
</pre>

<p><div class="rs-tip-major">Poznámka: jedná se o tu nejjednodušší možnou formu
komunikace s&nbsp;LLM, protože nemáme nakonfigurovanou ani historii zpráv, ani
RAG databázi. Složitější příklady si uvedeme příště.</div></p>



<p><a name="k11"></a></p>
<h2 id="k11">11. Úplný zdrojový kód klienta</h2>

<p>Úplný zdrojový kód klienta popsaného <a href="#k10">v&nbsp;předchozí
kapitole</a> vypadá následovně:</p>

<pre>
from llama_stack_client import LlamaStackClient
&nbsp;
PROMPT = "Say Hello"
&nbsp;
client = LlamaStackClient(base_url="http://localhost:8321")
&nbsp;
print(f"Using Llama Stack version {client._version}")
&nbsp;
response = client.inference.chat_completion(
    messages=[{"role": "user", "content": PROMPT}],
    model_id=client.models.list()[0].identifier,
)
&nbsp;
text = response.completion_message.content
print(f"LLM response: {text}")
</pre>

<p><div class="rs-tip-major">Poznámka: povšimněte si, že se skutečně jedná o
pouhých několik řádků zdrojového kódu, protože veškerá konfigurace, definice
připojení k&nbsp;LLM atd.&nbsp;je řešena Llama Stackem. Klient pouze komunikuje
s&nbsp;již nakonfigurovaným stackem.</div></p>



<p><a name="k12"></a></p>
<h2 id="k12">12. Spuštění klienta</h2>

<p>Předpokládejme, že náš <i>stack</i> již běží a je dostupný na portu 8321.
V&nbsp;této situaci můžeme spustit klienta, který bude s&nbsp;tímto
<i>stackem</i> komunikovat &ndash; pošle otázku do LLM a zobrazí obdrženou
odpověď. Klienta budeme muset spustit ve virtuálním prostředí Pythonu;
konkrétně použijeme (opět) správce projektů PDM:</p>

<pre>
$ <strong>pdm run client1.py</strong>
</pre>

<p>Na terminálu by se měly postupně objevit čtyři informace:</p>

<ol>
<li>Verze Llama stacku</li>
<li>Logovací informace o tom, že se přes REST API zjistil seznam dostupných modelů</li>
<li>Logovací informace o tom, že se prvnímu vybranému modelu poslal dotaz</li>
<li>Odpověď LLM serveru</li>
</ol>

<p>Konkrétně bude terminál obsahovat tyto čtyři řádky:</p>

<pre>
Using Llama Stack version 0.2.10
INFO:httpx:HTTP Request: GET http://localhost:8321/v1/models "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://localhost:8321/v1/inference/chat-completion "HTTP/1.1 200 OK"
LLM response: Hello! How can I assist you today?
</pre>

<p>Na straně <i>stacku</i> se taktéž objeví množství logovacích informací:</p>

<pre>
INFO     2025-06-06 10:52:24,555 LiteLLM:3043 uncategorized:                                                            
         LiteLLM completion() model= gpt-4-turbo; provider = openai                                                     
INFO     2025-06-06 10:52:26,027 LiteLLM:1215 uncategorized: Wrapper: Completed Call, calling success_handler           
INFO     2025-06-06 10:52:26,030 LiteLLM:655 uncategorized: selected model name for cost calculation:                   
         openai/gpt-4-turbo-2024-04-09                                                                                  
INFO     2025-06-06 10:52:26,033 LiteLLM:655 uncategorized: selected model name for cost calculation:                   
         openai/gpt-4-turbo-2024-04-09                                                                                  
INFO     2025-06-06 10:52:26,037 LiteLLM:655 uncategorized: selected model name for cost calculation:                   
         openai/gpt-4-turbo-2024-04-09                                                                                  
INFO:     ::1:57018 - "POST /v1/inference/chat-completion HTTP/1.1" 200 OK
[2m08:52:26.046[0m [35m[END][0m [2m/v1/inference/chat-completion[0m[0m [StatusCode.OK][0m (1501.47ms)
INFO     2025-06-06 10:52:40,419 llama_stack.distribution.server.server:158 server: Shutting down                       
INFO     2025-06-06 10:52:40,422 llama_stack.distribution.server.server:142 server: Shutting down TelemetryAdapter      
INFO     2025-06-06 10:52:40,423 llama_stack.distribution.server.server:142 server: Shutting down ModelsRoutingTable    
INFO     2025-06-06 10:52:40,425 llama_stack.distribution.server.server:142 server: Shutting down InferenceRouter       
INFO     2025-06-06 10:52:40,427 llama_stack.distribution.server.server:142 server: Shutting down                       
         DistributionInspectImpl                                                                                        
INFO     2025-06-06 10:52:40,428 llama_stack.distribution.server.server:142 server: Shutting down ProviderImpl          
</pre>

<p><div class="rs-tip-major">Poznámka: pochopitelně je možné úroveň logovacích
informací změnit a například namísto úrovně <i>INFO</i> nastavit <i>WARNING</i>
nebo <i>ERROR</i>. Ovšem při vývoji se mi osvědčilo ponechat právě úroveň
<i>INFO</i> nebo dokonce i ještě podrobnější <i>DEBUG</i> (výstup je potom
ještě mnohem více ukecaný).</div></p>



<p><a name="k13"></a></p>
<h2 id="k13">13. Klient zjišťující, které LLM modely je možné využít</h2>

<p>Druhý klient, který komunikuje se <i>stackem</i>, zjišťuje všechny LLM
modely, které je možné využít pro <i>inferenci</i> (zjednodušeně řečeno: dotazy
a odpovědi). Ve skutečnosti se vypíšou všechny dostupné modely, nejenom LLM</p>

<pre>
from llama_stack_client import LlamaStackClient
&nbsp;
client = LlamaStackClient(base_url="http://localhost:8321")
&nbsp;
print(f"Using Llama Stack version {client._version}")
&nbsp;
models = client.models.list()
&nbsp;
for model in models:
    print(model)
</pre>

<p>Výsledek by měl vypadat zhruba následovně (pouze jsem ho naformátoval pro
větší čitelnost):</p>

<pre>
Model(
    identifier='gpt-4-turbo',
    metadata={},
    api_model_type='llm',
    provider_id='openai',
    type='model',
    provider_resource_id='gpt-4-turbo',
    model_type='llm'
)
</pre>

<p><div class="rs-tip-major">Poznámka: povšimněte si, že je nastaven i atribut
<strong>model_type</strong>, podle kterého je možné určit typ modelu (prozatím
nás zajímají jen klasické LLM).</div></p>



<p><a name="k14"></a></p>
<h2 id="k14">14. Použití Llama Stacku v&nbsp;roli Pythonní knihovny (bez serveru)</h2>

<p>V&nbsp;předchozích kapitolách jsme si ukázali, jakým způsobem je možné
komunikovat se stackem, který běží v&nbsp;samostatném procesu. Ovšem existují
situace, v&nbsp;nichž není vhodné spouštět dva procesory (jeden se stackem,
druhý s&nbsp;klientem). V&nbsp;těchto případech je možné stack spustit přímo
v&nbsp;rámci klientské aplikace. Namísto třídy
<strong>LlamaStackClient</strong> se v&nbsp;tomto případě musí použít třída
<strong>LlamaStackAsLibraryClient</strong> a vlastní inicializace je tedy
pochopitelně odlišná &ndash; nemusí se zadávat host a port, ale naopak cesta ke
konfiguračnímu souboru:</p>

<pre>
class LlamaStackAsLibraryClient(llama_stack_client.LlamaStackClient)
 |  LlamaStackAsLibraryClient(config_path_or_template_name: str, skip_logger_removal: bool = False, custom_provider_registry: dict[llama_stack.apis.datatypes.Api, dict[str, llama_stack.providers.datatypes.ProviderSpec]] | None = None, provider_data: dict[str, typing.Any] | None = None)
 |
 |  Method resolution order:
 |      LlamaStackAsLibraryClient
 |      llama_stack_client.LlamaStackClient
 |      llama_stack_client._base_client.SyncAPIClient
 |      llama_stack_client._base_client.BaseClient
 |      typing.Generic
 |      builtins.object
</pre>

<p>Ukázka bude uvedena v&nbsp;navazující kapitole.</p>



<p><a name="k15"></a></p>
<h2 id="k15">15. Úplný zdrojový kód třetí verze klienta</h2>

<p>Realizace klienta, který pro svůj běh <i>nebude</i> potřebovat samostatně
běžící <i>stack</i>, ale naopak vše bude realizovat v&nbsp;rámci jediného
procesu, vypadá následovně. Povšimněte si, že namísto připojení ke
<i>stacku</i> musíme pouze zkonstruovat instanci třídy
<strong>LlamaStackAsLibraryClient</strong> s&nbsp;předáním konfiguračního
souboru a následně pouze provést inicializaci. Další práce se stackem je již
totožná s&nbsp;konfigurací, kdy <i>stack</i> běžel jako samostatný server:</p>

<pre>
from llama_stack.distribution.library_client import LlamaStackAsLibraryClient
from llama_stack_client import LlamaStackClient
&nbsp;
client = LlamaStackAsLibraryClient("run.yaml")
client.initialize()
&nbsp;
print(f"Using Llama Stack version {client._version}")
&nbsp;
models = client.models.list()
&nbsp;
for model in models:
    print(model)
</pre>



<p><a name="k16"></a></p>
<h2 id="k16">16. Spuštění třetí verze klienta</h2>

<p>Klienta, jehož zdrojový kód byl ukázán v&nbsp;předchozí kapitole, spustíme
příkazem:</p>

<pre>
$ <strong>pdm run client3.py</strong>
</pre>

<p>Nejprve by se měla vypsat konfigurace stacku, která byla načtena
z&nbsp;konfiguračního souboru:</p>

<pre>
Using config run.yaml:
apis:
- inference
- telemetry
benchmarks: []
container_image: null
datasets: []
external_providers_dir: null
image_name: simplest-llama-stack-configuration
inference_store: null
logging: null
metadata_store: null
models:
- metadata: {}
  model_id: gpt-4-turbo
  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType
  - llm
  provider_id: openai
  provider_model_id: gpt-4-turbo
providers:
  inference:
  - config:
      api_key: '********'
    provider_id: openai
    provider_type: remote::openai
  telemetry:
  - config:
      sinks:
      - console
    provider_id: meta-reference
    provider_type: inline::meta-reference
scoring_fns: []
server:
  auth: null
  host: null
  port: 8321
  quota: null
  tls_cafile: null
  tls_certfile: null
  tls_keyfile: null
shields: []
tool_groups: []
vector_dbs: []
version: '2'
</pre>

<p>Následně se provede kód klienta, tedy výpis verze Llama Stacku a seznam
dostupných modelů:</p>

<pre>
Using Llama Stack version 0.2.10
Model(identifier='gpt-4-turbo', metadata={}, api_model_type='llm', provider_id='openai', type='model', provider_resource_id='gpt-4-turbo', model_type='llm')
</pre>

<p><div class="rs-tip-major">Poznámka: naprosto stejným postupem můžeme
zajistit volání LLM atd.</div></p>



<p><a name="k17"></a></p>
<h2 id="k17">17. Další možnosti konfigurace <i>stacku</i></h2>

<p>Konfigurace <i>stacku</i> může být i poměrně komplikovaná.
V&nbsp;navazujícím článku si například popíšeme následující konfiguraci, která
je mj.&nbsp;použitá v&nbsp;reálně nasaditelné službě:</p>

<pre>
version: '2'
image_name: simplest-llamastack-app
&nbsp;
apis:
  - agents
  - datasetio
  - eval
  - inference
  - post_training
  - safety
  - scoring
  - telemetry
  - tool_runtime
  - vector_io
benchmarks: []
container_image: null
datasets: []
external_providers_dir: null
inference_store:
  db_path: /home/ptisnovs/.llama/distributions/ollama/inference_store.db
  type: sqlite
logging: null
metadata_store:
  db_path: /home/ptisnovs/.llama/distributions/ollama/registry.db
  namespace: null
  type: sqlite
providers:
  agents:
  - config:
      persistence_store:
        db_path: /home/ptisnovs/.llama/distributions/ollama/agents_store.db
        namespace: null
        type: sqlite
      responses_store:
        db_path: /home/ptisnovs/.llama/distributions/ollama/responses_store.db
        type: sqlite
    provider_id: meta-reference
    provider_type: inline::meta-reference
  datasetio:
  - config:
      kvstore:
        db_path: /home/ptisnovs/.llama/distributions/ollama/huggingface_datasetio.db
        namespace: null
        type: sqlite
    provider_id: huggingface
    provider_type: remote::huggingface
  - config:
      kvstore:
        db_path: /home/ptisnovs/.llama/distributions/ollama/localfs_datasetio.db
        namespace: null
        type: sqlite
    provider_id: localfs
    provider_type: inline::localfs
  eval:
  - config:
      kvstore:
        db_path: /home/ptisnovs/.llama/distributions/ollama/meta_reference_eval.db
        namespace: null
        type: sqlite
    provider_id: meta-reference
    provider_type: inline::meta-reference
  inference:
    - provider_id: openai
      provider_type: remote::openai
      config:
        api_key: ${env.OPENAI_API_KEY}
  post_training:
  - config:
      checkpoint_format: huggingface
      device: cpu
      distributed_backend: null
    provider_id: huggingface
    provider_type: inline::huggingface
  safety:
  - config:
      excluded_categories: []
    provider_id: llama-guard
    provider_type: inline::llama-guard
  scoring:
  - config: {}
    provider_id: basic
    provider_type: inline::basic
  - config: {}
    provider_id: llm-as-judge
    provider_type: inline::llm-as-judge
  - config:
      openai_api_key: '********'
    provider_id: braintrust
    provider_type: inline::braintrust
  telemetry:
</pre>



<p><a name="k18"></a></p>
<h2 id="k18">18. Příloha: seznam poskytovatelů (<i>providers</i>)</h2>

<p>V&nbsp;této příloze je uveden seznam oficiálně podporovaných poskytovatelů
(<i>providers</i>):</p>

<table>
<tr><th>Typ</th><th>Jméno poskytovatele</th></tr>
<tr><td>API Type     </td><td>Provider Type                  </td></tr>
<tr><td>agents       </td><td>inline::meta-reference         </td></tr>
<tr><td>datasetio    </td><td>inline::localfs                </td></tr>
<tr><td>datasetio    </td><td>remote::huggingface            </td></tr>
<tr><td>datasetio    </td><td>remote::nvidia                 </td></tr>
<tr><td>eval         </td><td>inline::meta-reference         </td></tr>
<tr><td>eval         </td><td>remote::nvidia                 </td></tr>
<tr><td>files        </td><td>inline::localfs                </td></tr>
<tr><td>inference    </td><td>inline::meta-reference         </td></tr>
<tr><td>inference    </td><td>inline::sentence-transformers  </td></tr>
<tr><td>inference    </td><td>inline::vllm                   </td></tr>
<tr><td>inference    </td><td>remote::anthropic              </td></tr>
<tr><td>inference    </td><td>remote::bedrock                </td></tr>
<tr><td>inference    </td><td>remote::cerebras               </td></tr>
<tr><td>inference    </td><td>remote::cerebras-openai-compat </td></tr>
<tr><td>inference    </td><td>remote::databricks             </td></tr>
<tr><td>inference    </td><td>remote::fireworks              </td></tr>
<tr><td>inference    </td><td>remote::fireworks-openai-compat</td></tr>
<tr><td>inference    </td><td>remote::gemini                 </td></tr>
<tr><td>inference    </td><td>remote::groq                   </td></tr>
<tr><td>inference    </td><td>remote::groq-openai-compat     </td></tr>
<tr><td>inference    </td><td>remote::hf::endpoint           </td></tr>
<tr><td>inference    </td><td>remote::hf::serverless         </td></tr>
<tr><td>inference    </td><td>remote::llama-openai-compat    </td></tr>
<tr><td>inference    </td><td>remote::nvidia                 </td></tr>
<tr><td>inference    </td><td>remote::ollama                 </td></tr>
<tr><td>inference    </td><td>remote::openai                 </td></tr>
<tr><td>inference    </td><td>remote::passthrough            </td></tr>
<tr><td>inference    </td><td>remote::runpod                 </td></tr>
<tr><td>inference    </td><td>remote::sambanova              </td></tr>
<tr><td>inference    </td><td>remote::sambanova-openai-compat</td></tr>
<tr><td>inference    </td><td>remote::tgi                    </td></tr>
<tr><td>inference    </td><td>remote::together               </td></tr>
<tr><td>inference    </td><td>remote::together-openai-compat </td></tr>
<tr><td>inference    </td><td>remote::vllm                   </td></tr>
<tr><td>inference    </td><td>remote::watsonx                </td></tr>
<tr><td>post_training</td><td>inline::huggingface            </td></tr>
<tr><td>post_training</td><td>inline::torchtune              </td></tr>
<tr><td>post_training</td><td>remote::nvidia                 </td></tr>
<tr><td>safety       </td><td>inline::code-scanner           </td></tr>
<tr><td>safety       </td><td>inline::llama-guard            </td></tr>
<tr><td>safety       </td><td>inline::prompt-guard           </td></tr>
<tr><td>safety       </td><td>remote::bedrock                </td></tr>
<tr><td>safety       </td><td>remote::nvidia                 </td></tr>
<tr><td>safety       </td><td>remote::sambanova              </td></tr>
<tr><td>scoring      </td><td>inline::basic                  </td></tr>
<tr><td>scoring      </td><td>inline::braintrust             </td></tr>
<tr><td>scoring      </td><td>inline::llm-as-judge           </td></tr>
<tr><td>telemetry    </td><td>inline::meta-reference         </td></tr>
<tr><td>tool_runtime </td><td>inline::rag-runtime            </td></tr>
<tr><td>tool_runtime </td><td>remote::bing-search            </td></tr>
<tr><td>tool_runtime </td><td>remote::brave-search           </td></tr>
<tr><td>tool_runtime </td><td>remote::model-context-protocol </td></tr>
<tr><td>tool_runtime </td><td>remote::tavily-search          </td></tr>
<tr><td>tool_runtime </td><td>remote::wolfram-alpha          </td></tr>
<tr><td>vector_io    </td><td>inline::chromadb               </td></tr>
<tr><td>vector_io    </td><td>inline::faiss                  </td></tr>
<tr><td>vector_io    </td><td>inline::meta-reference         </td></tr>
<tr><td>vector_io    </td><td>inline::milvus                 </td></tr>
<tr><td>vector_io    </td><td>inline::qdrant                 </td></tr>
<tr><td>vector_io    </td><td>inline::sqlite-vec             </td></tr>
<tr><td>vector_io    </td><td>inline::sqlite_vec             </td></tr>
<tr><td>vector_io    </td><td>remote::chromadb               </td></tr>
<tr><td>vector_io    </td><td>remote::milvus                 </td></tr>
<tr><td>vector_io    </td><td>remote::pgvector               </td></tr>
<tr><td>vector_io    </td><td>remote::qdrant                 </td></tr>
<tr><td>vector_io    </td><td>remote::weaviate               </td></tr>
</table>



<p><a name="k19"></a></p>
<h2 id="k19">19. Repositář s&nbsp;demonstračními příklady</h2>

<p>Projekty popsané v&nbsp;předchozích kapitolách je možné nalézt
v&nbsp;repositáři <a
href="https://github.com/tisnik/most-popular-python-libs">https://github.com/tisnik/most-popular-python-libs</a>.
Následují odkazy na jednotlivé soubory s&nbsp;jejich stručným popisem:</p>

<h3>První projekt: volání LLM modelu přes stack</h3>

<table>
<tr><th>#<th>Příklad</th><th>Stručný popis</th><th>Adresa příkladu</th></tr></i>
<tr><td>1</td><td>demo1/pyproject.toml</td><td>konfigurační soubor s&nbsp;definicí projektu</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo1/pyproject.toml">https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo1/pyproject.toml</a></td></tr>
<tr><td>2</td><td>demo1/pdm.lock</td><td>soubor se seznamem a haši nainstalovaných balíčků</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo1/pdm.lock">https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo1/pdm.lock</a></td></tr>
<tr><td>3</td><td>demo1/run.yaml</td><td>konfigurace našeho <i>stacku</i></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo1/run.yaml">https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo1/run.yaml</a></td></tr>
<tr><td>4</td><td>demo1/client1.py</td><td>skript v&nbsp;Pythonu s&nbsp;realizovaným klientem</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo1/client1.py">https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo1/client1.py</a></td></tr>
</table>

<h3>Druhý projekt: výpis všech dostupných modelů</h3>

<table>
<tr><th>#<th>Příklad</th><th>Stručný popis</th><th>Adresa příkladu</th></tr></i>
<tr><td>1</td><td>demo2/pyproject.toml</td><td>konfigurační soubor s&nbsp;definicí projektu</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo2/pyproject.toml">https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo2/pyproject.toml</a></td></tr>
<tr><td>2</td><td>demo2/pdm.lock</td><td>soubor se seznamem a haši nainstalovaných balíčků</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo2/pdm.lock">https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo2/pdm.lock</a></td></tr>
<tr><td>3</td><td>demo2/run.yaml</td><td>konfigurace našeho <i>stacku</i></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo2/run.yaml">https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo2/run.yaml</a></td></tr>
<tr><td>4</td><td>demo2/client2.py</td><td>skript v&nbsp;Pythonu s&nbsp;realizovaným klientem</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo2/client2.py">https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo2/client2.py</a></td></tr>
</table>

<h3>Třetí projekt: využití Llama Stacku jako Pythonní knihovny</h3>

<table>
<tr><th>#<th>Příklad</th><th>Stručný popis</th><th>Adresa příkladu</th></tr></i>
<tr><td>1</td><td>demo3/pyproject.toml</td><td>konfigurační soubor s&nbsp;definicí projektu</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo3/pyproject.toml">https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo3/pyproject.toml</a></td></tr>
<tr><td>2</td><td>demo3/pdm.lock</td><td>soubor se seznamem a haši nainstalovaných balíčků</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo3/pdm.lock">https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo3/pdm.lock</a></td></tr>
<tr><td>3</td><td>demo3/run.yaml</td><td>konfigurace našeho <i>stacku</i></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo3/run.yaml">https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo3/run.yaml</a></td></tr>
<tr><td>4</td><td>demo3/client3.py</td><td>skript v&nbsp;Pythonu s&nbsp;realizovaným klientem</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo3/client3.py">https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo3/client3.py</a></td></tr>
</table>



<p><a name="k20"></a></p>
<h2 id="k20">20. Odkazy na Internetu</h2>

<ol>

<li>llama-stack na GitHubu<br />
<a href="https://github.com/meta-llama/llama-stack">https://github.com/meta-llama/llama-stack</a>
</li>

<li>llama-stack na PyPi<br />
<a href="https://pypi.org/project/llama-stack/">https://pypi.org/project/llama-stack/</a>
</li>

<li>Configuring a "Stack"<br />
<a href="https://llama-stack.readthedocs.io/en/latest/distributions/configuration.html#">https://llama-stack.readthedocs.io/en/latest/distributions/configuration.html#</a>
</li>

<li>Generativní umělá inteligence<br />
<a href="https://cs.wikipedia.org/wiki/Generativn%C3%AD_um%C4%9Bl%C3%A1_inteligence">https://cs.wikipedia.org/wiki/Generativn%C3%AD_um%C4%9Bl%C3%A1_inteligence</a>
</li>

<li>Generative artificial intelligence<br />
<a href="https://en.wikipedia.org/wiki/Generative_artificial_intelligence">https://en.wikipedia.org/wiki/Generative_artificial_intelligence</a>
</li>

<li>Generativní AI a LLM: (R)evoluce v umělé inteligenci?<br />
<a href="https://www.itbiz.cz/clanky/generativni-ai-a-llm-revoluce-v-umele-inteligenci/">https://www.itbiz.cz/clanky/generativni-ai-a-llm-revoluce-v-umele-inteligenci/</a>
</li>

<li>langchain<br />
<a href="https://python.langchain.com/docs/introduction/">https://python.langchain.com/docs/introduction/</a>
</li>

<li>langgraph<br />
<a href="https://github.com/langchain-ai/langgraph">https://github.com/langchain-ai/langgraph</a>
</li>

<li>autogen<br />
<a href="https://github.com/microsoft/autogen">https://github.com/microsoft/autogen</a>
</li>

<li>metaGPT<br />
<a href="https://github.com/geekan/MetaGPT">https://github.com/geekan/MetaGPT</a>
</li>

<li>phidata<br />
<a href="https://github.com/phidatahq/phidata">https://github.com/phidatahq/phidata</a>
</li>

<li>CrewAI<br />
<a href="https://github.com/crewAIInc/crewAI">https://github.com/crewAIInc/crewAI</a>
</li>

<li>pydanticAI<br />
<a href="https://github.com/pydantic/pydantic-ai">https://github.com/pydantic/pydantic-ai</a>
</li>

<li>controlflow<br />
<a href="https://github.com/PrefectHQ/ControlFlow">https://github.com/PrefectHQ/ControlFlow</a>
</li>

<li>langflow<br />
<a href="https://github.com/langflow-ai/langflow">https://github.com/langflow-ai/langflow</a>
</li>

<li>LiteLLM<br />
<a href="https://github.com/BerriAI/litellm">https://github.com/BerriAI/litellm</a>
</li>

<li>Llama Stack<br />
<a href="https://github.com/meta-llama/llama-stack">https://github.com/meta-llama/llama-stack</a>
</li>

<li>uv<br />
<a href="https://docs.astral.sh/uv/">https://docs.astral.sh/uv/</a>
</li>

<li>Python na Root.cz<br />
<a href="https://www.root.cz/n/python/">https://www.root.cz/n/python/</a>
</li>

<li>PDM: moderní správce balíčků a virtuálních prostředí Pythonu<br />
<a href="https://www.root.cz/clanky/pdm-moderni-spravce-balicku-a-virtualnich-prostredi-pythonu/">https://www.root.cz/clanky/pdm-moderni-spravce-balicku-a-virtualnich-prostredi-pythonu/</a>
</li>

<li>YAML<br />
<a href="https://en.wikipedia.org/wiki/YAML">https://en.wikipedia.org/wiki/YAML</a>
</li>

<li>Top 11 LLM API Providers in 2025<br />
<a href="https://www.helicone.ai/blog/llm-api-providers">https://www.helicone.ai/blog/llm-api-providers</a>
</li>

<li>Zpracování dat reprezentovaných ve formátu JSON nástrojem jq<br />
<a href="https://www.root.cz/clanky/zpracovani-dat-reprezentovanych-ve-formatu-json-nastrojem-jq/">https://www.root.cz/clanky/zpracovani-dat-reprezentovanych-ve-formatu-json-nastrojem-jq/</a>
</li>

<li>LLama Stack SDK pro jazyk Python<br />
<a href="https://github.com/meta-llama/llama-stack-client-python">https://github.com/meta-llama/llama-stack-client-python</a>
</li>

<li>LLama Stack SDK pro jazyk Swift<br />
<a href="https://github.com/meta-llama/llama-stack-client-swift/tree/latest-release">https://github.com/meta-llama/llama-stack-client-swift/tree/latest-release</a>
</li>

<li>LLama Stack SDK pro ekosystém Node.js<br />
<a href="https://github.com/meta-llama/llama-stack-client-node">https://github.com/meta-llama/llama-stack-client-node</a>
</li>

<li>LLama Stack SDK pro jazyk Kotlin<br />
<a href="https://github.com/meta-llama/llama-stack-client-kotlin/tree/latest-release">https://github.com/meta-llama/llama-stack-client-kotlin/tree/latest-release</a>
</li>

</ol>



<p></p><p></p>
<p><small>Autor: <a href="https://github.com/tisnik/">Pavel Tišnovský</a> &nbsp; 2025</small></p>
</body>
</html>

