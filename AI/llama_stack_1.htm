<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
        "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html>
<head>
<title></title>
<meta name="Author" content="Pavel Tisnovsky" />
<meta name="Generator" content="vim" />
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<style type="text/css">
         body {color:#000000; background:#ffffff;}
         h1  {font-family: arial, helvetica, sans-serif; color:#ffffff; background-color:#c00000; text-align:center; padding-left:1em}
         h2  {font-family: arial, helvetica, sans-serif; color:#ffffff; background-color:#0000c0; padding-left:1em; text-align:left}
         h3  {font-family: arial, helvetica, sans-serif; color:#000000; background-color:#c0c0c0; padding-left:1em; text-align:left}
         h4  {font-family: arial, helvetica, sans-serif; color:#000000; background-color:#e0e0e0; padding-left:1em; text-align:left}
         a   {font-family: arial, helvetica, sans-serif;}
         li  {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify; width:450px;}
         ol  {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify; width:450px;}
         ul  {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify; width:450px;}
         p   {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify;}
         pre {background:#e0e0e0}
</style>
</head>

<body>

<h1></h1>

<h3>Pavel Ti≈°novsk√Ω</h3>

<p></p>

<h1>√övodn√≠k</h1>

<p></p>



<h2>Obsah</h2>

<p><a href="#k01">*** 1. </a></p>
<p><a href="#k02">*** 2. </a></p>
<p><a href="#k03">*** 3. P≈ô√≠prava projektu pro spu≈°tƒõn√≠ a otestov√°n√≠ Llama Stacku</a></p>
<p><a href="#k04">*** 4. P≈ôid√°n√≠ v≈°ech pot≈ôebn√Ωch z√°vislost√≠ do projektu</a></p>
<p><a href="#k05">*** 5. Kontrola, zda instalace Llama stacku dopadla √∫spƒõ≈°nƒõ</a></p>
<p><a href="#k06">*** 6. Nejjednodu≈°≈°√≠ funkƒçn√≠ konfigurace Llama stacku</a></p>
<p><a href="#k07">*** 7. </a></p>
<p><a href="#k08">*** 8. </a></p>
<p><a href="#k09">*** 9. </a></p>
<p><a href="#k10">*** 10. </a></p>
<p><a href="#k11">*** 11. </a></p>
<p><a href="#k12">*** 12. </a></p>
<p><a href="#k13">*** 13. </a></p>
<p><a href="#k14">*** 14. </a></p>
<p><a href="#k15">*** 15. </a></p>
<p><a href="#k16">*** 16. </a></p>
<p><a href="#k17">*** 17. </a></p>
<p><a href="#k18">*** 18. </a></p>
<p><a href="#k19">*** 19. Reposit√°≈ô s&nbsp;demonstraƒçn√≠mi p≈ô√≠klady</a></p>
<p><a href="#k20">*** 20. Odkazy na Internetu</a></p>



<p><a name="k01"></a></p>
<h2 id="k01">1. </h2>



<p><a name="k02"></a></p>
<h2 id="k02">2. </h2>



<p><a name="k03"></a></p>
<h2 id="k03">3. P≈ô√≠prava projektu pro spu≈°tƒõn√≠ a otestov√°n√≠ Llama Stacku</h2>

<p>V&nbsp;prvn√≠m kroku si vytvo≈ô√≠me kostru projektu, kter√Ω postupnƒõ uprav√≠me do
fin√°ln√≠ podoby. Pro spr√°vu projekt≈Ø vyu≈æijeme n√°stroj <i>PDM</i>, kter√Ω byl
pops√°n v&nbsp;ƒçl√°nku <a
href="https://www.root.cz/clanky/pdm-moderni-spravce-balicku-a-virtualnich-prostredi-pythonu/">PDM:
modern√≠ spr√°vce bal√≠ƒçk≈Ø a virtu√°ln√≠ch prost≈ôed√≠ Pythonu</a> (ov≈°em stejnƒõ dob≈ôe
m≈Ø≈æete pou≈æ√≠t i klasick√Ω <strong>pip</strong> zkombinovan√Ω
s&nbsp;<strong>venv</strong> nebo <strong>virtualenv</strong>). Kostra nov√©ho
projektu se vytvo≈ô√≠ p≈ô√≠kazem:</p>

<pre>
$ <strong>pdm init</strong>
</pre>

<p>Po zad√°n√≠ tohoto p≈ô√≠kazu se syst√©m PDM zept√° na nƒõkolik ot√°zek. Odpovƒõdi
jsou zv√Ωraznƒõny tuƒçn√Ωm p√≠smem:</p>

<pre>
Creating a pyproject.toml for PDM...
Please enter the Python interpreter to use
 0. cpython@3.12 (/usr/bin/python)
 1. cpython@3.13 (/usr/bin/python3.13)
 2. cpython@3.12 (/usr/bin/python3.12)
 3. cpython@3.11 (/usr/bin/python3.11)
Please select (0): <strong>0</strong>
Virtualenv is created successfully at /tmp/ramdisk/llama-stack-demo/.venv
Project name (llama-stack-demo):  <strong>[Enter]</strong>
Project version (0.1.0):  <strong>[Enter]</strong>
Do you want to build this project for distribution(such as wheel)?
If yes, it will be installed by default when running `pdm install`. [y/n] (n):  <strong>[Enter]</strong>
License(SPDX name) (MIT):  <strong>[Enter]</strong>
Author name (Pavel Tisnovsky):  <strong>[Enter]</strong>
Author email (tisnik@nowhere.com):  <strong>[Enter]</strong>
Python requires('*' to allow any) (==3.12.*):  <strong>[Enter]</strong>
Project is initialized successfully
</pre>

<p><div class="rs-tip-major">Pozn√°mka: nab√≠dka interpretr≈Ø jazyka Python a
nab√≠zen√© jm√©no a e-mail se pochopitelnƒõ budou li≈°it.</div></p>

<p>V√Ωsledkem by mƒõl b√Ωt projektov√Ω soubor nazvan√Ω
<strong>pyproject.toml</strong> s&nbsp;n√°sleduj√≠c√≠m obsahem:</p>

<pre>
[project]
name = "llama-stack-demo"
version = "0.1.0"
description = "Default template for PDM package"
authors = [
    {name = "Pavel Tisnovsky", email = "tisnik@nowhere.com"},
]
dependencies = []
requires-python = "==3.12.*"
readme = "README.md"
license = {text = "MIT"}
&nbsp;
&nbsp;
[tool.pdm]
distribution = false
</pre>



<p><a name="k04"></a></p>
<h2 id="k04">4. P≈ôid√°n√≠ v≈°ech pot≈ôebn√Ωch z√°vislost√≠ do projektu</h2>

<p></p>

<pre>
$ <strong>pdm add llama-stack==0.2.8 fastapi opentelemetry-sdk opentelemetry-exporter-otlp opentelemetry-instrumentation aiosqlite litellm uvicorn blobfile</strong>
</pre>

<pre>
Adding packages to default dependencies: llama-stack==0.2.8, fastapi, opentelemetry-sdk, opentelemetry-exporter-otlp,
opentelemetry-instrumentation, aiosqlite, litellm, uvicorn
  0:01:51 üîí Lock successful.
Changes are written to pyproject.toml.
Synchronizing working set with resolved packages: 87 to add, 0 to update, 0 to remove
&nbsp;
  ‚úî Install setuptools 80.9.0 successful
  ‚úî Install annotated-types 0.7.0 successful
  ‚úî Install distro 1.9.0 successful
  ‚úî Install click 8.2.1 successful
  ‚úî Install h11 0.16.0 successful
  ...
  ...
  ...
  ‚úî Install multidict 6.4.4 successful
  ‚úî Install tokenizers 0.21.1 successful
  ‚úî Install pillow 11.2.1 successful
  ‚úî Install numpy 2.2.6 successful
  ‚úî Install rpds-py 0.25.1 successful
  ‚úî Install yarl 1.20.0 successful
  ‚úî Install regex 2024.11.6 successful
  ‚úî Install grpcio 1.72.1 successful
  ‚úî Install pydantic-core 2.33.2 successful
  ‚úî Install aiohttp 3.12.9 successful
&nbsp;
  0:01:29 üéâ All complete! 87/87
</pre>

<pre>
[project]
name = "llama-stack-demo"
version = "0.1.0"
description = "Default template for PDM package"
authors = [
    {name = "Pavel Tisnovsky", email = "tisnik@nowhere.com"},
]
dependencies = [
    "llama-stack==0.2.8",
    "fastapi&gt;=0.115.12",
    "opentelemetry-sdk&gt;=1.34.0",
    "opentelemetry-exporter-otlp&gt;=1.34.0",
    "opentelemetry-instrumentation&gt;=0.55b0",
    "aiosqlite&gt;=0.21.0",
    "litellm&gt;=1.72.1",
    "uvicorn&gt;=0.34.3",
    "blobfile&gt;=3.0.0"]
requires-python = "==3.12.*"
readme = "README.md"
license = {text = "MIT"}
&nbsp;
&nbsp;
[tool.pdm]
distribution = false
</pre>



<p><a name="k05"></a></p>
<h2 id="k05">5. Kontrola, zda instalace Llama stacku dopadla √∫spƒõ≈°nƒõ</h2>

<pre>
$ <strong>pdm run llama</strong>
</pre>

<pre>
usage: llama [-h] {model,stack,download,verify-download} ...
&nbsp;
Welcome to the Llama CLI
&nbsp;
options:
  -h, --help            show this help message and exit
&nbsp;
subcommands:
  {model,stack,download,verify-download}
&nbsp;
  model                 Work with llama models
  stack                 Operations for the Llama Stack / Distributions
  download              Download a model from llama.meta.com or Hugging Face Hub
  verify-download       Verify integrity of downloaded model files
</pre>



<p><a name="k06"></a></p>
<h2 id="k06">6. Nejjednodu≈°≈°√≠ funkƒçn√≠ konfigurace Llama stacku</h2>

<pre>
version: '2'
image_name: simplest-llama-stack-configuration
container_image: null

distribution_spec:
  local:
    services:
      - inference
      - telemetry

apis:
  - inference
  - telemetry

providers:
  inference:
    - provider_id: openai
      provider_type: remote::openai
      config:
        api_key: ${env.OPENAI_API_KEY}
  telemetry:
    - provider_id: meta-reference
      provider_type: inline::meta-reference
      config:
        sinks: ['console']

models:
  - model_id: gpt-4-turbo
    provider_id: openai
    model_type: llm
    provider_model_id: gpt-4-turbo

server:
  port: 8321
</pre>



<p><a name="k07"></a></p>
<h2 id="k07">7. Spu≈°tƒõn√≠ Llama stacku</h2>

<pre>
$ <strong>pdm run llama stack run run.yaml</strong>
</pre>

<pre>
INFO     2025-06-06 10:51:57,634 llama_stack.cli.stack.run:125 server: Using run configuration: run.yaml                
INFO     2025-06-06 10:51:57,640 llama_stack.cli.stack.run:146 server: No image type or image name provided. Assuming   
         environment packages.                                                                                          
INFO     2025-06-06 10:51:58,282 llama_stack.distribution.server.server:422 server: Using config file: run.yaml         
INFO     2025-06-06 10:51:58,285 llama_stack.distribution.server.server:424 server: Run configuration:                  
INFO     2025-06-06 10:51:58,290 llama_stack.distribution.server.server:426 server: apis:                               
         - inference                                                                                                    
         - telemetry                                                                                                    
         benchmarks: []                                                                                                 
         container_image: null                                                                                          
         datasets: []                                                                                                   
         external_providers_dir: null                                                                                   
         image_name: simplest-llama-stack-configuration                                                                 
         inference_store: null                                                                                          
         logging: null                                                                                                  
         metadata_store: null                                                                                           
         models:                                                                                                        
         - metadata: {}                                                                                                 
           model_id: gpt-4-turbo                                                                                        
           model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType                                   
           - llm                                                                                                        
           provider_id: openai                                                                                          
           provider_model_id: gpt-4-turbo                                                                               
         providers:                                                                                                     
           inference:                                                                                                   
           - config:                                                                                                    
               api_key: '********'                                                                                      
             provider_id: openai                                                                                        
             provider_type: remote::openai                                                                              
           telemetry:                                                                                                   
           - config:                                                                                                    
               sinks:                                                                                                   
               - console                                                                                                
             provider_id: meta-reference                                                                                
             provider_type: inline::meta-reference                                                                      
         scoring_fns: []                                                                                                
         server:                                                                                                        
           auth: null                                                                                                   
           host: null                                                                                                   
           port: 8321                                                                                                   
           quota: null                                                                                                  
           tls_cafile: null                                                                                             
           tls_certfile: null                                                                                           
           tls_keyfile: null                                                                                            
         shields: []                                                                                                    
         tool_groups: []                                                                                                
         vector_dbs: []                                                                                                 
         version: '2'                                                                                                   
                                                                                                                        
INFO     2025-06-06 10:52:00,166 llama_stack.distribution.server.server:564 server: Listening on ['::', '0.0.0.0']:8321 
INFO     2025-06-06 10:52:00,176 llama_stack.distribution.server.server:156 server: Starting up                         
INFO:     ::1:57018 - "GET /v1/models HTTP/1.1" 200 OK
</pre>



<p><a name="k08"></a></p>
<h2 id="k08">8. </h2>

<pre>
from llama_stack_client import LlamaStackClient

PROMPT = "Say Hello"

client = LlamaStackClient(base_url="http://localhost:8321")

response = client.inference.chat_completion(
    messages=[{"role": "user", "content": PROMPT}],
    model_id=client.models.list()[0].identifier,
)

print(response.completion_message.content)
</pre>



<p><a name="k09"></a></p>
<h2 id="k09">9. </h2>

<pre>
$ pdm run client1.py
INFO:httpx:HTTP Request: GET http://localhost:8321/v1/models "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://localhost:8321/v1/inference/chat-completion "HTTP/1.1 200 OK"
Hello! How can I assist you today?
</pre>

<pre>
[2m08:52:24.539[0m [35m[START][0m [2m/v1/models[0m
[2m08:52:24.540[0m [35m[END][0m [2m/v1/models[0m[0m [StatusCode.OK][0m (1.42ms)
[2m08:52:24.545[0m [35m[START][0m [2m/v1/inference/chat-completion[0m
INFO     2025-06-06 10:52:24,555 LiteLLM:3043 uncategorized:                                                            
         LiteLLM completion() model= gpt-4-turbo; provider = openai                                                     
INFO     2025-06-06 10:52:26,027 LiteLLM:1215 uncategorized: Wrapper: Completed Call, calling success_handler           
INFO     2025-06-06 10:52:26,030 LiteLLM:655 uncategorized: selected model name for cost calculation:                   
         openai/gpt-4-turbo-2024-04-09                                                                                  
INFO     2025-06-06 10:52:26,033 LiteLLM:655 uncategorized: selected model name for cost calculation:                   
         openai/gpt-4-turbo-2024-04-09                                                                                  
INFO     2025-06-06 10:52:26,037 LiteLLM:655 uncategorized: selected model name for cost calculation:                   
         openai/gpt-4-turbo-2024-04-09                                                                                  
INFO:     ::1:57018 - "POST /v1/inference/chat-completion HTTP/1.1" 200 OK
[2m08:52:26.046[0m [35m[END][0m [2m/v1/inference/chat-completion[0m[0m [StatusCode.OK][0m (1501.47ms)
INFO     2025-06-06 10:52:40,419 llama_stack.distribution.server.server:158 server: Shutting down                       
INFO     2025-06-06 10:52:40,422 llama_stack.distribution.server.server:142 server: Shutting down TelemetryAdapter      
INFO     2025-06-06 10:52:40,423 llama_stack.distribution.server.server:142 server: Shutting down ModelsRoutingTable    
INFO     2025-06-06 10:52:40,425 llama_stack.distribution.server.server:142 server: Shutting down InferenceRouter       
INFO     2025-06-06 10:52:40,427 llama_stack.distribution.server.server:142 server: Shutting down                       
         DistributionInspectImpl                                                                                        
INFO     2025-06-06 10:52:40,428 llama_stack.distribution.server.server:142 server: Shutting down ProviderImpl          
</pre>


<p><a name="k10"></a></p>
<h2 id="k10">10. </h2>



<p><a name="k11"></a></p>
<h2 id="k11">11. </h2>



<p><a name="k12"></a></p>
<h2 id="k12">12. </h2>



<p><a name="k13"></a></p>
<h2 id="k13">13. </h2>



<p><a name="k14"></a></p>
<h2 id="k14">14. </h2>



<p><a name="k15"></a></p>
<h2 id="k15">15. </h2>



<p><a name="k16"></a></p>
<h2 id="k16">16. </h2>



<p><a name="k17"></a></p>
<h2 id="k17">17. </h2>



<p><a name="k18"></a></p>
<h2 id="k18">18. </h2>



<p><a name="k19"></a></p>
<h2 id="k19">19. Reposit√°≈ô s&nbsp;demonstraƒçn√≠mi p≈ô√≠klady</h2>

<p>Projekt popsan√Ω v&nbsp;p≈ôedchoz√≠ch kapitol√°ch je mo≈æn√© nal√©zt
v&nbsp;reposit√°≈ôi <a
href="https://github.com/tisnik/most-popular-python-libs">https://github.com/tisnik/most-popular-python-libs</a>.
N√°sleduj√≠ odkazy na jednotliv√© soubory s&nbsp;jejich struƒçn√Ωm popisem:</p>

<table>
<tr><th>#<th>P≈ô√≠klad</th><th>Struƒçn√Ω popis</th><th>Adresa p≈ô√≠kladu</th></tr></i>
<tr><td>1</td><td></td><td></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/">https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/</a></td></tr>
<tr><td>2</td><td></td><td></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/">https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/</a></td></tr>
<tr><td>3</td><td></td><td></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/">https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/</a></td></tr>
<tr><td>4</td><td></td><td></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/">https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/</a></td></tr>
</table>



<p><a name="k20"></a></p>
<h2 id="k20">20. Odkazy na Internetu</h2>

<ol>

<li>llama-stack na GitHubu<br />
<a href="https://github.com/meta-llama/llama-stack">https://github.com/meta-llama/llama-stack</a>
</li>

<li>llama-stack na PyPi<br />
<a href="https://pypi.org/project/llama-stack/">https://pypi.org/project/llama-stack/</a>
</li>

<li>Configuring a "Stack"<br />
<a href="https://llama-stack.readthedocs.io/en/latest/distributions/configuration.html#">https://llama-stack.readthedocs.io/en/latest/distributions/configuration.html#</a>
</li>

</ol>



<p></p><p></p>
<p><small>Autor: <a href="https://github.com/tisnik/">Pavel Ti≈°novsk√Ω</a> &nbsp; 2025</small></p>
</body>
</html>

