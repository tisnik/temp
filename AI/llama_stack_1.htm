<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
        "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html>
<head>
<title>Llama Stack: framework pro tvorbu aplikac√≠ s generativn√≠ AI</title>
<meta name="Author" content="Pavel Tisnovsky" />
<meta name="Generator" content="vim" />
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<style type="text/css">
         body {color:#000000; background:#ffffff;}
         h1  {font-family: arial, helvetica, sans-serif; color:#ffffff; background-color:#c00000; text-align:center; padding-left:1em}
         h2  {font-family: arial, helvetica, sans-serif; color:#ffffff; background-color:#0000c0; padding-left:1em; text-align:left}
         h3  {font-family: arial, helvetica, sans-serif; color:#000000; background-color:#c0c0c0; padding-left:1em; text-align:left}
         h4  {font-family: arial, helvetica, sans-serif; color:#000000; background-color:#e0e0e0; padding-left:1em; text-align:left}
         a   {font-family: arial, helvetica, sans-serif;}
         li  {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify; width:450px;}
         ol  {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify; width:450px;}
         ul  {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify; width:450px;}
         p   {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify;}
         pre {background:#e0e0e0}
</style>
</head>

<body>

<h1>Llama Stack: framework pro tvorbu aplikac√≠ s generativn√≠ AI</h1>

<h3>Pavel Ti≈°novsk√Ω</h3>

<p></p>

<h1>√övodn√≠k</h1>

<p>V dne≈°n√≠m ƒçl√°nku se sezn√°m√≠me se z√°klady pou≈æit√≠ frameworku Llama Stack, kter√Ω je urƒçen pro tvorbu aplikac√≠ a n√°stroj≈Ø vybaven√Ωch generativn√≠ AI. Do t√©to oblasti spadaj√≠ i velk√© jazykov√© modely zpopularizovan√© slu≈æbami jako ChatGPT, Gemini nebo Cursor.</p>



<h2>Obsah</h2>

<p><a href="#k01">1. Llama Stack: framework pro tvorbu aplikac√≠ s&nbsp;generativn√≠ AI</a></p>
<p><a href="#k02">2. Framework Llama Stack se p≈ôedstavuje</a></p>
<p><a href="#k03">3. P≈ô√≠prava projektu pro spu≈°tƒõn√≠ a otestov√°n√≠ Llama Stacku</a></p>
<p><a href="#k04">4. P≈ôid√°n√≠ v≈°ech pot≈ôebn√Ωch z√°vislost√≠ do projektu</a></p>
<p><a href="#k05">5. Kontrola, zda instalace Llama Stacku dopadla √∫spƒõ≈°nƒõ</a></p>
<p><a href="#k06">6. Nejjednodu≈°≈°√≠ funkƒçn√≠ konfigurace Llama Stacku</a></p>
<p><a href="#k07">7. Prvn√≠ spu≈°tƒõn√≠ Llama Stacku</a></p>
<p><a href="#k08">8. REST API Llama Stacku</a></p>
<p><a href="#k09">9. P≈ô√≠stup ke <i>stacku</i> z&nbsp;aplikac√≠ psan√Ωch v&nbsp;Pythonu</a></p>
<p><a href="#k10">10. Jednoduch√Ω klient komunikuj√≠c√≠ se stackem</a></p>
<p><a href="#k11">11. √öpln√Ω zdrojov√Ω k√≥d klienta</a></p>
<p><a href="#k12">12. Spu≈°tƒõn√≠ klienta</a></p>
<p><a href="#k13">13. Klient zji≈°≈•uj√≠c√≠, kter√© LLM modely je mo≈æn√© vyu≈æ√≠t</a></p>
<p><a href="#k14">14. Pou≈æit√≠ Llama Stacku v&nbsp;roli Pythonn√≠ knihovny (bez serveru)</a></p>
<p><a href="#k15">15. √öpln√Ω zdrojov√Ω k√≥d t≈ôet√≠ verze klienta</a></p>
<p><a href="#k16">16. Spu≈°tƒõn√≠ t≈ôet√≠ verze klienta</a></p>
<p><a href="#k17">17. Dal≈°√≠ mo≈ænosti konfigurace <i>stacku</i></a></p>
<p><a href="#k18">18. P≈ô√≠loha: seznam poskytovatel≈Ø (<i>providers</i>)</a></p>
<p><a href="#k19">19. Reposit√°≈ô s&nbsp;demonstraƒçn√≠mi p≈ô√≠klady</a></p>
<p><a href="#k20">20. Odkazy na Internetu</a></p>



<p><a name="k01"></a></p>
<h2 id="k01">1. Llama Stack: framework pro tvorbu aplikac√≠ s&nbsp;generativn√≠ AI</h2>

<p>Pravdƒõpodobnƒõ naprost√° vƒõt≈°ina lid√≠, kte≈ô√≠ se pohybuj√≠ v&nbsp;oblasti
informatiky, zaznamenali revoluci, kterou v&nbsp;posledn√≠ch dvou ƒçi t≈ô√≠ letech
p≈ôinesla generativn√≠ umƒõl√° inteligence (<i>generative AI</i>).
V&nbsp;souƒçasnosti se zd√°, ≈æe prakticky ka≈æd√° nov√° verze vƒõt≈°iny aplikac√≠ je
vybavena v√≠ce ƒçi m√©nƒõ sofistikovanou umƒõlou inteligenc√≠, co≈æ pochopitelnƒõ
p≈ôin√°≈°√≠ jak klady, tak i z√°pory (kter√© mnohdy p≈ôeva≈æuj√≠ nad klady, co≈æ je v≈°ak
t√©ma na samostatn√Ω ƒçl√°nek). A pochopitelnƒõ nastala situace bƒõ≈æn√° v&nbsp;cel√©m
IT: spoleƒçnƒõ s&nbsp;rozvojem generativn√≠ AI (nejv√≠ce pak velk√Ωch jazykov√Ωch
model≈Ø neboli LLM) vzniklo i velk√© mno≈æstv√≠ knihoven a framework≈Ø urƒçen√Ωch pro
v√Ωvoj aplikac√≠ vybaven√Ωch nƒõjakou formou AI, p≈ôiƒçem≈æ mnohdy nejsou tyto
knihovny roz≈°i≈ôiteln√©, maj√≠ mnoho chyb, nepodporuj√≠ v≈°echny pot≈ôebn√© vlastnosti
(z&nbsp;posledn√≠ doby agent-to-agent neboli A2A) apod.</p>

<p>Abychom si uk√°zali, jak je situace okolo generativn√≠ AI (a LLM) rozs√°hl√° a
chaotick√°, pod√≠vejme se pouze na oblast velk√Ωch jazykov√Ωch model≈Ø (co≈æ je
podmno≈æina generativn√≠ AI) a nav√≠c se omezme jen na ekosyst√©m programovac√≠ho
jazyka <a
href="https://www.itbiz.cz/clanky/generativni-ai-a-llm-revoluce-v-umele-inteligenci/">Python</a>.
Zde se m≈Ø≈æeme se setkat s&nbsp;velkou ≈ôadou knihoven a framework≈Ø, zejm√©na
pak:</p>

<ol>
<li>LangChain</li>
<li>langgraph</li>
<li>autogen</li>
<li>metaGPT</li>
<li>phidata</li>
<li>CrewAI</li>
<li>pydanticAI</li>
<li>controlflow</li>
<li>langflow</li>
<li>LiteLLM</li>
<li>Llama Stack</li>
</ol>

<p><div class="rs-tip-major">Pozn√°mka: vybral jsem jen ty n√°stroje,
s&nbsp;nimi≈æ m√°m alespo≈à nƒõjakou zku≈°enost a kter√© jsme otestovali.</div></p>



<p><a name="k02"></a></p>
<h2 id="k02">2. Framework Llama Stack se p≈ôedstavuje</h2>

<p>V&nbsp;seznamu, kter√Ω byl uveden <a href="#k01">v&nbsp;p≈ôedchoz√≠
kapitole</a>, nalezneme jak knihovny (p≈ô√≠kladem m≈Ø≈æe b√Ωt <i>LangChain</i>), tak
i v√≠ce ƒçi m√©nƒõ sofistikovan√© frameworky. A pr√°vƒõ jedn√≠m z&nbsp;framework≈Ø
urƒçen√Ωch pro tvorbu aplikac√≠, kter√© v&nbsp;nƒõjak√© m√≠≈ôe vyu≈æ√≠vaj√≠ generativn√≠
AI, je i framework nazvan√Ω <i>Llama Stack</i>, za jeho≈æ v√Ωvojem stoj√≠ firma
Meta spoleƒçnƒõ s&nbsp;dal≈°√≠mi p≈ôispƒõvateli (co≈æ jsou jak firmy, tak i
jednotlivci). Tento framework je mo≈æn√© vyu≈æ√≠t nƒõkolika zp≈Øsoby. Buƒè je mo≈æn√©
cel√Ω <i>stack</i> (tj.&nbsp;r≈Øzn√© moduly) spustit jako samostatn√Ω server,
k&nbsp;nƒõmu≈æ je mo≈æn√© se p≈ôipojit vhodnƒõ nakonfigurovan√Ωm klientem, nebo se
<i>Llama Stack</i> pou≈æije jako bƒõ≈æn√° knihovna a v√Ωsledkem tak bude aplikace,
kter√° bude cel√Ω stack obsahovat (ve skuteƒçnosti m≈Ø≈æe b√Ωt cel√° konfigurace je≈°tƒõ
slo≈æitƒõj≈°√≠, k&nbsp;tomuto t√©matu se je≈°tƒõ pozdƒõji vr√°t√≠me).</p>

<p>Samotn√Ω <i>Llama Stack</i> je skuteƒçnƒõ jen &bdquo;pouh√Ωm&ldquo; frameworkem,
kter√Ω je v≈°ak pomƒõrnƒõ jednodu≈°e roz≈°i≈ôiteln√Ω o dal≈°√≠ pluginy, kter√© se naz√Ωvaj√≠
<i>poskytovatel√©</i> neboli anglicky <i>providers</i>. V&nbsp;navazuj√≠c√≠ch
kapitol√°ch si uk√°≈æeme realizaci jednoduch√©ho chatu (podobn√©ho ChatGPT atd.),
kter√Ω vy≈æaduje konfiguraci minim√°lnƒõ dvou poskytovatel≈Ø. V&nbsp;prvn√≠ ≈ôadƒõ se
jedn√° o poskytovatele vlastn√≠ho inference serveru, v&nbsp;na≈°em p≈ô√≠padƒõ
konkr√©tnƒõ OpenAI. A druh√Ωm poskytovatelem, kter√©ho nakonfigurujeme, je
poskytovatel pro telemetrii, logov√°n√≠ a trasov√°n√≠ (ten je nutn√© nakonfigurovat
minim√°lnƒõ proto, aby se vypisovaly logovac√≠ informace). To znamen√°, ≈æe n√°≈°
<i>stack</i> m≈Ø≈æe vypadat takto:</p>

<pre>
providers:
  inference:
    - konfigurace poskytovatele
  telemetry:
    - konfigurace poskytovatele
</pre>

<p>Konkr√©tnƒõ pro OpenAI a logov√°n√≠:</p>

<pre>
providers:
  inference:
    - provider_id: openai
      provider_type: remote::openai
      config:
        api_key: <u>${env.OPENAI_API_KEY}</u>
  telemetry:
    - provider_id: meta-reference
      provider_type: inline::meta-reference
      config:
        sinks: ['console']
</pre>

<p><div class="rs-tip-major">Pozn√°mka: jak je z&nbsp;tƒõchto v√Ωpis≈Ø patrn√©, je
cel√Ω <i>stack</i> pops√°n v&nbsp;konfiguraƒçn√≠m souboru typu YAML.</div></p>



<p><a name="k03"></a></p>
<h2 id="k03">3. P≈ô√≠prava projektu pro spu≈°tƒõn√≠ a otestov√°n√≠ Llama Stacku</h2>

<p>Dne≈°n√≠ ƒçl√°nek je zamƒõ≈ôen sp√≠≈°e prakticky, co≈æ znamen√°, ≈æe se v&nbsp;nƒõm
sezn√°m√≠me se z√°kladn√≠m zp≈Øsobem vyu≈æit√≠ Llama Stacku. V&nbsp;prvn√≠m kroku si
mus√≠me nechat vytvo≈ôit kostru projektu, kter√Ω postupnƒõ uprav√≠me do fin√°ln√≠
podoby. Pro spr√°vu projekt≈Ø vyu≈æijeme n√°stroj <i>PDM</i>, kter√Ω byl pops√°n
v&nbsp;ƒçl√°nku <a
href="https://www.root.cz/clanky/pdm-moderni-spravce-balicku-a-virtualnich-prostredi-pythonu/">PDM:
modern√≠ spr√°vce bal√≠ƒçk≈Ø a virtu√°ln√≠ch prost≈ôed√≠ Pythonu</a> (ov≈°em stejnƒõ dob≈ôe
m≈Ø≈æete pou≈æ√≠t i klasick√Ω <strong>pip</strong> zkombinovan√Ω
s&nbsp;<strong>venv</strong> nebo <strong>virtualenv</strong>; velmi dobrou
alternativou je pak <a href="https://docs.astral.sh/uv/">uv</a>).</p>

<p>Nejprve si nainstalujte samotn√© PDM:</p>

<pre>
$ <strong>pip install --user pdm</strong>
</pre>

<p>Otestujeme, ≈æe je p≈ô√≠kaz <strong>pdm</strong> dostupn√Ω:</p>

<pre>
$ <strong>pdm --version</strong>
&nbsp;
PDM, version 2.22.2
</pre>

<p>Kostra nov√©ho projektu se vytvo≈ô√≠ p≈ô√≠kazem:</p>

<pre>
$ <strong>pdm init</strong>
</pre>

<p>Po zad√°n√≠ tohoto p≈ô√≠kazu se syst√©m PDM zept√° na nƒõkolik ot√°zek. Odpovƒõdi,
kter√© mus√≠ poskytnou program√°tor, jsou zv√Ωraznƒõny tuƒçn√Ωm p√≠smem:</p>

<pre>
Creating a pyproject.toml for PDM...
Please enter the Python interpreter to use
 0. cpython@3.12 (/usr/bin/python)
 1. cpython@3.13 (/usr/bin/python3.13)
 2. cpython@3.12 (/usr/bin/python3.12)
 3. cpython@3.11 (/usr/bin/python3.11)
Please select (0): <strong>0</strong>
Virtualenv is created successfully at /tmp/ramdisk/llama-stack-demo/.venv
Project name (llama-stack-demo):  <strong>[Enter]</strong>
Project version (0.1.0):  <strong>[Enter]</strong>
Do you want to build this project for distribution(such as wheel)?
If yes, it will be installed by default when running `pdm install`. [y/n] (n):  <strong>[Enter]</strong>
License(SPDX name) (MIT):  <strong>[Enter]</strong>
Author name (Pavel Tisnovsky):  <strong>[Enter]</strong>
Author email (tisnik@nowhere.com):  <strong>[Enter]</strong>
Python requires('*' to allow any) (==3.12.*):  <strong>[Enter]</strong>
Project is initialized successfully
</pre>

<p><div class="rs-tip-major">Pozn√°mka: nab√≠dka interpretr≈Ø jazyka Python a
nab√≠zen√© jm√©no a e-mail se pochopitelnƒõ budou li≈°it.</div></p>

<p>V√Ωsledkem v√Ω≈°e uveden√©ho kroku by mƒõl b√Ωt projektov√Ω soubor nazvan√Ω
<strong>pyproject.toml</strong> s&nbsp;n√°sleduj√≠c√≠m obsahem:</p>

<pre>
[project]
name = "llama-stack-demo"
version = "0.1.0"
description = "Default template for PDM package"
authors = [
    {name = "Pavel Tisnovsky", email = "tisnik@nowhere.com"},
]
dependencies = []
requires-python = "==3.12.*"
readme = "README.md"
license = {text = "MIT"}
&nbsp;
&nbsp;
[tool.pdm]
distribution = false
</pre>

<p><div class="rs-tip-major">Pozn√°mka: pov≈°imnƒõte si ≈ô√°dku, na kter√©m je
specifikov√°na verze Pythonu. Tento ≈ô√°dek m≈Ø≈æete zmƒõnit nap≈ô√≠klad do t√©to
podoby:</div></p>

<pre>
requires-python = "&gt;=3.11.1,&lt;=3.12.10"
</pre>



<p><a name="k04"></a></p>
<h2 id="k04">4. P≈ôid√°n√≠ v≈°ech pot≈ôebn√Ωch z√°vislost√≠ do projektu</h2>

<p>V&nbsp;dal≈°√≠m kroku je nutn√© do projektu p≈ôidat dal≈°√≠ z√°vislosti, mezi kter√©
pat≈ô√≠ samotn√Ω Llama stack a takt√©≈æ bal√≠ƒçky pou≈æ√≠van√© p≈ôi vol√°n√≠ LLM atd.
Prozat√≠m pou≈æijeme &bdquo;uzamƒçenou&ldquo; verzi Llama stacku 0.2.8 a teprve
pozdƒõji tuto verzi zv√Ω≈°√≠me. Llama stack se toti≈æ pomƒõrnƒõ bou≈ôlivƒõ vyv√≠j√≠ a i
p≈ôechod (nap≈ô√≠klad) mezi 0.2.8 a 0.2.10 zp≈Øsobuje nekompatibility (v√≠tejte ve
svƒõtƒõ Pythonu+AI).</p>

<pre>
$ <strong>pdm add llama-stack==0.2.8 fastapi opentelemetry-sdk opentelemetry-exporter-otlp opentelemetry-instrumentation aiosqlite litellm uvicorn blobfile</strong>
</pre>

<p>Pr≈Øbƒõh instalace do virtu√°ln√≠ho prost≈ôed√≠ Pythonu:</p>

<pre>
Adding packages to default dependencies: llama-stack==0.2.8, fastapi, opentelemetry-sdk, opentelemetry-exporter-otlp,
opentelemetry-instrumentation, aiosqlite, litellm, uvicorn
  0:01:51 üîí Lock successful.
Changes are written to pyproject.toml.
Synchronizing working set with resolved packages: 87 to add, 0 to update, 0 to remove
&nbsp;
  ‚úî Install setuptools 80.9.0 successful
  ‚úî Install annotated-types 0.7.0 successful
  ‚úî Install distro 1.9.0 successful
  ‚úî Install click 8.2.1 successful
  ‚úî Install h11 0.16.0 successful
  ...
  ...
  ...
  ‚úî Install multidict 6.4.4 successful
  ‚úî Install tokenizers 0.21.1 successful
  ‚úî Install pillow 11.2.1 successful
  ‚úî Install numpy 2.2.6 successful
  ‚úî Install rpds-py 0.25.1 successful
  ‚úî Install yarl 1.20.0 successful
  ‚úî Install regex 2024.11.6 successful
  ‚úî Install grpcio 1.72.1 successful
  ‚úî Install pydantic-core 2.33.2 successful
  ‚úî Install aiohttp 3.12.9 successful
&nbsp;
  0:01:29 üéâ All complete! 87/87
</pre>

<p>Nyn√≠ by mƒõl projektov√Ω soubor <strong>pyproject.toml</strong> vypadat
n√°sledovnƒõ:</p>

<pre>
[project]
name = "llama-stack-demo"
version = "0.1.0"
description = "Default template for PDM package"
authors = [
    {name = "Pavel Tisnovsky", email = "tisnik@nowhere.com"},
]
dependencies = [
    "llama-stack==0.2.8",
    "fastapi&gt;=0.115.12",
    "opentelemetry-sdk&gt;=1.34.0",
    "opentelemetry-exporter-otlp&gt;=1.34.0",
    "opentelemetry-instrumentation&gt;=0.55b0",
    "aiosqlite&gt;=0.21.0",
    "litellm&gt;=1.72.1",
    "uvicorn&gt;=0.34.3",
    "blobfile&gt;=3.0.0"]
requires-python = "==3.12.*"
readme = "README.md"
license = {text = "MIT"}
&nbsp;
&nbsp;
[tool.pdm]
distribution = false
</pre>

<p><div class="rs-tip-major">Pozn√°mka: nƒõkter√© verze n√°stroje PDM v≈°echny
z√°vislosti ulo≈æ√≠ na jedin√Ω ≈ô√°dek, to v≈°ak nem√° na v√Ωslednou funkcionalitu
projektu ≈æ√°dn√Ω vliv.</div></p>



<p><a name="k05"></a></p>
<h2 id="k05">5. Kontrola, zda instalace Llama Stacku dopadla √∫spƒõ≈°nƒõ</h2>

<p>V&nbsp;dal≈°√≠m kroku si ovƒõ≈ô√≠me, ≈æe je mo≈æn√© spustit n√°stroj nazvan√Ω
<strong>llama</strong>. Ten byl nainstalov√°n do virtu√°ln√≠ho prost≈ôed√≠ Pythonu a
proto ho mus√≠me spou≈°tƒõt p≈ôes <strong>pdm run</strong>:</p>

<pre>
$ <strong>pdm run llama</strong>
</pre>

<p>Pokud instalace probƒõhla √∫spƒõ≈°nƒõ, mƒõly by se na termin√°l vypsat n√°sleduj√≠c√≠
zpr√°vy:</p>

<pre>
usage: llama [-h] {model,stack,download,verify-download} ...
&nbsp;
Welcome to the Llama CLI
&nbsp;
options:
  -h, --help            show this help message and exit
&nbsp;
subcommands:
  {model,stack,download,verify-download}
&nbsp;
  model                 Work with llama models
  stack                 Operations for the Llama Stack / Distributions
  download              Download a model from llama.meta.com or Hugging Face Hub
  verify-download       Verify integrity of downloaded model files
</pre>

<p><div class="rs-tip-major">Pozn√°mka: pozdƒõji vyu≈æijeme mo≈ænost manipulace
s&nbsp;cel√Ωm &bdquo;stackem&ldquo;, ov≈°em prozat√≠m n√°m bude postaƒçovat jeho
spu≈°tƒõn√≠. Ov≈°em nejd≈ô√≠ve je nutn√© cel√Ω &bdquo;stack&ldquo; vhodn√Ωm zp≈Øsobem
nakonfigurovat, co≈æ je t√©ma navazuj√≠c√≠ kapitoly.</div></p>



<p><a name="k06"></a></p>
<h2 id="k06">6. Nejjednodu≈°≈°√≠ funkƒçn√≠ konfigurace Llama Stacku</h2>

<p>V&nbsp;p≈ô√≠padƒõ, ≈æe se pokus√≠me spustit Llama Stack bez jeho konfigurace,
vyp√≠≈°e se pouze informace o v√Ωjimce (co≈æ tedy nen√≠ p≈ô√≠li≈° u≈æivatelsky
p≈ô√≠jemn√©):</p>

<pre>
$ <strong>pdm run llama stack run</strong>
&nbsp;
INFO     2025-06-27 14:36:57,618 llama_stack.cli.stack.run:146 server: No image type or image name provided. Assuming   
         environment packages.                                                                                          
Traceback (most recent call last):
  File "/tmp/ramdisk/llama-stack-demo/demo1/.venv/bin/llama", line 10, in &lt;module&gt;
    sys.exit(main())
             ^^^^^^
  File "/tmp/ramdisk/llama-stack-demo/demo1/.venv/lib/python3.12/site-packages/llama_stack/cli/llama.py", line 53, in main
    parser.run(args)
  File "/tmp/ramdisk/llama-stack-demo/demo1/.venv/lib/python3.12/site-packages/llama_stack/cli/llama.py", line 47, in run
    args.func(args)
  File "/tmp/ramdisk/llama-stack-demo/demo1/.venv/lib/python3.12/site-packages/llama_stack/cli/stack/run.py", line 160, in _run_stack_run_cmd
    server_main(server_args)
  File "/tmp/ramdisk/llama-stack-demo/demo1/.venv/lib/python3.12/site-packages/llama_stack/distribution/server/server.py", line 399, in main
    elif args.template:
         ^^^^^^^^^^^^^
AttributeError: 'Namespace' object has no attribute 'template'
</pre>

<p>Aby bylo mo≈æn√© Llama Stack skuteƒçnƒõ spustit a provozovat, mus√≠me si n√°≈°
&bdquo;stack&ldquo; vhodn√Ωm zp≈Øsobem nakonfigurovat. Prozat√≠m si vystaƒç√≠me
s&nbsp;velmi jednoduchou konfigurac√≠, kter√° bude m√≠t specifikovan√© pouze dva
poskytovatele (<i>providers</i>), konkr√©tnƒõ poskytovatele, kter√Ω bude na
konzoli vypisovat telemetrick√© informace a d√°le poskytovatele s&nbsp;LLM,
konkr√©tnƒõ s&nbsp;rozhran√≠m pro OpenAI (ale m≈Ø≈æeme pou≈æ√≠t i dal≈°√≠ poskytovatele
LLM, nap≈ô√≠klad Azure Open AI, Gemini, WatsonX, lok√°lnƒõ bƒõ≈æ√≠c√≠ LLM atd. atd.).
Pov≈°imnƒõte si, ≈æe je v&nbsp;konfiguraƒçn√≠m souboru mo≈æn√© specifikovat i to, ≈æe
nƒõkter√° volba (zde konkr√©tnƒõ p≈ô√≠stupov√Ω kl√≠ƒç k&nbsp;OpenAI) bude naƒçtena
z&nbsp;promƒõnn√© prost≈ôed√≠:</p>

<pre>
version: '2'
image_name: simplest-llama-stack-configuration
container_image: null
&nbsp;
distribution_spec:
  local:
    services:
      - inference
      - telemetry
&nbsp;
apis:
  - inference
  - telemetry
&nbsp;
providers:
  inference:
    - provider_id: openai
      provider_type: remote::openai
      config:
        api_key: <u>${env.OPENAI_API_KEY}</u>
  telemetry:
    - provider_id: meta-reference
      provider_type: inline::meta-reference
      config:
        sinks: ['console']
&nbsp;
models:
  - model_id: gpt-4-turbo
    provider_id: openai
    model_type: llm
    provider_model_id: gpt-4-turbo
&nbsp;
server:
  port: 8321
</pre>

<p>Tato konfigurace p≈ôedpokl√°d√°, ≈æe je≈°tƒõ p≈ôed vlastn√≠m spu≈°tƒõn√≠m serveru dojde
k&nbsp;nastaven√≠ kl√≠ƒçe pro OpenAI do promƒõnn√© prost≈ôed√≠ nazvan√©
<strong>OPENAI_API_KEY</strong>. Toto nastaven√≠ bude vypadat n√°sledovnƒõ:</p>

<pre>
$ export OPENAI_API_KEY="sk-xyzzy1234567890xyzzy1234567890xyzzy1234567890..."
</pre>

<p><div class="rs-tip-major">Pozn√°mka: v&nbsp;sekci <strong>server</strong> je
uvedeno, ≈æe Llama Stack po sv√©m spu≈°tƒõn√≠ bude dostupn√Ω pr√°vƒõ na portu ƒç√≠slo
8321.</div></p>



<p><a name="k07"></a></p>
<h2 id="k07">7. Prvn√≠ spu≈°tƒõn√≠ Llama Stacku</h2>

<p>Nyn√≠, kdy≈æ ji≈æ m√°me p≈ôipraven konfiguraƒçn√≠ soubor pro n√°≈° <i>stack</i>, je
mo≈æn√© spustit server s&nbsp;t√≠mto stackem. Pro tento √∫ƒçel pou≈æijeme p≈ô√≠kaz:</p>

<pre>
$ <strong>pdm run llama stack run run.yaml</strong>
</pre>

<p>Konfigurace je korektn√≠ (nebo by alespo≈à mƒõla b√Ωt korektn√≠), tak≈æe by se
nejprve mƒõly vypsat informace o naƒç√≠t√°n√≠ konfiguraƒçn√≠ho souboru a n√°slednƒõ i
aktu√°ln√≠ konfigurace cel√©ho serveru:</p>

<pre>
INFO     2025-06-06 10:51:57,634 llama_stack.cli.stack.run:125 server: Using run configuration: run.yaml                
INFO     2025-06-06 10:51:57,640 llama_stack.cli.stack.run:146 server: No image type or image name provided. Assuming   
         environment packages.                                                                                          
INFO     2025-06-06 10:51:58,282 llama_stack.distribution.server.server:422 server: Using config file: run.yaml         
INFO     2025-06-06 10:51:58,285 llama_stack.distribution.server.server:424 server: Run configuration:                  
INFO     2025-06-06 10:51:58,290 llama_stack.distribution.server.server:426 server: apis:                               
         - inference                                                                                                    
         - telemetry                                                                                                    
         benchmarks: []                                                                                                 
         container_image: null                                                                                          
         datasets: []                                                                                                   
         external_providers_dir: null                                                                                   
         image_name: simplest-llama-stack-configuration                                                                 
         inference_store: null                                                                                          
         logging: null                                                                                                  
         metadata_store: null                                                                                           
         models:                                                                                                        
         - metadata: {}                                                                                                 
           model_id: gpt-4-turbo                                                                                        
           model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType                                   
           - llm                                                                                                        
           provider_id: openai                                                                                          
           provider_model_id: gpt-4-turbo                                                                               
         providers:                                                                                                     
           inference:                                                                                                   
           - config:                                                                                                    
               api_key: '********'                                                                                      
             provider_id: openai                                                                                        
             provider_type: remote::openai                                                                              
           telemetry:                                                                                                   
           - config:                                                                                                    
               sinks:                                                                                                   
               - console                                                                                                
             provider_id: meta-reference                                                                                
             provider_type: inline::meta-reference                                                                      
         scoring_fns: []                                                                                                
         server:                                                                                                        
           auth: null                                                                                                   
           host: null                                                                                                   
           port: 8321                                                                                                   
           quota: null                                                                                                  
           tls_cafile: null                                                                                             
           tls_certfile: null                                                                                           
           tls_keyfile: null                                                                                            
         shields: []                                                                                                    
         tool_groups: []                                                                                                
         vector_dbs: []                                                                                                 
         version: '2'                                                                                                   
&nbsp;                                                                                                                        
INFO     2025-06-06 10:52:00,166 llama_stack.distribution.server.server:564 server: Listening on ['::', '0.0.0.0']:8321 
INFO     2025-06-06 10:52:00,176 llama_stack.distribution.server.server:156 server: Starting up                         
INFO:     ::1:57018 - "GET /v1/models HTTP/1.1" 200 OK
</pre>



<p><a name="k08"></a></p>
<h2 id="k08">8. REST API Llama Stacku</h2>

<p>Server naslouch√° na portu 8321. K&nbsp;dispozici je popis REST API dostupn√Ω
ve formƒõ OpenAPI (endpoint <strong>/openapi.json</strong>), ov≈°em vyu≈æ√≠t lze i
dal≈°√≠ endpointy. Nap≈ô√≠klad endpoint <strong>/v1/models</strong> vrac√≠ seznam AI
model≈Ø, vƒçetnƒõ n√°mi nakonfigurovan√©ho LLM modelu OpenAI:</p>

<pre>
$ <strong>curl localhost:8321/v1/models</strong>
&nbsp;
{"data":[{"identifier":"gpt-4-turbo","provider_resource_id":"gpt-4-turbo","provider_id":"openai","type":"model","metadata":{},"model_type":"llm"}]}
</pre>

<p><div class="rs-tip-major">Pozn√°mka: v&nbsp;dal≈°√≠ch p≈ô√≠kladech budeme kromƒõ
n√°stroje <strong>curl</strong> vyu≈æ√≠vat i n√°stroj <strong>jq</strong>, kter√Ωm
si nech√°me naform√°tovat v√Ωsledn√Ω JSON, kter√Ω je serverem pos√≠l√°n jako odpovƒõƒè.
T√≠mto n√°strojem jsme se ji≈æ zab√Ωvali v&nbsp;ƒçl√°nku <a
href="https://www.root.cz/clanky/zpracovani-dat-reprezentovanych-ve-formatu-json-nastrojem-jq/">Zpracov√°n√≠
dat reprezentovan√Ωch ve form√°tu JSON n√°strojem jq </a>.</div></p>

<p>Podrobnƒõj≈°√≠ informace modelech, ale v&nbsp;ƒçiteln√© podobnƒõ:</p>

<pre>
$ <strong>curl localhost:8321/v1/models | jq .</strong>
&nbsp;
{
  "data": [
    {
      "identifier": "gpt-4-turbo",
      "provider_resource_id": "gpt-4-turbo",
      "provider_id": "openai",
      "type": "model",
      "metadata": {},
      "model_type": "llm"
    }
  ]
}
</pre>

<p>Informace o OpenAI modelech. My m√°me nakonfigurov√°n pouze model
<i>gpt-4-turbo</i>, tak≈æe se vr√°t√≠ pole s&nbsp;jedin√Ωm prvkem:</p>

<pre>
$ <strong>curl localhost:8321/v1/openai/v1/models | jq .</strong>
&nbsp;
{
  "data": [
    {
      "id": "gpt-4-turbo",
      "object": "model",
      "created": 1751115324,
      "owned_by": "llama_stack"
    }
  ]
}
</pre>

<p>Z√≠sk√°n√≠ informace o verzi samotn√©ho Llama Stacku:</p>

<pre>
$ <strong>curl localhost:8321/v1/version | jq .</strong>
&nbsp;
{
  "version": "0.2.8"
}
</pre>

<p>Pop≈ô.&nbsp;se selektorem:</p>

<pre>
$ <strong>curl localhost:8321/v1/version | jq .version</strong>
&nbsp;
"0.2.8"
</pre>

<p>V&nbsp;posledn√≠m p≈ô√≠kladu zjist√≠me, zda je v≈°e v&nbsp;po≈ô√°dku:</p>

<pre>
$ <strong>curl localhost:8321/v1/health | jq .</strong>
&nbsp;
{
  "status": "OK"
}
</pre>



<p><a name="k09"></a></p>
<h2 id="k09">9. P≈ô√≠stup ke <i>stacku</i> z&nbsp;aplikac√≠ psan√Ωch v&nbsp;Pythonu</h2>

<p>Teoreticky je mo≈æn√© spustit <i>stack</i> zp≈Øsobem naznaƒçen√Ωm v√Ω≈°e a
p≈ôistupovat k&nbsp;nƒõmu p≈ôes REST API. Ov≈°em nam√≠sto p≈ô√≠m√© komunikace p≈ôes REST
API (a form√°t JSON) existuje je≈°tƒõ lep≈°√≠ alternativa. Ta je zalo≈æena na
takzvan√©m Llama Stack Clientovi. Jedn√° se o knihovnu dostupnou pro <a
href="https://github.com/meta-llama/llama-stack-client-python">Python</a>, <a
href="https://github.com/meta-llama/llama-stack-client-swift/tree/latest-release">Swift</a>,
<a href="https://github.com/meta-llama/llama-stack-client-node">Node.js</a>
nebo <a
href="https://github.com/meta-llama/llama-stack-client-kotlin/tree/latest-release">Kotlin</a>
(mimochodem: zaj√≠mav√Ω v√Ωbƒõr jazyk≈Ø), kter√° REST API stacku vhodn√Ωm zp≈Øsobem
&bdquo;obaluje&ldquo;, co≈æ je pro mnoho aplikac√≠ jednodu≈°≈°√≠.
V&nbsp;navazuj√≠c√≠ch kapitol√°ch si uk√°≈æeme, jak m≈Ø≈æe takov√Ω klient vypadat.
Uvid√≠me, ≈æe se nejedn√° o nijak komplikovanou technologii, sp√≠≈° naopak.</p>

<p>V≈°echny uk√°zky budou naprogramov√°ny v&nbsp;Pythonu, ov≈°em jejich p≈ôepis do
ostatn√≠ch zm√≠nƒõn√Ωch jazyk≈Ø nen√≠ p≈ô√≠li≈° komplikovan√Ω, proto≈æe jm√©na t≈ô√≠d a
jejich metod jsou podobn√° ƒçi dokonce stejn√°.</p>



<p><a name="k10"></a></p>
<h2 id="k10">10. Jednoduch√Ω klient komunikuj√≠c√≠ se stackem</h2>

<p>Pod√≠vejme se nyn√≠ na zp≈Øsob realizace velmi jednoduch√©ho klienta
naprogramovan√©ho v&nbsp;Pythonu. Nejprve mus√≠me pochopitelnƒõ naimportovat
modul, kter√Ω zprost≈ôedkov√°v√° spojen√≠ se stackem. Tento modul se jmenuje
<strong>LlamaStackClient</strong>:</p>

<pre>
from llama_stack_client import LlamaStackClient
</pre>

<p>D√°le se pokus√≠me o p≈ôipojen√≠ k&nbsp;bƒõ≈æ√≠c√≠mu <i>stacku</i>. V√≠me, ≈æe je
dostupn√Ω na portu 8321, tak≈æe spojen√≠ realizujeme snadno:</p>

<pre>
client = LlamaStackClient(base_url="http://localhost:8321")
</pre>

<p>V&nbsp;t√©to chv√≠li je ji≈æ mo≈æn√© se <i>stackem</i> komunikovat a nap≈ô√≠klad
zjistit verzi frameworku <i>Llama Stack</i>:</p>

<pre>
print(f"Using Llama Stack version {client._version}")
</pre>

<p>Ov≈°em u≈æiteƒçnƒõj≈°√≠ je nav√°z√°n√≠ (nep≈ô√≠m√©) komunikace se zvolen√Ωm LLM modelem,
posl√°n√≠ ot√°zky do modelu a p≈ôeƒçten√≠ jeho odpovƒõdi. V&nbsp;n√°sleduj√≠c√≠m k√≥du se
do LLM po≈°le tento text:</p>

<pre>
PROMPT = "Say Hello"
</pre>

<p>V&nbsp;k√≥du vybereme prvn√≠ (a jedin√Ω nakonfigurovan√Ω) model a po≈°leme
dotaz:</p>

<pre>
response = client.inference.chat_completion(
    messages=[{"role": "user", "content": PROMPT}],
    model_id=client.models.list()[0].identifier,
)
</pre>

<p>Odpovƒõƒè je ulo≈æena v&nbsp;promƒõnn√© <strong>response</strong>. Kromƒõ metadat
zde nalezneme i vlastn√≠ odpovƒõƒè LLM modelu, kterou m≈Ø≈æeme vypsat ƒçi jinak
zpracovat:</p>

<pre>
text = response.completion_message.content
print(f"LLM response: {text}")
</pre>

<p><div class="rs-tip-major">Pozn√°mka: jedn√° se o tu nejjednodu≈°≈°√≠ mo≈ænou formu
komunikace s&nbsp;LLM, proto≈æe nem√°me nakonfigurovanou ani historii zpr√°v, ani
RAG datab√°zi. Slo≈æitƒõj≈°√≠ p≈ô√≠klady si uvedeme p≈ô√≠≈°tƒõ.</div></p>



<p><a name="k11"></a></p>
<h2 id="k11">11. √öpln√Ω zdrojov√Ω k√≥d klienta</h2>

<p>√öpln√Ω zdrojov√Ω k√≥d klienta popsan√©ho <a href="#k10">v&nbsp;p≈ôedchoz√≠
kapitole</a> vypad√° n√°sledovnƒõ:</p>

<pre>
from llama_stack_client import LlamaStackClient
&nbsp;
PROMPT = "Say Hello"
&nbsp;
client = LlamaStackClient(base_url="http://localhost:8321")
&nbsp;
print(f"Using Llama Stack version {client._version}")
&nbsp;
response = client.inference.chat_completion(
    messages=[{"role": "user", "content": PROMPT}],
    model_id=client.models.list()[0].identifier,
)
&nbsp;
text = response.completion_message.content
print(f"LLM response: {text}")
</pre>

<p><div class="rs-tip-major">Pozn√°mka: pov≈°imnƒõte si, ≈æe se skuteƒçnƒõ jedn√° o
pouh√Ωch nƒõkolik ≈ô√°dk≈Ø zdrojov√©ho k√≥du, proto≈æe ve≈°ker√° konfigurace, definice
p≈ôipojen√≠ k&nbsp;LLM atd.&nbsp;je ≈ôe≈°ena Llama Stackem. Klient pouze komunikuje
s&nbsp;ji≈æ nakonfigurovan√Ωm stackem.</div></p>



<p><a name="k12"></a></p>
<h2 id="k12">12. Spu≈°tƒõn√≠ klienta</h2>

<p>P≈ôedpokl√°dejme, ≈æe n√°≈° <i>stack</i> ji≈æ bƒõ≈æ√≠ a je dostupn√Ω na portu 8321.
V&nbsp;t√©to situaci m≈Ø≈æeme spustit klienta, kter√Ω bude s&nbsp;t√≠mto
<i>stackem</i> komunikovat &ndash; po≈°le ot√°zku do LLM a zobraz√≠ obdr≈æenou
odpovƒõƒè. Klienta budeme muset spustit ve virtu√°ln√≠m prost≈ôed√≠ Pythonu;
konkr√©tnƒõ pou≈æijeme (opƒõt) spr√°vce projekt≈Ø PDM:</p>

<pre>
$ <strong>pdm run client1.py</strong>
</pre>

<p>Na termin√°lu by se mƒõly postupnƒõ objevit ƒçty≈ôi informace:</p>

<ol>
<li>Verze Llama stacku</li>
<li>Logovac√≠ informace o tom, ≈æe se p≈ôes REST API zjistil seznam dostupn√Ωch model≈Ø</li>
<li>Logovac√≠ informace o tom, ≈æe se prvn√≠mu vybran√©mu modelu poslal dotaz</li>
<li>Odpovƒõƒè LLM serveru</li>
</ol>

<p>Konkr√©tnƒõ bude termin√°l obsahovat tyto ƒçty≈ôi ≈ô√°dky:</p>

<pre>
Using Llama Stack version 0.2.10
INFO:httpx:HTTP Request: GET http://localhost:8321/v1/models "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://localhost:8321/v1/inference/chat-completion "HTTP/1.1 200 OK"
LLM response: Hello! How can I assist you today?
</pre>

<p>Na stranƒõ <i>stacku</i> se takt√©≈æ objev√≠ mno≈æstv√≠ logovac√≠ch informac√≠:</p>

<pre>
INFO     2025-06-06 10:52:24,555 LiteLLM:3043 uncategorized:                                                            
         LiteLLM completion() model= gpt-4-turbo; provider = openai                                                     
INFO     2025-06-06 10:52:26,027 LiteLLM:1215 uncategorized: Wrapper: Completed Call, calling success_handler           
INFO     2025-06-06 10:52:26,030 LiteLLM:655 uncategorized: selected model name for cost calculation:                   
         openai/gpt-4-turbo-2024-04-09                                                                                  
INFO     2025-06-06 10:52:26,033 LiteLLM:655 uncategorized: selected model name for cost calculation:                   
         openai/gpt-4-turbo-2024-04-09                                                                                  
INFO     2025-06-06 10:52:26,037 LiteLLM:655 uncategorized: selected model name for cost calculation:                   
         openai/gpt-4-turbo-2024-04-09                                                                                  
INFO:     ::1:57018 - "POST /v1/inference/chat-completion HTTP/1.1" 200 OK
[2m08:52:26.046[0m [35m[END][0m [2m/v1/inference/chat-completion[0m[0m [StatusCode.OK][0m (1501.47ms)
INFO     2025-06-06 10:52:40,419 llama_stack.distribution.server.server:158 server: Shutting down                       
INFO     2025-06-06 10:52:40,422 llama_stack.distribution.server.server:142 server: Shutting down TelemetryAdapter      
INFO     2025-06-06 10:52:40,423 llama_stack.distribution.server.server:142 server: Shutting down ModelsRoutingTable    
INFO     2025-06-06 10:52:40,425 llama_stack.distribution.server.server:142 server: Shutting down InferenceRouter       
INFO     2025-06-06 10:52:40,427 llama_stack.distribution.server.server:142 server: Shutting down                       
         DistributionInspectImpl                                                                                        
INFO     2025-06-06 10:52:40,428 llama_stack.distribution.server.server:142 server: Shutting down ProviderImpl          
</pre>

<p><div class="rs-tip-major">Pozn√°mka: pochopitelnƒõ je mo≈æn√© √∫rove≈à logovac√≠ch
informac√≠ zmƒõnit a nap≈ô√≠klad nam√≠sto √∫rovnƒõ <i>INFO</i> nastavit <i>WARNING</i>
nebo <i>ERROR</i>. Ov≈°em p≈ôi v√Ωvoji se mi osvƒõdƒçilo ponechat pr√°vƒõ √∫rove≈à
<i>INFO</i> nebo dokonce i je≈°tƒõ podrobnƒõj≈°√≠ <i>DEBUG</i> (v√Ωstup je potom
je≈°tƒõ mnohem v√≠ce ukecan√Ω).</div></p>



<p><a name="k13"></a></p>
<h2 id="k13">13. Klient zji≈°≈•uj√≠c√≠, kter√© LLM modely je mo≈æn√© vyu≈æ√≠t</h2>

<p>Druh√Ω klient, kter√Ω komunikuje se <i>stackem</i>, zji≈°≈•uje v≈°echny LLM
modely, kter√© je mo≈æn√© vyu≈æ√≠t pro <i>inferenci</i> (zjednodu≈°enƒõ ≈ôeƒçeno: dotazy
a odpovƒõdi). Ve skuteƒçnosti se vyp√≠≈°ou v≈°echny dostupn√© modely, nejenom LLM</p>

<pre>
from llama_stack_client import LlamaStackClient
&nbsp;
client = LlamaStackClient(base_url="http://localhost:8321")
&nbsp;
print(f"Using Llama Stack version {client._version}")
&nbsp;
models = client.models.list()
&nbsp;
for model in models:
    print(model)
</pre>

<p>V√Ωsledek by mƒõl vypadat zhruba n√°sledovnƒõ (pouze jsem ho naform√°toval pro
vƒõt≈°√≠ ƒçitelnost):</p>

<pre>
Model(
    identifier='gpt-4-turbo',
    metadata={},
    api_model_type='llm',
    provider_id='openai',
    type='model',
    provider_resource_id='gpt-4-turbo',
    model_type='llm'
)
</pre>

<p><div class="rs-tip-major">Pozn√°mka: pov≈°imnƒõte si, ≈æe je nastaven i atribut
<strong>model_type</strong>, podle kter√©ho je mo≈æn√© urƒçit typ modelu (prozat√≠m
n√°s zaj√≠maj√≠ jen klasick√© LLM).</div></p>



<p><a name="k14"></a></p>
<h2 id="k14">14. Pou≈æit√≠ Llama Stacku v&nbsp;roli Pythonn√≠ knihovny (bez serveru)</h2>

<p>V&nbsp;p≈ôedchoz√≠ch kapitol√°ch jsme si uk√°zali, jak√Ωm zp≈Øsobem je mo≈æn√©
komunikovat se stackem, kter√Ω bƒõ≈æ√≠ v&nbsp;samostatn√©m procesu. Ov≈°em existuj√≠
situace, v&nbsp;nich≈æ nen√≠ vhodn√© spou≈°tƒõt dva procesory (jeden se stackem,
druh√Ω s&nbsp;klientem). V&nbsp;tƒõchto p≈ô√≠padech je mo≈æn√© stack spustit p≈ô√≠mo
v&nbsp;r√°mci klientsk√© aplikace. Nam√≠sto t≈ô√≠dy
<strong>LlamaStackClient</strong> se v&nbsp;tomto p≈ô√≠padƒõ mus√≠ pou≈æ√≠t t≈ô√≠da
<strong>LlamaStackAsLibraryClient</strong> a vlastn√≠ inicializace je tedy
pochopitelnƒõ odli≈°n√° &ndash; nemus√≠ se zad√°vat host a port, ale naopak cesta ke
konfiguraƒçn√≠mu souboru:</p>

<pre>
class LlamaStackAsLibraryClient(llama_stack_client.LlamaStackClient)
 |  LlamaStackAsLibraryClient(config_path_or_template_name: str, skip_logger_removal: bool = False, custom_provider_registry: dict[llama_stack.apis.datatypes.Api, dict[str, llama_stack.providers.datatypes.ProviderSpec]] | None = None, provider_data: dict[str, typing.Any] | None = None)
 |
 |  Method resolution order:
 |      LlamaStackAsLibraryClient
 |      llama_stack_client.LlamaStackClient
 |      llama_stack_client._base_client.SyncAPIClient
 |      llama_stack_client._base_client.BaseClient
 |      typing.Generic
 |      builtins.object
</pre>

<p>Uk√°zka bude uvedena v&nbsp;navazuj√≠c√≠ kapitole.</p>



<p><a name="k15"></a></p>
<h2 id="k15">15. √öpln√Ω zdrojov√Ω k√≥d t≈ôet√≠ verze klienta</h2>

<p>Realizace klienta, kter√Ω pro sv≈Øj bƒõh <i>nebude</i> pot≈ôebovat samostatnƒõ
bƒõ≈æ√≠c√≠ <i>stack</i>, ale naopak v≈°e bude realizovat v&nbsp;r√°mci jedin√©ho
procesu, vypad√° n√°sledovnƒõ. Pov≈°imnƒõte si, ≈æe nam√≠sto p≈ôipojen√≠ ke
<i>stacku</i> mus√≠me pouze zkonstruovat instanci t≈ô√≠dy
<strong>LlamaStackAsLibraryClient</strong> s&nbsp;p≈ôed√°n√≠m konfiguraƒçn√≠ho
souboru a n√°slednƒõ pouze prov√©st inicializaci. Dal≈°√≠ pr√°ce se stackem je ji≈æ
toto≈æn√° s&nbsp;konfigurac√≠, kdy <i>stack</i> bƒõ≈æel jako samostatn√Ω server:</p>

<pre>
from llama_stack.distribution.library_client import LlamaStackAsLibraryClient
from llama_stack_client import LlamaStackClient
&nbsp;
client = LlamaStackAsLibraryClient("run.yaml")
client.initialize()
&nbsp;
print(f"Using Llama Stack version {client._version}")
&nbsp;
models = client.models.list()
&nbsp;
for model in models:
    print(model)
</pre>



<p><a name="k16"></a></p>
<h2 id="k16">16. Spu≈°tƒõn√≠ t≈ôet√≠ verze klienta</h2>

<p>Klienta, jeho≈æ zdrojov√Ω k√≥d byl uk√°z√°n v&nbsp;p≈ôedchoz√≠ kapitole, spust√≠me
p≈ô√≠kazem:</p>

<pre>
$ <strong>pdm run client3.py</strong>
</pre>

<p>Nejprve by se mƒõla vypsat konfigurace stacku, kter√° byla naƒçtena
z&nbsp;konfiguraƒçn√≠ho souboru:</p>

<pre>
Using config run.yaml:
apis:
- inference
- telemetry
benchmarks: []
container_image: null
datasets: []
external_providers_dir: null
image_name: simplest-llama-stack-configuration
inference_store: null
logging: null
metadata_store: null
models:
- metadata: {}
  model_id: gpt-4-turbo
  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType
  - llm
  provider_id: openai
  provider_model_id: gpt-4-turbo
providers:
  inference:
  - config:
      api_key: '********'
    provider_id: openai
    provider_type: remote::openai
  telemetry:
  - config:
      sinks:
      - console
    provider_id: meta-reference
    provider_type: inline::meta-reference
scoring_fns: []
server:
  auth: null
  host: null
  port: 8321
  quota: null
  tls_cafile: null
  tls_certfile: null
  tls_keyfile: null
shields: []
tool_groups: []
vector_dbs: []
version: '2'
</pre>

<p>N√°slednƒõ se provede k√≥d klienta, tedy v√Ωpis verze Llama Stacku a seznam
dostupn√Ωch model≈Ø:</p>

<pre>
Using Llama Stack version 0.2.10
Model(identifier='gpt-4-turbo', metadata={}, api_model_type='llm', provider_id='openai', type='model', provider_resource_id='gpt-4-turbo', model_type='llm')
</pre>

<p><div class="rs-tip-major">Pozn√°mka: naprosto stejn√Ωm postupem m≈Ø≈æeme
zajistit vol√°n√≠ LLM atd.</div></p>



<p><a name="k17"></a></p>
<h2 id="k17">17. Dal≈°√≠ mo≈ænosti konfigurace <i>stacku</i></h2>

<p>Konfigurace <i>stacku</i> m≈Ø≈æe b√Ωt i pomƒõrnƒõ komplikovan√°.
V&nbsp;navazuj√≠c√≠m ƒçl√°nku si nap≈ô√≠klad pop√≠≈°eme n√°sleduj√≠c√≠ konfiguraci, kter√°
je mj.&nbsp;pou≈æit√° v&nbsp;re√°lnƒõ nasaditeln√© slu≈æbƒõ:</p>

<pre>
version: '2'
image_name: simplest-llamastack-app
&nbsp;
apis:
  - agents
  - datasetio
  - eval
  - inference
  - post_training
  - safety
  - scoring
  - telemetry
  - tool_runtime
  - vector_io
benchmarks: []
container_image: null
datasets: []
external_providers_dir: null
inference_store:
  db_path: /home/ptisnovs/.llama/distributions/ollama/inference_store.db
  type: sqlite
logging: null
metadata_store:
  db_path: /home/ptisnovs/.llama/distributions/ollama/registry.db
  namespace: null
  type: sqlite
providers:
  agents:
  - config:
      persistence_store:
        db_path: /home/ptisnovs/.llama/distributions/ollama/agents_store.db
        namespace: null
        type: sqlite
      responses_store:
        db_path: /home/ptisnovs/.llama/distributions/ollama/responses_store.db
        type: sqlite
    provider_id: meta-reference
    provider_type: inline::meta-reference
  datasetio:
  - config:
      kvstore:
        db_path: /home/ptisnovs/.llama/distributions/ollama/huggingface_datasetio.db
        namespace: null
        type: sqlite
    provider_id: huggingface
    provider_type: remote::huggingface
  - config:
      kvstore:
        db_path: /home/ptisnovs/.llama/distributions/ollama/localfs_datasetio.db
        namespace: null
        type: sqlite
    provider_id: localfs
    provider_type: inline::localfs
  eval:
  - config:
      kvstore:
        db_path: /home/ptisnovs/.llama/distributions/ollama/meta_reference_eval.db
        namespace: null
        type: sqlite
    provider_id: meta-reference
    provider_type: inline::meta-reference
  inference:
    - provider_id: openai
      provider_type: remote::openai
      config:
        api_key: ${env.OPENAI_API_KEY}
  post_training:
  - config:
      checkpoint_format: huggingface
      device: cpu
      distributed_backend: null
    provider_id: huggingface
    provider_type: inline::huggingface
  safety:
  - config:
      excluded_categories: []
    provider_id: llama-guard
    provider_type: inline::llama-guard
  scoring:
  - config: {}
    provider_id: basic
    provider_type: inline::basic
  - config: {}
    provider_id: llm-as-judge
    provider_type: inline::llm-as-judge
  - config:
      openai_api_key: '********'
    provider_id: braintrust
    provider_type: inline::braintrust
  telemetry:
</pre>



<p><a name="k18"></a></p>
<h2 id="k18">18. P≈ô√≠loha: seznam poskytovatel≈Ø (<i>providers</i>)</h2>

<p>V&nbsp;t√©to p≈ô√≠loze je uveden seznam ofici√°lnƒõ podporovan√Ωch poskytovatel≈Ø
(<i>providers</i>):</p>

<table>
<tr><th>Typ</th><th>Jm√©no poskytovatele</th></tr>
<tr><td>API Type     </td><td>Provider Type                  </td></tr>
<tr><td>agents       </td><td>inline::meta-reference         </td></tr>
<tr><td>datasetio    </td><td>inline::localfs                </td></tr>
<tr><td>datasetio    </td><td>remote::huggingface            </td></tr>
<tr><td>datasetio    </td><td>remote::nvidia                 </td></tr>
<tr><td>eval         </td><td>inline::meta-reference         </td></tr>
<tr><td>eval         </td><td>remote::nvidia                 </td></tr>
<tr><td>files        </td><td>inline::localfs                </td></tr>
<tr><td>inference    </td><td>inline::meta-reference         </td></tr>
<tr><td>inference    </td><td>inline::sentence-transformers  </td></tr>
<tr><td>inference    </td><td>inline::vllm                   </td></tr>
<tr><td>inference    </td><td>remote::anthropic              </td></tr>
<tr><td>inference    </td><td>remote::bedrock                </td></tr>
<tr><td>inference    </td><td>remote::cerebras               </td></tr>
<tr><td>inference    </td><td>remote::cerebras-openai-compat </td></tr>
<tr><td>inference    </td><td>remote::databricks             </td></tr>
<tr><td>inference    </td><td>remote::fireworks              </td></tr>
<tr><td>inference    </td><td>remote::fireworks-openai-compat</td></tr>
<tr><td>inference    </td><td>remote::gemini                 </td></tr>
<tr><td>inference    </td><td>remote::groq                   </td></tr>
<tr><td>inference    </td><td>remote::groq-openai-compat     </td></tr>
<tr><td>inference    </td><td>remote::hf::endpoint           </td></tr>
<tr><td>inference    </td><td>remote::hf::serverless         </td></tr>
<tr><td>inference    </td><td>remote::llama-openai-compat    </td></tr>
<tr><td>inference    </td><td>remote::nvidia                 </td></tr>
<tr><td>inference    </td><td>remote::ollama                 </td></tr>
<tr><td>inference    </td><td>remote::openai                 </td></tr>
<tr><td>inference    </td><td>remote::passthrough            </td></tr>
<tr><td>inference    </td><td>remote::runpod                 </td></tr>
<tr><td>inference    </td><td>remote::sambanova              </td></tr>
<tr><td>inference    </td><td>remote::sambanova-openai-compat</td></tr>
<tr><td>inference    </td><td>remote::tgi                    </td></tr>
<tr><td>inference    </td><td>remote::together               </td></tr>
<tr><td>inference    </td><td>remote::together-openai-compat </td></tr>
<tr><td>inference    </td><td>remote::vllm                   </td></tr>
<tr><td>inference    </td><td>remote::watsonx                </td></tr>
<tr><td>post_training</td><td>inline::huggingface            </td></tr>
<tr><td>post_training</td><td>inline::torchtune              </td></tr>
<tr><td>post_training</td><td>remote::nvidia                 </td></tr>
<tr><td>safety       </td><td>inline::code-scanner           </td></tr>
<tr><td>safety       </td><td>inline::llama-guard            </td></tr>
<tr><td>safety       </td><td>inline::prompt-guard           </td></tr>
<tr><td>safety       </td><td>remote::bedrock                </td></tr>
<tr><td>safety       </td><td>remote::nvidia                 </td></tr>
<tr><td>safety       </td><td>remote::sambanova              </td></tr>
<tr><td>scoring      </td><td>inline::basic                  </td></tr>
<tr><td>scoring      </td><td>inline::braintrust             </td></tr>
<tr><td>scoring      </td><td>inline::llm-as-judge           </td></tr>
<tr><td>telemetry    </td><td>inline::meta-reference         </td></tr>
<tr><td>tool_runtime </td><td>inline::rag-runtime            </td></tr>
<tr><td>tool_runtime </td><td>remote::bing-search            </td></tr>
<tr><td>tool_runtime </td><td>remote::brave-search           </td></tr>
<tr><td>tool_runtime </td><td>remote::model-context-protocol </td></tr>
<tr><td>tool_runtime </td><td>remote::tavily-search          </td></tr>
<tr><td>tool_runtime </td><td>remote::wolfram-alpha          </td></tr>
<tr><td>vector_io    </td><td>inline::chromadb               </td></tr>
<tr><td>vector_io    </td><td>inline::faiss                  </td></tr>
<tr><td>vector_io    </td><td>inline::meta-reference         </td></tr>
<tr><td>vector_io    </td><td>inline::milvus                 </td></tr>
<tr><td>vector_io    </td><td>inline::qdrant                 </td></tr>
<tr><td>vector_io    </td><td>inline::sqlite-vec             </td></tr>
<tr><td>vector_io    </td><td>inline::sqlite_vec             </td></tr>
<tr><td>vector_io    </td><td>remote::chromadb               </td></tr>
<tr><td>vector_io    </td><td>remote::milvus                 </td></tr>
<tr><td>vector_io    </td><td>remote::pgvector               </td></tr>
<tr><td>vector_io    </td><td>remote::qdrant                 </td></tr>
<tr><td>vector_io    </td><td>remote::weaviate               </td></tr>
</table>



<p><a name="k19"></a></p>
<h2 id="k19">19. Reposit√°≈ô s&nbsp;demonstraƒçn√≠mi p≈ô√≠klady</h2>

<p>Projekty popsan√© v&nbsp;p≈ôedchoz√≠ch kapitol√°ch je mo≈æn√© nal√©zt
v&nbsp;reposit√°≈ôi <a
href="https://github.com/tisnik/most-popular-python-libs">https://github.com/tisnik/most-popular-python-libs</a>.
N√°sleduj√≠ odkazy na jednotliv√© soubory s&nbsp;jejich struƒçn√Ωm popisem:</p>

<h3>Prvn√≠ projekt: vol√°n√≠ LLM modelu p≈ôes stack</h3>

<table>
<tr><th>#<th>P≈ô√≠klad</th><th>Struƒçn√Ω popis</th><th>Adresa p≈ô√≠kladu</th></tr></i>
<tr><td>1</td><td>demo1/pyproject.toml</td><td>konfiguraƒçn√≠ soubor s&nbsp;definic√≠ projektu</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo1/pyproject.toml">https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo1/pyproject.toml</a></td></tr>
<tr><td>2</td><td>demo1/pdm.lock</td><td>soubor se seznamem a ha≈°i nainstalovan√Ωch bal√≠ƒçk≈Ø</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo1/pdm.lock">https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo1/pdm.lock</a></td></tr>
<tr><td>3</td><td>demo1/run.yaml</td><td>konfigurace na≈°eho <i>stacku</i></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo1/run.yaml">https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo1/run.yaml</a></td></tr>
<tr><td>4</td><td>demo1/client1.py</td><td>skript v&nbsp;Pythonu s&nbsp;realizovan√Ωm klientem</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo1/client1.py">https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo1/client1.py</a></td></tr>
</table>

<h3>Druh√Ω projekt: v√Ωpis v≈°ech dostupn√Ωch model≈Ø</h3>

<table>
<tr><th>#<th>P≈ô√≠klad</th><th>Struƒçn√Ω popis</th><th>Adresa p≈ô√≠kladu</th></tr></i>
<tr><td>1</td><td>demo2/pyproject.toml</td><td>konfiguraƒçn√≠ soubor s&nbsp;definic√≠ projektu</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo2/pyproject.toml">https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo2/pyproject.toml</a></td></tr>
<tr><td>2</td><td>demo2/pdm.lock</td><td>soubor se seznamem a ha≈°i nainstalovan√Ωch bal√≠ƒçk≈Ø</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo2/pdm.lock">https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo2/pdm.lock</a></td></tr>
<tr><td>3</td><td>demo2/run.yaml</td><td>konfigurace na≈°eho <i>stacku</i></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo2/run.yaml">https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo2/run.yaml</a></td></tr>
<tr><td>4</td><td>demo2/client2.py</td><td>skript v&nbsp;Pythonu s&nbsp;realizovan√Ωm klientem</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo2/client2.py">https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo2/client2.py</a></td></tr>
</table>

<h3>T≈ôet√≠ projekt: vyu≈æit√≠ Llama Stacku jako Pythonn√≠ knihovny</h3>

<table>
<tr><th>#<th>P≈ô√≠klad</th><th>Struƒçn√Ω popis</th><th>Adresa p≈ô√≠kladu</th></tr></i>
<tr><td>1</td><td>demo3/pyproject.toml</td><td>konfiguraƒçn√≠ soubor s&nbsp;definic√≠ projektu</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo3/pyproject.toml">https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo3/pyproject.toml</a></td></tr>
<tr><td>2</td><td>demo3/pdm.lock</td><td>soubor se seznamem a ha≈°i nainstalovan√Ωch bal√≠ƒçk≈Ø</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo3/pdm.lock">https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo3/pdm.lock</a></td></tr>
<tr><td>3</td><td>demo3/run.yaml</td><td>konfigurace na≈°eho <i>stacku</i></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo3/run.yaml">https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo3/run.yaml</a></td></tr>
<tr><td>4</td><td>demo3/client3.py</td><td>skript v&nbsp;Pythonu s&nbsp;realizovan√Ωm klientem</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo3/client3.py">https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo3/client3.py</a></td></tr>
</table>



<p><a name="k20"></a></p>
<h2 id="k20">20. Odkazy na Internetu</h2>

<ol>

<li>llama-stack na GitHubu<br />
<a href="https://github.com/meta-llama/llama-stack">https://github.com/meta-llama/llama-stack</a>
</li>

<li>llama-stack na PyPi<br />
<a href="https://pypi.org/project/llama-stack/">https://pypi.org/project/llama-stack/</a>
</li>

<li>Configuring a "Stack"<br />
<a href="https://llama-stack.readthedocs.io/en/latest/distributions/configuration.html#">https://llama-stack.readthedocs.io/en/latest/distributions/configuration.html#</a>
</li>

<li>Generativn√≠ umƒõl√° inteligence<br />
<a href="https://cs.wikipedia.org/wiki/Generativn%C3%AD_um%C4%9Bl%C3%A1_inteligence">https://cs.wikipedia.org/wiki/Generativn%C3%AD_um%C4%9Bl%C3%A1_inteligence</a>
</li>

<li>Generative artificial intelligence<br />
<a href="https://en.wikipedia.org/wiki/Generative_artificial_intelligence">https://en.wikipedia.org/wiki/Generative_artificial_intelligence</a>
</li>

<li>Generativn√≠ AI a LLM: (R)evoluce v umƒõl√© inteligenci?<br />
<a href="https://www.itbiz.cz/clanky/generativni-ai-a-llm-revoluce-v-umele-inteligenci/">https://www.itbiz.cz/clanky/generativni-ai-a-llm-revoluce-v-umele-inteligenci/</a>
</li>

<li>langchain<br />
<a href="https://python.langchain.com/docs/introduction/">https://python.langchain.com/docs/introduction/</a>
</li>

<li>langgraph<br />
<a href="https://github.com/langchain-ai/langgraph">https://github.com/langchain-ai/langgraph</a>
</li>

<li>autogen<br />
<a href="https://github.com/microsoft/autogen">https://github.com/microsoft/autogen</a>
</li>

<li>metaGPT<br />
<a href="https://github.com/geekan/MetaGPT">https://github.com/geekan/MetaGPT</a>
</li>

<li>phidata<br />
<a href="https://github.com/phidatahq/phidata">https://github.com/phidatahq/phidata</a>
</li>

<li>CrewAI<br />
<a href="https://github.com/crewAIInc/crewAI">https://github.com/crewAIInc/crewAI</a>
</li>

<li>pydanticAI<br />
<a href="https://github.com/pydantic/pydantic-ai">https://github.com/pydantic/pydantic-ai</a>
</li>

<li>controlflow<br />
<a href="https://github.com/PrefectHQ/ControlFlow">https://github.com/PrefectHQ/ControlFlow</a>
</li>

<li>langflow<br />
<a href="https://github.com/langflow-ai/langflow">https://github.com/langflow-ai/langflow</a>
</li>

<li>LiteLLM<br />
<a href="https://github.com/BerriAI/litellm">https://github.com/BerriAI/litellm</a>
</li>

<li>Llama Stack<br />
<a href="https://github.com/meta-llama/llama-stack">https://github.com/meta-llama/llama-stack</a>
</li>

<li>uv<br />
<a href="https://docs.astral.sh/uv/">https://docs.astral.sh/uv/</a>
</li>

<li>Python na Root.cz<br />
<a href="https://www.root.cz/n/python/">https://www.root.cz/n/python/</a>
</li>

<li>PDM: modern√≠ spr√°vce bal√≠ƒçk≈Ø a virtu√°ln√≠ch prost≈ôed√≠ Pythonu<br />
<a href="https://www.root.cz/clanky/pdm-moderni-spravce-balicku-a-virtualnich-prostredi-pythonu/">https://www.root.cz/clanky/pdm-moderni-spravce-balicku-a-virtualnich-prostredi-pythonu/</a>
</li>

<li>YAML<br />
<a href="https://en.wikipedia.org/wiki/YAML">https://en.wikipedia.org/wiki/YAML</a>
</li>

<li>Top 11 LLM API Providers in 2025<br />
<a href="https://www.helicone.ai/blog/llm-api-providers">https://www.helicone.ai/blog/llm-api-providers</a>
</li>

<li>Zpracov√°n√≠ dat reprezentovan√Ωch ve form√°tu JSON n√°strojem jq<br />
<a href="https://www.root.cz/clanky/zpracovani-dat-reprezentovanych-ve-formatu-json-nastrojem-jq/">https://www.root.cz/clanky/zpracovani-dat-reprezentovanych-ve-formatu-json-nastrojem-jq/</a>
</li>

<li>LLama Stack SDK pro jazyk Python<br />
<a href="https://github.com/meta-llama/llama-stack-client-python">https://github.com/meta-llama/llama-stack-client-python</a>
</li>

<li>LLama Stack SDK pro jazyk Swift<br />
<a href="https://github.com/meta-llama/llama-stack-client-swift/tree/latest-release">https://github.com/meta-llama/llama-stack-client-swift/tree/latest-release</a>
</li>

<li>LLama Stack SDK pro ekosyst√©m Node.js<br />
<a href="https://github.com/meta-llama/llama-stack-client-node">https://github.com/meta-llama/llama-stack-client-node</a>
</li>

<li>LLama Stack SDK pro jazyk Kotlin<br />
<a href="https://github.com/meta-llama/llama-stack-client-kotlin/tree/latest-release">https://github.com/meta-llama/llama-stack-client-kotlin/tree/latest-release</a>
</li>

</ol>



<p></p><p></p>
<p><small>Autor: <a href="https://github.com/tisnik/">Pavel Ti≈°novsk√Ω</a> &nbsp; 2025</small></p>
</body>
</html>

