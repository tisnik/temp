<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
        "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html>
<head>
<title>Knihovna FAISS a embedding: základ jazykových modelů</title>
<meta name="Author" content="Pavel Tisnovsky" />
<meta name="Generator" content="vim" />
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<style type="text/css">
         body {color:#000000; background:#ffffff;}
         h1  {font-family: arial, helvetica, sans-serif; color:#ffffff; background-color:#c00000; text-align:center; padding-left:1em}
         h2  {font-family: arial, helvetica, sans-serif; color:#ffffff; background-color:#0000c0; padding-left:1em; text-align:left}
         h3  {font-family: arial, helvetica, sans-serif; color:#000000; background-color:#c0c0c0; padding-left:1em; text-align:left}
         h4  {font-family: arial, helvetica, sans-serif; color:#000000; background-color:#e0e0e0; padding-left:1em; text-align:left}
         a   {font-family: arial, helvetica, sans-serif;}
         li  {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify; width:450px;}
         ol  {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify; width:450px;}
         ul  {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify; width:450px;}
         p   {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify;}
         pre {background:#e0e0e0}
</style>
</head>

<body>

<h1>Knihovna FAISS a embedding: základ jazykových modelů</h1>

<h3>Pavel Tišnovský</h3>

<p></p>

<h1>Úvodník</h1>

<p></p>



<h2>Obsah</h2>

<p><a href="#k01">*** 1. Knihovna FAISS a embedding: základ jazykových modelů</a></p>
<p><a href="#k02">2. Příprava projektu</a></p>
<p><a href="#k03">3. Instalace balíčku <strong>sentence-transformers</strong> i jeho závislostí do virtuálního prostředí</a></p>
<p><a href="#k04">4. Načtení modelu</a></p>
<p><a href="#k05">5. Načtení odlišných modelů</a></p>
<p><a href="#k06">6. Velikost stažených modelů</a></p>
<p><a href="#k07">7. Převod vět do vektorizované podoby (<i>embeddings</i>)</a></p>
<p><a href="#k08">8. Podoba výsledné matice</a></p>
<p><a href="#k09">9. Podrobnější informace o vypočtené matici</a></p>
<p><a href="#k10">10. Výpočet míry podobnosti jednotlivých vět</a></p>
<p><a href="#k11">*** 11. Využití knihovny Faiss pro získání původní věty na základě předaného vzorku</a></p>
<p><a href="#k12">12. Instalace knihovny Faiss do virtuálního prostředí</a></p>
<p><a href="#k13">*** 13. </a></p>
<p><a href="#k14">*** 14. </a></p>
<p><a href="#k15">*** 15. </a></p>
<p><a href="#k16">*** 16. </a></p>
<p><a href="#k17">*** 17. </a></p>
<p><a href="#k18">*** 18. </a></p>
<p><a href="#k19">19. Repositář s&nbsp;demonstračními příklady</a></p>
<p><a href="#k20">20. Odkazy na Internetu</a></p>



<p><a name="k01"></a></p>
<h2 id="k01">1. Knihovna FAISS a embedding: základ jazykových modelů</h2>

contextual based embedding


<p><a name="k02"></a></p>
<h2 id="k02">2. Příprava projektu</h2>

<p>V&nbsp;prvním kroku si, naprosto stejným způsobem, jako jsme to provedli u
demonstračních příkladů využívajících knihovnu FAISS, připravíme projekt
v&nbsp;Pythonu a následně do něj nainstalujeme všechny potřebné knihovny. Pro
vytvoření projektu použijeme buď nástroj <i>PDM</i> &ndash; viz též <a
href="https://www.root.cz/clanky/pdm-moderni-spravce-balicku-a-virtualnich-prostredi-pythonu/">PDM:
moderní správce balíčků a virtuálních prostředí Pythonu</a> nebo (což je
v&nbsp;současnosti výhodnější) nástroj <a
href="https://docs.astral.sh/uv/">uv</a>. Projekt bude vytvořen v&nbsp;novém
(původně prázdném) adresáři a jeho projektový soubor
<strong>pyproject.toml</strong> může vypadat následovně:</p>

<pre>
[project]
name = "sentence-transformer"
version = "0.1.0"
description = "Sentence transformer demos"
authors = [
    {name = "Pavel Tisnovsky", email = "tisnik@nikdo.nikde.cz"},
]
dependencies = [
]
requires-python = "==3.12.*"
readme = "README.md"
license = {text = "MIT"}
&nbsp;
&nbsp;
[tool.pdm]
distribution = false
</pre>



<p><a name="k03"></a></p>
<h2 id="k03">3. Instalace balíčku <strong>sentence-transformers</strong> i jeho závislostí do virtuálního prostředí</h2>

<p>Ve druhém kroku do virtuálního prostředí vytvořeného pro nový projekt
přidáme balíček nazvaný <strong>sentence-transformers</strong>. Později do
projektu přidáme i knihovnu <strong>faiss-cpu</strong>, to ovšem prozatím
nebude nutné:</p>

<pre>
<strong>uv add sentence_transformers</strong>
</pre>

<p>Tato knihovna má poměrně velké množství závislostí, včetně cca 6GB balíčků
on NVidie. To je ostatně patrné i z&nbsp;průběhu instalace:</p>

<pre>
Installed 46 packages in 2.73s
 + certifi==2025.7.14
 + charset-normalizer==3.4.2
 + filelock==3.18.0
 + fsspec==2025.7.0
 + hf-xet==1.1.5
 + huggingface-hub==0.34.1
 + idna==3.10
 + jinja2==3.1.6
 + joblib==1.5.1
 + markupsafe==3.0.2
 + mpmath==1.3.0
 + networkx==3.5
 + numpy==2.3.2
 + nvidia-cublas-cu12==12.6.4.1
 + nvidia-cuda-cupti-cu12==12.6.80
 + nvidia-cuda-nvrtc-cu12==12.6.77
 + nvidia-cuda-runtime-cu12==12.6.77
 + nvidia-cudnn-cu12==9.5.1.17
 + nvidia-cufft-cu12==11.3.0.4
 + nvidia-cufile-cu12==1.11.1.6
 + nvidia-curand-cu12==10.3.7.77
 + nvidia-cusolver-cu12==11.7.1.2
 + nvidia-cusparse-cu12==12.5.4.2
 + nvidia-cusparselt-cu12==0.6.3
 + nvidia-nccl-cu12==2.26.2
 + nvidia-nvjitlink-cu12==12.6.85
 + nvidia-nvtx-cu12==12.6.77
 + packaging==25.0
 + pillow==11.3.0
 + pyyaml==6.0.2
 + regex==2024.11.6
 + requests==2.32.4
 + safetensors==0.5.3
 + scikit-learn==1.7.1
 + scipy==1.16.0
 + sentence-transformers==5.0.0
 + setuptools==80.9.0
 + sympy==1.14.0
 + threadpoolctl==3.6.0
 + tokenizers==0.21.2
 + torch==2.7.1
 + tqdm==4.67.1
 + transformers==4.53.3
 + triton==3.3.1
 + typing-extensions==4.14.1
 + urllib3==2.5.0
</pre>

<p>Projektový soubor by se měl změnit do následující podoby:</p>

<pre>
[project]
name = "sentence-transformer"
version = "0.1.0"
description = "Sentence transformer demos"
authors = [
    {name = "Pavel Tisnovsky", email = "tisnik@nikdo.nikde.cz"},
]
dependencies = [
    "sentence-transformers&gt;=5.0.0",
]
requires-python = "==3.12.*"
readme = "README.md"
license = {text = "MIT"}
&nbsp;
&nbsp;
[tool.pdm]
distribution = false
</pre>

<p>Otestujeme, zda je možné právě nainstalovaný balíček naimportovat:</p>

<pre>
$ <strong>uv run python</strong>
&nbsp;
Python 3.12.10 (main, Apr 22 2025, 00:00:00) [GCC 14.2.1 20240912 (Red Hat 14.2.1-3)] on linux
Type "help", "copyright", "credits" or "license" for more information.
&nbsp;
&gt;&gt;&gt; import sentence_transformers
&nbsp;
&gt;&gt;&gt; help(sentence_transformers)
&nbsp;
Help on package sentence_transformers:
&nbsp;
NAME
    sentence_transformers
&nbsp;
PACKAGE CONTENTS
    LoggingHandler
    SentenceTransformer
    backend
    cross_encoder (package)
    data_collator
    datasets (package)
    evaluation (package)
    fit_mixin
    losses (package)
    model_card
    model_card_templates
    models (package)
    peft_mixin
    quantization
    readers (package)
    sampler
    similarity_functions
    sparse_encoder (package)
    trainer
    training_args
    util
&nbsp;
    ...
    ...
    ...
</pre>



<p><a name="k04"></a></p>
<h2 id="k04">4. Načtení modelu</h2>

<p>Jedinou funkcí, kterou v&nbsp;dnešním článku budeme od balíčku
<strong>sentence-transformers</strong> vyžadovat, je transformace textů
(v&nbsp;našem případě jednoduchých vět) do formy vektorů pevné délky. Hodnoty
prvků těchto vektorů nějakým způsobem popisují význam původních vět. To
znamená, že, pokud situaci značně zjednodušíme, může jeden z&nbsp;prvků
(v&nbsp;závislosti na modelu) určovat, jestli se v&nbsp;původní větě mluví o
zvířatech (popř.&nbsp;o jakých zvířatech), další prvek určuje, že ve větě je
zmínka o počasí atd. Prvky vektorů jsou přitom typicky reálnými čísly
určujícími váhy příslušných kategorií a v&nbsp;závislosti na modelu mohou být
vektory normalizovány či nikoli (normalizované vektory lze přímo použít při
hledání podobných vektorů s&nbsp;využitím skalárního součinu).</p>

<p>Jedním z&nbsp;modelů, které je možné pro tvorbu vektorové databáze použít,
je model nazvaný <strong>paraphrase-MiniLM-L6-v2</strong>. Můžeme se pokusit
tento model načíst a vypsat si jeho základní vlastnosti:</p>

<pre>
from sentence_transformers import SentenceTransformer
&nbsp;
model = SentenceTransformer("paraphrase-MiniLM-L6-v2")
&nbsp;
print(model)
</pre>

<p>Po první spuštění tohoto skriptu se model načte z&nbsp;Internetu (což může
nějakou dobu trvat) a posléze by se měly vypsat jeho základní vlastnosti:</p>

<pre>
SentenceTransformer(
  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False, 'architecture': 'BertModel'})
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
)
</pre>

<p>Důležitá je pro nás především informace o délce vektorů, tedy o počtu
dimenzí. Ta je v&nbsp;tomto modelu rovna hodnotě 384. S&nbsp;touto hodnotou se
setkáme později. Zajímavá je taktéž informace o použité architektuře <a
href="https://en.wikipedia.org/wiki/BERT_(language_model)">BERT</a> (což by
bylo téma na samostatný článek).</p>

<p><div class="rs-tip-major">Poznámka: na jednu stranu modely s&nbsp;vektory
s&nbsp;větším počtem dimenzí dokážou popsat původní věty přesněji, na stranu
druhou mají tyto modely větší paměťové i výpočetní nároky. Z&nbsp;tohoto důvodu
se velmi často setkáme s&nbsp;modely, jejichž vektory mají právě 384 dimenzí,
ovšem pochopitelně existují i modely menší (typicky specializované na jednu
oblast) nebo naopak větší. U takových modelů je již velmi složité určit, co
která dimenze reprezentuje (už se nebude jednat o slova, ale spíše o ucelené a
zobecněné významy).</div></p>



<p><a name="k05"></a></p>
<h2 id="k05">5. Načtení odlišných modelů</h2>

<p>Na <a href="https://huggingface.co/sentence-transformers">stránkách projektu
Hugging Face</a> můžeme nalézt velké množství dalších modelů, které je možné
použít pro vektorizaci textů. Některé z&nbsp;těchto modelů se snaží být
univerzální (a to včetně podpory mnoha jazyků), další jsou specializované, atd.
Modely se taktéž liší délkou (počtem dimenzí) výsledných vektorů.</p>

<p>Poměrně často se setkáme s&nbsp;modelem nazvaným
<strong>all-mpnet-base-v2</strong>, který lze inicializovat tímto způsobem:</p>

<pre>
from sentence_transformers import SentenceTransformer
&nbsp;
model = SentenceTransformer("all-mpnet-base-v2")
&nbsp;
print(model)
</pre>

<p>Po prvním spuštění tohoto skriptu se celý model stáhne a následně se vypíšou
jeho základní charakteristiky:</p>

<pre>
SentenceTransformer(
  (0): Transformer({'max_seq_length': 384, 'do_lower_case': False, 'architecture': 'MPNetModel'})
  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
  (2): Normalize()
)
</pre>

<p><div class="rs-tip-major">Poznámka: povšimněte si, že dimenzionalita
výsledných vektorů je nyní rovna hodnotě 768, což znamená, že model bude
<i>kvalitnější</i> (zaznamená více kontextu ze zpracovávaných textů), ovšem
bude vyžadovat více paměti i více strojového času při práci
s&nbsp;ním.</div></p>

<p>K&nbsp;dispozici jsou ovšem i modely určené pro konkrétní jazyky, tedy
nikoli primárně pro angličtinu. Příkladem může být model od Seznamu, který si
opět můžeme stáhnout a vypsat jeho základní vlastnosti:</p>

<pre>
from sentence_transformers import SentenceTransformer
&nbsp;
model = SentenceTransformer("Seznam/small-e-czech")
&nbsp;
print(model)
</pre>

<p>Opět se bude lišit dimezinonalita výsledných vektorů:</p>

<pre>
SentenceTransformer(
  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False, 'architecture': 'ElectraModel'})
  (1): Pooling({'word_embedding_dimension': 256, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
)
</pre>



<p><a name="k06"></a></p>
<h2 id="k06">6. Velikost stažených modelů</h2>

<p>V&nbsp;rámci předchozích kapitol bylo staženo několik modelů, ke kterým jsem
ještě přidal model <strong>all-MiniLM-L6-v2</strong>. Tyto modely jsou, včetně
jejich metadat, uloženy v&nbsp;podadresáři
<strong>~/.cache/huggingface/hug</strong>, takže bude snadné zjistit, kolik
místa na disku vlastně vyžadují:</p>

<pre>
$ <strong>du --summarize --human-readable ~/.cache/huggingface/hub/*</strong>
&nbsp;
88M     /home/ptisnovs/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2
419M    /home/ptisnovs/.cache/huggingface/hub/models--sentence-transformers--all-mpnet-base-v2
88M     /home/ptisnovs/.cache/huggingface/hub/models--sentence-transformers--paraphrase-MiniLM-L6-v2
105M    /home/ptisnovs/.cache/huggingface/hub/models--Seznam--small-e-czech
</pre>

<p>Povšimněte si, že obsazené místo na disku do určité míry záleží na
dimenzionalitě výsledných vektorů (což je pochopitelné), ovšem nejedná se o
jedinou veličinu, protože záleží i na interní složitosti modelu atd. Ovšem je
patrné, že modely nazvané &bdquo;mini&ldquo; nebo v&nbsp;případě českého modelu
&bdquo;small&ldquo; jsou skutečně menší, než všeobjímající model
<strong>all-mpnet-base-v2</strong>.</p>

<p><div class="rs-tip-major">Poznámka: pokud vám zdánlivě &bdquo;mizí&ldquo;
místo na disku, podívejte se i na adresář
<strong>~/.cache/huggingface/xet</strong>, ve kterém se mohou nacházet částečně
stažené soubory. Pokud například při stahování modelu o velikosti 500MB dojde
před koncem stahování k&nbsp;přerušení připojení (stránky Hugging Face bývají
přetíženy), může se ve výše zmíněném podadresáři nacházet částečně stažený
soubor.</div></p>



<p><a name="k07"></a></p>
<h2 id="k07">7. Převod vět do vektorizované podoby (<i>embeddings</i>)</h2>

<p>Balíček <strong>sentence-transformers</strong> i modely z&nbsp;Hugging Face
jsme instalovali z&nbsp;jediného důvodu &ndash; aby bylo možné převádět věty
(pro jednoduchost prozatím řekněme anglické věty) do vektorizované podoby.
V&nbsp;případě, že použijeme model <strong>paraphrase-MiniLM-L6-v2</strong>,
měla by být každá věta převedena do vektoru s&nbsp;384 prvky, jejichž hodnoty
by měly ve více či méně dokonalé podobě vyjadřovat význam věty.</p>

<p>Podívejme se nyní, jak tato vektorizace probíhá. Je to vlastně velmi
jednoduché:</p>

<ol>

<li>Načteme vybraný model, v&nbsp;našem konkrétním případě výše zmíněný model
<strong>paraphrase-MiniLM-L6-v2</strong></li>

<li>Zavoláme metodu modelu nazvanou <strong>encode</strong> a předáme jí seznam
(nebo sekvenci) vět v&nbsp;textové podobě. Tato metoda má velké množství
doplňujících parametrů, ty ovšem prozatím nebudeme používat a ponecháme
explicitní hodnoty.</li>

<li>Výsledkem bude matice (reálných hodnot) o <i>n</i> řádcích a <i>m</i>
sloupcích, kde <i>n</i> odpovídá počtu předaných vět a <i>m</i> dimenzionalitě
vektorů (takže například 256, 384, 512, 768 atd. &ndash; závisí na použitém
modelu)</li>

</ol>

<p>V&nbsp;Pythonu celý postup vypadá následovně:</p>

<pre>
from sentence_transformers import SentenceTransformer
&nbsp;
model = SentenceTransformer("paraphrase-MiniLM-L6-v2")
&nbsp;
print(model)
&nbsp;
sentences = [
    "The rain in Spain falls mainly on the plain",
    "The tesselated polygon is a special type of polygon",
    "The quick brown fox jumps over the lazy dog",
    "To be or not to be, that is the question",
    "It is a truth universally acknowledged...",
    "The goat ran down the hill"
]
&nbsp;
embeddings = model.encode(sentences)
print(f"Embeddings shape: {embeddings.shape}")
&nbsp;
print(f"Data type: {type(embeddings)}")
&nbsp;
print(embeddings)
</pre>



<p><a name="k08"></a></p>
<h2 id="k08">8. Podoba výsledné matice</h2>

<p>Skript popsaný <a href="#k07">v&nbsp;předchozí kapitole</a> spustíme. Přitom
musíme myslet na to, že balíček <strong>sentence-transformers</strong> byl
nainstalován do virtuálního prostředí a proto pro spuštění skriptu použijeme
nástroj <strong>pdm</strong> nebo <strong>uv</strong>:</p>

<pre>
$ <strong>uv run transformer4.py </strong>
</pre>

<p>Nejprve se vypíšou vlastnosti vybraného modelu. Ten se již nemusí stahovat,
takže by tento výpis měl proběhnout relativně rychle:</p>

<pre>
SentenceTransformer(
  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False, 'architecture': 'BertModel'})
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
)
</pre>

<p>Dále se vypíšou informace o výsledné matici, tj.&nbsp;počet řádků a počet
sloupců. Vypíšeme (pro jistotu) i typ výsledné hodnoty a následně i celou
matici (výpis bude automaticky zkrácený tak, aby se vlezl na terminál):</p>

<pre>
Embeddings shape: (6, 384)
Data type: &lt;class 'numpy.ndarray'&gt;
&nbsp;
[[ 0.3478464   0.16516833  0.80891246 ...  0.30405566 -0.03139801
   0.01735949]
 [ 0.1602744  -0.4778426   0.14267132 ...  0.35347039  0.0946853
   0.16065337]
 [ 0.6181423  -0.24776645 -0.23564643 ...  0.17215663  1.1165967
   0.63950485]
 [ 0.6088223   0.5830838   0.4046087  ...  0.08260182  0.8130692
   0.16788083]
 [ 0.11015981  0.10045841 -0.24472675 ... -0.06199175  0.46151286
  -0.12090196]
 [ 0.15316609 -0.05734093 -0.27605703 ...  0.01458593  0.24355106
  -0.44814658]]
</pre>



<p><a name="k09"></a></p>
<h2 id="k09">9. Podrobnější informace o vypočtené matici</h2>

<p>Další informace o matici lze získat zavoláním funkce
<strong>numpy.info()</strong>:</p>

<pre>
from sentence_transformers import SentenceTransformer
import numpy as np
&nbsp;
model = SentenceTransformer("paraphrase-MiniLM-L6-v2")
&nbsp;
print(model)
&nbsp;
sentences = [
    "The rain in Spain falls mainly on the plain",
    "The tesselated polygon is a special type of polygon",
    "The quick brown fox jumps over the lazy dog",
    "To be or not to be, that is the question",
    "It is a truth universally acknowledged...",
    "The goat ran down the hill"
]
&nbsp;
<i># Generate embeddings for the sentences</i>
embeddings = model.encode(sentences)
&nbsp;
np.info(embeddings)
</pre>

<p>Vypíšou se spíše interní informace o matici, které však mohou být
v&nbsp;některých případech užitečné:</p>

<pre>
class:  ndarray
shape:  (6, 384)
strides:  (1536, 4)
itemsize:  4
aligned:  True
contiguous:  True
fortran:  False
data pointer: 0x55a259b4e060
byteorder:  little
byteswap:  False
type: float32
</pre>



<p><a name="k10"></a></p>
<h2 id="k10">10. Výpočet míry podobnosti jednotlivých vět</h2>

<p>Ve chvíli, kdy máme připravenu matici s&nbsp;&bdquo;vektorizovanými&ldquo;
větami, můžeme se pokusit o provedení dalších výpočtů. S&nbsp;využitím metody
nazvané <strong>model.similarity</strong> lze vypočítat (de facto) matici,
která bude obsahovat míry podobnosti jednotlivých vět. Pro původních šest vět
bude mít tato matice velikost 6&times;6 prvků, bude (logicky) symetrická okolo
hlavní diagonály a současně bude mít na hlavní diagonále jedničky (což je opět
pochopitelné &ndash; věta je nejvíce podobná sobě samé).</p>

<pre>
from sentence_transformers import SentenceTransformer
&nbsp;
model = SentenceTransformer("paraphrase-MiniLM-L6-v2")
&nbsp;
print(model)
&nbsp;
sentences = [
    "The rain in Spain falls mainly on the plain",
    "The tesselated polygon is a special type of polygon",
    "The quick brown fox jumps over the lazy dog",
    "To be or not to be, that is the question",
    "It is a truth universally acknowledged...",
    "The goat ran down the hill"
]
&nbsp;
embeddings = model.encode(sentences)
print(f"Embeddings shape: {embeddings.shape}")
&nbsp;
print(embeddings)
similarities = model.similarity(embeddings, embeddings)
print(similarities)
</pre>

<p>Po spuštění tohoto skriptu se nejdříve opět vypíše informace o modelu:</p>

<pre>
SentenceTransformer(
  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False, 'architecture': 'BertModel'})
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
)
</pre>

<p>Následně se vypíše informace o vytvořené matici (vektorizovaný text):</p>

<pre>
Embeddings shape: (6, 384)
[[ 0.3478464   0.16516833  0.80891246 ...  0.30405566 -0.03139801
   0.01735949]
 [ 0.1602744  -0.4778426   0.14267132 ...  0.35347039  0.0946853
   0.16065337]
 [ 0.6181423  -0.24776645 -0.23564643 ...  0.17215663  1.1165967
   0.63950485]
 [ 0.6088223   0.5830838   0.4046087  ...  0.08260182  0.8130692
   0.16788083]
 [ 0.11015981  0.10045841 -0.24472675 ... -0.06199175  0.46151286
  -0.12090196]
 [ 0.15316609 -0.05734093 -0.27605703 ...  0.01458593  0.24355106
  -0.44814658]]
</pre>

<p>A nakonec výpočtem získáme matici s&nbsp;podobnostmi jednotlivých vět
(nenechte se zmást, že typem je tenzor, tento tenzor druhého řádu je
maticí):</p>

<pre>
tensor([[ 1.0000,  0.0134, -0.0661,  0.0697,  0.0171,  0.0515],
        [ 0.0134,  1.0000, -0.0014, -0.0654, -0.0510,  0.0028],
        [-0.0661, -0.0014,  1.0000, -0.1279, -0.0577,  0.3191],
        [ 0.0697, -0.0654, -0.1279,  1.0000,  0.1503, -0.0457],
        [ 0.0171, -0.0510, -0.0577,  0.1503,  1.0000, -0.0364],
        [ 0.0515,  0.0028,  0.3191, -0.0457, -0.0364,  1.0000]])
</pre>



<p><a name="k11"></a></p>
<h2 id="k11">11. Využití knihovny Faiss pro získání původní věty na základě předaného vzorku</h2>

<p></p>



<p><a name="k12"></a></p>
<h2 id="k12">12. Instalace knihovny Faiss do virtuálního prostředí</h2>

<p>Příkazem <strong>uv add faiss-cpu</strong> doinstalujeme do našeho projektu
knihovnu Faiss:</p>

<pre>
$ <strong>uv add faiss-cpu</strong>
</pre>

<p>Projektový soubor <strong>pyproject.toml</strong> by se měl změnit do této
podoby (viz podtržený řádek):</p>

<pre>
[project]
name = "sentence-transformer"
version = "0.1.0"
description = "Sentence transformer demos"
authors = [
    {name = "Pavel Tisnovsky", email = "ptisnovs@redhat.com"},
]
dependencies = [
    <strong>"faiss-cpu&gt;=1.11.0.post1",</strong>
    "sentence-transformers&gt;=5.0.0",
]
requires-python = "==3.12.*"
readme = "README.md"
license = {text = "MIT"}
&nbsp;
&nbsp;
[tool.pdm]
distribution = false
</pre>

<p><div class="rs-tip-major">Poznámka: pochopitelně se změní i obsah souboru
<strong>uv.lock</strong>, nyní však nemusíme řešit konkrétní verze knihoven,
takže se jím nebudeme zabývat.</div></p>



<p><a name="k13"></a></p>
<h2 id="k13">13. Získání vět z&nbsp;původního seznamu na základě dotazu</h2>

<pre>
DIMENSIONS = embeddings.shape[1]

index = faiss.IndexFlatL2(DIMENSIONS)
index.add(embeddings)

print(f"Index: {index.ntotal}")


def find_similar_sentences(query_sentence, k):
    query_embedding = model.encode([query_sentence])
    distances, indices = index.search(query_embedding, k)
    print("-"*40)
    print(f"Query: {query_sentence}")
    print(f"Most {k} similar sentences:")
    for i, idx in enumerate(indices[0]):
        print(f"{i + 1}: {sentences[idx]} (Distance: {distances[0][i]})")
</pre>



<p><a name="k14"></a></p>
<h2 id="k14">14. Úplný zdrojový kód demonstračního příkladu</h2>

<pre>
from sentence_transformers import SentenceTransformer
import faiss

model = SentenceTransformer("paraphrase-MiniLM-L6-v2")

print(model)

sentences = [
    "The rain in Spain falls mainly on the plain",
    "The tesselated polygon is a special type of polygon",
    "The quick brown fox jumps over the lazy dog",
    "To be or not to be, that is the question",
    "It is a truth universally acknowledged...",
    "The goat ran down the hill"
]

# Generate embeddings for the sentences
embeddings = model.encode(sentences)
print(f"Embeddings shape: {embeddings.shape}")

similarities = model.similarity(embeddings, embeddings)

DIMENSIONS = embeddings.shape[1]

index = faiss.IndexFlatL2(DIMENSIONS)
index.add(embeddings)

print(f"Index: {index.ntotal}")


def find_similar_sentences(query_sentence, k):
    query_embedding = model.encode([query_sentence])
    distances, indices = index.search(query_embedding, k)
    print("-"*40)
    print(f"Query: {query_sentence}")
    print(f"Most {k} similar sentences:")
    for i, idx in enumerate(indices[0]):
        print(f"{i + 1}: {sentences[idx]} (Distance: {distances[0][i]})")


find_similar_sentences("The quick brown fox jumps over the lazy dog", 3)
find_similar_sentences("quick brown fox jumps over lazy dog", 3)
find_similar_sentences("The quick brown fox jumps over the angry dog", 3)
find_similar_sentences("The quick brown cat jumps over the lazy dog", 3)
</pre>



<p><a name="k15"></a></p>
<h2 id="k15">15. Výsledky dotazů</h2>



<p><a name="k16"></a></p>
<h2 id="k16">16. Jsou věty vybírány na základě textové podobnosti nebo jejich významu?</h2>

<pre>
from sentence_transformers import SentenceTransformer
import faiss

model = SentenceTransformer("paraphrase-MiniLM-L6-v2")

print(model)

sentences = [
    "The rain in Spain falls mainly on the plain",
    "The tesselated polygon is a special type of polygon",
    "The quick brown fox jumps over the lazy dog",
    "To be or not to be, that is the question",
    "It is a truth universally acknowledged...",
    "The goat ran down the hill"
]

embeddings = model.encode(sentences)
print(f"Embeddings shape: {embeddings.shape}")

similarities = model.similarity(embeddings, embeddings)

DIMENSIONS = embeddings.shape[1]

index = faiss.IndexFlatL2(DIMENSIONS)
index.add(embeddings)

print(f"Index: {index.ntotal}")


def find_similar_sentences(query_sentence, k):
    query_embedding = model.encode([query_sentence])
    distances, indices = index.search(query_embedding, k)
    print("-"*40)
    print(f"Query: {query_sentence}")
    print(f"Most {k} similar sentences:")
    for i, idx in enumerate(indices[0]):
        print(f"{i + 1}: {sentences[idx]} (Distance: {distances[0][i]})")


find_similar_sentences("The rain in Spain falls mainly on the plain", 3)
find_similar_sentences("The rain in Czechia falls mainly on the plain", 3)
find_similar_sentences("rainy weather in Spain, especially on plains", 3)
</pre>



<p><a name="k17"></a></p>
<h2 id="k17">17. Věty s&nbsp;podobným význame, ovšem zapsané naprosto odlišným způsobem</h2>

<pre>
from sentence_transformers import SentenceTransformer
import faiss

model = SentenceTransformer("paraphrase-MiniLM-L6-v2")

print(model)

sentences = [
    "The rain in Spain falls mainly on the plain",
    "The tesselated polygon is a special type of polygon",
    "The quick brown fox jumps over the lazy dog",
    "To be or not to be, that is the question",
    "It is a truth universally acknowledged...",
    "How old are you?",
    "The goat ran down the hill"
]

embeddings = model.encode(sentences)
print(f"Embeddings shape: {embeddings.shape}")

similarities = model.similarity(embeddings, embeddings)

DIMENSIONS = embeddings.shape[1]

index = faiss.IndexFlatL2(DIMENSIONS)
index.add(embeddings)

print(f"Index: {index.ntotal}")


def find_similar_sentences(query_sentence, k):
    query_embedding = model.encode([query_sentence])
    distances, indices = index.search(query_embedding, k)
    print("-"*40)
    print(f"Query: {query_sentence}")
    print(f"Most {k} similar sentences:")
    for i, idx in enumerate(indices[0]):
        print(f"{i + 1}: {sentences[idx]} (Distance: {distances[0][i]})")


find_similar_sentences("What is your age?", 3)
</pre>



<p><a name="k18"></a></p>
<h2 id="k18">18. Vyhledávání na základě kontextu (zobecnění provedené modelem)</h2>

<pre>
from sentence_transformers import SentenceTransformer
import faiss

model = SentenceTransformer("paraphrase-MiniLM-L6-v2")

print(model)

sentences = [
    "The rain in Spain falls mainly on the plain",
    "The tesselated polygon is a special type of polygon",
    "The quick brown fox jumps over the lazy dog",
    "To be or not to be, that is the question",
    "It is a truth universally acknowledged...",
    "How old are you?",
    "The goat ran down the hill"
]

embeddings = model.encode(sentences)
print(f"Embeddings shape: {embeddings.shape}")

similarities = model.similarity(embeddings, embeddings)

DIMENSIONS = embeddings.shape[1]

index = faiss.IndexFlatL2(DIMENSIONS)
index.add(embeddings)

print(f"Index: {index.ntotal}")


def find_similar_sentences(query_sentence, k):
    query_embedding = model.encode([query_sentence])
    distances, indices = index.search(query_embedding, k)
    print("-"*40)
    print(f"Query: {query_sentence}")
    print(f"Most {k} similar sentences:")
    for i, idx in enumerate(indices[0]):
        print(f"{i + 1}: {sentences[idx]} (Distance: {distances[0][i]})")


find_similar_sentences("Shakespeare", 3)
find_similar_sentences("animal", 3)
find_similar_sentences("geometry", 3)
find_similar_sentences("weather", 3)
</pre>



<p><a name="k19"></a></p>
<h2 id="k19">19. Repositář s&nbsp;demonstračními příklady</h2>

<p>Demonstrační příklady z&nbsp;dnešního článku lze nalézt na následujících
odkazech:</p>

<table>
<tr><th> #</th><th>Příklad</th><th>Stručný popis</th><th>Adresa</th></tr>
<tr><td> 1</td><td>transformer1.py</td><td>inicializace modelu <strong>paraphrase-MiniLM-L6-v2</strong> s&nbsp;výpisem základních informací o něm</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transformers/transformer1.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transfomers/transformer1.py</a></td></tr>
<tr><td> 2</td><td>transformer2.py</td><td>inicializace modelu <strong>all-mpnet-base-v2</strong> s&nbsp;výpisem základních informací o něm</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transformers/transformer2.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transfomers/transformer2.py</a></td></tr>
<tr><td> 3</td><td>transformer3.py</td><td>inicializace modelu <strong>Seznam/small-e-czech</strong> s&nbsp;výpisem základních informací o něm</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transformers/transformer3.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transfomers/transformer3.py</a></td></tr>
<tr><td> 4</td><td>transformer4.py</td><td>vektorizace textů (vět) s&nbsp;využitím zvoleného modelu</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transformers/transformer4.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transfomers/transformer4.py</a></td></tr>
<tr><td> 5</td><td>transformer5.py</td><td>informace o vypočtené matici</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transformers/transformer5.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transfomers/transformer5.py</a></td></tr>
<tr><td> 6</td><td>transformer6.py</td><td>výpočet a zobrazení vypočtené tabulky podobností</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transformers/transformer6.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transfomers/transformer6.py</a></td></tr>
<tr><td> 7</td><td>transformer7.py</td><td>nalezení významově nejpodobnějších vět s&nbsp;využitím vektorizované databáze textů</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transformers/transformer7.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transfomers/transformer7.py</a></td></tr>
<tr><td> 8</td><td>transformer8.py</td><td>nalezení významově nejpodobnějších vět s&nbsp;využitím vektorizované databáze textů</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transformers/transformer8.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transfomers/transformer8.py</a></td></tr>
<tr><td> 9</td><td>transformer9.py</td><td>nalezení vět nejbližších k&nbsp;zadanému termínu</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transformers/transformer9.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transfomers/transformer9.py</a></td></tr>
<tr><td>10</td><td>transformerA.py</td><td>sémantická podobnost</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transformers/transformerA.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transfomers/transformerA.py</a></td></tr>
<tr><td>11</td><td>pyproject.toml</td><td>soubor s&nbsp;projektem a definicí všech potřebných závislostí</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transformers/pyproject.toml">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transfomers/pyproject.toml</a></td></tr>
</table>



<p>Demonstrační příklady vytvořené v&nbsp;Pythonu a popsané <a
href="https://www.root.cz/clanky/faiss-knihovna-pro-rychle-a-efektivni-vyhledavani-podobnych-vektoru/">v&nbsp;předchozím</a>
i v&nbsp;článku <a
href="https://www.root.cz/clanky/faiss-knihovna-pro-rychle-a-efektivni-vyhledavani-podobnych-vektoru-2-cast/">FAISS:
knihovna pro rychlé a efektivní vyhledávání podobných vektorů (2. část)</a> <a
href="https://github.com/tisnik/most-popular-python-libs/">https://github.com/tisnik/most-popular-python-libs/</a>.
Následují odkazy na jednotlivé příklady:</p>

<table>
<tr><th> #</th><th>Příklad</th><th>Stručný popis</th><th>Adresa</th></tr>
<tr><td> 1</td><td>faiss-1.py</td><td>seznamy souřadnic bodů v&nbsp;rovině</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-1.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-1.py</a></td></tr>
<tr><td> 2</td><td>faiss-2.py</td><td>konstrukce matice se souřadnicemi bodů</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-2.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-2.py</a></td></tr>
<tr><td> 3</td><td>faiss-3.py</td><td>konstrukce indexu pro vyhledávání na základě vzdálenosti</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-3.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-3.py</a></td></tr>
<tr><td> 4</td><td>faiss-4.py</td><td>nalezení nejbližších bodů k&nbsp;zadaným souřadnicím &ndash; výpis indexů nalezených bodů</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-4.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-4.py</a></td></tr>
<tr><td> 5</td><td>faiss-5.py</td><td>nalezení nejbližších bodů k&nbsp;zadaným souřadnicím &ndash; výpis souřadnic nalezených bodů</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-5.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-5.py</a></td></tr>
<tr><td> 6</td><td>faiss-6.py</td><td>vyhledávání bodů na základě skalárního součinu bez normalizace vektorů</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-6.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-6.py</a></td></tr>
<tr><td> 7</td><td>faiss-7.py</td><td>vyhledávání bodů na základě skalárního součinu s&nbsp;normalizací vektorů</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-7.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-7.py</a></td></tr>
<tr><td> 8</td><td>faiss-8.py</td><td>jednoduchý benchmark rychlosti vyhledávání knihovnou FAISS</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-8.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-8.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td> 9</td><td>faiss-9.py</td><td>vizualizace koncových bodů vektorů v&nbsp;rovině</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-9.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-9.py</a></td></tr>
<tr><td>10</td><td>faiss-A.py</td><td>vykreslení nejpodobnějších vektorů získaných na základě L2 metriky</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-A.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-A.py</a></td></tr>
<tr><td>11</td><td>faiss-B.py</td><td>nalezení nejpodobnějších vektorů získaných na základě skalárního součinu: varianta s&nbsp;nenormovanými vektory</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-B.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-B.py</a></td></tr>
<tr><td>12</td><td>faiss-C.py</td><td>nalezení nejpodobnějších vektorů získaných na základě skalárního součinu: varianta s&nbsp;normovanými vektory</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-C.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-C.py</a></td></tr>
<tr><td>13</td><td>faiss-D.py</td><td>vykreslení nejpodobnějších vektorů před jejich normalizací</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-D.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-D.py</a></td></tr>
<tr><td>14</td><td>faiss-E.py</td><td>vykreslení vektorů formou orientovaných šipek</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-E.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-E.py</a></td></tr>
<tr><td>15</td><td>faiss-F.py</td><td>vykreslení vektorů po jejich normalizaci formou orientovaných šipek</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-F.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-F.py</a></td></tr>
<tr><td>16</td><td>faiss-G.py</td><td>vyhledání a vykreslení nejvíce NEpodobných vektorů</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-G.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-G.py</a></td></tr>
<tr><td>17</td><td>faiss-H.py</td><td>vyhledání podobných vektorů se složkami typu <i>float16</i></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-H.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-H.py</a></td></tr>
<tr><td>18</td><td>faiss-I.py</td><td>jednoduchý benchmark rychlosti vyhledávání knihovnou FAISS: rozdíly mezi <i>float16</i> a <i>float32</i></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-I.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-I.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>19</td><td>pyproject.toml</td><td>soubor s&nbsp;projektem a definicí závislostí</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/pyproject.toml">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/pyproject.toml</a></td></tr>
</table>



<p><a name="k20"></a></p>
<h2 id="k20">20. Odkazy na Internetu</h2>

<ol>

<li>FAISS (Facebook AI Similarity Search)<br />
<a href="https://en.wikipedia.org/wiki/FAISS">https://en.wikipedia.org/wiki/FAISS</a>
</li>

<li>FAISS documentation<br />
<a href="https://faiss.ai/">https://faiss.ai/</a>
</li>

<li>Introduction to Facebook AI Similarity Search (Faiss)<br />
<a href="https://www.pinecone.io/learn/series/faiss/faiss-tutorial/">https://www.pinecone.io/learn/series/faiss/faiss-tutorial/</a>
</li>

<li>Faiss: Efficient Similarity Search and Clustering of Dense Vectors<br />
<a href="https://medium.com/@pankaj_pandey/faiss-efficient-similarity-search-and-clustering-of-dense-vectors-dace1df1e235">https://medium.com/@pankaj_pandey/faiss-efficient-similarity-search-and-clustering-of-dense-vectors-dace1df1e235</a>
</li>

<li>Cosine Distance vs Dot Product vs Euclidean in vector similarity search<br />
<a href="https://medium.com/data-science-collective/cosine-distance-vs-dot-product-vs-euclidean-in-vector-similarity-search-227a6db32edb">https://medium.com/data-science-collective/cosine-distance-vs-dot-product-vs-euclidean-in-vector-similarity-search-227a6db32edb</a>
</li>

<li>F16C<br />
<a href="https://en.wikipedia.org/wiki/F16C">https://en.wikipedia.org/wiki/F16C</a>
</li>

<li>FP16 (AVX-512)<br />
<a href="https://en.wikipedia.org/wiki/AVX-512#FP16">https://en.wikipedia.org/wiki/AVX-512#FP16</a>
</li>

<li>Top 8 Vector Databases in 2025: Features, Use Cases, and Comparisons<br />
<a href="https://synapsefabric.com/top-8-vector-databases-in-2025-features-use-cases-and-comparisons/">https://synapsefabric.com/top-8-vector-databases-in-2025-features-use-cases-and-comparisons/</a>
</li>

<li>Is FAISS a Vector Database? Complete Guide<br />
<a href="https://mljourney.com/is-faiss-a-vector-database-complete-guide/">https://mljourney.com/is-faiss-a-vector-database-complete-guide/</a>
</li>

<li>Vector database<br />
<a href="https://en.wikipedia.org/wiki/Vector_database">https://en.wikipedia.org/wiki/Vector_database</a>
</li>

<li>Similarity search<br />
<a href="https://en.wikipedia.org/wiki/Similarity_search">https://en.wikipedia.org/wiki/Similarity_search</a>
</li>

<li>Nearest neighbor search<br />
<a href="https://en.wikipedia.org/wiki/Nearest_neighbor_search#Approximation_methods">https://en.wikipedia.org/wiki/Nearest_neighbor_search#Approximation_methods</a>
</li>

<li>Decoding Similarity Search with FAISS: A Practical Approach<br />
<a href="https://www.luminis.eu/blog/decoding-similarity-search-with-faiss-a-practical-approach/">https://www.luminis.eu/blog/decoding-similarity-search-with-faiss-a-practical-approach/</a>
</li>

<li>MetricType and distances<br />
<a href="https://github.com/facebookresearch/faiss/wiki/MetricType-and-distances">https://github.com/facebookresearch/faiss/wiki/MetricType-and-distances</a>
</li>

<li>RAG - Retrieval-augmented generation<br />
<a href="https://en.wikipedia.org/wiki/Retrieval-augmented_generation">https://en.wikipedia.org/wiki/Retrieval-augmented_generation</a>
</li>

<li>pgvector na GitHubu<br />
<a href="https://github.com/pgvector/pgvector">https://github.com/pgvector/pgvector</a>
</li>

<li>Why we replaced Pinecone with PGVector<br />
<a href="https://www.confident-ai.com/blog/why-we-replaced-pinecone-with-pgvector">https://www.confident-ai.com/blog/why-we-replaced-pinecone-with-pgvector</a>
</li>

<li>PostgreSQL as VectorDB - Beginner Tutorial<br />
<a href="https://www.youtube.com/watch?v=Ff3tJ4pJEa4">https://www.youtube.com/watch?v=Ff3tJ4pJEa4</a>
</li>

<li>What is a Vector Database? (neobsahuje odpověď na otázku v titulku :-)<br />
<a href="https://www.youtube.com/watch?v=t9IDoenf-lo">https://www.youtube.com/watch?v=t9IDoenf-lo</a>
</li>

<li>PGVector: Turn PostgreSQL Into A Vector Database<br />
<a href="https://www.youtube.com/watch?v=j1QcPSLj7u0">https://www.youtube.com/watch?v=j1QcPSLj7u0</a>
</li>

<li>Milvus<br />
<a href="https://milvus.io/">https://milvus.io/</a>
</li>

<li>Vector Databases simply explained! (Embeddings &amp; Indexes)<br />
<a href="https://www.youtube.com/watch?v=dN0lsF2cvm4">https://www.youtube.com/watch?v=dN0lsF2cvm4</a>
</li>

<li>Vector databases are so hot right now. WTF are they?<br />
<a href="https://www.youtube.com/watch?v=klTvEwg3oJ4">https://www.youtube.com/watch?v=klTvEwg3oJ4</a>
</li>

<li>Step-by-Step Guide to Installing “pgvector” and Loading Data in PostgreSQL<br />
<a href="https://medium.com/@besttechreads/step-by-step-guide-to-installing-pgvector-and-loading-data-in-postgresql-f2cffb5dec43">https://medium.com/@besttechreads/step-by-step-guide-to-installing-pgvector-and-loading-data-in-postgresql-f2cffb5dec43</a>
</li>

<li>Best 17 Vector Databases for 2025<br />
<a href="https://lakefs.io/blog/12-vector-databases-2023/">https://lakefs.io/blog/12-vector-databases-2023/</a>
</li>

<li>Top 15 Vector Databases that You Must Try in 2025<br />
<a href="https://www.geeksforgeeks.org/top-vector-databases/">https://www.geeksforgeeks.org/top-vector-databases/</a>
</li>

<li>Picking a vector database: a comparison and guide for 2023<br />
<a href="https://benchmark.vectorview.ai/vectordbs.html">https://benchmark.vectorview.ai/vectordbs.html</a>
</li>

<li>Top 9 Vector Databases as of Feburary 2025<br />
<a href="https://www.shakudo.io/blog/top-9-vector-databases">https://www.shakudo.io/blog/top-9-vector-databases</a>
</li>

<li>What is a vector database?<br />
<a href="https://www.ibm.com/think/topics/vector-database">https://www.ibm.com/think/topics/vector-database</a>
</li>

<li>SQL injection<br />
<a href="https://en.wikipedia.org/wiki/SQL_injection">https://en.wikipedia.org/wiki/SQL_injection</a>
</li>

<li>Cosine similarity<br />
<a href="https://en.wikipedia.org/wiki/Cosine_similarity">https://en.wikipedia.org/wiki/Cosine_similarity</a>
</li>

<li>Euclidean distance<br />
<a href="https://en.wikipedia.org/wiki/Euclidean_distance">https://en.wikipedia.org/wiki/Euclidean_distance</a>
</li>

<li>Dot product<br />
<a href="https://en.wikipedia.org/wiki/Dot_product">https://en.wikipedia.org/wiki/Dot_product</a>
</li>

<li>Hammingova vzdálenost<br />
<a href="https://cs.wikipedia.org/wiki/Hammingova_vzd%C3%A1lenost">https://cs.wikipedia.org/wiki/Hammingova_vzd%C3%A1lenost</a>
</li>

<li>Jaccard index<br />
<a href="https://en.wikipedia.org/wiki/Jaccard_index">https://en.wikipedia.org/wiki/Jaccard_index</a>
</li>

<li>Manhattanská metrika<br />
<a href="https://cs.wikipedia.org/wiki/Manhattansk%C3%A1_metrika">https://cs.wikipedia.org/wiki/Manhattansk%C3%A1_metrika</a>
</li>

<li>pgvector: vektorová databáze postavená na Postgresu<br />
<a href="https://www.root.cz/clanky/pgvector-vektorova-databaze-postavena-na-postgresu/">https://www.root.cz/clanky/pgvector-vektorova-databaze-postavena-na-postgresu/</a>
</li>

<li>Matplotlib Home Page<br />
<a href="http://matplotlib.org/">http://matplotlib.org/</a>
</li>

<li>matplotlib (Wikipedia)<br />
<a href="https://en.wikipedia.org/wiki/Matplotlib">https://en.wikipedia.org/wiki/Matplotlib</a>
</li>

<li>Dot Product<br />
<a href="https://mathworld.wolfram.com/DotProduct.html">https://mathworld.wolfram.com/DotProduct.html</a>
</li>

<li>FAISS and sentence-transformers in 5 Minutes<br />
<a href="https://www.stephendiehl.com/posts/faiss/">https://www.stephendiehl.com/posts/faiss/</a>
</li>

<li>Sentence Transformer: Quickstart<br />
<a href="https://sbert.net/docs/quickstart.html#sentence-transformer">https://sbert.net/docs/quickstart.html#sentence-transformer</a>
</li>

<li>Sentence Transformers: Embeddings, Retrieval, and Reranking<br />
<a href="https://pypi.org/project/sentence-transformers/">https://pypi.org/project/sentence-transformers/</a>
</li>

<li>uv<br />
<a href="https://docs.astral.sh/uv/">https://docs.astral.sh/uv/</a>
</li>

<li>A Gentle Introduction to Retrieval Augmented Generation (RAG)<br />
<a href="https://wandb.ai/cosmo3769/RAG/reports/A-Gentle-Introduction-to-Retrieval-Augmented-Generation-RAG---Vmlldzo1MjM4Mjk1">https://wandb.ai/cosmo3769/RAG/reports/A-Gentle-Introduction-to-Retrieval-Augmented-Generation-RAG---Vmlldzo1MjM4Mjk1</a>
</li>

<li>The Beginner’s Guide to Text Embeddings<br />
<a href="https://www.deepset.ai/blog/the-beginners-guide-to-text-embeddings">https://www.deepset.ai/blog/the-beginners-guide-to-text-embeddings</a>
</li>

<li>What are Word Embeddings?<br />
<a href="https://www.youtube.com/watch?v=wgfSDrqYMJ4">https://www.youtube.com/watch?v=wgfSDrqYMJ4</a>
</li>

<li>How to choose an embedding model<br />
<a href="https://www.youtube.com/watch?v=djp4205tHGU">https://www.youtube.com/watch?v=djp4205tHGU</a>
</li>

<li>What is a Vector Database? Powering Semantic Search &amp; AI Applications<br />
<a href="https://www.youtube.com/watch?v=gl1r1XV0SLw">https://www.youtube.com/watch?v=gl1r1XV0SLw</a>
</li>

<li>How do Sentence Transformers differ from traditional word embedding models like Word2Vec or GloVe?<br />
<a href="https://zilliz.com/ai-faq/how-do-sentence-transformers-differ-from-traditional-word-embedding-models-like-word2vec-or-glove">https://zilliz.com/ai-faq/how-do-sentence-transformers-differ-from-traditional-word-embedding-models-like-word2vec-or-glove</a>
</li>

<li>BERT (language model)<br />
<a href="https://en.wikipedia.org/wiki/BERT_(language_model)">https://en.wikipedia.org/wiki/BERT_(language_model)</a>
</li>

</ol>



<p></p><p></p>
<p><small>Autor: <a href="https://github.com/tisnik/">Pavel Tišnovský</a> &nbsp; 2025</small></p>
</body>
</html>

