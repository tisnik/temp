<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
        "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html>
<head>
<title></title>
<meta name="Author" content="Pavel Tisnovsky" />
<meta name="Generator" content="vim" />
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<style type="text/css">
         body {color:#000000; background:#ffffff;}
         h1  {font-family: arial, helvetica, sans-serif; color:#ffffff; background-color:#c00000; text-align:center; padding-left:1em}
         h2  {font-family: arial, helvetica, sans-serif; color:#ffffff; background-color:#0000c0; padding-left:1em; text-align:left}
         h3  {font-family: arial, helvetica, sans-serif; color:#000000; background-color:#c0c0c0; padding-left:1em; text-align:left}
         h4  {font-family: arial, helvetica, sans-serif; color:#000000; background-color:#e0e0e0; padding-left:1em; text-align:left}
         a   {font-family: arial, helvetica, sans-serif;}
         li  {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify; width:450px;}
         ol  {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify; width:450px;}
         ul  {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify; width:450px;}
         p   {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify;}
         pre {background:#e0e0e0}
</style>
</head>

<body>

<h1></h1>

<h3>Pavel Tišnovský</h3>

<p></p>

<h1>Úvodník</h1>

<p></p>



<h2>Obsah</h2>

<p><a href="#k01">*** 1. </a></p>
<p><a href="#k02">*** 2. </a></p>
<p><a href="#k03">*** 3. </a></p>
<p><a href="#k04">*** 4. </a></p>
<p><a href="#k05">*** 5. </a></p>
<p><a href="#k06">*** 6. </a></p>
<p><a href="#k07">*** 7. </a></p>
<p><a href="#k08">*** 8. </a></p>
<p><a href="#k09">*** 9. </a></p>
<p><a href="#k10">*** 10. </a></p>
<p><a href="#k11">*** 11. </a></p>
<p><a href="#k12">*** 12. </a></p>
<p><a href="#k13">*** 13. </a></p>
<p><a href="#k14">*** 14. </a></p>
<p><a href="#k15">*** 15. </a></p>
<p><a href="#k16">*** 16. </a></p>
<p><a href="#k17">*** 17. </a></p>
<p><a href="#k18">*** 18. </a></p>
<p><a href="#k19">*** 19. Repositář s&nbsp;demonstračními příklady</a></p>
<p><a href="#k20">*** 20. Odkazy na Internetu</a></p>



<p><a name="k01"></a></p>
<h2 id="k01">1. </h2>

<pre>
from datasets import load_dataset

dataset_id = "polygraf-ai/human-sentences-1M-sample-v2"

dataset = load_dataset(dataset_id)

print(dataset)
</pre>



<p><a name="k02"></a></p>
<h2 id="k02">2. </h2>

<pre>
from datasets import load_dataset

dataset_id = "polygraf-ai/human-sentences-1M-sample-v2"

dataset = load_dataset(dataset_id, split="train")

print(dataset.features)
</pre>



<p><a name="k03"></a></p>
<h2 id="k03">3. </h2>

<pre>
from datasets import load_dataset

dataset_id = "polygraf-ai/human-sentences-1M-sample-v2"

dataset = load_dataset(dataset_id, split="train")

print("Building sentences")
sentences = [sentence for sentence in dataset["text"]]

print(f"{len(sentences)} sentences created")
</pre>



<p><a name="k04"></a></p>
<h2 id="k04">4. </h2>

<pre>
from datasets import load_dataset

dataset_id = "polygraf-ai/human-sentences-1M-sample-v2"

dataset = load_dataset(dataset_id, split="train")

print("Building sentences")
sentences = [sentence for sentence in dataset["text"][0:100]]

print(f"{len(sentences)} sentences created")
</pre>



<p><a name="k05"></a></p>
<h2 id="k05">5. </h2>



<p><a name="k06"></a></p>
<h2 id="k06">6. </h2>

<pre>
from sentence_transformers import SentenceTransformer
import faiss

model = SentenceTransformer("paraphrase-MiniLM-L6-v2")

print(model)

sentences = [
    "The rain in Spain falls mainly on the plain",
    "The tesselated polygon is a special type of polygon",
    "The quick brown fox jumps over the lazy dog",
    "To be or not to be, that is the question",
    "It is a truth universally acknowledged...",
    "How old are you?",
    "The goat ran down the hill"
]

embeddings = model.encode(sentences)
print(f"Embeddings shape: {embeddings.shape}")

similarities = model.similarity(embeddings, embeddings)

DIMENSIONS = embeddings.shape[1]

index = faiss.IndexFlatL2(DIMENSIONS)
index.add(embeddings)

print(f"Index: {index.ntotal}")


def find_similar_sentences(query_sentence, k):
    query_embedding = model.encode([query_sentence])
    distances, indices = index.search(query_embedding, k)
    print("-"*40)
    print(f"Query: {query_sentence}")
    print(f"Most {k} similar sentences:")
    for i, idx in enumerate(indices[0]):
        print(f"{i + 1}: {sentences[idx]} (Distance: {distances[0][i]})")


find_similar_sentences("Shakespeare", 3)
find_similar_sentences("animal", 3)
find_similar_sentences("geometry", 3)
find_similar_sentences("weather", 3)
</pre>

<pre>
SentenceTransformer(
  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False, 'architecture': 'BertModel'})
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
)
Embeddings shape: (7, 384)
Index: 7
----------------------------------------
Query: Shakespeare
Most 3 similar sentences:
1: To be or not to be, that is the question (Distance: 83.53305053710938)
2: The tesselated polygon is a special type of polygon (Distance: 112.60221862792969)
3: The rain in Spain falls mainly on the plain (Distance: 114.61812591552734)
----------------------------------------
Query: animal
Most 3 similar sentences:
1: The goat ran down the hill (Distance: 67.3703384399414)
2: The quick brown fox jumps over the lazy dog (Distance: 68.25883483886719)
3: To be or not to be, that is the question (Distance: 82.82962036132812)
----------------------------------------
Query: geometry
Most 3 similar sentences:
1: The tesselated polygon is a special type of polygon (Distance: 51.57851791381836)
2: To be or not to be, that is the question (Distance: 91.08709716796875)
3: The goat ran down the hill (Distance: 109.95413208007812)
----------------------------------------
Query: weather
Most 3 similar sentences:
1: The rain in Spain falls mainly on the plain (Distance: 73.53900146484375)
2: To be or not to be, that is the question (Distance: 86.14794921875)
3: How old are you? (Distance: 101.28636169433594)
</pre>


<p><a name="k07"></a></p>
<h2 id="k07">7. </h2>



<p><a name="k08"></a></p>
<h2 id="k08">8. </h2>

<pre>
from sentence_transformers import SentenceTransformer
import faiss
from datasets import load_dataset


model = SentenceTransformer("paraphrase-MiniLM-L6-v2")
print(model)

dataset = load_dataset("polygraf-ai/human-sentences-1M-sample-v2", split="train")
print(dataset)

sentences = [sentence for sentence in dataset["text"][0:1000]]
print(f"{len(sentences)} sentences created")

embeddings = model.encode(sentences)
print(f"Embeddings shape: {embeddings.shape}")

DIMENSIONS = embeddings.shape[1]
index = faiss.IndexFlatL2(DIMENSIONS)
index.add(embeddings)
print(f"Index: {index.ntotal}")


def find_similar_sentences(query_sentence, k):
    query_embedding = model.encode([query_sentence])
    distances, indices = index.search(query_embedding, k)
    print("-"*40)
    print(f"Query: {query_sentence}")
    print(f"Most {k} similar sentences:")
    for i, idx in enumerate(indices[0]):
        print(f"{i + 1}: {sentences[idx]} (Distance: {distances[0][i]})")


find_similar_sentences("city", 3)
find_similar_sentences("animal", 3)
find_similar_sentences("geometry", 3)
find_similar_sentences("weather", 3)
find_similar_sentences("game", 3)
find_similar_sentences("school", 3)
</pre>



<p><a name="k09"></a></p>
<h2 id="k09">9. </h2>



<p><a name="k10"></a></p>
<h2 id="k10">10. </h2>

<pre>
from sentence_transformers import SentenceTransformer
import faiss
from datasets import load_dataset

MODEL_NAME = "paraphrase-MiniLM-L6-v2"
DATASET_ID = "polygraf-ai/human-sentences-1M-sample-v2"


def initialize_model(model_name):
    print("Model initialization started")
    model = SentenceTransformer(model_name)
    print(model)
    print("Model initialization finished")
    return model


def load_dataset_by_id(dataset_id):
    print("Loading dataset started")
    dataset = load_dataset(dataset_id, split="train")
    print("Loading dataset finished")
    return dataset


def build_sentences(dataset, from_, to_):
    print("Building sentences")
    sentences = [sentence for sentence in dataset["text"][from_:to_]]
    print(f"{len(sentences)} sentences created")
    return sentences


def create_embeddings(model, sentences):
    print("Embedding started")
    embeddings = model.encode(sentences)
    print(f"Embeddings shape: {embeddings.shape}")
    print("Embedding finished")
    return embeddings


def create_faiss_index(embeddings):
    print("FAISS index construction started")
    DIMENSIONS = embeddings.shape[1]
    index = faiss.IndexFlatL2(DIMENSIONS)
    index.add(embeddings)
    print(f"Index: {index.ntotal}")
    print("FAISS index construction finished")
    return index


def find_similar_sentences(model, index, query_sentence, k):
    query_embedding = model.encode([query_sentence])
    distances, indices = index.search(query_embedding, k)
    print("-"*40)
    print(f"Query: {query_sentence}")
    print(f"Most {k} similar sentences:")
    for i, idx in enumerate(indices[0]):
        print(f"{i + 1}: {sentences[idx]} (Distance: {distances[0][i]})")


model = initialize_model(MODEL_NAME)
dataset = load_dataset_by_id(DATASET_ID)
sentences = build_sentences(dataset, 0, 1000)
embeddings = create_embeddings(model, sentences)
index = create_faiss_index(embeddings)

find_similar_sentences(model, index, "city", 3)
find_similar_sentences(model, index, "animal", 3)
find_similar_sentences(model, index, "geometry", 3)
find_similar_sentences(model, index, "weather", 3)
find_similar_sentences(model, index, "game", 3)
find_similar_sentences(model, index, "school", 3)
</pre>



<p><a name="k11"></a></p>
<h2 id="k11">11. </h2>



<p><a name="k12"></a></p>
<h2 id="k12">12. </h2>

<pre>
from sentence_transformers import SentenceTransformer
import faiss
from datasets import load_dataset
from time import time

MODEL_NAME = "paraphrase-MiniLM-L6-v2"
DATASET_ID = "polygraf-ai/human-sentences-1M-sample-v2"


def initialize_model(model_name):
    print("Model initialization started")
    model = SentenceTransformer(model_name)
    print(model)
    print("Model initialization finished")
    return model


def load_dataset_by_id(dataset_id):
    print("Loading dataset started")
    dataset = load_dataset(dataset_id, split="train")
    print("Loading dataset finished")
    return dataset


def build_sentences(dataset, from_, to_):
    print("Building sentences")
    sentences = [sentence for sentence in dataset["text"][from_:to_]]
    print(f"{len(sentences)} sentences created")
    return sentences


def create_embeddings(model, sentences):
    print("Embedding started")
    embeddings = model.encode(sentences)
    print(f"Embeddings shape: {embeddings.shape}")
    print("Embedding finished")
    return embeddings


def create_faiss_index(embeddings):
    print("FAISS index construction started")
    DIMENSIONS = embeddings.shape[1]
    index = faiss.IndexFlatL2(DIMENSIONS)
    index.add(embeddings)
    print(f"Index: {index.ntotal}")
    print("FAISS index construction finished")
    return index


def find_similar_sentences(model, index, query_sentence, k):
    t1 = time()
    query_embedding = model.encode([query_sentence])
    distances, indices = index.search(query_embedding, k)
    print("-"*40)
    print(f"Query: {query_sentence}")
    print(f"Most {k} similar sentences:")
    for i, idx in enumerate(indices[0]):
        print(f"{i + 1}: {sentences[idx]} (Distance: {distances[0][i]})")
    t2 = time()
    print(f"Finished in {t2-t1:3.2} seconds")


model = initialize_model(MODEL_NAME)
dataset = load_dataset_by_id(DATASET_ID)
sentences = build_sentences(dataset, 0, 100000)
embeddings = create_embeddings(model, sentences)
index = create_faiss_index(embeddings)

find_similar_sentences(model, index, "city", 3)
find_similar_sentences(model, index, "animal", 3)
find_similar_sentences(model, index, "geometry", 3)
find_similar_sentences(model, index, "weather", 3)
find_similar_sentences(model, index, "game", 3)
find_similar_sentences(model, index, "school", 3)
</pre>



<p><a name="k13"></a></p>
<h2 id="k13">13. </h2>



<p><a name="k14"></a></p>
<h2 id="k14">14. </h2>



<p><a name="k15"></a></p>
<h2 id="k15">15. </h2>



<p><a name="k16"></a></p>
<h2 id="k16">16. </h2>



<p><a name="k17"></a></p>
<h2 id="k17">17. </h2>



<p><a name="k18"></a></p>
<h2 id="k18">18. </h2>



<p><a name="k19"></a></p>
<h2 id="k19">19. Repositář s&nbsp;demonstračními příklady</h2>

<p>Demonstrační příklady <a
href="https://www.root.cz/clanky/knihovna-faiss-a-embedding-zaklad-jazykovych-modelu/">z&nbsp;předchozího</a>
i dnešního článku lze nalézt na následujících odkazech:</p>

<table>
<tr><th> #</th><th>Příklad</th><th>Stručný popis</th><th>Adresa</th></tr>
<tr><td> 1</td><td>transformer1.py</td><td>inicializace modelu <strong>paraphrase-MiniLM-L6-v2</strong> přes knihovnu sentence-transformers s&nbsp;výpisem základních informací o něm</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss_transformers/transformer1.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transfomers/transformer1.py</a></td></tr>
<tr><td> 2</td><td>transformer2.py</td><td>inicializace modelu <strong>all-mpnet-base-v2</strong> přes knihovnu sentence-transformers s&nbsp;výpisem základních informací o něm</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss_transformers/transformer2.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transfomers/transformer2.py</a></td></tr>
<tr><td> 3</td><td>transformer3.py</td><td>inicializace modelu <strong>Seznam/small-e-czech</strong> přes knihovnu sentence-transformers s&nbsp;výpisem základních informací o něm</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss_transformers/transformer3.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transfomers/transformer3.py</a></td></tr>
<tr><td> 4</td><td>transformer4.py</td><td>vektorizace textů (vět) přes knihovnu sentence-transformers s&nbsp;využitím zvoleného modelu</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss_transformers/transformer4.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transfomers/transformer4.py</a></td></tr>
<tr><td> 5</td><td>transformer5.py</td><td>informace o vypočtené matici s&nbsp;vektorizovaným textem (<i>embedding</i>)</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss_transformers/transformer5.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transfomers/transformer5.py</a></td></tr>
<tr><td> 6</td><td>transformer6.py</td><td>výpočet a zobrazení vypočtené tabulky podobností</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss_transformers/transformer6.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transfomers/transformer6.py</a></td></tr>
<tr><td> 7</td><td>transformer7.py</td><td>nalezení významově nejpodobnějších vět s&nbsp;využitím vektorizované databáze textů</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss_transformers/transformer7.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transfomers/transformer7.py</a></td></tr>
<tr><td> 8</td><td>transformer8.py</td><td>nalezení významově nejpodobnějších vět s&nbsp;využitím vektorizované databáze textů</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss_transformers/transformer8.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transfomers/transformer8.py</a></td></tr>
<tr><td> 9</td><td>transformer9.py</td><td>nalezení vět nejbližších k&nbsp;zadanému termínu</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss_transformers/transformer9.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transfomers/transformer9.py</a></td></tr>
<tr><td>10</td><td>transformerA.py</td><td>podpora pro hledání na základě sémantické podobnosti</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss_transformers/transformerA.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transfomers/transformerA.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>11</td><td>pyproject.toml</td><td>soubor s&nbsp;projektem a definicí všech potřebných závislostí</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss_transformers/pyproject.toml">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transfomers/pyproject.toml</a></td></tr>
</table>



<p>Demonstrační příklady vytvořené v&nbsp;Pythonu a popsané <a
href="https://www.root.cz/clanky/faiss-knihovna-pro-rychle-a-efektivni-vyhledavani-podobnych-vektoru/">v&nbsp;předchozím</a>
i v&nbsp;článku <a
href="https://www.root.cz/clanky/faiss-knihovna-pro-rychle-a-efektivni-vyhledavani-podobnych-vektoru-2-cast/">FAISS:
knihovna pro rychlé a efektivní vyhledávání podobných vektorů (2. část)</a> <a
href="https://github.com/tisnik/most-popular-python-libs/">https://github.com/tisnik/most-popular-python-libs/</a>.
Následují odkazy na jednotlivé příklady:</p>

<table>
<tr><th> #</th><th>Příklad</th><th>Stručný popis</th><th>Adresa</th></tr>
<tr><td> 1</td><td>faiss-1.py</td><td>seznamy souřadnic bodů v&nbsp;rovině</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-1.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-1.py</a></td></tr>
<tr><td> 2</td><td>faiss-2.py</td><td>konstrukce matice se souřadnicemi bodů</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-2.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-2.py</a></td></tr>
<tr><td> 3</td><td>faiss-3.py</td><td>konstrukce indexu pro vyhledávání na základě vzdálenosti</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-3.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-3.py</a></td></tr>
<tr><td> 4</td><td>faiss-4.py</td><td>nalezení nejbližších bodů k&nbsp;zadaným souřadnicím &ndash; výpis indexů nalezených bodů</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-4.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-4.py</a></td></tr>
<tr><td> 5</td><td>faiss-5.py</td><td>nalezení nejbližších bodů k&nbsp;zadaným souřadnicím &ndash; výpis souřadnic nalezených bodů</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-5.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-5.py</a></td></tr>
</table>



<p><a name="k20"></a></p>
<h2 id="k20">20. Odkazy na Internetu</h2>

<ol>

<li>FAISS: knihovna pro rychlé a efektivní vyhledávání podobných vektorů<br />
<a href="https://www.root.cz/clanky/faiss-knihovna-pro-rychle-a-efektivni-vyhledavani-podobnych-vektoru/">https://www.root.cz/clanky/faiss-knihovna-pro-rychle-a-efektivni-vyhledavani-podobnych-vektoru/</a>
</li>

<li>FAISS: knihovna pro rychlé a efektivní vyhledávání podobných vektorů (2. část)<br />
<a href="https://www.root.cz/clanky/faiss-knihovna-pro-rychle-a-efektivni-vyhledavani-podobnych-vektoru-2-cast/">https://www.root.cz/clanky/faiss-knihovna-pro-rychle-a-efektivni-vyhledavani-podobnych-vektoru-2-cast/</a>
</li>

<li>Knihovna FAISS a embedding: základ jazykových modelů<br />
<a href="https://www.root.cz/clanky/knihovna-faiss-a-embedding-zaklad-jazykovych-modelu/">https://www.root.cz/clanky/knihovna-faiss-a-embedding-zaklad-jazykovych-modelu/</a>
</li>

<li>FAISS (Facebook AI Similarity Search)<br />
<a href="https://en.wikipedia.org/wiki/FAISS">https://en.wikipedia.org/wiki/FAISS</a>
</li>

<li>FAISS documentation<br />
<a href="https://faiss.ai/">https://faiss.ai/</a>
</li>

<li>Introduction to Facebook AI Similarity Search (Faiss)<br />
<a href="https://www.pinecone.io/learn/series/faiss/faiss-tutorial/">https://www.pinecone.io/learn/series/faiss/faiss-tutorial/</a>
</li>

<li>Faiss: Efficient Similarity Search and Clustering of Dense Vectors<br />
<a href="https://medium.com/@pankaj_pandey/faiss-efficient-similarity-search-and-clustering-of-dense-vectors-dace1df1e235">https://medium.com/@pankaj_pandey/faiss-efficient-similarity-search-and-clustering-of-dense-vectors-dace1df1e235</a>
</li>

<li>Cosine Distance vs Dot Product vs Euclidean in vector similarity search<br />
<a href="https://medium.com/data-science-collective/cosine-distance-vs-dot-product-vs-euclidean-in-vector-similarity-search-227a6db32edb">https://medium.com/data-science-collective/cosine-distance-vs-dot-product-vs-euclidean-in-vector-similarity-search-227a6db32edb</a>
</li>

<li>F16C<br />
<a href="https://en.wikipedia.org/wiki/F16C">https://en.wikipedia.org/wiki/F16C</a>
</li>

<li>FP16 (AVX-512)<br />
<a href="https://en.wikipedia.org/wiki/AVX-512#FP16">https://en.wikipedia.org/wiki/AVX-512#FP16</a>
</li>

<li>Top 8 Vector Databases in 2025: Features, Use Cases, and Comparisons<br />
<a href="https://synapsefabric.com/top-8-vector-databases-in-2025-features-use-cases-and-comparisons/">https://synapsefabric.com/top-8-vector-databases-in-2025-features-use-cases-and-comparisons/</a>
</li>

<li>Is FAISS a Vector Database? Complete Guide<br />
<a href="https://mljourney.com/is-faiss-a-vector-database-complete-guide/">https://mljourney.com/is-faiss-a-vector-database-complete-guide/</a>
</li>

<li>Vector database<br />
<a href="https://en.wikipedia.org/wiki/Vector_database">https://en.wikipedia.org/wiki/Vector_database</a>
</li>

<li>Similarity search<br />
<a href="https://en.wikipedia.org/wiki/Similarity_search">https://en.wikipedia.org/wiki/Similarity_search</a>
</li>

<li>Nearest neighbor search<br />
<a href="https://en.wikipedia.org/wiki/Nearest_neighbor_search#Approximation_methods">https://en.wikipedia.org/wiki/Nearest_neighbor_search#Approximation_methods</a>
</li>

<li>Decoding Similarity Search with FAISS: A Practical Approach<br />
<a href="https://www.luminis.eu/blog/decoding-similarity-search-with-faiss-a-practical-approach/">https://www.luminis.eu/blog/decoding-similarity-search-with-faiss-a-practical-approach/</a>
</li>

<li>MetricType and distances<br />
<a href="https://github.com/facebookresearch/faiss/wiki/MetricType-and-distances">https://github.com/facebookresearch/faiss/wiki/MetricType-and-distances</a>
</li>

<li>RAG - Retrieval-augmented generation<br />
<a href="https://en.wikipedia.org/wiki/Retrieval-augmented_generation">https://en.wikipedia.org/wiki/Retrieval-augmented_generation</a>
</li>

<li>pgvector na GitHubu<br />
<a href="https://github.com/pgvector/pgvector">https://github.com/pgvector/pgvector</a>
</li>

<li>Why we replaced Pinecone with PGVector<br />
<a href="https://www.confident-ai.com/blog/why-we-replaced-pinecone-with-pgvector">https://www.confident-ai.com/blog/why-we-replaced-pinecone-with-pgvector</a>
</li>

<li>PostgreSQL as VectorDB - Beginner Tutorial<br />
<a href="https://www.youtube.com/watch?v=Ff3tJ4pJEa4">https://www.youtube.com/watch?v=Ff3tJ4pJEa4</a>
</li>

<li>What is a Vector Database? (neobsahuje odpověď na otázku v titulku :-)<br />
<a href="https://www.youtube.com/watch?v=t9IDoenf-lo">https://www.youtube.com/watch?v=t9IDoenf-lo</a>
</li>

<li>PGVector: Turn PostgreSQL Into A Vector Database<br />
<a href="https://www.youtube.com/watch?v=j1QcPSLj7u0">https://www.youtube.com/watch?v=j1QcPSLj7u0</a>
</li>

<li>Milvus<br />
<a href="https://milvus.io/">https://milvus.io/</a>
</li>

<li>Vector Databases simply explained! (Embeddings &amp; Indexes)<br />
<a href="https://www.youtube.com/watch?v=dN0lsF2cvm4">https://www.youtube.com/watch?v=dN0lsF2cvm4</a>
</li>

<li>Vector databases are so hot right now. WTF are they?<br />
<a href="https://www.youtube.com/watch?v=klTvEwg3oJ4">https://www.youtube.com/watch?v=klTvEwg3oJ4</a>
</li>

<li>Step-by-Step Guide to Installing “pgvector” and Loading Data in PostgreSQL<br />
<a href="https://medium.com/@besttechreads/step-by-step-guide-to-installing-pgvector-and-loading-data-in-postgresql-f2cffb5dec43">https://medium.com/@besttechreads/step-by-step-guide-to-installing-pgvector-and-loading-data-in-postgresql-f2cffb5dec43</a>
</li>

<li>Best 17 Vector Databases for 2025<br />
<a href="https://lakefs.io/blog/12-vector-databases-2023/">https://lakefs.io/blog/12-vector-databases-2023/</a>
</li>

<li>Top 15 Vector Databases that You Must Try in 2025<br />
<a href="https://www.geeksforgeeks.org/top-vector-databases/">https://www.geeksforgeeks.org/top-vector-databases/</a>
</li>

<li>Picking a vector database: a comparison and guide for 2023<br />
<a href="https://benchmark.vectorview.ai/vectordbs.html">https://benchmark.vectorview.ai/vectordbs.html</a>
</li>

<li>Top 9 Vector Databases as of Feburary 2025<br />
<a href="https://www.shakudo.io/blog/top-9-vector-databases">https://www.shakudo.io/blog/top-9-vector-databases</a>
</li>

<li>What is a vector database?<br />
<a href="https://www.ibm.com/think/topics/vector-database">https://www.ibm.com/think/topics/vector-database</a>
</li>

<li>SQL injection<br />
<a href="https://en.wikipedia.org/wiki/SQL_injection">https://en.wikipedia.org/wiki/SQL_injection</a>
</li>

<li>Cosine similarity<br />
<a href="https://en.wikipedia.org/wiki/Cosine_similarity">https://en.wikipedia.org/wiki/Cosine_similarity</a>
</li>

<li>Euclidean distance<br />
<a href="https://en.wikipedia.org/wiki/Euclidean_distance">https://en.wikipedia.org/wiki/Euclidean_distance</a>
</li>

<li>Dot product<br />
<a href="https://en.wikipedia.org/wiki/Dot_product">https://en.wikipedia.org/wiki/Dot_product</a>
</li>

<li>Hammingova vzdálenost<br />
<a href="https://cs.wikipedia.org/wiki/Hammingova_vzd%C3%A1lenost">https://cs.wikipedia.org/wiki/Hammingova_vzd%C3%A1lenost</a>
</li>

<li>Jaccard index<br />
<a href="https://en.wikipedia.org/wiki/Jaccard_index">https://en.wikipedia.org/wiki/Jaccard_index</a>
</li>

<li>Manhattanská metrika<br />
<a href="https://cs.wikipedia.org/wiki/Manhattansk%C3%A1_metrika">https://cs.wikipedia.org/wiki/Manhattansk%C3%A1_metrika</a>
</li>

<li>pgvector: vektorová databáze postavená na Postgresu<br />
<a href="https://www.root.cz/clanky/pgvector-vektorova-databaze-postavena-na-postgresu/">https://www.root.cz/clanky/pgvector-vektorova-databaze-postavena-na-postgresu/</a>
</li>

<li>Matplotlib Home Page<br />
<a href="http://matplotlib.org/">http://matplotlib.org/</a>
</li>

<li>matplotlib (Wikipedia)<br />
<a href="https://en.wikipedia.org/wiki/Matplotlib">https://en.wikipedia.org/wiki/Matplotlib</a>
</li>

<li>Dot Product<br />
<a href="https://mathworld.wolfram.com/DotProduct.html">https://mathworld.wolfram.com/DotProduct.html</a>
</li>

<li>FAISS and sentence-transformers in 5 Minutes<br />
<a href="https://www.stephendiehl.com/posts/faiss/">https://www.stephendiehl.com/posts/faiss/</a>
</li>

<li>Sentence Transformer: Quickstart<br />
<a href="https://sbert.net/docs/quickstart.html#sentence-transformer">https://sbert.net/docs/quickstart.html#sentence-transformer</a>
</li>

<li>Sentence Transformers: Embeddings, Retrieval, and Reranking<br />
<a href="https://pypi.org/project/sentence-transformers/">https://pypi.org/project/sentence-transformers/</a>
</li>

<li>uv<br />
<a href="https://docs.astral.sh/uv/">https://docs.astral.sh/uv/</a>
</li>

<li>A Gentle Introduction to Retrieval Augmented Generation (RAG)<br />
<a href="https://wandb.ai/cosmo3769/RAG/reports/A-Gentle-Introduction-to-Retrieval-Augmented-Generation-RAG---Vmlldzo1MjM4Mjk1">https://wandb.ai/cosmo3769/RAG/reports/A-Gentle-Introduction-to-Retrieval-Augmented-Generation-RAG---Vmlldzo1MjM4Mjk1</a>
</li>

<li>The Beginner’s Guide to Text Embeddings<br />
<a href="https://www.deepset.ai/blog/the-beginners-guide-to-text-embeddings">https://www.deepset.ai/blog/the-beginners-guide-to-text-embeddings</a>
</li>

<li>What are Word Embeddings?<br />
<a href="https://www.youtube.com/watch?v=wgfSDrqYMJ4">https://www.youtube.com/watch?v=wgfSDrqYMJ4</a>
</li>

<li>How to choose an embedding model<br />
<a href="https://www.youtube.com/watch?v=djp4205tHGU">https://www.youtube.com/watch?v=djp4205tHGU</a>
</li>

<li>What is a Vector Database? Powering Semantic Search &amp; AI Applications<br />
<a href="https://www.youtube.com/watch?v=gl1r1XV0SLw">https://www.youtube.com/watch?v=gl1r1XV0SLw</a>
</li>

<li>How do Sentence Transformers differ from traditional word embedding models like Word2Vec or GloVe?<br />
<a href="https://zilliz.com/ai-faq/how-do-sentence-transformers-differ-from-traditional-word-embedding-models-like-word2vec-or-glove">https://zilliz.com/ai-faq/how-do-sentence-transformers-differ-from-traditional-word-embedding-models-like-word2vec-or-glove</a>
</li>

<li>BERT (language model)<br />
<a href="https://en.wikipedia.org/wiki/BERT_(language_model)">https://en.wikipedia.org/wiki/BERT_(language_model)</a>
</li>

<li>Levenštejnova vzdálenost<br />
<a href="https://cs.wikipedia.org/wiki/Leven%C5%A1tejnova_vzd%C3%A1lenost">https://cs.wikipedia.org/wiki/Leven%C5%A1tejnova_vzd%C3%A1lenost</a>
</li>

</ol>



<p></p><p></p>
<p><small>Autor: <a href="https://github.com/tisnik/">Pavel Tišnovský</a> &nbsp; 2025</small></p>
</body>
</html>

