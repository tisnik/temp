<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
        "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html>
<head>
<title>Knihovna FAISS a embedding: základ jazykových modelů (2. část)</title>
<meta name="Author" content="Pavel Tisnovsky" />
<meta name="Generator" content="vim" />
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<style type="text/css">
         body {color:#000000; background:#ffffff;}
         h1  {font-family: arial, helvetica, sans-serif; color:#ffffff; background-color:#c00000; text-align:center; padding-left:1em}
         h2  {font-family: arial, helvetica, sans-serif; color:#ffffff; background-color:#0000c0; padding-left:1em; text-align:left}
         h3  {font-family: arial, helvetica, sans-serif; color:#000000; background-color:#c0c0c0; padding-left:1em; text-align:left}
         h4  {font-family: arial, helvetica, sans-serif; color:#000000; background-color:#e0e0e0; padding-left:1em; text-align:left}
         a   {font-family: arial, helvetica, sans-serif;}
         li  {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify; width:450px;}
         ol  {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify; width:450px;}
         ul  {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify; width:450px;}
         p   {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify;}
         pre {background:#e0e0e0}
</style>
</head>

<body>

<h1>Knihovna FAISS a embedding: základ jazykových modelů (2. část)</h1>

<h3>Pavel Tišnovský</h3>

<p></p>

<h1>Úvodník</h1>

<p>Dnes si vyzkoušíme, jak se knihovny FAISS a Sentence-transformers chovají v situaci, kdy je použita datová sada s velkým počtem vět, konkrétně s cca jedním milionem anglických vět. Při takovém rozsahu dat již bude zajímavé sledovat výkonnost obou zmíněných knihoven.</p>



<h2>Obsah</h2>

<p><a href="#k01">1. Knihovna FAISS a embedding: základ jazykových modelů (2. část)</a></p>
<p><a href="#k02">2. Úprava projektu &ndash; přidání nových závislostí</a></p>
<p><a href="#k03">3. Datová sada s&nbsp;jedním milionem anglických vět</a></p>
<p><a href="#k04">4. Získání vybrané datové sady <strong>polygraf-ai/human-sentences-1M-sample-v2</strong></a></p>
<p><a href="#k05">5. Slovník s&nbsp;datovými sadami vs. jedna datová sada</a></p>
<p><a href="#k06">6. Konstrukce běžného Pythonovského seznamu s&nbsp;větami</a></p>
<p><a href="#k07">7. Hledání nejpodobnějších vět v&nbsp;celé datové sadě</a></p>
<p><a href="#k08">8. Krátké připomenutí: vyhledávání na základě výrazů s&nbsp;využitím sémantické podobnosti</a></p>
<p><a href="#k09">9. Využití celé datové sady pro sémantické vyhledávání</a></p>
<p><a href="#k10">10. Úplný zdrojový kód demonstračního příkladu</a></p>
<p><a href="#k11">11. Refaktoring pro lepší čitelnost a další rozšiřování funkcionality</a></p>
<p><a href="#k12">12. Výsledky získané demonstračním příkladem</a></p>
<p><a href="#k13">13. Rychlost sémantického vyhledávání</a></p>
<p><a href="#k14">14. Rychlost embeddingu i konstrukce indexu</a></p>
<p><a href="#k15">15. Výsledky benchmarku</a></p>
<p><a href="#k16">16. Vektory s&nbsp;prvky typu <strong>float16</strong> nebo <strong>bfloat16</strong></a></p>
<p><a href="#k17">17. Výsledky benchmarku</a></p>
<p><a href="#k18">18. Velikost sady vektorů i indexu při použití prvků typu <strong>float32</strong>, <strong>float16</strong> a <strong>bfloat16</strong></a></p>
<p><a href="#k19">19. Repositář s&nbsp;demonstračními příklady</a></p>
<p><a href="#k20">20. Odkazy na Internetu</a></p>



<p><a name="k01"></a></p>
<h2 id="k01">1. Knihovna FAISS a embedding: základ jazykových modelů (2. část)</h2>

<p><a
href="https://www.root.cz/clanky/knihovna-faiss-a-embedding-zaklad-jazykovych-modelu/">V&nbsp;předchozím
článku</a> jsme si ukázali jeden dosti typický způsob využití knihovny FAISS.
S&nbsp;využitím FAISSu a vhodným způsobem &bdquo;vektorizovaného&ldquo; textu
je totiž možné zajistit takzvané <i>sémantické vyhledávání</i>, které není
založeno na (většinou dosti triviální) textové podobě či podobě tokenizovaného
textu, ale na významu, které věty či delší texty obsahují. Základem je přitom
převod původních textů s&nbsp;využitím vhodného modelu do podoby vektorů pevné
délky (typicky se jedná o délky 256, 384, 512, 768), přičemž do hodnot
konkrétních prvků je nějakým způsobem promítnuta sémantika (jak konkrétně či
naopak obecně je to provedeno, do značné míry záleží na použitém modelu,
kterých dnes existuje minimálně několik desítek, spíše však několik set).</p>

<p>Minule obsahovala naše &bdquo;textová databáze&ldquo; pouze přibližně deset
vět. Dnes si vyzkoušíme, jak se <i>FAISS</i> a taktéž knihovna
<i>Sentence-Transformers</i> chová v&nbsp;situaci, když použijeme datovou sadu
s&nbsp;mnohem větším počtem vět. Konkrétně využijeme sadu s&nbsp;přibližně
milionem anglických vět. Při takovém rozsahu dat již bude zajímavé sledovat
výkonnost obou zmíněných knihoven. A taktéž bude zajímavé zjistit, jak se
vlastně změní velikost modelu v&nbsp;případě, že prvky vektorů (po vektorizaci
textu) budou typu <strong>float16</strong> nebo <strong>bfloat16</strong> a
nikoli <strong>float32</strong> (což je výchozí typ).</p>

<p><div class="rs-tip-major">Poznámka: typy <strong>float16</strong> a
<strong>bfloat16</strong> jsme se zabývali v&nbsp;článku <a
href="https://www.root.cz/clanky/brain-floating-point-ndash-novy-format-ulozeni-cisel-pro-strojove-uceni-a-chytra-cidla/">Brain
Floating Point – nový formát uložení čísel pro strojové učení a chytrá čidla
</a>.</div></p>



<p><a name="k02"></a></p>
<h2 id="k02">2. Úprava projektu &ndash; přidání nových závislostí</h2>

<p>Ještě před tím, než si ukážeme další vlastnosti knihoven <i>Faiss</i> i
<i>sentence-transformers</i>, je nutné provést úpravu projektu. Přidáme do něj
další závislosti, konkrétně knihovny nazvané <i>datasets</i> a
<i>matplotlib</i>. To se provede snadno. Buď příkazy:</p>

<pre>
<strong>uv add datasets</strong>
<strong>uv add matplotlib</strong>
</pre>

<p>Nebo lze alternativně použít správce balíčků PDM:</p>

<pre>
<strong>uv add datasets</strong>
<strong>uv add matplotlib</strong>
</pre>

<p>Projektový soubor <strong>pyproject.toml</strong> by nyní měl vypadat
následovně:</p>

<pre>
[project]
name = "sentence-transformer"
version = "0.1.0"
description = "Sentence transformer demos"
authors = [
    {name = "Pavel Tisnovsky", email = "ptisnovs@redhat.com"},
]
dependencies = [
    "datasets&amp;=4.0.0",
    "faiss-cpu&amp;=1.11.0.post1",
    "matplotlib&amp;=3.10.5",
    "sentence-transformers&amp;=5.0.0",
]
requires-python = "==3.12.*"
readme = "README.md"
license = {text = "MIT"}
&nbsp;
&nbsp;
[tool.pdm]
distribution = false
</pre>

<p><div class="rs-tip-major">Poznámka: nástroj <strong>uv</strong> ve
skutečnosti všechny závislosti zapisuje na jediném řádku &ndash; neprovádí
formátování projektového souboru.</div></p>



<p><a name="k03"></a></p>
<h2 id="k03">3. Datová sada s&nbsp;jedním milionem anglických vět</h2>

<p>V&nbsp;demonstračních příkladech, které jsme si ukázali v&nbsp;předchozím
článku, se pracovalo pouze s&nbsp;minimalistickou &bdquo;databází&ldquo; textů,
která se skládala pouze z&nbsp;osmi vět, což skutečně při práci s&nbsp;reálným
mluveným jazykem není mnoho:</p>

<pre>
sentences = [
    "The rain in Spain falls mainly on the plain",
    "The tesselated polygon is a special type of polygon",
    "The quick brown fox jumps over the lazy dog",
    "To be or not to be, that is the question",
    "It is a truth universally acknowledged...",
    "How old are you?",
    "The goat ran down the hill"
]
</pre>

<p>Dnes již budeme pracovat s&nbsp;mnohem větší databází. Konkrétně se bude
jednat o datovou sadu <strong>polygraf-ai/human-sentences-1M-sample-v2</strong>
obsahující jeden milion anglických vět. Na tomto místě je vhodné poznamenat, že
se stále jedná o relativně malou datovou sadu (například pro trénink LLM je
<i>naprosto</i> nedostačující), ovšem pro otestování základních vlastností
knihoven <i>Faiss</i> a <i>sentence-transformers</i> bude její velikost
dostačující. Navíc je práce s&nbsp;touto datovou sadou relativně rychlá.</p>



<p><a name="k04"></a></p>
<h2 id="k04">4. Získání vybrané datové sady <strong>polygraf-ai/human-sentences-1M-sample-v2</strong></h2>

<p>Datovou sadu <strong>polygraf-ai/human-sentences-1M-sample-v2</strong> není
nutné stahovat ručně, protože tuto funkci zajišťuje přímo knihovna
<strong>dataset</strong>. Postačuje pouze zavolat funkci
<strong>load_dataset</strong> a předat jí jednoznačný identifikátor datové
sady. Knihovna automaticky zjistí, zda je datová sada dostupná na lokálním
úložišti a pokud tomu tak není, provede její stažení:</p>

<pre>
from datasets import load_dataset
&nbsp;
dataset_id = "polygraf-ai/human-sentences-1M-sample-v2"
&nbsp;
dataset = load_dataset(dataset_id)
&nbsp;
print(dataset)
</pre>

<p>Při prvním spuštění tohoto skriptu je patrné, že stažení opravdu
proběhlo (cca 100 MB!):</p>

<pre>
README.md: 100%|███████████████████████████████████████████████████████████████████████████████████████| 317/317 [00:00&lt;00:00, 1.31MB/s]
train-00000-of-00001.parquet: 100%|██████████████████████████████████████████████████████████████████| 100M/100M [00:09&lt;00:00, 10.1MB/s]
Generating train split: 100%|████████████████████████████████████████████████████████| 997593/997593 [00:01&lt;00:00, 926163.87 examples/s]
DatasetDict({
    train: Dataset({
        features: ['text', 'source'],
        num_rows: 997593
    })
})
</pre>

<p>Po každém dalším spuštění skriptu se pouze zobrazí informace o datové
sadě:</p>

<pre>
DatasetDict({
    train: Dataset({
        features: ['text', 'source'],
        num_rows: 997593
    })
})
</pre>

<p><div class="rs-tip-major">Poznámka: datová sada se uloží do adresáře
<strong>.cache/huggingface/datasets/</strong>. Nalezneme zde i soubor se všemi
anglickými větami ve formátu Arrow:</div></p>

<pre>
$ <strong>ls -lah human-sentences-1_m-sample-v2-train.arrow</strong>
&nbsp;
-rw-r--r--. 1 ptisnovs ptisnovs 145M Jul 31 18:46 human-sentences-1_m-sample-v2-train.arrow
</pre>



<p><a name="k05"></a></p>
<h2 id="k05">5. Slovník s&nbsp;datovými sadami vs. jedna datová sada</h2>

<p>Předchozí skript by měl po svém spuštění vypsat informaci o tom, že ve
skutečnosti nenačetl a nevrátil pouze jedinou datovou sadu, ale celý slovník
datových sad s&nbsp;jediným prvkem:</p>

<pre>
DatasetDict({
    train: Dataset({
        features: ['text', 'source'],
        num_rows: 997593
    })
})
</pre>

<p>Sadu nazvanou <strong>train</strong> získáme snadno &ndash; předáním
pojmenovaného parametru <strong>split="train"</strong> funkci
<strong>load_dataset</strong>:</p>

<pre>
from datasets import load_dataset
&nbsp;
dataset_id = "polygraf-ai/human-sentences-1M-sample-v2"
&nbsp;
dataset = load_dataset(dataset_id, <u>split="train"</u>)
&nbsp;
print(dataset.features)
</pre>

<p>Po spuštění tohoto skriptu by se měly vypsat informace o sloupcích
v&nbsp;tabulce, ve které je datová sada uložena:</p>

<pre>
{'text': Value('string'), 'source': Value('string')}
</pre>



<p><a name="k06"></a></p>
<h2 id="k06">6. Konstrukce běžného Pythonovského seznamu s&nbsp;větami</h2>

<p>Ukažme si ještě jeden mezikrok, který vlastně v&nbsp;praxi není nezbytně
nutné provádět, protože je výpočetně i paměťově náročný. Nicméně je vhodné
vědět, že stažená datová sada není nějakým &bdquo;magickým objektem&ldquo;, ale
skutečným zdrojem dat. Konkrétně si necháme z&nbsp;datové sady vygenerovat
běžný Pythonovský seznam se všemi větami (v&nbsp;čisté textové podobě):</p>

<pre>
from datasets import load_dataset
&nbsp;
dataset_id = "polygraf-ai/human-sentences-1M-sample-v2"
&nbsp;
dataset = load_dataset(dataset_id, split="train")
&nbsp;
print("Building sentences")
sentences = [sentence for sentence in dataset["text"]]
&nbsp;
print(f"{len(sentences)} sentences created")
</pre>

<p>Výsledek získaný po určité době (jednotky až desítky sekund):</p>

<pre>
Building sentences
997593 sentences created
</pre>

<p><div class="rs-tip-major">Poznámka: jméno datové sady tedy není zcela
přesné, protože počet vět je menší, než jeden milion.</div></p>

<p>Počet vět transformovaných do seznamu lze řídit s&nbsp;využitím operace řezu
(<i>slice</i>), a to <i>ještě</i> před vytvořením seznamu:</p>

<pre>
from datasets import load_dataset
&nbsp;
dataset_id = "polygraf-ai/human-sentences-1M-sample-v2"
&nbsp;
dataset = load_dataset(dataset_id, split="train")
&nbsp;
print("Building sentences")
sentences = [sentence for sentence in dataset["text"]<u>[0:100]</u>]
&nbsp;
print(f"{len(sentences)} sentences created")
</pre>

<p>Nyní získáme výsledky prakticky okamžitě:</p>

<pre>
Building sentences
100 sentences created
</pre>



<p><a name="k07"></a></p>
<h2 id="k07">7. Hledání nejpodobnějších vět v&nbsp;celé datové sadě</h2>

<p>V&nbsp;navazujících kapitolách si ukážeme, jakým způsobem je možné provádět
vyhledávání vět na základě podobnosti v&nbsp;rámci celé datové sady
s&nbsp;přibližně jedním milionem vět. Opět je však nutné zdůraznit, že 1000000
vět sice může vypadat jako velmi vysoká hodnota, ovšem v&nbsp;praxi se mnohdy
pracuje i s&nbsp;(mnohem) většími datovými sadami. Typickým příkladem je
trénink velkých jazykových modelů, což je však téma na samostatné články.</p>

<p>Na druhou stranu je ovšem tato hodnota již dostatečně vysoká na to, aby byly
jednotlivé kroky zpracování časově náročné. Z&nbsp;tohoto důvodu si otestujeme
rychlost (nebo spíše pomalost) vektorizace textů (<i>embedding</i>), rychlost
konstrukce indexu knihovnou FAISS a samozřejmě i rychlost vyhledávání
s&nbsp;využitím tohoto indexu.</p>



<p><a name="k08"></a></p>
<h2 id="k08">8. Krátké připomenutí: vyhledávání na základě výrazů s&nbsp;využitím sémantické podobnosti</h2>

<p>Jen pro připomenutí si ukažme demonstrační příklad <a
href="https://www.root.cz/clanky/knihovna-faiss-a-embedding-zaklad-jazykovych-modelu/">z&nbsp;předchozího
článku</a>. V&nbsp;tomto příkladu jsme si nechali zvoleným <i>embedding
modelem</i> &bdquo;vektorizovat&ldquo; sedm vět, poté jsme knihovnou FAISS
z&nbsp;těchto vektorů zkonstruovali <i>index</i> a následně jsme tento index
použili při vyhledávání výrazů či celých vět na základě jejich <i>sémantické
podobnosti</i> s&nbsp;původními větami:</p>

<pre>
from sentence_transformers import SentenceTransformer
import faiss
&nbsp;
model = SentenceTransformer("paraphrase-MiniLM-L6-v2")
&nbsp;
print(model)
&nbsp;
sentences = [
    "The rain in Spain falls mainly on the plain",
    "The tesselated polygon is a special type of polygon",
    "The quick brown fox jumps over the lazy dog",
    "To be or not to be, that is the question",
    "It is a truth universally acknowledged...",
    "How old are you?",
    "The goat ran down the hill"
]
&nbsp;
embeddings = model.encode(sentences)
print(f"Embeddings shape: {embeddings.shape}")
&nbsp;
similarities = model.similarity(embeddings, embeddings)
&nbsp;
DIMENSIONS = embeddings.shape[1]
&nbsp;
index = faiss.IndexFlatL2(DIMENSIONS)
index.add(embeddings)
&nbsp;
print(f"Index: {index.ntotal}")
&nbsp;
&nbsp;
def <strong>find_similar_sentences</strong>(query_sentence, k):
    query_embedding = model.encode([query_sentence])
    distances, indices = index.search(query_embedding, k)
    print("-"*40)
    print(f"Query: {query_sentence}")
    print(f"Most {k} similar sentences:")
    for i, idx in enumerate(indices[0]):
        print(f"{i + 1}: {sentences[idx]} (Distance: {distances[0][i]})")
&nbsp;
&nbsp;
find_similar_sentences("Shakespeare", 3)
find_similar_sentences("animal", 3)
find_similar_sentences("geometry", 3)
find_similar_sentences("weather", 3)
</pre>

<p>Příklad výsledků získaných po spuštění tohoto demonstračního příkladu:</p>

<pre>
SentenceTransformer(
  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False, 'architecture': 'BertModel'})
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
)
Embeddings shape: (7, 384)
Index: 7
----------------------------------------
Query: Shakespeare
Most 3 similar sentences:
1: To be or not to be, that is the question (Distance: 83.53305053710938)
2: The tesselated polygon is a special type of polygon (Distance: 112.60221862792969)
3: The rain in Spain falls mainly on the plain (Distance: 114.61812591552734)
----------------------------------------
Query: animal
Most 3 similar sentences:
1: The goat ran down the hill (Distance: 67.3703384399414)
2: The quick brown fox jumps over the lazy dog (Distance: 68.25883483886719)
3: To be or not to be, that is the question (Distance: 82.82962036132812)
----------------------------------------
Query: geometry
Most 3 similar sentences:
1: The tesselated polygon is a special type of polygon (Distance: 51.57851791381836)
2: To be or not to be, that is the question (Distance: 91.08709716796875)
3: The goat ran down the hill (Distance: 109.95413208007812)
----------------------------------------
Query: weather
Most 3 similar sentences:
1: The rain in Spain falls mainly on the plain (Distance: 73.53900146484375)
2: To be or not to be, that is the question (Distance: 86.14794921875)
3: How old are you? (Distance: 101.28636169433594)
</pre>



<p><a name="k09"></a></p>
<h2 id="k09">9. Využití celé datové sady pro sémantické vyhledávání</h2>

<p>Podívejme se nyní, jak se datová sada
<strong>polygraf-ai/human-sentences-1M-sample-v2</strong> využije v&nbsp;praxi
pro embedding i pro tvorbu indexu pro knihovnu FAISS. Celý skript je rozdělen
do několika samostatných kroků.</p>

<p>V&nbsp;prvním kroku inicializujeme model, jenž bude využitý pro embedding,
tedy pro vektorizaci celé datové sady:</p>

<pre>
from sentence_transformers import SentenceTransformer
&nbsp;
model = SentenceTransformer("paraphrase-MiniLM-L6-v2")
print(model)
</pre>

<p>V&nbsp;dalším kroku načteme, stejně jako v&nbsp;předchozích kapitolách,
datovou sadu s&nbsp;cca jedním milionem vět:</p>

<pre>
from datasets import load_dataset
&nbsp;
dataset = load_dataset("polygraf-ai/human-sentences-1M-sample-v2", split="train")
print(dataset)
</pre>

<p>Převedeme věty do formy běžného seznamu (můžeme i odstranit řez a převést
všechny věty):</p>

<pre>
sentences = [sentence for sentence in dataset["text"][0:1000]]
print(f"{len(sentences)} sentences created")
</pre>

<p>Dále si necháme knihovnou Sentence-transformers vytvořit vektorizovanou
formu vět (embedding):</p>

<pre>
embeddings = model.encode(sentences)
print(f"Embeddings shape: {embeddings.shape}")
</pre>

<p>Zbývá předposlední krok &ndash; tvorba FAISS indexu:</p>

<pre>
import faiss
&nbsp;
DIMENSIONS = embeddings.shape[1]
index = faiss.IndexFlatL2(DIMENSIONS)
index.add(embeddings)
print(f"Index: {index.ntotal}")
</pre>

<p>Ve chvíli, kdy je index vytvořen, již můžeme v&nbsp;celé databázi jednoho
milionu vět provádět sémantické vyhledávání:</p>

<pre>
def <strong>find_similar_sentences</strong>(query_sentence, k):
    query_embedding = model.encode([query_sentence])
    distances, indices = index.search(query_embedding, k)
    print("-"*40)
    print(f"Query: {query_sentence}")
    print(f"Most {k} similar sentences:")
    for i, idx in enumerate(indices[0]):
        print(f"{i + 1}: {sentences[idx]} (Distance: {distances[0][i]})")
&nbsp;
&nbsp;
find_similar_sentences("city", 3)
find_similar_sentences("animal", 3)
find_similar_sentences("geometry", 3)
find_similar_sentences("weather", 3)
find_similar_sentences("game", 3)
find_similar_sentences("school", 3)
</pre>



<p><a name="k10"></a></p>
<h2 id="k10">10. Úplný zdrojový kód demonstračního příkladu</h2>

<p>Úplný zdrojový kód demonstračního příkladu popsaného <a
href="#k09">v&nbsp;předchozí kapitole</a> vypadá následovně:</p>

<pre>
from sentence_transformers import SentenceTransformer
import faiss
from datasets import load_dataset
&nbsp;
&nbsp;
model = SentenceTransformer("paraphrase-MiniLM-L6-v2")
print(model)
&nbsp;
dataset = load_dataset("polygraf-ai/human-sentences-1M-sample-v2", split="train")
print(dataset)
&nbsp;
sentences = [sentence for sentence in dataset["text"][0:1000]]
print(f"{len(sentences)} sentences created")
&nbsp;
embeddings = model.encode(sentences)
print(f"Embeddings shape: {embeddings.shape}")
&nbsp;
DIMENSIONS = embeddings.shape[1]
index = faiss.IndexFlatL2(DIMENSIONS)
index.add(embeddings)
print(f"Index: {index.ntotal}")
&nbsp;
&nbsp;
def <strong>find_similar_sentences</strong>(query_sentence, k):
    query_embedding = model.encode([query_sentence])
    distances, indices = index.search(query_embedding, k)
    print("-"*40)
    print(f"Query: {query_sentence}")
    print(f"Most {k} similar sentences:")
    for i, idx in enumerate(indices[0]):
        print(f"{i + 1}: {sentences[idx]} (Distance: {distances[0][i]})")
&nbsp;
&nbsp;
find_similar_sentences("city", 3)
find_similar_sentences("animal", 3)
find_similar_sentences("geometry", 3)
find_similar_sentences("weather", 3)
find_similar_sentences("game", 3)
find_similar_sentences("school", 3)
</pre>



<p><a name="k11"></a></p>
<h2 id="k11">11. Refaktoring pro lepší čitelnost a další rozšiřování funkcionality</h2>

<p>Demonstrační příklad <a href="#k10">z&nbsp;předchozí kapitoly</a> je sice
funkční, ale není příliš čitelný ani rozšiřitelný. Z&nbsp;těchto důvodů ho
nepatrně přepíšeme takovým způsobem, že každý krok bude realizován ve zvláštní
funkci. Takto upravený zdrojový kód bude použitelný i pro další účely &ndash;
měření času, benchmarky, zjištění velikosti vektorizovaného textu i FAISS
indexu atd.:</p>

<pre>
from sentence_transformers import SentenceTransformer
import faiss
from datasets import load_dataset
&nbsp;
MODEL_NAME = "paraphrase-MiniLM-L6-v2"
DATASET_ID = "polygraf-ai/human-sentences-1M-sample-v2"
&nbsp;
&nbsp;
def <strong>initialize_model</strong>(model_name):
    print("Model initialization started")
    model = SentenceTransformer(model_name)
    print(model)
    print("Model initialization finished")
    return model
&nbsp;
&nbsp;
def <strong>load_dataset_by_id</strong>(dataset_id):
    print("Loading dataset started")
    dataset = load_dataset(dataset_id, split="train")
    print("Loading dataset finished")
    return dataset
&nbsp;
&nbsp;
def <strong>build_sentences</strong>(dataset, from_, to_):
    print("Building sentences")
    sentences = [sentence for sentence in dataset["text"][from_:to_]]
    print(f"{len(sentences)} sentences created")
    return sentences
&nbsp;
&nbsp;
def <strong>create_embeddings</strong>(model, sentences):
    print("Embedding started")
    embeddings = model.encode(sentences)
    print(f"Embeddings shape: {embeddings.shape}")
    print("Embedding finished")
    return embeddings
&nbsp;
&nbsp;
def <strong>create_faiss_index</strong>(embeddings):
    print("FAISS index construction started")
    DIMENSIONS = embeddings.shape[1]
    index = faiss.IndexFlatL2(DIMENSIONS)
    index.add(embeddings)
    print(f"Index: {index.ntotal}")
    print("FAISS index construction finished")
    return index
&nbsp;
&nbsp;
def <strong>find_similar_sentences</strong>(model, index, query_sentence, k):
    query_embedding = model.encode([query_sentence])
    distances, indices = index.search(query_embedding, k)
    print("-"*40)
    print(f"Query: {query_sentence}")
    print(f"Most {k} similar sentences:")
    for i, idx in enumerate(indices[0]):
        print(f"{i + 1}: {sentences[idx]} (Distance: {distances[0][i]})")
&nbsp;
&nbsp;
model = initialize_model(MODEL_NAME)
dataset = load_dataset_by_id(DATASET_ID)
sentences = build_sentences(dataset, 0, 1000)
embeddings = create_embeddings(model, sentences)
index = create_faiss_index(embeddings)
&nbsp;
find_similar_sentences(model, index, "city", 3)
find_similar_sentences(model, index, "animal", 3)
find_similar_sentences(model, index, "geometry", 3)
find_similar_sentences(model, index, "weather", 3)
find_similar_sentences(model, index, "game", 3)
find_similar_sentences(model, index, "school", 3)
</pre>



<p><a name="k12"></a></p>
<h2 id="k12">12. Výsledky získané demonstračním příkladem</h2>

<p>Podívejme se nyní na výsledky, které byly získány demonstračním příkladem
popsaným <a href="#k11">v&nbsp;předchozí kapitole</a>. Nejprve jsou vypsány
informace o tvorbě modelu, embeddingu, indexu atd:</p>

<pre>
Model initialization started
SentenceTransformer(
  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False, 'architecture': 'BertModel'})
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
)
Model initialization finished
Loading dataset started
Loading dataset finished
Building sentences
1000 sentences created
Embedding started
Embeddings shape: (1000, 384)
Embedding finished
FAISS index construction started
Index: 1000
FAISS index construction finished
</pre>

<p>Následují výsledky sémantického vyhledávání v&nbsp;prvních tisících
větách:</p>

<pre>
----------------------------------------
Query: city
Most 3 similar sentences:
1: Do you gamble that the city will stay quiet for a few hours, or do you risk sending an undermanned team into a dangerous situation? (Distance: 59.321781158447266)
2: "Please respond" said: "Sam - do yourself a favor and contact Pleasanton city management and they will confirm that personnel costs have increased from the 65% range to 77% due to the significant increase of personnel costs in and of themselves." (Distance: 59.33335876464844)
3: Mayor of Hartford and subsequent career Upon the death of John A. Pilgard—who died only nine days after being elected and before he could take office—Spellacy was elected mayor of Hartford in 1935 by the Board of Aldermen. (Distance: 59.40525817871094)
----------------------------------------
Query: animal
Most 3 similar sentences:
1: Prolific fly tyer Blane Chocklett of the TFO Advisory Staff, was integral in the design of the “Esox” rod and brings forth a wealth of knowledge from years of chasing toothy, carnivorous fish. (Distance: 55.286163330078125)
2: In contrast to tunicates and echinoderms, the MRCA of vertebrates is thought to have resembled a lancelet, which is pelagic and which has a more advanced nervous system than adult tunicates or echinoderms. (Distance: 56.6212158203125)
3: Rare Alnskan Bird’s Nest and Eggs Found Ornithologists have just succeeded in finding the nest and eggs of the rare surf bird of Alaska, though the bird itself has been known to science for a century and a half. (Distance: 58.14012145996094)
----------------------------------------
Query: geometry
Most 3 similar sentences:
1: In the present work we show that this is indeed the case and that for spherically symmetric spacetimes there are several inequivalent representations possible. (Distance: 53.43998718261719)
2: Geometric intuition plays an important role in many aspects of Hilbert space theory. (Distance: 54.873863220214844)
3: To further confirm these projection pulses are able to correctly construct a robust density matrix, a tomographic Rabi Chevron map is performed in Figure 2c,d, combined with the calibration technique described above. (Distance: 56.672767639160156)
----------------------------------------
Query: weather
Most 3 similar sentences:
1: In late fall and early winter, they’ve given that heat up, back into the atmosphere. (Distance: 48.415863037109375)
2: Traffic patterns are flown at one specific altitude, usually 800 or 1,000 ft (244 or 305 m) above ground level (AGL). (Distance: 58.13227081298828)
3: Considered an exploratory species Arabian oryx exhibit highly flexible home ranges with large distance migration being observed after rainfall (mostly during spring in Saudi Arabia) while it will contract again during hot and dry periods (e.g. (Distance: 65.33474731445312)
----------------------------------------
Query: game
Most 3 similar sentences:
1: But also to give my players a chance to continue playing with the character they are invested in. (Distance: 45.21950912475586)
2: In contrast, Go is ancient board game that consists of simple elements (a line and circle, black and white colors, and stone and wood materials) combined with simple rules that generate subtleties that have enthralled players for millennia \[[@CR5]\]. (Distance: 49.22539138793945)
3: Differing skill levels of players made it work. (Distance: 51.900543212890625)
----------------------------------------
Query: school
Most 3 similar sentences:
1: Three of my children have progressed through the PPS (soon my youngest will finish at Allderdice) and I was happy with their middle schools being separate from high school. (Distance: 44.28334045410156)
2: When afternoon school began Mr. Porson placed on the desk before him a packet done up in brown paper. (Distance: 54.15468978881836)
3: In response to these considerations, we propose to conduct an innovative multi-measure longitudinal study of a national sample of medical students in order to examine the impact of medical school factors, independent of individual medical student characteristics, on implicit and explicit racial bias in medical students' judgments and decisions. (Distance: 54.777183532714844)
</pre>



<p><a name="k13"></a></p>
<h2 id="k13">13. Rychlost sémantického vyhledávání</h2>

<p>V&nbsp;praxi je velmi často důležité zajistit, aby samotné sémantické
vyhledávání bylo co nejrychlejší, k&nbsp;čemuž v&nbsp;knihovně FAISS slouží
několik speciálních datových struktur a algoritmů (blíže se s&nbsp;nimi
seznámíme příště). Nepatrnou úpravou demonstračního příkladu můžeme zjistit
časy vyhledávání a ty následně vypsat:</p>

<pre>
from sentence_transformers import SentenceTransformer
import faiss
from datasets import load_dataset
from time import time
&nbsp;
MODEL_NAME = "paraphrase-MiniLM-L6-v2"
DATASET_ID = "polygraf-ai/human-sentences-1M-sample-v2"
&nbsp;
&nbsp;
def <strong>initialize_model</strong>(model_name):
    print("Model initialization started")
    model = SentenceTransformer(model_name)
    print(model)
    print("Model initialization finished")
    return model
&nbsp;
&nbsp;
def <strong>load_dataset_by_id</strong>(dataset_id):
    print("Loading dataset started")
    dataset = load_dataset(dataset_id, split="train")
    print("Loading dataset finished")
    return dataset
&nbsp;
&nbsp;
def <strong>build_sentences</strong>(dataset, from_, to_):
    print("Building sentences")
    sentences = [sentence for sentence in dataset["text"][from_:to_]]
    print(f"{len(sentences)} sentences created")
    return sentences
&nbsp;
&nbsp;
def <strong>create_embeddings</strong>(model, sentences):
    print("Embedding started")
    embeddings = model.encode(sentences)
    print(f"Embeddings shape: {embeddings.shape}")
    print("Embedding finished")
    return embeddings
&nbsp;
&nbsp;
def <strong>create_faiss_index</strong>(embeddings):
    print("FAISS index construction started")
    DIMENSIONS = embeddings.shape[1]
    index = faiss.IndexFlatL2(DIMENSIONS)
    index.add(embeddings)
    print(f"Index: {index.ntotal}")
    print("FAISS index construction finished")
    return index
&nbsp;
&nbsp;
def <strong>find_similar_sentences</strong>(model, index, query_sentence, k):
    t1 = time()
    query_embedding = model.encode([query_sentence])
    distances, indices = index.search(query_embedding, k)
    print("-"*40)
    print(f"Query: {query_sentence}")
    print(f"Most {k} similar sentences:")
    for i, idx in enumerate(indices[0]):
        print(f"{i + 1}: {sentences[idx]} (Distance: {distances[0][i]})")
    t2 = time()
    print(f"Finished in {t2-t1:3.2} seconds")
&nbsp;
&nbsp;
model = initialize_model(MODEL_NAME)
dataset = load_dataset_by_id(DATASET_ID)
sentences = build_sentences(dataset, 0, 100000)
embeddings = create_embeddings(model, sentences)
index = create_faiss_index(embeddings)
&nbsp;
find_similar_sentences(model, index, "city", 3)
find_similar_sentences(model, index, "animal", 3)
find_similar_sentences(model, index, "geometry", 3)
find_similar_sentences(model, index, "weather", 3)
find_similar_sentences(model, index, "game", 3)
find_similar_sentences(model, index, "school", 3)
</pre>

<p><div class="rs-tip-major">Poznámka: nyní je datová sada větší &ndash; 100000
vět.</div></p>

<p>Výsledky:</p>

<pre>
Model initialization started
SentenceTransformer(
  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False, 'architecture': 'BertModel'})
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
)
Model initialization finished
Loading dataset started
Loading dataset finished
Building sentences
100000 sentences created
Embedding started
Embeddings shape: (100000, 384)
Embedding finished
FAISS index construction started
Index: 100000
FAISS index construction finished
----------------------------------------
Query: city
Most 3 similar sentences:
1: Which is the best city in the world? (Distance: 34.68456268310547)
2: Which city do you come from? (Distance: 36.576637268066406)
3: That’s more than the population of the city. (Distance: 39.577919006347656)
Finished in 0.025 seconds
----------------------------------------
Query: animal
Most 3 similar sentences:
1: The animal comes from an early branch of the mammal family, and like mammals it is covered in fur and produces milk. (Distance: 36.88507843017578)
2: If an animal determines that you are indeed a human its game over. (Distance: 38.494911193847656)
3: State the animal mentioned in the sentence above." (Distance: 39.17326354980469)
Finished in 0.027 seconds
----------------------------------------
Query: geometry
Most 3 similar sentences:
1: One obtains a physical geometry as a deformation of some standard geometry, which is axiomatizable and physical simultaneously. (Distance: 36.97258377075195)
2: Later published in Discrete and Computational Geometry. (Distance: 40.11151123046875)
3: of Lagrange, Euler and collision points on a geometrically unfaithful depiction of the shape sphere S 2 . (Distance: 43.62152862548828)
Finished in 0.025 seconds
----------------------------------------
Query: weather
Most 3 similar sentences:
1: Daytime highs in the winter range between 64 and 75 °F (18 and 24 °C), with overnight lows between 30 and 44 °F (−1 and 7 °C). (Distance: 40.27581787109375)
2: Winters are cool and wet, with summers mild and also wet. (Distance: 40.89495086669922)
3: He sends rain in summer, snow in winter, and all the changes of weather in their season. (Distance: 42.138771057128906)
Finished in 0.031 seconds
----------------------------------------
Query: game
Most 3 similar sentences:
1: What would be the point of continuing the game? (Distance: 36.408992767333984)
2: The game has a lot of varieties that is either a curse or a blessing to the game. (Distance: 39.20246124267578)
3: Great games, but do not underestimate player frustrations. (Distance: 39.954978942871094)
Finished in 0.038 seconds
----------------------------------------
Query: school
Most 3 similar sentences:
1: But nonetheless, school is beginning or already under way for fully one in four American youngsters and adults enrolled in the nation’s more than 95,000 public elementary and secondary schools, 3,200 charter schools and nearly 4,300 degree-granting colleges, as well as for the 1.1 million who are home-schooled. (Distance: 36.799034118652344)
2: A great thing about this school is all of the clubs and activities available. (Distance: 37.57196044921875)
3: It now houses students in grades Year 1 to Year 11 and Kindergarten. (Distance: 38.36991500854492)
Finished in 0.029 seconds
</pre>

<p><div class="rs-tip-major">Poznámka: prozatím tedy časy vyhledávání
nepřesahují jednu desetinu sekundy, ovšem společně s&nbsp;rostoucím počtem
vektorů bude i tento čas narůstat.</div></p>



<p><a name="k14"></a></p>
<h2 id="k14">14. Rychlost embeddingu i konstrukce indexu</h2>

<p>Časy sémantického vyhledávání, které jsme zjišťovali v&nbsp;předchozím
textu, jsou velmi důležité v&nbsp;praxi, ovšem při tvorbě modelů a indexů (což
není tak častá operace) je důležité vědět i to, jak dlouho tyto dvě operace
trvají. Opět si pro tyto účely vytvoříme jednoduchý benchmark. Tentokrát budeme
zjišťovat, kolik času trvá vektorizace <i>n</i> vět i tvorba FAISS indexu
z&nbsp;výsledné sady vektorů. Výsledky budou zobrazeny knihovnou Matplotlib
pomocí dvojice grafů (každý totiž bude mít zcela odlišné časové měřítko na
vertikální ose, proto nemá velký význam vše zobrazit v&nbsp;grafu jediném).
Implementace benchmarku vypadá následovně:</p>

<pre>
from sentence_transformers import SentenceTransformer
import faiss
from datasets import load_dataset
from time import time
import numpy as np
&nbsp;
import matplotlib.pyplot as plt
&nbsp;
MODEL_NAME = "paraphrase-MiniLM-L6-v2"
DATASET_ID = "polygraf-ai/human-sentences-1M-sample-v2"
&nbsp;
&nbsp;
def <strong>initialize_model</strong>(model_name):
    print("Model initialization started")
    model = SentenceTransformer(model_name)
    print(model)
    print("Model initialization finished")
    return model
&nbsp;
&nbsp;
def <strong>load_dataset_by_id</strong>(dataset_id):
    print("Loading dataset started")
    dataset = load_dataset(dataset_id, split="train")
    print("Loading dataset finished")
    return dataset
&nbsp;
&nbsp;
def <strong>build_sentences</strong>(dataset, from_, to_):
    print("Building sentences")
    sentences = [sentence for sentence in dataset["text"][from_:to_]]
    print(f"{len(sentences)} sentences created")
    return sentences
&nbsp;
&nbsp;
def <strong>create_embeddings</strong>(model, sentences):
    t1 = time()
    print("Embedding started")
    embeddings = model.encode(sentences)
    print(f"Embeddings shape: {embeddings.shape}")
    print("Embedding finished")
    t2 = time()
    return embeddings, t2-t1
&nbsp;
&nbsp;
def <strong>create_faiss_index</strong>(embeddings):
    t1 = time()
    print("FAISS index construction started")
    DIMENSIONS = embeddings.shape[1]
    index = faiss.IndexFlatL2(DIMENSIONS)
    index.add(embeddings)
    print(f"Index: {index.ntotal}")
    print("FAISS index construction finished")
    t2 = time()
    return index, t2-t1
&nbsp;
&nbsp;
model = initialize_model(MODEL_NAME)
dataset = load_dataset_by_id(DATASET_ID)
&nbsp;
ns = []
ts_embeddings = []
ts_index = []
&nbsp;
for n in np.linspace(1000, 10000, 20):
    ns.append(n)
    sentences = build_sentences(dataset, 0, int(n))
    embeddings, t_embeddings = create_embeddings(model, sentences)
    index, t_index = create_faiss_index(embeddings)
    ts_embeddings.append(t_embeddings)
    ts_index.append(t_index)
&nbsp;
fig = plt.figure(figsize=(10, 10))
&nbsp;
plt.subplot(2, 1, 1)
plt.plot(ns, ts_embeddings, "b-")
plt.title("Embeddings creation")
&nbsp;
plt.subplot(2, 1, 2)
plt.plot(ns, ts_index, "m-", label="index creation")
plt.title("Index creation")
&nbsp;
<i># povolení zobrazení mřížky</i>
plt.grid(True)
&nbsp;
fig.savefig("embeddings.png")
&nbsp;
<i># zobrazení grafu</i>
fig.show()
</pre>



<p><a name="k15"></a></p>
<h2 id="k15">15. Výsledky benchmarku</h2>

<p>Výsledky benchmarku <a href="#k14">z&nbsp;předchozí kapitoly</a> (pro 1000
až 10000 vět, tedy pro relativně malé množství dat) jsou zobrazeny na
následující dvojici grafů:</p>

<div class="rs-img-center" style="margin-left: auto; margin-right: auto; max-width: 525px"><a href="https://www.root.cz/obrazek/1215474/"><img src="https://i.iinfo.cz/images/362/faiss-4-1-large.png" class="image-1215474" width="525" height="525" data-prev-filename="https://i.iinfo.cz/images/362/faiss-4-1-prev.png" data-prev-filename-webp="https://i.iinfo.cz/images/362/faiss-4-1-prev.webp" data-prev-width="270" data-prev-height="270" data-large-filename="https://i.iinfo.cz/images/362/faiss-4-1-large.png" data-large-filename-webp="https://i.iinfo.cz/images/362/faiss-4-1-large.webp" data-large-width="525" data-large-height="525" alt="Benchmark: tvorba vektorizovaného textu do podoby vektorů s prvky typu float32" data-description="Benchmark: tvorba vektorizovaného textu do podoby vektorů s prvky typu float32" title="Autor: tisnik, podle licence: &lt;a href=&quot;http://en.wikipedia.org/wiki/Rights_Managed&quot;&gt;Rights Managed&lt;/a&gt;" style="width: 100%; height: auto; max-width: 525px" /></a><p>Obrázek 1: Výsledek benchmarku - tvorba vektorizovaného textu do podoby vektorů s prvky typu float32<br class="_remove" /></p><p class="author">Autor: tisnik, podle licence: <a href="http://en.wikipedia.org/wiki/Rights_Managed">Rights Managed</a><br class="_remove" /></p></div>



<p><a name="k16"></a></p>
<h2 id="k16">16. Vektory s&nbsp;prvky typu <strong>float16</strong> nebo <strong>bfloat16</strong></h2>

<p>Prozatím jsme při vektorizaci textů vůbec nespecifikovali, jakého typu budou
prvky vektorů (na druhou stranu počet prvků určuje model, v&nbsp;našem případě
to je 384 prvků). Při výchozím nastavení je pro reprezentaci prvků vektorů
použit datový typ <strong>float32</strong> neboli <i>IEEE 754 single
precision</i>. Alternativně je ovšem možné namísto tohoto typu použít
<strong>float16</strong> (<i>IEEE 754 half precision</i>) nebo
<strong>bfloat16</strong> (<i>brain floating point</i>). Tyto datové typy
využívají pro uložení numerické hodnoty pouze šestnáct bitů a vektorizovaný
text by tak měl být teoreticky poloviční. Ovšem důležitější je, že výpočty
s&nbsp;těmito datovými typy budou při použití GPU rychlejší (do jaké míry
rychlejší &ndash; k&nbsp;tomu se ještě vrátíme). Na druhou stranu při SW
výpočtech (Torch CPU) bude pravděpodobně naopak docházet ke zpomalení, zejména
pokud nejsou k&nbsp;dispozici příslušná rozšíření instrukční sady (starší
mikroprocesory).</p>

<p>Ostatně se o tom můžeme snadno přesvědčit specifikací datového typu
<strong>float16</strong>:</p>

<pre>
model = SentenceTransformer(model_name, <u>model_kwargs={"torch_dtype": "float16"}</u>)
</pre>

<p>Celý benchmark je upraven do této podoby:</p>

<pre>
from sentence_transformers import SentenceTransformer
import faiss
from datasets import load_dataset
from time import time
import numpy as np
&nbsp;
import matplotlib.pyplot as plt
&nbsp;
MODEL_NAME = "paraphrase-MiniLM-L6-v2"
DATASET_ID = "polygraf-ai/human-sentences-1M-sample-v2"
&nbsp;
&nbsp;
def <strong>initialize_model</strong>(model_name):
    print("Model initialization started")
    model = SentenceTransformer(model_name, <u>model_kwargs={"torch_dtype": "float16"}</u>)
    print(model)
    print("Model initialization finished")
    return model
&nbsp;
&nbsp;
def <strong>load_dataset_by_id</strong>(dataset_id):
    print("Loading dataset started")
    dataset = load_dataset(dataset_id, split="train")
    print("Loading dataset finished")
    return dataset
&nbsp;
&nbsp;
def <strong>build_sentences</strong>(dataset, from_, to_):
    print("Building sentences")
    sentences = [sentence for sentence in dataset["text"][from_:to_]]
    print(f"{len(sentences)} sentences created")
    return sentences
&nbsp;
&nbsp;
def <strong>create_embeddings</strong>(model, sentences):
    t1 = time()
    print("Embedding started")
    embeddings = model.encode(sentences)
    print(f"Embeddings shape: {embeddings.shape}")
    print("Embedding finished")
    t2 = time()
    return embeddings, t2-t1
&nbsp;
&nbsp;
def <strong>create_faiss_index</strong>(embeddings):
    t1 = time()
    print("FAISS index construction started")
    DIMENSIONS = embeddings.shape[1]
    index = faiss.IndexFlatL2(DIMENSIONS)
    index.add(embeddings)
    print(f"Index: {index.ntotal}")
    print("FAISS index construction finished")
    t2 = time()
    return index, t2-t1
&nbsp;
&nbsp;
model = initialize_model(MODEL_NAME)
dataset = load_dataset_by_id(DATASET_ID)
&nbsp;
ns = []
ts_embeddings = []
ts_index = []
&nbsp;
for n in np.linspace(1000, 10000, 20):
    ns.append(n)
    sentences = build_sentences(dataset, 0, int(n))
    embeddings, t_embeddings = create_embeddings(model, sentences)
    index, t_index = create_faiss_index(embeddings)
    ts_embeddings.append(t_embeddings)
    ts_index.append(t_index)
&nbsp;
fig = plt.figure(figsize=(10, 10))
&nbsp;
plt.subplot(2, 1, 1)
plt.plot(ns, ts_embeddings, "b-")
plt.title("Embeddings creation")
&nbsp;
plt.subplot(2, 1, 2)
plt.plot(ns, ts_index, "m-", label="index creation")
plt.title("Index creation")
&nbsp;
<i># povolení zobrazení mřížky</i>
plt.grid(True)
&nbsp;
fig.savefig("embeddings.png")
&nbsp;
<i># zobrazení grafu</i>
fig.show()
</pre>



<p><a name="k17"></a></p>
<h2 id="k17">17. Výsledky benchmarku</h2>

<p>Výsledky benchmarku <a href="#k16">z&nbsp;předchozí kapitoly</a> jsou
zobrazeny na následující dvojici grafů:</p>

<div class="rs-img-center" style="margin-left: auto; margin-right: auto; max-width: 525px"><a href="https://www.root.cz/obrazek/1215477/"><img src="https://i.iinfo.cz/images/362/faiss-4-2-large.png" class="image-1215477" width="525" height="525" data-prev-filename="https://i.iinfo.cz/images/362/faiss-4-2-prev.png" data-prev-filename-webp="https://i.iinfo.cz/images/362/faiss-4-2-prev.webp" data-prev-width="270" data-prev-height="270" data-large-filename="https://i.iinfo.cz/images/362/faiss-4-2-large.png" data-large-filename-webp="https://i.iinfo.cz/images/362/faiss-4-2-large.webp" data-large-width="525" data-large-height="525" alt="Benchmark: tvorba vektorizovaného textu do podoby vektorů s prvky typu float32" data-description="Benchmark: tvorba vektorizovaného textu do podoby vektorů s prvky typu float32" title="Autor: tisnik, podle licence: &lt;a href=&quot;http://en.wikipedia.org/wiki/Rights_Managed&quot;&gt;Rights Managed&lt;/a&gt;" style="width: 100%; height: auto; max-width: 525px" /></a><p>Obrázek 2: Výsledek benchmarku - tvorba vektorizovaného textu do podoby vektorů s prvky typu float16. Povšimněte si velkého zpomalení při implementaci na CPU.</p><p class="author">Autor: tisnik, podle licence: <a href="http://en.wikipedia.org/wiki/Rights_Managed">Rights Managed</a><br class="_remove" /></p></div>

<p><div class="rs-tip-major">Poznámka: je vhodné si porovnat tento graf
s&nbsp;grafem <a href="#k14">ze čtrnácté kapitoly</a>. Nyní jsou časy mnohem
delší, protože veškeré výpočty jsou prováděny na CPU (viz měřítko na
vertikálních osách). Počet vstupných dat zůstává pochopitelně
zachován.</div></p>



<p><a name="k18"></a></p>
<h2 id="k18">18. Velikost sady vektorů i indexu při použití prvků typu <strong>float32</strong>, <strong>float16</strong> a <strong>bfloat16</strong></h2>

<p>Pro zjištění velikostí vektorizovaného textu i indexu při použití prvků
různých typů si upravíme původní skript do následující podoby, v&nbsp;níž se
provádí uložení modelu i indexu do souboru, pochopitelně pro každý datový typ
zvlášť (<strong>embeddings.dump</strong> vs.
<strong>faiss.write_index</strong>):</p>

<pre>
for dtype in ["float32", "float16", "bfloat16"]:
    model = initialize_model(MODEL_NAME, dtype)
    dataset = load_dataset_by_id(DATASET_ID)
&nbsp;
    sentences = build_sentences(dataset, 0, 10)
    embeddings = create_embeddings(model, sentences)
    index = create_faiss_index(embeddings)
&nbsp;
    embeddings.dump(f"embeddings.{dtype}")
    faiss.write_index(index, f"index.{dtype}")
</pre>

<p>Z&nbsp;výsledků je patrné, že vektorizovaná datová sada je (podle očekávání)
zhruba poloviční při použití <strong>float16</strong> namísto
<strong>float32</strong> (23225/11668=1.987080767, tedy přibližně 2.0).
Zajímavé je, že pro typ <strong>bfloat16</strong> dosahuje velikost 19454
bajtů, což si vyžádá hlubší prozkoumání. Ovšem FAISS indexy jsou stejně velké:
15405 bajtů. Proč tomu tak je? Při výpočtech podobnosti pomocí L2 metriky je
vyžadováno <i>4&times;d&times;n</i> bajtů, kde <i>d</i> je dimenze vektoru a
<i>n</i> je počet vektorů. V&nbsp;našem konkrétním případě tedy
4&times;384&times;10=15360, což po doplnění hlavičky vychází na 15405
bajtů:</p>

<table>
<tr><th>Typ prvků vektorů</th><th>Embeggings</th><th>FAISS index</th></tr>
<tr><td>bfloat16</td><td>19454</td><td>15405</td></tr>
<tr><td>float16 </td><td>11668</td><td>15405</td></tr>
<tr><td>float32 </td><td>23225</td><td>15405</td></tr>
</table>

<p>Celý skript vypadá následovně:</p>

<pre>
from sentence_transformers import SentenceTransformer
import faiss
from datasets import load_dataset
&nbsp;
MODEL_NAME = "paraphrase-MiniLM-L6-v2"
DATASET_ID = "polygraf-ai/human-sentences-1M-sample-v2"
&nbsp;
&nbsp;
def <strong>initialize_model</strong>(model_name, dtype):
    print("Model initialization started")
    model = SentenceTransformer(model_name, model_kwargs={"torch_dtype": dtype})
    print(model)
    print("Model initialization finished")
    return model
&nbsp;
&nbsp;
def <strong>load_dataset_by_id</strong>(dataset_id):
    print("Loading dataset started")
    dataset = load_dataset(dataset_id, split="train")
    print("Loading dataset finished")
    return dataset
&nbsp;
&nbsp;
def <strong>build_sentences</strong>(dataset, from_, to_):
    print("Building sentences")
    sentences = [sentence for sentence in dataset["text"][from_:to_]]
    print(f"{len(sentences)} sentences created")
    return sentences
&nbsp;
&nbsp;
def <strong>create_embeddings</strong>(model, sentences):
    print("Embedding started")
    embeddings = model.encode(sentences)
    print(f"Embeddings shape: {embeddings.shape}")
    print("Embedding finished")
    return embeddings
&nbsp;
&nbsp;
def <strong>create_faiss_index</strong>(embeddings):
    print("FAISS index construction started")
    DIMENSIONS = embeddings.shape[1]
    index = faiss.IndexFlatL2(DIMENSIONS)
    index.add(embeddings)
    print(f"Index: {index.ntotal}")
    print("FAISS index construction finished")
    return index
&nbsp;
&nbsp;
for dtype in ["float32", "float16", "bfloat16"]:
    model = initialize_model(MODEL_NAME, dtype)
    dataset = load_dataset_by_id(DATASET_ID)
&nbsp;
    sentences = build_sentences(dataset, 0, 1000)
    embeddings = create_embeddings(model, sentences)
    index = create_faiss_index(embeddings)
&nbsp;
    embeddings.dump(f"embeddings.{dtype}")
    faiss.write_index(index, f"index.{dtype}")
</pre>



<p><a name="k19"></a></p>
<h2 id="k19">19. Repositář s&nbsp;demonstračními příklady</h2>

<p>Demonstrační příklady <a
href="https://www.root.cz/clanky/knihovna-faiss-a-embedding-zaklad-jazykovych-modelu/">z&nbsp;předchozího</a>
i dnešního článku lze nalézt na následujících odkazech:</p>

<table>
<tr><th> #</th><th>Příklad</th><th>Stručný popis</th><th>Adresa</th></tr>
<tr><td> 1</td><td>transformer1.py</td><td>inicializace modelu <strong>paraphrase-MiniLM-L6-v2</strong> přes knihovnu sentence-transformers s&nbsp;výpisem základních informací o něm</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss_transformers/transformer1.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transfomers/transformer1.py</a></td></tr>
<tr><td> 2</td><td>transformer2.py</td><td>inicializace modelu <strong>all-mpnet-base-v2</strong> přes knihovnu sentence-transformers s&nbsp;výpisem základních informací o něm</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss_transformers/transformer2.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transfomers/transformer2.py</a></td></tr>
<tr><td> 3</td><td>transformer3.py</td><td>inicializace modelu <strong>Seznam/small-e-czech</strong> přes knihovnu sentence-transformers s&nbsp;výpisem základních informací o něm</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss_transformers/transformer3.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transfomers/transformer3.py</a></td></tr>
<tr><td> 4</td><td>transformer4.py</td><td>vektorizace textů (vět) přes knihovnu sentence-transformers s&nbsp;využitím zvoleného modelu</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss_transformers/transformer4.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transfomers/transformer4.py</a></td></tr>
<tr><td> 5</td><td>transformer5.py</td><td>informace o vypočtené matici s&nbsp;vektorizovaným textem (<i>embedding</i>)</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss_transformers/transformer5.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transfomers/transformer5.py</a></td></tr>
<tr><td> 6</td><td>transformer6.py</td><td>výpočet a zobrazení vypočtené tabulky podobností</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss_transformers/transformer6.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transfomers/transformer6.py</a></td></tr>
<tr><td> 7</td><td>transformer7.py</td><td>nalezení významově nejpodobnějších vět s&nbsp;využitím vektorizované databáze textů</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss_transformers/transformer7.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transfomers/transformer7.py</a></td></tr>
<tr><td> 8</td><td>transformer8.py</td><td>nalezení významově nejpodobnějších vět s&nbsp;využitím vektorizované databáze textů</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss_transformers/transformer8.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transfomers/transformer8.py</a></td></tr>
<tr><td> 9</td><td>transformer9.py</td><td>nalezení vět nejbližších k&nbsp;zadanému termínu</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss_transformers/transformer9.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transfomers/transformer9.py</a></td></tr>
<tr><td>10</td><td>transformerA.py</td><td>podpora pro hledání na základě sémantické podobnosti</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss_transformers/transformerA.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transfomers/transformerA.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>11</td><td>dataset1.py</td><td>získání datové sady s&nbsp;milionem anglických vět</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss_transformers/dataset1.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transfomers/dataset1.py</a></td></tr>
<tr><td>12</td><td>dataset2.py</td><td>získání tréninkových dat s&nbsp;milionem anglických vět</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss_transformers/dataset2.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transfomers/dataset2.py</a></td></tr>
<tr><td>13</td><td>dataset3.py</td><td>konstrukce Pythonovského seznamu s&nbsp;větami</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss_transformers/dataset3.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transfomers/dataset3.py</a></td></tr>
<tr><td>14</td><td>dataset4.py</td><td>konstrukce Pythonovského seznamu s&nbsp;větami, omezení počtu vět</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss_transformers/dataset4.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transfomers/dataset4.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>15</td><td>transformerB.py</td><td>vytvoření indexu z&nbsp;datové sady s&nbsp;milionem anglických vět</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss_transformers/transformerB.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transfomers/transformerB.py</a></td></tr>
<tr><td>16</td><td>transformerC.py</td><td>refaktoring předchozího demonstračního příkladu</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss_transformers/transformerC.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transfomers/transformerC.py</a></td></tr>
<tr><td>17</td><td>transformerD.py</td><td>zjištění rychlosti nalezení nejpodobnějších vět</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss_transformers/transformerD.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transfomers/transformerD.py</a></td></tr>
<tr><td>18</td><td>transformerE.py</td><td>benchmark: rychlost embeddingu a konstrukce indexu (využití typu <strong>float32</strong>)</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss_transformers/transformerE.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transfomers/transformerE.py</a></td></tr>
<tr><td>19</td><td>transformerF.py</td><td>benchmark: rychlost embeddingu a konstrukce indexu (využití typu <strong>float16</strong>)</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss_transformers/transformerF.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transfomers/transformerF.py</a></td></tr>
<tr><td>20</td><td>transformerG.py</td><td>uložení modelu i indexu do souboru pro porovnání velikostí</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss_transformers/transformerG.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transfomers/transformerG.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>21</td><td>pyproject.toml</td><td>soubor s&nbsp;projektem a definicí všech potřebných závislostí</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss_transformers/pyproject.toml">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transfomers/pyproject.toml</a></td></tr>
</table>

<p>Demonstrační příklady vytvořené v&nbsp;Pythonu a popsané <a
href="https://www.root.cz/clanky/faiss-knihovna-pro-rychle-a-efektivni-vyhledavani-podobnych-vektoru/">v&nbsp;předchozím</a>
i v&nbsp;článku <a
href="https://www.root.cz/clanky/faiss-knihovna-pro-rychle-a-efektivni-vyhledavani-podobnych-vektoru-2-cast/">FAISS:
knihovna pro rychlé a efektivní vyhledávání podobných vektorů (2. část)</a> <a
href="https://github.com/tisnik/most-popular-python-libs/">https://github.com/tisnik/most-popular-python-libs/</a>.
Následují odkazy na jednotlivé příklady:</p>

<table>
<tr><th> #</th><th>Příklad</th><th>Stručný popis</th><th>Adresa</th></tr>
<tr><td> 1</td><td>faiss-1.py</td><td>seznamy souřadnic bodů v&nbsp;rovině</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-1.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-1.py</a></td></tr>
<tr><td> 2</td><td>faiss-2.py</td><td>konstrukce matice se souřadnicemi bodů</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-2.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-2.py</a></td></tr>
<tr><td> 3</td><td>faiss-3.py</td><td>konstrukce indexu pro vyhledávání na základě vzdálenosti</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-3.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-3.py</a></td></tr>
<tr><td> 4</td><td>faiss-4.py</td><td>nalezení nejbližších bodů k&nbsp;zadaným souřadnicím &ndash; výpis indexů nalezených bodů</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-4.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-4.py</a></td></tr>
<tr><td> 5</td><td>faiss-5.py</td><td>nalezení nejbližších bodů k&nbsp;zadaným souřadnicím &ndash; výpis souřadnic nalezených bodů</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-5.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-5.py</a></td></tr>
<tr><td> 6</td><td>faiss-6.py</td><td>vyhledávání bodů na základě skalárního součinu bez normalizace vektorů</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-6.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-6.py</a></td></tr>
<tr><td> 7</td><td>faiss-7.py</td><td>vyhledávání bodů na základě skalárního součinu s&nbsp;normalizací vektorů</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-7.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-7.py</a></td></tr>
<tr><td> 8</td><td>faiss-8.py</td><td>jednoduchý benchmark rychlosti vyhledávání knihovnou FAISS</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-8.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-8.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td> 9</td><td>faiss-9.py</td><td>vizualizace koncových bodů vektorů v&nbsp;rovině</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-9.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-9.py</a></td></tr>
<tr><td>10</td><td>faiss-A.py</td><td>vykreslení nejpodobnějších vektorů získaných na základě L2 metriky</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-A.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-A.py</a></td></tr>
<tr><td>11</td><td>faiss-B.py</td><td>nalezení nejpodobnějších vektorů získaných na základě skalárního součinu: varianta s&nbsp;nenormovanými vektory</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-B.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-B.py</a></td></tr>
<tr><td>12</td><td>faiss-C.py</td><td>nalezení nejpodobnějších vektorů získaných na základě skalárního součinu: varianta s&nbsp;normovanými vektory</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-C.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-C.py</a></td></tr>
<tr><td>13</td><td>faiss-D.py</td><td>vykreslení nejpodobnějších vektorů před jejich normalizací</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-D.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-D.py</a></td></tr>
<tr><td>14</td><td>faiss-E.py</td><td>vykreslení vektorů formou orientovaných šipek</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-E.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-E.py</a></td></tr>
<tr><td>15</td><td>faiss-F.py</td><td>vykreslení vektorů po jejich normalizaci formou orientovaných šipek</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-F.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-F.py</a></td></tr>
<tr><td>16</td><td>faiss-G.py</td><td>vyhledání a vykreslení nejvíce NEpodobných vektorů</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-G.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-G.py</a></td></tr>
<tr><td>17</td><td>faiss-H.py</td><td>vyhledání podobných vektorů se složkami typu <i>float16</i></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-H.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-H.py</a></td></tr>
<tr><td>18</td><td>faiss-I.py</td><td>jednoduchý benchmark rychlosti vyhledávání knihovnou FAISS: rozdíly mezi <i>float16</i> a <i>float32</i></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-I.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-I.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>19</td><td>pyproject.toml</td><td>soubor s&nbsp;projektem a definicí závislostí</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/pyproject.toml">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/pyproject.toml</a></td></tr>
</table>



<p><a name="k20"></a></p>
<h2 id="k20">20. Odkazy na Internetu</h2>

<ol>

<li>FAISS: knihovna pro rychlé a efektivní vyhledávání podobných vektorů<br />
<a href="https://www.root.cz/clanky/faiss-knihovna-pro-rychle-a-efektivni-vyhledavani-podobnych-vektoru/">https://www.root.cz/clanky/faiss-knihovna-pro-rychle-a-efektivni-vyhledavani-podobnych-vektoru/</a>
</li>

<li>FAISS: knihovna pro rychlé a efektivní vyhledávání podobných vektorů (2. část)<br />
<a href="https://www.root.cz/clanky/faiss-knihovna-pro-rychle-a-efektivni-vyhledavani-podobnych-vektoru-2-cast/">https://www.root.cz/clanky/faiss-knihovna-pro-rychle-a-efektivni-vyhledavani-podobnych-vektoru-2-cast/</a>
</li>

<li>Knihovna FAISS a embedding: základ jazykových modelů<br />
<a href="https://www.root.cz/clanky/knihovna-faiss-a-embedding-zaklad-jazykovych-modelu/">https://www.root.cz/clanky/knihovna-faiss-a-embedding-zaklad-jazykovych-modelu/</a>
</li>

<li>FAISS (Facebook AI Similarity Search)<br />
<a href="https://en.wikipedia.org/wiki/FAISS">https://en.wikipedia.org/wiki/FAISS</a>
</li>

<li>FAISS documentation<br />
<a href="https://faiss.ai/">https://faiss.ai/</a>
</li>

<li>Introduction to Facebook AI Similarity Search (Faiss)<br />
<a href="https://www.pinecone.io/learn/series/faiss/faiss-tutorial/">https://www.pinecone.io/learn/series/faiss/faiss-tutorial/</a>
</li>

<li>Faiss: Efficient Similarity Search and Clustering of Dense Vectors<br />
<a href="https://medium.com/@pankaj_pandey/faiss-efficient-similarity-search-and-clustering-of-dense-vectors-dace1df1e235">https://medium.com/@pankaj_pandey/faiss-efficient-similarity-search-and-clustering-of-dense-vectors-dace1df1e235</a>
</li>

<li>Cosine Distance vs Dot Product vs Euclidean in vector similarity search<br />
<a href="https://medium.com/data-science-collective/cosine-distance-vs-dot-product-vs-euclidean-in-vector-similarity-search-227a6db32edb">https://medium.com/data-science-collective/cosine-distance-vs-dot-product-vs-euclidean-in-vector-similarity-search-227a6db32edb</a>
</li>

<li>F16C<br />
<a href="https://en.wikipedia.org/wiki/F16C">https://en.wikipedia.org/wiki/F16C</a>
</li>

<li>FP16 (AVX-512)<br />
<a href="https://en.wikipedia.org/wiki/AVX-512#FP16">https://en.wikipedia.org/wiki/AVX-512#FP16</a>
</li>

<li>Top 8 Vector Databases in 2025: Features, Use Cases, and Comparisons<br />
<a href="https://synapsefabric.com/top-8-vector-databases-in-2025-features-use-cases-and-comparisons/">https://synapsefabric.com/top-8-vector-databases-in-2025-features-use-cases-and-comparisons/</a>
</li>

<li>Is FAISS a Vector Database? Complete Guide<br />
<a href="https://mljourney.com/is-faiss-a-vector-database-complete-guide/">https://mljourney.com/is-faiss-a-vector-database-complete-guide/</a>
</li>

<li>Vector database<br />
<a href="https://en.wikipedia.org/wiki/Vector_database">https://en.wikipedia.org/wiki/Vector_database</a>
</li>

<li>Similarity search<br />
<a href="https://en.wikipedia.org/wiki/Similarity_search">https://en.wikipedia.org/wiki/Similarity_search</a>
</li>

<li>Nearest neighbor search<br />
<a href="https://en.wikipedia.org/wiki/Nearest_neighbor_search#Approximation_methods">https://en.wikipedia.org/wiki/Nearest_neighbor_search#Approximation_methods</a>
</li>

<li>Decoding Similarity Search with FAISS: A Practical Approach<br />
<a href="https://www.luminis.eu/blog/decoding-similarity-search-with-faiss-a-practical-approach/">https://www.luminis.eu/blog/decoding-similarity-search-with-faiss-a-practical-approach/</a>
</li>

<li>MetricType and distances<br />
<a href="https://github.com/facebookresearch/faiss/wiki/MetricType-and-distances">https://github.com/facebookresearch/faiss/wiki/MetricType-and-distances</a>
</li>

<li>RAG - Retrieval-augmented generation<br />
<a href="https://en.wikipedia.org/wiki/Retrieval-augmented_generation">https://en.wikipedia.org/wiki/Retrieval-augmented_generation</a>
</li>

<li>pgvector na GitHubu<br />
<a href="https://github.com/pgvector/pgvector">https://github.com/pgvector/pgvector</a>
</li>

<li>Why we replaced Pinecone with PGVector<br />
<a href="https://www.confident-ai.com/blog/why-we-replaced-pinecone-with-pgvector">https://www.confident-ai.com/blog/why-we-replaced-pinecone-with-pgvector</a>
</li>

<li>PostgreSQL as VectorDB - Beginner Tutorial<br />
<a href="https://www.youtube.com/watch?v=Ff3tJ4pJEa4">https://www.youtube.com/watch?v=Ff3tJ4pJEa4</a>
</li>

<li>What is a Vector Database? (neobsahuje odpověď na otázku v titulku :-)<br />
<a href="https://www.youtube.com/watch?v=t9IDoenf-lo">https://www.youtube.com/watch?v=t9IDoenf-lo</a>
</li>

<li>PGVector: Turn PostgreSQL Into A Vector Database<br />
<a href="https://www.youtube.com/watch?v=j1QcPSLj7u0">https://www.youtube.com/watch?v=j1QcPSLj7u0</a>
</li>

<li>Milvus<br />
<a href="https://milvus.io/">https://milvus.io/</a>
</li>

<li>Vector Databases simply explained! (Embeddings &amp; Indexes)<br />
<a href="https://www.youtube.com/watch?v=dN0lsF2cvm4">https://www.youtube.com/watch?v=dN0lsF2cvm4</a>
</li>

<li>Vector databases are so hot right now. WTF are they?<br />
<a href="https://www.youtube.com/watch?v=klTvEwg3oJ4">https://www.youtube.com/watch?v=klTvEwg3oJ4</a>
</li>

<li>Step-by-Step Guide to Installing “pgvector” and Loading Data in PostgreSQL<br />
<a href="https://medium.com/@besttechreads/step-by-step-guide-to-installing-pgvector-and-loading-data-in-postgresql-f2cffb5dec43">https://medium.com/@besttechreads/step-by-step-guide-to-installing-pgvector-and-loading-data-in-postgresql-f2cffb5dec43</a>
</li>

<li>Best 17 Vector Databases for 2025<br />
<a href="https://lakefs.io/blog/12-vector-databases-2023/">https://lakefs.io/blog/12-vector-databases-2023/</a>
</li>

<li>Top 15 Vector Databases that You Must Try in 2025<br />
<a href="https://www.geeksforgeeks.org/top-vector-databases/">https://www.geeksforgeeks.org/top-vector-databases/</a>
</li>

<li>Picking a vector database: a comparison and guide for 2023<br />
<a href="https://benchmark.vectorview.ai/vectordbs.html">https://benchmark.vectorview.ai/vectordbs.html</a>
</li>

<li>Top 9 Vector Databases as of Feburary 2025<br />
<a href="https://www.shakudo.io/blog/top-9-vector-databases">https://www.shakudo.io/blog/top-9-vector-databases</a>
</li>

<li>What is a vector database?<br />
<a href="https://www.ibm.com/think/topics/vector-database">https://www.ibm.com/think/topics/vector-database</a>
</li>

<li>SQL injection<br />
<a href="https://en.wikipedia.org/wiki/SQL_injection">https://en.wikipedia.org/wiki/SQL_injection</a>
</li>

<li>Cosine similarity<br />
<a href="https://en.wikipedia.org/wiki/Cosine_similarity">https://en.wikipedia.org/wiki/Cosine_similarity</a>
</li>

<li>Euclidean distance<br />
<a href="https://en.wikipedia.org/wiki/Euclidean_distance">https://en.wikipedia.org/wiki/Euclidean_distance</a>
</li>

<li>Dot product<br />
<a href="https://en.wikipedia.org/wiki/Dot_product">https://en.wikipedia.org/wiki/Dot_product</a>
</li>

<li>Hammingova vzdálenost<br />
<a href="https://cs.wikipedia.org/wiki/Hammingova_vzd%C3%A1lenost">https://cs.wikipedia.org/wiki/Hammingova_vzd%C3%A1lenost</a>
</li>

<li>Jaccard index<br />
<a href="https://en.wikipedia.org/wiki/Jaccard_index">https://en.wikipedia.org/wiki/Jaccard_index</a>
</li>

<li>Manhattanská metrika<br />
<a href="https://cs.wikipedia.org/wiki/Manhattansk%C3%A1_metrika">https://cs.wikipedia.org/wiki/Manhattansk%C3%A1_metrika</a>
</li>

<li>pgvector: vektorová databáze postavená na Postgresu<br />
<a href="https://www.root.cz/clanky/pgvector-vektorova-databaze-postavena-na-postgresu/">https://www.root.cz/clanky/pgvector-vektorova-databaze-postavena-na-postgresu/</a>
</li>

<li>Matplotlib Home Page<br />
<a href="http://matplotlib.org/">http://matplotlib.org/</a>
</li>

<li>matplotlib (Wikipedia)<br />
<a href="https://en.wikipedia.org/wiki/Matplotlib">https://en.wikipedia.org/wiki/Matplotlib</a>
</li>

<li>Dot Product<br />
<a href="https://mathworld.wolfram.com/DotProduct.html">https://mathworld.wolfram.com/DotProduct.html</a>
</li>

<li>FAISS and sentence-transformers in 5 Minutes<br />
<a href="https://www.stephendiehl.com/posts/faiss/">https://www.stephendiehl.com/posts/faiss/</a>
</li>

<li>Sentence Transformer: Quickstart<br />
<a href="https://sbert.net/docs/quickstart.html#sentence-transformer">https://sbert.net/docs/quickstart.html#sentence-transformer</a>
</li>

<li>Sentence Transformers: Embeddings, Retrieval, and Reranking<br />
<a href="https://pypi.org/project/sentence-transformers/">https://pypi.org/project/sentence-transformers/</a>
</li>

<li>uv<br />
<a href="https://docs.astral.sh/uv/">https://docs.astral.sh/uv/</a>
</li>

<li>A Gentle Introduction to Retrieval Augmented Generation (RAG)<br />
<a href="https://wandb.ai/cosmo3769/RAG/reports/A-Gentle-Introduction-to-Retrieval-Augmented-Generation-RAG---Vmlldzo1MjM4Mjk1">https://wandb.ai/cosmo3769/RAG/reports/A-Gentle-Introduction-to-Retrieval-Augmented-Generation-RAG---Vmlldzo1MjM4Mjk1</a>
</li>

<li>The Beginner’s Guide to Text Embeddings<br />
<a href="https://www.deepset.ai/blog/the-beginners-guide-to-text-embeddings">https://www.deepset.ai/blog/the-beginners-guide-to-text-embeddings</a>
</li>

<li>What are Word Embeddings?<br />
<a href="https://www.youtube.com/watch?v=wgfSDrqYMJ4">https://www.youtube.com/watch?v=wgfSDrqYMJ4</a>
</li>

<li>How to choose an embedding model<br />
<a href="https://www.youtube.com/watch?v=djp4205tHGU">https://www.youtube.com/watch?v=djp4205tHGU</a>
</li>

<li>What is a Vector Database? Powering Semantic Search &amp; AI Applications<br />
<a href="https://www.youtube.com/watch?v=gl1r1XV0SLw">https://www.youtube.com/watch?v=gl1r1XV0SLw</a>
</li>

<li>How do Sentence Transformers differ from traditional word embedding models like Word2Vec or GloVe?<br />
<a href="https://zilliz.com/ai-faq/how-do-sentence-transformers-differ-from-traditional-word-embedding-models-like-word2vec-or-glove">https://zilliz.com/ai-faq/how-do-sentence-transformers-differ-from-traditional-word-embedding-models-like-word2vec-or-glove</a>
</li>

<li>BERT (language model)<br />
<a href="https://en.wikipedia.org/wiki/BERT_(language_model)">https://en.wikipedia.org/wiki/BERT_(language_model)</a>
</li>

<li>Levenštejnova vzdálenost<br />
<a href="https://cs.wikipedia.org/wiki/Leven%C5%A1tejnova_vzd%C3%A1lenost">https://cs.wikipedia.org/wiki/Leven%C5%A1tejnova_vzd%C3%A1lenost</a>
</li>

</ol>



<p></p><p></p>
<p><small>Autor: <a href="https://github.com/tisnik/">Pavel Tišnovský</a> &nbsp; 2025</small></p>
</body>
</html>

