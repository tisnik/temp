<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
        "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html>
<head>
<title>Knihovna FAISS a embedding: základ jazykových modelů (2. část)</title>
<meta name="Author" content="Pavel Tisnovsky" />
<meta name="Generator" content="vim" />
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<style type="text/css">
         body {color:#000000; background:#ffffff;}
         h1  {font-family: arial, helvetica, sans-serif; color:#ffffff; background-color:#c00000; text-align:center; padding-left:1em}
         h2  {font-family: arial, helvetica, sans-serif; color:#ffffff; background-color:#0000c0; padding-left:1em; text-align:left}
         h3  {font-family: arial, helvetica, sans-serif; color:#000000; background-color:#c0c0c0; padding-left:1em; text-align:left}
         h4  {font-family: arial, helvetica, sans-serif; color:#000000; background-color:#e0e0e0; padding-left:1em; text-align:left}
         a   {font-family: arial, helvetica, sans-serif;}
         li  {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify; width:450px;}
         ol  {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify; width:450px;}
         ul  {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify; width:450px;}
         p   {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify;}
         pre {background:#e0e0e0}
</style>
</head>

<body>

<h1>Knihovna FAISS a embedding: základ jazykových modelů (2. část)</h1>

<h3>Pavel Tišnovský</h3>

<p></p>

<h1>Úvodník</h1>

<p></p>



<h2>Obsah</h2>

<p><a href="#k01">*** 1. Knihovna FAISS a embedding: základ jazykových modelů (2. část)</a></p>
<p><a href="#k02">2. Úprava projektu &ndash; přidání nových závislostí</a></p>
<p><a href="#k03">3. Datová sada s&nbsp;jedním milionem anglických vět</a></p>
<p><a href="#k04">4. Získání vybrané datové sady <strong>polygraf-ai/human-sentences-1M-sample-v2</strong></a></p>
<p><a href="#k05">5. Slovník s&nbsp;datovými sadami vs. jedna datová sada</a></p>
<p><a href="#k06">6. Konstrukce běžného Pythonovského seznamu s&nbsp;větami</a></p>
<p><a href="#k07">7. Hledání nejpodobnějších vět v&nbsp;celé datové sadě</a></p>
<p><a href="#k08">8. Krátké připomenutí: vyhledávání na základě výrazů s&nbsp;využitím sémantické podobnosti</a></p>
<p><a href="#k09">*** 9. Využití celé datové sady pro sémantické vyhledávání</a></p>
<p><a href="#k10">10. Úplný zdrojový kód demonstračního příkladu</a></p>
<p><a href="#k11">*** 11. Refaktoring pro lepší čitelnost a další rozšiřování funkcionality</a></p>
<p><a href="#k12">*** 12. Výsledky získané demonstračním příkladem</a></p>
<p><a href="#k13">*** 13. Rychlost sémantického vyhledávání</a></p>
<p><a href="#k14">*** 14. </a></p>
<p><a href="#k15">*** 15. </a></p>
<p><a href="#k16">*** 16. </a></p>
<p><a href="#k17">*** 17. </a></p>
<p><a href="#k18">*** 18. </a></p>
<p><a href="#k19">19. Repositář s&nbsp;demonstračními příklady</a></p>
<p><a href="#k20">20. Odkazy na Internetu</a></p>



<p><a name="k01"></a></p>
<h2 id="k01">1. Knihovna FAISS a embedding: základ jazykových modelů (2. část)</h2>



<p><a name="k02"></a></p>
<h2 id="k02">2. Úprava projektu &ndash; přidání nových závislostí</h2>

<p>Ještě před tím, než si ukážeme další vlastnosti knihoven <i>Faiss</i> i
<i>sentence-transformers</i>, je nutné provést úpravu projektu. Přidáme do něj
další závislosti, konkrétně knihovny nazvané <i>datasets</i> a
<i>matplotlib</i>. To se provede snadno. Buď příkazy:</p>

<pre>
<strong>uv add datasets</strong>
<strong>uv add matplotlib</strong>
</pre>

<p>Nebo lze alternativně použít správce balíčků PDM:</p>

<pre>
<strong>uv add datasets</strong>
<strong>uv add matplotlib</strong>
</pre>

<p>Projektový soubor <strong>pyproject.toml</strong> by nyní měl vypadat
následovně:</p>

<pre>
[project]
name = "sentence-transformer"
version = "0.1.0"
description = "Sentence transformer demos"
authors = [
    {name = "Pavel Tisnovsky", email = "ptisnovs@redhat.com"},
]
dependencies = [
    "datasets&amp;=4.0.0",
    "faiss-cpu&amp;=1.11.0.post1",
    "matplotlib&amp;=3.10.5",
    "sentence-transformers&amp;=5.0.0",
]
requires-python = "==3.12.*"
readme = "README.md"
license = {text = "MIT"}
&nbsp;
&nbsp;
[tool.pdm]
distribution = false
</pre>

<p><div class="rs-tip-major">Poznámka: nástroj <strong>uv</strong> ve
skutečnosti všechny závislosti zapisuje na jediném řádku &ndash; neprovádí
formátování projektového souboru.</div></p>



<p><a name="k03"></a></p>
<h2 id="k03">3. Datová sada s&nbsp;jedním milionem anglických vět</h2>

<p>V&nbsp;demonstračních příkladech, které jsme si ukázali v&nbsp;předchozím
článku, se pracovalo pouze s&nbsp;minimalistickou &bdquo;databází&ldquo; textů,
která se skládala pouze z&nbsp;osmi vět, což skutečně při práci s&nbsp;reálným
mluveným jazykem není mnoho:</p>

<pre>
sentences = [
    "The rain in Spain falls mainly on the plain",
    "The tesselated polygon is a special type of polygon",
    "The quick brown fox jumps over the lazy dog",
    "To be or not to be, that is the question",
    "It is a truth universally acknowledged...",
    "How old are you?",
    "The goat ran down the hill"
]
</pre>

<p>Dnes již budeme pracovat s&nbsp;mnohem větší databází. Konkrétně se bude
jednat o datovou sadu <strong>polygraf-ai/human-sentences-1M-sample-v2</strong>
obsahující jeden milion anglických vět. Na tomto místě je vhodné poznamenat, že
se stále jedná o relativně malou datovou sadu (například pro trénink LLM je
<i>naprosto</i> nedostačující), ovšem pro otestování základních vlastností
knihoven <i>Faiss</i> a <i>sentence-transformers</i> bude její velikost
dostačující. Navíc je práce s&nbsp;touto datovou sadou relativně rychlá.</p>



<p><a name="k04"></a></p>
<h2 id="k04">4. Získání vybrané datové sady <strong>polygraf-ai/human-sentences-1M-sample-v2</strong></h2>

<p>Datovou sadu <strong>polygraf-ai/human-sentences-1M-sample-v2</strong> není
nutné stahovat ručně, protože tuto funkci zajišťuje přímo knihovna
<strong>dataset</strong>. Postačuje pouze zavolat funkci
<strong>load_dataset</strong> a předat jí jednoznačný identifikátor datové
sady. Knihovna automaticky zjistí, zda je datová sada dostupná na lokálním
úložišti a pokud tomu tak není, provede její stažení:</p>

<pre>
from datasets import load_dataset
&nbsp;
dataset_id = "polygraf-ai/human-sentences-1M-sample-v2"
&nbsp;
dataset = load_dataset(dataset_id)
&nbsp;
print(dataset)
</pre>

<p>Při prvním spuštění tohoto skriptu je patrné, že stažení opravdu
proběhlo (cca 100 MB!):</p>

<pre>
README.md: 100%|███████████████████████████████████████████████████████████████████████████████████████| 317/317 [00:00&lt;00:00, 1.31MB/s]
train-00000-of-00001.parquet: 100%|██████████████████████████████████████████████████████████████████| 100M/100M [00:09&lt;00:00, 10.1MB/s]
Generating train split: 100%|████████████████████████████████████████████████████████| 997593/997593 [00:01&lt;00:00, 926163.87 examples/s]
DatasetDict({
    train: Dataset({
        features: ['text', 'source'],
        num_rows: 997593
    })
})
</pre>

<p>Po každém dalším spuštění skriptu se pouze zobrazí informace o datové
sadě:</p>

<pre>
DatasetDict({
    train: Dataset({
        features: ['text', 'source'],
        num_rows: 997593
    })
})
</pre>

<p><div class="rs-tip-major">Poznámka: datová sada se uloží do adresáře
<strong>.cache/huggingface/datasets/</strong>. Nalezneme zde i soubor se všemi
anglickými větami ve formátu Arrow:</div></p>

<pre>
$ <strong>ls -lah human-sentences-1_m-sample-v2-train.arrow</strong>
&nbsp;
-rw-r--r--. 1 ptisnovs ptisnovs 145M Jul 31 18:46 human-sentences-1_m-sample-v2-train.arrow
</pre>



<p><a name="k05"></a></p>
<h2 id="k05">5. Slovník s&nbsp;datovými sadami vs. jedna datová sada</h2>

<p>Předchozí skript by měl po svém spuštění vypsat informaci o tom, že ve
skutečnosti nenačetl a nevrátil pouze jedinou datovou sadu, ale celý slovník
datových sad s&nbsp;jediným prvkem:</p>

<pre>
DatasetDict({
    train: Dataset({
        features: ['text', 'source'],
        num_rows: 997593
    })
})
</pre>

<p>Sadu nazvanou <strong>train</strong> získáme snadno &ndash; předáním
pojmenovaného parametru <strong>split="train"</strong> funkci
<strong>load_dataset</strong>:</p>

<pre>
from datasets import load_dataset
&nbsp;
dataset_id = "polygraf-ai/human-sentences-1M-sample-v2"
&nbsp;
dataset = load_dataset(dataset_id, <u>split="train"</u>)
&nbsp;
print(dataset.features)
</pre>

<p>Po spuštění tohoto skriptu by se měly vypsat informace o sloupcích
v&nbsp;tabulce, ve které je datová sada uložena:</p>

<pre>
{'text': Value('string'), 'source': Value('string')}
</pre>



<p><a name="k06"></a></p>
<h2 id="k06">6. Konstrukce běžného Pythonovského seznamu s&nbsp;větami</h2>

<p>Ukažme si ještě jeden mezikrok, který vlastně v&nbsp;praxi není nezbytně
nutné provádět, protože je výpočetně i paměťově náročný. Nicméně je vhodné
vědět, že stažená datová sada není nějakým &bdquo;magickým objektem&ldquo;, ale
skutečným zdrojem dat. Konkrétně si necháme z&nbsp;datové sady vygenerovat
běžný Pythonovský seznam se všemi větami (v&nbsp;čisté textové podobě):</p>

<pre>
from datasets import load_dataset
&nbsp;
dataset_id = "polygraf-ai/human-sentences-1M-sample-v2"
&nbsp;
dataset = load_dataset(dataset_id, split="train")
&nbsp;
print("Building sentences")
sentences = [sentence for sentence in dataset["text"]]
&nbsp;
print(f"{len(sentences)} sentences created")
</pre>

<p>Výsledek získaný po určité době (jednotky až desítky sekund):</p>

<pre>
Building sentences
997593 sentences created
</pre>

<p><div class="rs-tip-major">Poznámka: jméno datové sady tedy není zcela
přesné, protože počet vět je menší, než jeden milion.</div></p>

<p>Počet vět transformovaných do seznamu lze řídit s&nbsp;využitím operace řezu
(<i>slice</i>), a to <i>ještě</i> před vytvořením seznamu:</p>

<pre>
from datasets import load_dataset
&nbsp;
dataset_id = "polygraf-ai/human-sentences-1M-sample-v2"
&nbsp;
dataset = load_dataset(dataset_id, split="train")
&nbsp;
print("Building sentences")
sentences = [sentence for sentence in dataset["text"]<u>[0:100]</u>]
&nbsp;
print(f"{len(sentences)} sentences created")
</pre>

<p>Nyní získáme výsledky prakticky okamžitě:</p>

<pre>
Building sentences
100 sentences created
</pre>



<p><a name="k07"></a></p>
<h2 id="k07">7. Hledání nejpodobnějších vět v&nbsp;celé datové sadě</h2>

<p>V&nbsp;navazujících kapitolách si ukážeme, jakým způsobem je možné provádět
vyhledávání vět na základě podobnosti v&nbsp;rámci celé datové sady
s&nbsp;přibližně jedním milionem vět. Opět je však nutné zdůraznit, že 1000000
vět sice může vypadat jako velmi vysoká hodnota, ovšem v&nbsp;praxi se mnohdy
pracuje i s&nbsp;(mnohem) většími datovými sadami. Typickým příkladem je
trénink velkých jazykových modelů, což je však téma na samostatné články.</p>

<p>Na druhou stranu je ovšem tato hodnota již dostatečně vysoká na to, aby byly
jednotlivé kroky zpracování časově náročné. Z&nbsp;tohoto důvodu si otestujeme
rychlost (nebo spíše pomalost) vektorizace textů (<i>embedding</i>), rychlost
konstrukce indexu knihovnou FAISS a samozřejmě i rychlost vyhledávání
s&nbsp;využitím tohoto indexu.</p>



<p><a name="k08"></a></p>
<h2 id="k08">8. Krátké připomenutí: vyhledávání na základě výrazů s&nbsp;využitím sémantické podobnosti</h2>

<p>Jen pro připomenutí si ukažme demonstrační příklad <a
href="https://www.root.cz/clanky/knihovna-faiss-a-embedding-zaklad-jazykovych-modelu/">z&nbsp;předchozího
článku</a>. V&nbsp;tomto příkladu jsme si nechali zvoleným <i>embedding
modelem</i> &bdquo;vektorizovat&ldquo; sedm vět, poté jsme knihovnou FAISS
z&nbsp;těchto vektorů zkonstruovali <i>index</i> a následně jsme tento index
použili při vyhledávání výrazů či celých vět na základě jejich <i>sémantické
podobnosti</i> s&nbsp;původními větami:</p>

<pre>
from sentence_transformers import SentenceTransformer
import faiss
&nbsp;
model = SentenceTransformer("paraphrase-MiniLM-L6-v2")
&nbsp;
print(model)
&nbsp;
sentences = [
    "The rain in Spain falls mainly on the plain",
    "The tesselated polygon is a special type of polygon",
    "The quick brown fox jumps over the lazy dog",
    "To be or not to be, that is the question",
    "It is a truth universally acknowledged...",
    "How old are you?",
    "The goat ran down the hill"
]
&nbsp;
embeddings = model.encode(sentences)
print(f"Embeddings shape: {embeddings.shape}")
&nbsp;
similarities = model.similarity(embeddings, embeddings)
&nbsp;
DIMENSIONS = embeddings.shape[1]
&nbsp;
index = faiss.IndexFlatL2(DIMENSIONS)
index.add(embeddings)
&nbsp;
print(f"Index: {index.ntotal}")
&nbsp;
&nbsp;
def <strong>find_similar_sentences</strong>(query_sentence, k):
    query_embedding = model.encode([query_sentence])
    distances, indices = index.search(query_embedding, k)
    print("-"*40)
    print(f"Query: {query_sentence}")
    print(f"Most {k} similar sentences:")
    for i, idx in enumerate(indices[0]):
        print(f"{i + 1}: {sentences[idx]} (Distance: {distances[0][i]})")
&nbsp;
&nbsp;
find_similar_sentences("Shakespeare", 3)
find_similar_sentences("animal", 3)
find_similar_sentences("geometry", 3)
find_similar_sentences("weather", 3)
</pre>

<p>Příklad výsledků získaných po spuštění tohoto demonstračního příkladu:</p>

<pre>
SentenceTransformer(
  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False, 'architecture': 'BertModel'})
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
)
Embeddings shape: (7, 384)
Index: 7
----------------------------------------
Query: Shakespeare
Most 3 similar sentences:
1: To be or not to be, that is the question (Distance: 83.53305053710938)
2: The tesselated polygon is a special type of polygon (Distance: 112.60221862792969)
3: The rain in Spain falls mainly on the plain (Distance: 114.61812591552734)
----------------------------------------
Query: animal
Most 3 similar sentences:
1: The goat ran down the hill (Distance: 67.3703384399414)
2: The quick brown fox jumps over the lazy dog (Distance: 68.25883483886719)
3: To be or not to be, that is the question (Distance: 82.82962036132812)
----------------------------------------
Query: geometry
Most 3 similar sentences:
1: The tesselated polygon is a special type of polygon (Distance: 51.57851791381836)
2: To be or not to be, that is the question (Distance: 91.08709716796875)
3: The goat ran down the hill (Distance: 109.95413208007812)
----------------------------------------
Query: weather
Most 3 similar sentences:
1: The rain in Spain falls mainly on the plain (Distance: 73.53900146484375)
2: To be or not to be, that is the question (Distance: 86.14794921875)
3: How old are you? (Distance: 101.28636169433594)
</pre>



<p><a name="k09"></a></p>
<h2 id="k09">9. Využití celé datové sady pro sémantické vyhledávání</h2>



<p><a name="k10"></a></p>
<h2 id="k10">10. Úplný zdrojový kód demonstračního příkladu</h2>

<p>Úplný zdrojový kód demonstračního příkladu popsaného <a
href="#k09">v&nbsp;předchozí kapitole</a> vypadá následovně:</p>

<pre>
from sentence_transformers import SentenceTransformer
import faiss
from datasets import load_dataset
&nbsp;
&nbsp;
model = SentenceTransformer("paraphrase-MiniLM-L6-v2")
print(model)
&nbsp;
dataset = load_dataset("polygraf-ai/human-sentences-1M-sample-v2", split="train")
print(dataset)
&nbsp;
sentences = [sentence for sentence in dataset["text"][0:1000]]
print(f"{len(sentences)} sentences created")
&nbsp;
embeddings = model.encode(sentences)
print(f"Embeddings shape: {embeddings.shape}")
&nbsp;
DIMENSIONS = embeddings.shape[1]
index = faiss.IndexFlatL2(DIMENSIONS)
index.add(embeddings)
print(f"Index: {index.ntotal}")
&nbsp;
&nbsp;
def <strong>find_similar_sentences</strong>(query_sentence, k):
    query_embedding = model.encode([query_sentence])
    distances, indices = index.search(query_embedding, k)
    print("-"*40)
    print(f"Query: {query_sentence}")
    print(f"Most {k} similar sentences:")
    for i, idx in enumerate(indices[0]):
        print(f"{i + 1}: {sentences[idx]} (Distance: {distances[0][i]})")
&nbsp;
&nbsp;
find_similar_sentences("city", 3)
find_similar_sentences("animal", 3)
find_similar_sentences("geometry", 3)
find_similar_sentences("weather", 3)
find_similar_sentences("game", 3)
find_similar_sentences("school", 3)
</pre>



<p><a name="k11"></a></p>
<h2 id="k11">11. Refaktoring pro lepší čitelnost a další rozšiřování funkcionality</h2>

<pre>
from sentence_transformers import SentenceTransformer
import faiss
from datasets import load_dataset

MODEL_NAME = "paraphrase-MiniLM-L6-v2"
DATASET_ID = "polygraf-ai/human-sentences-1M-sample-v2"


def initialize_model(model_name):
    print("Model initialization started")
    model = SentenceTransformer(model_name)
    print(model)
    print("Model initialization finished")
    return model


def load_dataset_by_id(dataset_id):
    print("Loading dataset started")
    dataset = load_dataset(dataset_id, split="train")
    print("Loading dataset finished")
    return dataset


def build_sentences(dataset, from_, to_):
    print("Building sentences")
    sentences = [sentence for sentence in dataset["text"][from_:to_]]
    print(f"{len(sentences)} sentences created")
    return sentences


def create_embeddings(model, sentences):
    print("Embedding started")
    embeddings = model.encode(sentences)
    print(f"Embeddings shape: {embeddings.shape}")
    print("Embedding finished")
    return embeddings


def create_faiss_index(embeddings):
    print("FAISS index construction started")
    DIMENSIONS = embeddings.shape[1]
    index = faiss.IndexFlatL2(DIMENSIONS)
    index.add(embeddings)
    print(f"Index: {index.ntotal}")
    print("FAISS index construction finished")
    return index


def find_similar_sentences(model, index, query_sentence, k):
    query_embedding = model.encode([query_sentence])
    distances, indices = index.search(query_embedding, k)
    print("-"*40)
    print(f"Query: {query_sentence}")
    print(f"Most {k} similar sentences:")
    for i, idx in enumerate(indices[0]):
        print(f"{i + 1}: {sentences[idx]} (Distance: {distances[0][i]})")


model = initialize_model(MODEL_NAME)
dataset = load_dataset_by_id(DATASET_ID)
sentences = build_sentences(dataset, 0, 1000)
embeddings = create_embeddings(model, sentences)
index = create_faiss_index(embeddings)

find_similar_sentences(model, index, "city", 3)
find_similar_sentences(model, index, "animal", 3)
find_similar_sentences(model, index, "geometry", 3)
find_similar_sentences(model, index, "weather", 3)
find_similar_sentences(model, index, "game", 3)
find_similar_sentences(model, index, "school", 3)
</pre>



<p><a name="k12"></a></p>
<h2 id="k12">12. Výsledky získané demonstračním příkladem</h2>



<p><a name="k13"></a></p>
<h2 id="k13">13. Rychlost sémantického vyhledávání</h2>

<pre>
from sentence_transformers import SentenceTransformer
import faiss
from datasets import load_dataset
from time import time

MODEL_NAME = "paraphrase-MiniLM-L6-v2"
DATASET_ID = "polygraf-ai/human-sentences-1M-sample-v2"


def initialize_model(model_name):
    print("Model initialization started")
    model = SentenceTransformer(model_name)
    print(model)
    print("Model initialization finished")
    return model


def load_dataset_by_id(dataset_id):
    print("Loading dataset started")
    dataset = load_dataset(dataset_id, split="train")
    print("Loading dataset finished")
    return dataset


def build_sentences(dataset, from_, to_):
    print("Building sentences")
    sentences = [sentence for sentence in dataset["text"][from_:to_]]
    print(f"{len(sentences)} sentences created")
    return sentences


def create_embeddings(model, sentences):
    print("Embedding started")
    embeddings = model.encode(sentences)
    print(f"Embeddings shape: {embeddings.shape}")
    print("Embedding finished")
    return embeddings


def create_faiss_index(embeddings):
    print("FAISS index construction started")
    DIMENSIONS = embeddings.shape[1]
    index = faiss.IndexFlatL2(DIMENSIONS)
    index.add(embeddings)
    print(f"Index: {index.ntotal}")
    print("FAISS index construction finished")
    return index


def find_similar_sentences(model, index, query_sentence, k):
    t1 = time()
    query_embedding = model.encode([query_sentence])
    distances, indices = index.search(query_embedding, k)
    print("-"*40)
    print(f"Query: {query_sentence}")
    print(f"Most {k} similar sentences:")
    for i, idx in enumerate(indices[0]):
        print(f"{i + 1}: {sentences[idx]} (Distance: {distances[0][i]})")
    t2 = time()
    print(f"Finished in {t2-t1:3.2} seconds")


model = initialize_model(MODEL_NAME)
dataset = load_dataset_by_id(DATASET_ID)
sentences = build_sentences(dataset, 0, 100000)
embeddings = create_embeddings(model, sentences)
index = create_faiss_index(embeddings)

find_similar_sentences(model, index, "city", 3)
find_similar_sentences(model, index, "animal", 3)
find_similar_sentences(model, index, "geometry", 3)
find_similar_sentences(model, index, "weather", 3)
find_similar_sentences(model, index, "game", 3)
find_similar_sentences(model, index, "school", 3)
</pre>



<p><a name="k14"></a></p>
<h2 id="k14">14. </h2>

<pre>
from sentence_transformers import SentenceTransformer
import faiss
from datasets import load_dataset
from time import time
import numpy as np

import matplotlib.pyplot as plt

MODEL_NAME = "paraphrase-MiniLM-L6-v2"
DATASET_ID = "polygraf-ai/human-sentences-1M-sample-v2"


def initialize_model(model_name):
    print("Model initialization started")
    model = SentenceTransformer(model_name)
    print(model)
    print("Model initialization finished")
    return model


def load_dataset_by_id(dataset_id):
    print("Loading dataset started")
    dataset = load_dataset(dataset_id, split="train")
    print("Loading dataset finished")
    return dataset


def build_sentences(dataset, from_, to_):
    print("Building sentences")
    sentences = [sentence for sentence in dataset["text"][from_:to_]]
    print(f"{len(sentences)} sentences created")
    return sentences


def create_embeddings(model, sentences):
    t1 = time()
    print("Embedding started")
    embeddings = model.encode(sentences)
    print(f"Embeddings shape: {embeddings.shape}")
    print("Embedding finished")
    t2 = time()
    return embeddings, t2-t1


def create_faiss_index(embeddings):
    t1 = time()
    print("FAISS index construction started")
    DIMENSIONS = embeddings.shape[1]
    index = faiss.IndexFlatL2(DIMENSIONS)
    index.add(embeddings)
    print(f"Index: {index.ntotal}")
    print("FAISS index construction finished")
    t2 = time()
    return index, t2-t1


model = initialize_model(MODEL_NAME)
dataset = load_dataset_by_id(DATASET_ID)

ns = []
ts_embeddings = []
ts_index = []

for n in np.linspace(1000, 10000, 20):
    ns.append(n)
    sentences = build_sentences(dataset, 0, int(n))
    embeddings, t_embeddings = create_embeddings(model, sentences)
    index, t_index = create_faiss_index(embeddings)
    ts_embeddings.append(t_embeddings)
    ts_index.append(t_index)

fig = plt.figure(figsize=(10, 10))

plt.subplot(2, 1, 1)
plt.plot(ns, ts_embeddings, "b-")
plt.title("Embeddings creation")

plt.subplot(2, 1, 2)
plt.plot(ns, ts_index, "m-", label="index creation")
plt.title("Index creation")

# povolení zobrazení mřížky
plt.grid(True)

fig.savefig("embeddings.png")

# zobrazení grafu
fig.show()
</pre>



<p><a name="k15"></a></p>
<h2 id="k15">15. </h2>



<p><a name="k16"></a></p>
<h2 id="k16">16. </h2>



<p><a name="k17"></a></p>
<h2 id="k17">17. </h2>



<p><a name="k18"></a></p>
<h2 id="k18">18. </h2>



<p><a name="k19"></a></p>
<h2 id="k19">19. Repositář s&nbsp;demonstračními příklady</h2>

<p>Demonstrační příklady <a
href="https://www.root.cz/clanky/knihovna-faiss-a-embedding-zaklad-jazykovych-modelu/">z&nbsp;předchozího</a>
i dnešního článku lze nalézt na následujících odkazech:</p>

<table>
<tr><th> #</th><th>Příklad</th><th>Stručný popis</th><th>Adresa</th></tr>
<tr><td> 1</td><td>transformer1.py</td><td>inicializace modelu <strong>paraphrase-MiniLM-L6-v2</strong> přes knihovnu sentence-transformers s&nbsp;výpisem základních informací o něm</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss_transformers/transformer1.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transfomers/transformer1.py</a></td></tr>
<tr><td> 2</td><td>transformer2.py</td><td>inicializace modelu <strong>all-mpnet-base-v2</strong> přes knihovnu sentence-transformers s&nbsp;výpisem základních informací o něm</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss_transformers/transformer2.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transfomers/transformer2.py</a></td></tr>
<tr><td> 3</td><td>transformer3.py</td><td>inicializace modelu <strong>Seznam/small-e-czech</strong> přes knihovnu sentence-transformers s&nbsp;výpisem základních informací o něm</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss_transformers/transformer3.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transfomers/transformer3.py</a></td></tr>
<tr><td> 4</td><td>transformer4.py</td><td>vektorizace textů (vět) přes knihovnu sentence-transformers s&nbsp;využitím zvoleného modelu</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss_transformers/transformer4.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transfomers/transformer4.py</a></td></tr>
<tr><td> 5</td><td>transformer5.py</td><td>informace o vypočtené matici s&nbsp;vektorizovaným textem (<i>embedding</i>)</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss_transformers/transformer5.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transfomers/transformer5.py</a></td></tr>
<tr><td> 6</td><td>transformer6.py</td><td>výpočet a zobrazení vypočtené tabulky podobností</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss_transformers/transformer6.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transfomers/transformer6.py</a></td></tr>
<tr><td> 7</td><td>transformer7.py</td><td>nalezení významově nejpodobnějších vět s&nbsp;využitím vektorizované databáze textů</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss_transformers/transformer7.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transfomers/transformer7.py</a></td></tr>
<tr><td> 8</td><td>transformer8.py</td><td>nalezení významově nejpodobnějších vět s&nbsp;využitím vektorizované databáze textů</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss_transformers/transformer8.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transfomers/transformer8.py</a></td></tr>
<tr><td> 9</td><td>transformer9.py</td><td>nalezení vět nejbližších k&nbsp;zadanému termínu</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss_transformers/transformer9.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transfomers/transformer9.py</a></td></tr>
<tr><td>10</td><td>transformerA.py</td><td>podpora pro hledání na základě sémantické podobnosti</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss_transformers/transformerA.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transfomers/transformerA.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>11</td><td>dataset1.py</td><td>získání datové sady s&nbsp;milionem anglických vět</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss_transformers/dataset1.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transfomers/dataset1.py</a></td></tr>
<tr><td>12</td><td>dataset2.py</td><td>získání tréninkových dat s&nbsp;milionem anglických vět</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss_transformers/dataset2.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transfomers/dataset2.py</a></td></tr>
<tr><td>13</td><td>dataset3.py</td><td>konstrukce Pythonovského seznamu s&nbsp;větami</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss_transformers/dataset3.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transfomers/dataset3.py</a></td></tr>
<tr><td>14</td><td>dataset4.py</td><td>konstrukce Pythonovského seznamu s&nbsp;větami, omezení počtu vět</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss_transformers/dataset4.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transfomers/dataset4.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>15</td><td>transformerB.py</td><td>vytvoření indexu z&nbsp;datové sady s&nbsp;milionem anglických vět</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss_transformers/transformerB.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transfomers/transformerB.py</a></td></tr>
<tr><td>16</td><td>transformerC.py</td><td>refaktoring předchozího demonstračního příkladu</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss_transformers/transformerC.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transfomers/transformerC.py</a></td></tr>
<tr><td>17</td><td>transformerD.py</td><td>zjištění rychlosti nalezení nejpodobnějších vět</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss_transformers/transformerD.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transfomers/transformerD.py</a></td></tr>
<tr><td>18</td><td>transformerE.py</td><td>benchmark: rychlost embeddingu a konstrukce indexu (využití typu <strong>float32</strong>)</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss_transformers/transformerE.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transfomers/transformerE.py</a></td></tr>
<tr><td>19</td><td>transformerF.py</td><td>benchmark: rychlost embeddingu a konstrukce indexu (využití typu <strong>float16</strong>)</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss_transformers/transformerF.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transfomers/transformerF.py</a></td></tr>
<tr><td>20</td><td>transformerG.py</td><td>uložení modelu i indexu do souboru pro porovnání velikostí</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss_transformers/transformerG.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transfomers/transformerG.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>21</td><td>pyproject.toml</td><td>soubor s&nbsp;projektem a definicí všech potřebných závislostí</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss_transformers/pyproject.toml">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss-transfomers/pyproject.toml</a></td></tr>
</table>

<p>Demonstrační příklady vytvořené v&nbsp;Pythonu a popsané <a
href="https://www.root.cz/clanky/faiss-knihovna-pro-rychle-a-efektivni-vyhledavani-podobnych-vektoru/">v&nbsp;předchozím</a>
i v&nbsp;článku <a
href="https://www.root.cz/clanky/faiss-knihovna-pro-rychle-a-efektivni-vyhledavani-podobnych-vektoru-2-cast/">FAISS:
knihovna pro rychlé a efektivní vyhledávání podobných vektorů (2. část)</a> <a
href="https://github.com/tisnik/most-popular-python-libs/">https://github.com/tisnik/most-popular-python-libs/</a>.
Následují odkazy na jednotlivé příklady:</p>

<table>
<tr><th> #</th><th>Příklad</th><th>Stručný popis</th><th>Adresa</th></tr>
<tr><td> 1</td><td>faiss-1.py</td><td>seznamy souřadnic bodů v&nbsp;rovině</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-1.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-1.py</a></td></tr>
<tr><td> 2</td><td>faiss-2.py</td><td>konstrukce matice se souřadnicemi bodů</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-2.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-2.py</a></td></tr>
<tr><td> 3</td><td>faiss-3.py</td><td>konstrukce indexu pro vyhledávání na základě vzdálenosti</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-3.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-3.py</a></td></tr>
<tr><td> 4</td><td>faiss-4.py</td><td>nalezení nejbližších bodů k&nbsp;zadaným souřadnicím &ndash; výpis indexů nalezených bodů</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-4.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-4.py</a></td></tr>
<tr><td> 5</td><td>faiss-5.py</td><td>nalezení nejbližších bodů k&nbsp;zadaným souřadnicím &ndash; výpis souřadnic nalezených bodů</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-5.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-5.py</a></td></tr>
<tr><td> 6</td><td>faiss-6.py</td><td>vyhledávání bodů na základě skalárního součinu bez normalizace vektorů</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-6.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-6.py</a></td></tr>
<tr><td> 7</td><td>faiss-7.py</td><td>vyhledávání bodů na základě skalárního součinu s&nbsp;normalizací vektorů</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-7.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-7.py</a></td></tr>
<tr><td> 8</td><td>faiss-8.py</td><td>jednoduchý benchmark rychlosti vyhledávání knihovnou FAISS</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-8.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-8.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td> 9</td><td>faiss-9.py</td><td>vizualizace koncových bodů vektorů v&nbsp;rovině</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-9.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-9.py</a></td></tr>
<tr><td>10</td><td>faiss-A.py</td><td>vykreslení nejpodobnějších vektorů získaných na základě L2 metriky</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-A.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-A.py</a></td></tr>
<tr><td>11</td><td>faiss-B.py</td><td>nalezení nejpodobnějších vektorů získaných na základě skalárního součinu: varianta s&nbsp;nenormovanými vektory</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-B.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-B.py</a></td></tr>
<tr><td>12</td><td>faiss-C.py</td><td>nalezení nejpodobnějších vektorů získaných na základě skalárního součinu: varianta s&nbsp;normovanými vektory</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-C.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-C.py</a></td></tr>
<tr><td>13</td><td>faiss-D.py</td><td>vykreslení nejpodobnějších vektorů před jejich normalizací</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-D.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-D.py</a></td></tr>
<tr><td>14</td><td>faiss-E.py</td><td>vykreslení vektorů formou orientovaných šipek</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-E.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-E.py</a></td></tr>
<tr><td>15</td><td>faiss-F.py</td><td>vykreslení vektorů po jejich normalizaci formou orientovaných šipek</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-F.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-F.py</a></td></tr>
<tr><td>16</td><td>faiss-G.py</td><td>vyhledání a vykreslení nejvíce NEpodobných vektorů</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-G.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-G.py</a></td></tr>
<tr><td>17</td><td>faiss-H.py</td><td>vyhledání podobných vektorů se složkami typu <i>float16</i></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-H.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-H.py</a></td></tr>
<tr><td>18</td><td>faiss-I.py</td><td>jednoduchý benchmark rychlosti vyhledávání knihovnou FAISS: rozdíly mezi <i>float16</i> a <i>float32</i></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-I.py">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/faiss-I.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>19</td><td>pyproject.toml</td><td>soubor s&nbsp;projektem a definicí závislostí</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/pyproject.toml">https://github.com/tisnik/most-popular-python-libs/blob/master/faiss/pyproject.toml</a></td></tr>
</table>



<p><a name="k20"></a></p>
<h2 id="k20">20. Odkazy na Internetu</h2>

<ol>

<li>FAISS: knihovna pro rychlé a efektivní vyhledávání podobných vektorů<br />
<a href="https://www.root.cz/clanky/faiss-knihovna-pro-rychle-a-efektivni-vyhledavani-podobnych-vektoru/">https://www.root.cz/clanky/faiss-knihovna-pro-rychle-a-efektivni-vyhledavani-podobnych-vektoru/</a>
</li>

<li>FAISS: knihovna pro rychlé a efektivní vyhledávání podobných vektorů (2. část)<br />
<a href="https://www.root.cz/clanky/faiss-knihovna-pro-rychle-a-efektivni-vyhledavani-podobnych-vektoru-2-cast/">https://www.root.cz/clanky/faiss-knihovna-pro-rychle-a-efektivni-vyhledavani-podobnych-vektoru-2-cast/</a>
</li>

<li>Knihovna FAISS a embedding: základ jazykových modelů<br />
<a href="https://www.root.cz/clanky/knihovna-faiss-a-embedding-zaklad-jazykovych-modelu/">https://www.root.cz/clanky/knihovna-faiss-a-embedding-zaklad-jazykovych-modelu/</a>
</li>

<li>FAISS (Facebook AI Similarity Search)<br />
<a href="https://en.wikipedia.org/wiki/FAISS">https://en.wikipedia.org/wiki/FAISS</a>
</li>

<li>FAISS documentation<br />
<a href="https://faiss.ai/">https://faiss.ai/</a>
</li>

<li>Introduction to Facebook AI Similarity Search (Faiss)<br />
<a href="https://www.pinecone.io/learn/series/faiss/faiss-tutorial/">https://www.pinecone.io/learn/series/faiss/faiss-tutorial/</a>
</li>

<li>Faiss: Efficient Similarity Search and Clustering of Dense Vectors<br />
<a href="https://medium.com/@pankaj_pandey/faiss-efficient-similarity-search-and-clustering-of-dense-vectors-dace1df1e235">https://medium.com/@pankaj_pandey/faiss-efficient-similarity-search-and-clustering-of-dense-vectors-dace1df1e235</a>
</li>

<li>Cosine Distance vs Dot Product vs Euclidean in vector similarity search<br />
<a href="https://medium.com/data-science-collective/cosine-distance-vs-dot-product-vs-euclidean-in-vector-similarity-search-227a6db32edb">https://medium.com/data-science-collective/cosine-distance-vs-dot-product-vs-euclidean-in-vector-similarity-search-227a6db32edb</a>
</li>

<li>F16C<br />
<a href="https://en.wikipedia.org/wiki/F16C">https://en.wikipedia.org/wiki/F16C</a>
</li>

<li>FP16 (AVX-512)<br />
<a href="https://en.wikipedia.org/wiki/AVX-512#FP16">https://en.wikipedia.org/wiki/AVX-512#FP16</a>
</li>

<li>Top 8 Vector Databases in 2025: Features, Use Cases, and Comparisons<br />
<a href="https://synapsefabric.com/top-8-vector-databases-in-2025-features-use-cases-and-comparisons/">https://synapsefabric.com/top-8-vector-databases-in-2025-features-use-cases-and-comparisons/</a>
</li>

<li>Is FAISS a Vector Database? Complete Guide<br />
<a href="https://mljourney.com/is-faiss-a-vector-database-complete-guide/">https://mljourney.com/is-faiss-a-vector-database-complete-guide/</a>
</li>

<li>Vector database<br />
<a href="https://en.wikipedia.org/wiki/Vector_database">https://en.wikipedia.org/wiki/Vector_database</a>
</li>

<li>Similarity search<br />
<a href="https://en.wikipedia.org/wiki/Similarity_search">https://en.wikipedia.org/wiki/Similarity_search</a>
</li>

<li>Nearest neighbor search<br />
<a href="https://en.wikipedia.org/wiki/Nearest_neighbor_search#Approximation_methods">https://en.wikipedia.org/wiki/Nearest_neighbor_search#Approximation_methods</a>
</li>

<li>Decoding Similarity Search with FAISS: A Practical Approach<br />
<a href="https://www.luminis.eu/blog/decoding-similarity-search-with-faiss-a-practical-approach/">https://www.luminis.eu/blog/decoding-similarity-search-with-faiss-a-practical-approach/</a>
</li>

<li>MetricType and distances<br />
<a href="https://github.com/facebookresearch/faiss/wiki/MetricType-and-distances">https://github.com/facebookresearch/faiss/wiki/MetricType-and-distances</a>
</li>

<li>RAG - Retrieval-augmented generation<br />
<a href="https://en.wikipedia.org/wiki/Retrieval-augmented_generation">https://en.wikipedia.org/wiki/Retrieval-augmented_generation</a>
</li>

<li>pgvector na GitHubu<br />
<a href="https://github.com/pgvector/pgvector">https://github.com/pgvector/pgvector</a>
</li>

<li>Why we replaced Pinecone with PGVector<br />
<a href="https://www.confident-ai.com/blog/why-we-replaced-pinecone-with-pgvector">https://www.confident-ai.com/blog/why-we-replaced-pinecone-with-pgvector</a>
</li>

<li>PostgreSQL as VectorDB - Beginner Tutorial<br />
<a href="https://www.youtube.com/watch?v=Ff3tJ4pJEa4">https://www.youtube.com/watch?v=Ff3tJ4pJEa4</a>
</li>

<li>What is a Vector Database? (neobsahuje odpověď na otázku v titulku :-)<br />
<a href="https://www.youtube.com/watch?v=t9IDoenf-lo">https://www.youtube.com/watch?v=t9IDoenf-lo</a>
</li>

<li>PGVector: Turn PostgreSQL Into A Vector Database<br />
<a href="https://www.youtube.com/watch?v=j1QcPSLj7u0">https://www.youtube.com/watch?v=j1QcPSLj7u0</a>
</li>

<li>Milvus<br />
<a href="https://milvus.io/">https://milvus.io/</a>
</li>

<li>Vector Databases simply explained! (Embeddings &amp; Indexes)<br />
<a href="https://www.youtube.com/watch?v=dN0lsF2cvm4">https://www.youtube.com/watch?v=dN0lsF2cvm4</a>
</li>

<li>Vector databases are so hot right now. WTF are they?<br />
<a href="https://www.youtube.com/watch?v=klTvEwg3oJ4">https://www.youtube.com/watch?v=klTvEwg3oJ4</a>
</li>

<li>Step-by-Step Guide to Installing “pgvector” and Loading Data in PostgreSQL<br />
<a href="https://medium.com/@besttechreads/step-by-step-guide-to-installing-pgvector-and-loading-data-in-postgresql-f2cffb5dec43">https://medium.com/@besttechreads/step-by-step-guide-to-installing-pgvector-and-loading-data-in-postgresql-f2cffb5dec43</a>
</li>

<li>Best 17 Vector Databases for 2025<br />
<a href="https://lakefs.io/blog/12-vector-databases-2023/">https://lakefs.io/blog/12-vector-databases-2023/</a>
</li>

<li>Top 15 Vector Databases that You Must Try in 2025<br />
<a href="https://www.geeksforgeeks.org/top-vector-databases/">https://www.geeksforgeeks.org/top-vector-databases/</a>
</li>

<li>Picking a vector database: a comparison and guide for 2023<br />
<a href="https://benchmark.vectorview.ai/vectordbs.html">https://benchmark.vectorview.ai/vectordbs.html</a>
</li>

<li>Top 9 Vector Databases as of Feburary 2025<br />
<a href="https://www.shakudo.io/blog/top-9-vector-databases">https://www.shakudo.io/blog/top-9-vector-databases</a>
</li>

<li>What is a vector database?<br />
<a href="https://www.ibm.com/think/topics/vector-database">https://www.ibm.com/think/topics/vector-database</a>
</li>

<li>SQL injection<br />
<a href="https://en.wikipedia.org/wiki/SQL_injection">https://en.wikipedia.org/wiki/SQL_injection</a>
</li>

<li>Cosine similarity<br />
<a href="https://en.wikipedia.org/wiki/Cosine_similarity">https://en.wikipedia.org/wiki/Cosine_similarity</a>
</li>

<li>Euclidean distance<br />
<a href="https://en.wikipedia.org/wiki/Euclidean_distance">https://en.wikipedia.org/wiki/Euclidean_distance</a>
</li>

<li>Dot product<br />
<a href="https://en.wikipedia.org/wiki/Dot_product">https://en.wikipedia.org/wiki/Dot_product</a>
</li>

<li>Hammingova vzdálenost<br />
<a href="https://cs.wikipedia.org/wiki/Hammingova_vzd%C3%A1lenost">https://cs.wikipedia.org/wiki/Hammingova_vzd%C3%A1lenost</a>
</li>

<li>Jaccard index<br />
<a href="https://en.wikipedia.org/wiki/Jaccard_index">https://en.wikipedia.org/wiki/Jaccard_index</a>
</li>

<li>Manhattanská metrika<br />
<a href="https://cs.wikipedia.org/wiki/Manhattansk%C3%A1_metrika">https://cs.wikipedia.org/wiki/Manhattansk%C3%A1_metrika</a>
</li>

<li>pgvector: vektorová databáze postavená na Postgresu<br />
<a href="https://www.root.cz/clanky/pgvector-vektorova-databaze-postavena-na-postgresu/">https://www.root.cz/clanky/pgvector-vektorova-databaze-postavena-na-postgresu/</a>
</li>

<li>Matplotlib Home Page<br />
<a href="http://matplotlib.org/">http://matplotlib.org/</a>
</li>

<li>matplotlib (Wikipedia)<br />
<a href="https://en.wikipedia.org/wiki/Matplotlib">https://en.wikipedia.org/wiki/Matplotlib</a>
</li>

<li>Dot Product<br />
<a href="https://mathworld.wolfram.com/DotProduct.html">https://mathworld.wolfram.com/DotProduct.html</a>
</li>

<li>FAISS and sentence-transformers in 5 Minutes<br />
<a href="https://www.stephendiehl.com/posts/faiss/">https://www.stephendiehl.com/posts/faiss/</a>
</li>

<li>Sentence Transformer: Quickstart<br />
<a href="https://sbert.net/docs/quickstart.html#sentence-transformer">https://sbert.net/docs/quickstart.html#sentence-transformer</a>
</li>

<li>Sentence Transformers: Embeddings, Retrieval, and Reranking<br />
<a href="https://pypi.org/project/sentence-transformers/">https://pypi.org/project/sentence-transformers/</a>
</li>

<li>uv<br />
<a href="https://docs.astral.sh/uv/">https://docs.astral.sh/uv/</a>
</li>

<li>A Gentle Introduction to Retrieval Augmented Generation (RAG)<br />
<a href="https://wandb.ai/cosmo3769/RAG/reports/A-Gentle-Introduction-to-Retrieval-Augmented-Generation-RAG---Vmlldzo1MjM4Mjk1">https://wandb.ai/cosmo3769/RAG/reports/A-Gentle-Introduction-to-Retrieval-Augmented-Generation-RAG---Vmlldzo1MjM4Mjk1</a>
</li>

<li>The Beginner’s Guide to Text Embeddings<br />
<a href="https://www.deepset.ai/blog/the-beginners-guide-to-text-embeddings">https://www.deepset.ai/blog/the-beginners-guide-to-text-embeddings</a>
</li>

<li>What are Word Embeddings?<br />
<a href="https://www.youtube.com/watch?v=wgfSDrqYMJ4">https://www.youtube.com/watch?v=wgfSDrqYMJ4</a>
</li>

<li>How to choose an embedding model<br />
<a href="https://www.youtube.com/watch?v=djp4205tHGU">https://www.youtube.com/watch?v=djp4205tHGU</a>
</li>

<li>What is a Vector Database? Powering Semantic Search &amp; AI Applications<br />
<a href="https://www.youtube.com/watch?v=gl1r1XV0SLw">https://www.youtube.com/watch?v=gl1r1XV0SLw</a>
</li>

<li>How do Sentence Transformers differ from traditional word embedding models like Word2Vec or GloVe?<br />
<a href="https://zilliz.com/ai-faq/how-do-sentence-transformers-differ-from-traditional-word-embedding-models-like-word2vec-or-glove">https://zilliz.com/ai-faq/how-do-sentence-transformers-differ-from-traditional-word-embedding-models-like-word2vec-or-glove</a>
</li>

<li>BERT (language model)<br />
<a href="https://en.wikipedia.org/wiki/BERT_(language_model)">https://en.wikipedia.org/wiki/BERT_(language_model)</a>
</li>

<li>Levenštejnova vzdálenost<br />
<a href="https://cs.wikipedia.org/wiki/Leven%C5%A1tejnova_vzd%C3%A1lenost">https://cs.wikipedia.org/wiki/Leven%C5%A1tejnova_vzd%C3%A1lenost</a>
</li>

</ol>



<p></p><p></p>
<p><small>Autor: <a href="https://github.com/tisnik/">Pavel Tišnovský</a> &nbsp; 2025</small></p>
</body>
</html>

