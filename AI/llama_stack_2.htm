<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
        "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html>
<head>
<title></title>
<meta name="Author" content="Pavel Tisnovsky" />
<meta name="Generator" content="vim" />
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<style type="text/css">
         body {color:#000000; background:#ffffff;}
         h1  {font-family: arial, helvetica, sans-serif; color:#ffffff; background-color:#c00000; text-align:center; padding-left:1em}
         h2  {font-family: arial, helvetica, sans-serif; color:#ffffff; background-color:#0000c0; padding-left:1em; text-align:left}
         h3  {font-family: arial, helvetica, sans-serif; color:#000000; background-color:#c0c0c0; padding-left:1em; text-align:left}
         h4  {font-family: arial, helvetica, sans-serif; color:#000000; background-color:#e0e0e0; padding-left:1em; text-align:left}
         a   {font-family: arial, helvetica, sans-serif;}
         li  {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify; width:450px;}
         ol  {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify; width:450px;}
         ul  {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify; width:450px;}
         p   {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify;}
         pre {background:#e0e0e0}
</style>
</head>

<body>

<h1></h1>

<h3>Pavel Tišnovský</h3>

<p></p>

<h1>Úvodník</h1>

<p></p>



<h2>Obsah</h2>

<p><a href="#k01">*** 1. </a></p>
<p><a href="#k02">2. Základní konfigurace Llama Stacku</a></p>
<p><a href="#k03">3. Spuštění Llama Stacku v&nbsp;režimu serveru</a></p>
<p><a href="#k04">4. Komunikace s&nbsp;LLM přes Llama Stack</a></p>
<p><a href="#k05">5. Nové rozhraní pro komunikaci s&nbsp;LLM</a></p>
<p><a href="#k06">6. Složitější konfigurace Llama Stacku</a></p>
<p><a href="#k07">7. Výpis všech dostupných API</a></p>
<p><a href="#k08">8. Poskytovatelé (<i>providers</i>)</a></p>
<p><a href="#k09">9. Výpis všech dostupných poskytovatelů</a></p>
<p><a href="#k10">10. Přidání závislostí do projektového souboru a úprava konfiguračního souboru Llama Stacku</a></p>
<p><a href="#k11">*** 11. Pomocné tabulky vytvářené a používané při běhu Llama Stacku</a></p>
<p><a href="#k12">*** 12. </a></p>
<p><a href="#k13">*** 13. </a></p>
<p><a href="#k14">*** 14. </a></p>
<p><a href="#k15">*** 15. </a></p>
<p><a href="#k16">*** 16. </a></p>
<p><a href="#k17">*** 17. </a></p>
<p><a href="#k18">*** 18. </a></p>
<p><a href="#k19">*** 19. Repositář s&nbsp;demonstračními příklady</a></p>
<p><a href="#k20">20. Odkazy na Internetu</a></p>



<p><a name="k01"></a></p>
<h2 id="k01">1. </h2>

<p>Na <a href="https://www.root.cz/clanky/llama-stack-framework-pro-tvorbu-aplikaci-s-generativni-ai/"></a></p>

<pre>
[project]
name = "llama-stack-demo"
version = "0.1.0"
description = "Default template for PDM package"
authors = [
    {name = "Pavel Tisnovsky", email = "ptisnovs@redhat.com"},
]
dependencies = [
    "llama-stack==0.2.23",
    "fastapi&gt;=0.115.12",
    "opentelemetry-sdk&gt;=1.34.0",
    "opentelemetry-exporter-otlp&gt;=1.34.0",
    "opentelemetry-instrumentation&gt;=0.55b0",
    "aiosqlite&gt;=0.21.0",
    "litellm&gt;=1.72.1",
    "uvicorn&gt;=0.34.3",
    "blobfile&gt;=3.0.0"
]
requires-python = "==3.12.*"
readme = "README.md"
license = {text = "MIT"}
&nbsp;
&nbsp;
[tool.pdm]
distribution = false
</pre>



<p><a name="k02"></a></p>
<h2 id="k02">2. Základní konfigurace Llama Stacku</h2>

<p>V&nbsp;úvodním článku jsme si kromě dalších informací uvedli i minimální
(ale stále plně funkční) konfiguraci Llama Stacku. Tato konfigurace by měla být
uložena v&nbsp;souboru s&nbsp;názvem <strong>run.yaml</strong> vypadá
následovně:</p>

<pre>
version: '2'
image_name: simplest-llama-stack-configuration
container_image: null
&nbsp;
distribution_spec:
  local:
    services:
      - inference
      - telemetry
&nbsp;
apis:
  - inference
  - telemetry
&nbsp;
providers:
  inference:
    - provider_id: openai
      provider_type: remote::openai
      config:
        api_key: <u>${env.OPENAI_API_KEY}</u>
  telemetry:
    - provider_id: meta-reference
      provider_type: inline::meta-reference
      config:
        sinks: ['console']
&nbsp;
models:
  - model_id: gpt-4-turbo
    provider_id: openai
    model_type: llm
    provider_model_id: gpt-4-turbo
&nbsp;
server:
  port: 8321
</pre>

<p>Důležité je, že je nakonfigurován LLM server, který se má volat
(v&nbsp;tomto kontextu se setkáme s&nbsp;pojmem poskytovatel a v&nbsp;tomto
případě se jedná o společnosti OpenAI). Tuto konfiguraci nalezneme v&nbsp;sekci
<strong>providers</strong>, protože LLM servery jsou z&nbsp;pohledu Llama
Stacku skutečně považovány za jeden (z&nbsp;mnoha typů) poskytovatelů:</p>

<pre>
providers:
  inference:
    - provider_id: openai
      provider_type: remote::openai
      config:
        api_key: <u>${env.OPENAI_API_KEY}</u>
</pre>

<p>Druhým nakonfigurovaným poskytovatelem je poskytovatel telemetrie. Současná
verze Llama Stacku vyžaduje, aby byl nakonfigurován, i když telemetrii
nepoužijete.</p>

<p>To však není postačující, protože kromě LLM serveru je nutné nakonfigurovat
i model, který bude volán. Poskytovatel OpenAI nabízí relativně velké množství
LLM modelů, mezi než patří i <strong>gpt-4-turbo</strong>. Konfigurace tohoto
modelu je uvedena v&nbsp;samostatné sekci <strong>models</strong> a vypadá
následovně:</p>

<pre>
models:
  - model_id: gpt-4-turbo
    provider_id: openai
    model_type: llm
    provider_model_id: gpt-4-turbo
</pre>

<p>Tato konfigurace předpokládá, že ještě před vlastním spuštěním serveru dojde
k&nbsp;nastavení klíče pro OpenAI do proměnné prostředí nazvané
<strong>OPENAI_API_KEY</strong>. Toto nastavení bude vypadat následovně:</p>

<pre>
$ export OPENAI_API_KEY="sk-xyzzy1234567890xyzzy1234567890xyzzy1234567890..."
</pre>

<p><div class="rs-tip-major">Poznámka: v&nbsp;sekci <strong>server</strong> je
uvedeno, že Llama Stack po svém spuštění bude dostupný na portu číslo 8321.
Přes tento port budeme k&nbsp;běžícímu Llama Stacku přistupovat.</div></p>



<p><a name="k03"></a></p>
<h2 id="k03">3. Spuštění Llama Stacku v&nbsp;režimu serveru</h2>

<p>V&nbsp;adresáři, ve kterém je uložen projektový soubor
<strong>pyproject.toml</strong> i konfigurační soubor <strong>run.yaml</strong>
zmíněný <a href="#k02">v&nbsp;předchozí kapitole</a>, spustíme Llama Stack
v&nbsp;režimu serveru. Pro tento účel je možné použít například nástroj
<strong>PDM</strong> nebo <strong>uv</strong> (oba tyto nástroje provádí
shodnou činnost, ovšem <strong>uv</strong> je značně rychlejší). Při použití
PDM vypadá spuštění Llama Stacku následovně:</p>

<pre>
$ <strong>pdm run llama stack run run.yaml</strong>
</pre>

<p>Naopak při použití nástroje <strong>uv</strong> se změní použitý příkaz
(ovšem jeho parametry zůstanou stejné):</p>

<pre>
$ <strong>uv run llama stack run run.yaml</strong>
</pre>

<p>Llama Stack by měl po svém spuštění vypsat aktuální konfiguraci načtenou ze
souboru <strong>run.yaml</strong>, a na konci taktéž port, na které běží:</p>

<pre>
INFO     2025-09-27 16:28:11,355 llama_stack.cli.stack.run:125 server: Using run configuration: run.yaml                
INFO     2025-09-27 16:28:11,362 llama_stack.cli.stack.run:146 server: No image type or image name provided. Assuming   
         environment packages.                                                                                          
INFO     2025-09-27 16:28:11,924 llama_stack.distribution.server.server:422 server: Using config file: run.yaml         
INFO     2025-09-27 16:28:11,925 llama_stack.distribution.server.server:424 server: Run configuration:                  
INFO     2025-09-27 16:28:11,928 llama_stack.distribution.server.server:426 server: apis:                               
         - inference                                                                                                    
         - telemetry                                                                                                    
         benchmarks: []                                                                                                 
         container_image: null                                                                                          
         datasets: []                                                                                                   
         external_providers_dir: null                                                                                   
         image_name: simplest-llama-stack-configuration                                                                 
         inference_store: null                                                                                          
         logging: null                                                                                                  
         metadata_store: null                                                                                           
         models:                                                                                                        
         - metadata: {}                                                                                                 
           model_id: gpt-4-turbo                                                                                        
           model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType                                   
           - llm                                                                                                        
           provider_id: openai                                                                                          
           provider_model_id: gpt-4-turbo                                                                               
         providers:                                                                                                     
           inference:                                                                                                   
           - config:                                                                                                    
               api_key: '********'                                                                                      
             provider_id: openai                                                                                        
             provider_type: remote::openai                                                                              
           telemetry:                                                                                                   
           - config:                                                                                                    
               sinks:                                                                                                   
               - console                                                                                                
             provider_id: meta-reference                                                                                
             provider_type: inline::meta-reference                                                                      
         scoring_fns: []                                                                                                
         server:                                                                                                        
           auth: null                                                                                                   
           host: null                                                                                                   
           port: 8321                                                                                                   
           quota: null                                                                                                  
           tls_cafile: null                                                                                             
           tls_certfile: null                                                                                           
           tls_keyfile: null                                                                                            
         shields: []                                                                                                    
         tool_groups: []                                                                                                
         vector_dbs: []                                                                                                 
         version: '2'                                                                                                   
&nbsp;                                                                                                                        
INFO     2025-09-27 16:28:13,560 llama_stack.distribution.server.server:564 server: Listening on ['::', '0.0.0.0']:8321 
INFO     2025-09-27 16:28:13,568 llama_stack.distribution.server.server:156 server: Starting up                         
</pre>

<p>Pro jistotu si přímo z&nbsp;příkazové řádky (pochopitelně z&nbsp;jiného
terminálu) ověříme, že Llama Stack skutečně běží na portu 8321 a současně má
nakonfigurován LLM model <strong>gpt-4-turbo</strong> nabízený OpenAI
(<i>provider_id</i>):</p>

<pre>
$ <strong>curl localhost:8321/v1/models | jq .</strong>
&nbsp;
{
  "data": [
    {
      "identifier": "gpt-4-turbo",
      "provider_resource_id": "gpt-4-turbo",
      "provider_id": "openai",
      "type": "model",
      "metadata": {},
      "model_type": "llm"
    }
  ]
}
</pre>

<p>Novější verze Llama Stacku vrátí všechny nabízené modely, nikoli pouze model
nakonfigurovaný (pozor &ndash; podobných nekompatibilit nalezneme celou
řadu!):</p>

<pre>
{
  "data": [
    {
      "identifier": "gpt-4-turbo",
      "provider_resource_id": "gpt-4-turbo",
      "provider_id": "openai",
      "type": "model",
      "metadata": {},
      "model_type": "llm"
    },
    {
      "identifier": "openai/gpt-4-turbo",
      "provider_resource_id": "gpt-4-turbo",
      "provider_id": "openai",
      "type": "model",
      "metadata": {},
      "model_type": "llm"
    },
    {
      "identifier": "openai/gpt-3.5-turbo-0125",
      "provider_resource_id": "gpt-3.5-turbo-0125",
      "provider_id": "openai",
      "type": "model",
      "metadata": {},
      "model_type": "llm"
    },
    {
      "identifier": "openai/gpt-3.5-turbo",
      "provider_resource_id": "gpt-3.5-turbo",
      "provider_id": "openai",
      "type": "model",
      "metadata": {},
      "model_type": "llm"
    },
    {
      "identifier": "openai/gpt-3.5-turbo-instruct",
      "provider_resource_id": "gpt-3.5-turbo-instruct",
      "provider_id": "openai",
      "type": "model",
      "metadata": {},
      "model_type": "llm"
    },
    {
      "identifier": "openai/gpt-4",
      "provider_resource_id": "gpt-4",
      "provider_id": "openai",
      "type": "model",
      "metadata": {},
      "model_type": "llm"
    },
    {
      "identifier": "openai/gpt-4o",
      "provider_resource_id": "gpt-4o",
      "provider_id": "openai",
      "type": "model",
      "metadata": {},
      "model_type": "llm"
    },
    {
      "identifier": "openai/gpt-4o-2024-08-06",
      "provider_resource_id": "gpt-4o-2024-08-06",
      "provider_id": "openai",
      "type": "model",
      "metadata": {},
      "model_type": "llm"
    },
    {
      "identifier": "openai/gpt-4o-mini",
      "provider_resource_id": "gpt-4o-mini",
      "provider_id": "openai",
      "type": "model",
      "metadata": {},
      "model_type": "llm"
    },
    {
      "identifier": "openai/gpt-4o-audio-preview",
      "provider_resource_id": "gpt-4o-audio-preview",
      "provider_id": "openai",
      "type": "model",
      "metadata": {},
      "model_type": "llm"
    },
    {
      "identifier": "openai/chatgpt-4o-latest",
      "provider_resource_id": "chatgpt-4o-latest",
      "provider_id": "openai",
      "type": "model",
      "metadata": {},
      "model_type": "llm"
    },
</pre>



<p><a name="k04"></a></p>
<h2 id="k04">4. Komunikace s&nbsp;LLM přes Llama Stack</h2>

<p>Připomeňme si ještě, jak vypadá program (skript) naprogramovaný
v&nbsp;Pythonu, který přes Llama Stack zavolá LLM (nakonfigurovaný
v&nbsp;souboru <strong>run.yaml</strong>), předá zvolenému jazykovému modelu
otázku a nakonec zobrazí jeho odpověď:</p>

<pre>
from llama_stack_client import LlamaStackClient
&nbsp;
PROMPT = "<strong>Say Hello in Czech</strong>"
&nbsp;
client = LlamaStackClient(base_url="http://localhost:8321")
&nbsp;
print(f"Using Llama Stack version {client._version}")
&nbsp;
response = <u>client.inference.chat_completion</u>(
    messages=[{"role": "user", "content": PROMPT}],
    model_id=client.models.list()[0].identifier,
)
&nbsp;
text = response.completion_message.content
print(f"LLM response: {text}")
</pre>

<p>Spuštění tohoto skriptu se opět provede přes nástroje PDM nebo uv:</p>

<pre>
$ <strong>pdm run client1.py</strong>
</pre>

<p>Alternativně:</p>

<pre>
$ <strong>uv run client1.py</strong>
</pre>

<p>Po spuštění skriptu se zobrazí informace o verzi Llama Stacku, dále
informace o tom, které koncové body Llama Stacku se volají, následuje varování
o použití staršího API (viz další kapitolu) a posléze se již zobrazí odpověď
získaná z&nbsp;LLM:</p>

<pre>
Using Llama Stack version 0.2.23
INFO:httpx:HTTP Request: GET http://localhost:8321/v1/models "HTTP/1.1 200 OK"
/tmp/ramdisk/yyyy/client1.py:9: DeprecationWarning: /v1/inference/chat-completion is deprecated. Please use /v1/openai/v1/chat/completions.
  response = client.inference.chat_completion(
INFO:httpx:HTTP Request: POST http://localhost:8321/v1/inference/chat-completion "HTTP/1.1 200 OK"
LLM response: <strong>Hello in Czech is "Ahoj" for informal situations or "Dobrý den" for more formal occasions.</strong>
</pre>



<p><a name="k05"></a></p>
<h2 id="k05">5. Nové rozhraní pro komunikaci s&nbsp;LLM</h2>

<p>Skript uvedený <a href="#k04">z&nbsp;předchozí kapitole</a> je sice funkční,
ovšem vypisuje varování o tom, že se volá nepodporovaný REST API endpoint (sice
nepřímo, ale je tomu tak).  Llama Stack se totiž poměrně rychle vyvíjí a proto
je nutné v&nbsp;aplikacích, které ho využívají, provádět mnohdy i poměrně
razantní změny (což je na tak mladý projekt dosti nepříjemné).</p>

<p>Úprava skriptu tak, aby používal novější (snad již stabilní) REST API
endpoint, by mohla vypadat následovně:</p>

<pre>
from llama_stack_client import LlamaStackClient
&nbsp;
client = LlamaStackClient(base_url="http://localhost:8321")
&nbsp;
print(f"Using Llama Stack version {client._version}")
&nbsp;
models = client.models.list()
model_id = models[0].identifier
&nbsp;
print(f"Using model {model_id}")
&nbsp;
response = <strong>client.chat.completions.create</strong>(
    model=model_id,
    messages=[{"role": "user", "content": "What is the capital of France?"}]
)
&nbsp;
print(response.to_json())
</pre>

<p>Aby bylo zřejmé, že odpověď LLM serveru obsahuje i mnohé další
(meta)informace, necháme si celou odpověď zobrazit ve formátu JSON. Samotná
informace o hlavním městě Francie je zvýrazněna:</p>

<pre>
{
  "id": "chatcmpl-CJPFSGyinwPahGe7s2y2FWtI9fqAZ",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "<strong>The capital of France is Paris</strong>.",
        "tool_calls": null,
        "refusal": null,
        "annotations": [],
        "audio": null,
        "function_call": null
      },
      "logprobs": null
    }
  ],
  "created": 1758741322,
  "model": "gpt-4-turbo-2024-04-09",
  "object": "chat.completion",
  "service_tier": "default",
  "system_fingerprint": "fp_de235176ee",
  "usage": {
    "completion_tokens": 7,
    "prompt_tokens": 14,
    "total_tokens": 21,
    "completion_tokens_details": {
      "accepted_prediction_tokens": 0,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": 0
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 0
    }
  },
  "metrics": [
    {
      "trace_id": "494b4862ce1339456957d19b46224579",
      "span_id": "664de9d2c854e534",
      "timestamp": "2025-09-24T19:15:23.602894Z",
      "attributes": {
        "model_id": "openai/gpt-4-turbo",
        "provider_id": "openai"
      },
      "type": "metric",
      "metric": "prompt_tokens",
      "value": 14,
      "unit": "tokens"
    },
    {
      "trace_id": "494b4862ce1339456957d19b46224579",
      "span_id": "664de9d2c854e534",
      "timestamp": "2025-09-24T19:15:23.602943Z",
      "attributes": {
        "model_id": "openai/gpt-4-turbo",
        "provider_id": "openai"
      },
      "type": "metric",
      "metric": "completion_tokens",
      "value": 7,
      "unit": "tokens"
    },
    {
      "trace_id": "494b4862ce1339456957d19b46224579",
      "span_id": "664de9d2c854e534",
      "timestamp": "2025-09-24T19:15:23.602961Z",
      "attributes": {
        "model_id": "openai/gpt-4-turbo",
        "provider_id": "openai"
      },
      "type": "metric",
      "metric": "total_tokens",
      "value": 21,
      "unit": "tokens"
    }
  ]
}
</pre>



<p><a name="k06"></a></p>
<h2 id="k06">6. Složitější konfigurace Llama Stacku</h2>

<p>Samotná konfigurace Llama Stacku, která byla ukázána <a href="#k02">ve druhé
kapitole</a>, je pochopitelně modifikovatelná a rozšiřitelná. Změna či
rozšíření konfigurace se týká dvou hlavních oblastí: API (lze povolit či
zakázat jednotlivé skupiny API Llama Stacku) a takzvaných poskytovatelů
(<i>providers</i>). V&nbsp;následujících kapitolách si alespoň ve stručnosti
ukážeme, jak může taková konfigurace vypadat.</p>

<p><div class="rs-tip-major">Poznámka: na tomto místě je vhodné upozornit na
to, že samotná dokumentace Llama Stacku je (prozatím) neúplná a zejména
informace o dostupných poskytovatelích by mohla být mnohem lepší a
přehlednější.</div></p>



<p><a name="k07"></a></p>
<h2 id="k07">7. Výpis všech dostupných API</h2>

<p><a href="#k07">Ve druhé kapitole</a> jsme povolili pouze dvě skupiny
API:</p>

<pre>
apis:
  - inference
  - telemetry
</pre>

<p>Ve skutečnosti je možné pracovat s&nbsp;více skupinami API. O které skupiny
se jedná, lze zjistit na příkazové řádce zadáním příkazu:</p>

<pre>
$ <strong>uv run llama stack list-apis</strong>
</pre>

<p>nebo:</p>

<pre>
$ <strong>pdm run llama stack list-apis</strong>
</pre>

<p>Výsledný seznam dostupných API bude v&nbsp;Llama Stacku verze 0.2.22 vypadat
následovně:</p>

<pre>
┏━━━━━━━━━━━━━━━━━━━┓
┃ API               ┃
┡━━━━━━━━━━━━━━━━━━━┩
│ providers         │
├───────────────────┤
│ inference         │
├───────────────────┤
│ safety            │
├───────────────────┤
│ agents            │
├───────────────────┤
│ batches           │
├───────────────────┤
│ vector_io         │
├───────────────────┤
│ datasetio         │
├───────────────────┤
│ scoring           │
├───────────────────┤
│ eval              │
├───────────────────┤
│ post_training     │
├───────────────────┤
│ tool_runtime      │
├───────────────────┤
│ telemetry         │
├───────────────────┤
│ models            │
├───────────────────┤
│ shields           │
├───────────────────┤
│ vector_dbs        │
├───────────────────┤
│ datasets          │
├───────────────────┤
│ scoring_functions │
├───────────────────┤
│ benchmarks        │
├───────────────────┤
│ tool_groups       │
├───────────────────┤
│ files             │
├───────────────────┤
│ prompts           │
├───────────────────┤
│ inspect           │
└───────────────────┘
</pre>

<p><div class="rs-tip-major">Poznámka: v&nbsp;navazujících kapitolách využijeme
pouze některé z&nbsp;těchto API.</div></p>



<p><a name="k08"></a></p>
<h2 id="k08">8. Poskytovatelé (<i>providers</i>)</h2>

<p>V&nbsp;praxi je mnohdy nutné kromě LLM serveru přímo či nepřímo používat i
další <i>poskytovatele</i> (<i>providers</i>). Těch existuje celá řada a dělí
se do několika skupin:</p>

<table>
<tr><th>Označení</th><th>Stručný popis</th></tr>
<tr><td>Agents</td><td>interakce se systémem agentů (samotní agenti mohou provádět různé operace)</td></tr>
<tr><td>Inference</td><td>rozhraní k&nbsp;LLM modelům i k&nbsp;embedding modelům</td></tr>
<tr><td>VectorIO</td><td>původně rozhraní k&nbsp;vektorovým databázím (hledání podobných vektorů), nyní i fulltext hledání</td></tr>
<tr><td>Safety</td><td>detekce dotazů s&nbsp;nevhodným či nepovoleným obsahem apod.</td></tr>
<tr><td>Telemetry</td><td>telemetrie (OpenTelemetry ale i další)</td></tr>
<tr><td>Eval</td><td>vyhodnocení odpovědí LLM modelů atd. (způsoby použití se různí)</td></tr>
<tr><td>DatasetIO</td><td>čtení a zápisy datových sad z/do zvoleného systému (může být i lokální souborový systém)</td></tr>
</table>



<p><a name="k09"></a></p>
<h2 id="k09">9. Výpis všech dostupných poskytovatelů</h2>

<p>Podobně jako dokáže Llama Stack vypsat seznam dostupných API, může vypsat i
dostupné poskytovatele. Postačuje použít příkaz:</p>

<pre>
$ <strong>uv run llama stack list-providers</strong>
</pre>

<p>nebo:</p>

<pre>
$ <strong>pdm run llama stack list-providers</strong>
</pre>

<p>Opět by se měla zobrazit tabulka, ve které je u každého poskytovatele
vypsána jak jeho skupina (například VectorIO), tak i vyžadované závislosti:</p>

<pre>
┏━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ API Type      ┃ Provider Type                  ┃ PIP Package Dependencies                               ┃
┡━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ batches       │ inline::reference              │ openai                                                 │
├───────────────┼────────────────────────────────┼────────────────────────────────────────────────────────┤
│ datasetio     │ inline::localfs                │ pandas                                                 │
├───────────────┼────────────────────────────────┼────────────────────────────────────────────────────────┤
│ datasetio     │ remote::huggingface            │ datasets                                               │
├───────────────┼────────────────────────────────┼────────────────────────────────────────────────────────┤
│ datasetio     │ remote::nvidia                 │ datasets                                               │
├───────────────┼────────────────────────────────┼────────────────────────────────────────────────────────┤
│ eval          │ inline::meta-reference         │ tree_sitter,pythainlp,langdetect,emoji,nltk            │
├───────────────┼────────────────────────────────┼────────────────────────────────────────────────────────┤
│ eval          │ remote::nvidia                 │ requests                                               │
├───────────────┼────────────────────────────────┼────────────────────────────────────────────────────────┤
│ files         │ inline::localfs                │ sqlalchemy,aiosqlite,asyncpg                           │
├───────────────┼────────────────────────────────┼────────────────────────────────────────────────────────┤
│ files         │ remote::s3                     │ boto3,sqlalchemy,aiosqlite,asyncpg                     │
├───────────────┼────────────────────────────────┼────────────────────────────────────────────────────────┤
│ inference     │ inline::meta-reference         │ accelerate,fairscale,torch,torchvision,transformers,z… │
├───────────────┼────────────────────────────────┼────────────────────────────────────────────────────────┤
│ inference     │ remote::anthropic              │ litellm                                                │
├───────────────┼────────────────────────────────┼────────────────────────────────────────────────────────┤
│ inference     │ remote::bedrock                │ boto3                                                  │
├───────────────┼────────────────────────────────┼────────────────────────────────────────────────────────┤
</pre>

<p><div class="rs-tip-major">Poznámka: tabulka je zkrácena, protože její
celková velikost v&nbsp;poslední oficiální verzi Llama Stacku (0.2.23)
přesahuje třicet kilobajtů.</div></p>



<p><a name="k10"></a></p>
<h2 id="k10">10. Přidání závislostí do projektového souboru a úprava konfiguračního souboru Llama Stacku</h2>

<p>Abychom mohli do Llama Stacku zaregistrovat další poskytovatele, je nutné do
projektového souboru používaného pro spuštění Llama Stacku přidat všechny
potřebné závislosti. Ty se přidávají buď ručně nebo příkazem <strong>uv add
balíček</strong> popř.&nbsp;<strong>pdm add balíček</strong>. Pro ukázku
přidáme následující závislosti:</p>

<pre>
blobfile&gt;=3.0.0
datasets&gt;=3.6.0
sqlalchemy&gt;=2.0.41
faiss-cpu&gt;=1.11.0
mcp&gt;=1.9.4
autoevals&gt;=0.0.129
psutil&gt;=7.0.0
torch&gt;=2.7.1
peft&gt;=0.15.2
trl&gt;=0.18.2
</pre>

<p>Výsledný obsah projektového souboru <strong>pyproject.toml</strong> může
vypadat následovně:</p>

<pre>
[project]
name = "llama-stack-demo"
version = "0.1.0"
description = "Default template for PDM package"
authors = [
    {name = "Pavel Tisnovsky", email = "ptisnovs@redhat.com"},
]
dependencies = [
    "llama-stack==0.2.14",
    "fastapi&gt;=0.115.12",
    "opentelemetry-sdk&gt;=1.34.0",
    "opentelemetry-exporter-otlp&gt;=1.34.0",
    "opentelemetry-instrumentation&gt;=0.55b0",
    "aiosqlite&gt;=0.21.0",
    "litellm&gt;=1.72.1",
    "uvicorn&gt;=0.34.3",
    "blobfile&gt;=3.0.0",
    "datasets&gt;=3.6.0",
    "sqlalchemy&gt;=2.0.41",
    "faiss-cpu&gt;=1.11.0",
    "mcp&gt;=1.9.4",
    "autoevals&gt;=0.0.129",
    "psutil&gt;=7.0.0",
    "torch&gt;=2.7.1",
    "peft&gt;=0.15.2",
    "trl&gt;=0.18.2"]
requires-python = "==3.12.*"
readme = "README.md"
license = {text = "MIT"}
&nbsp;
&nbsp;
[tool.pdm]
distribution = false
</pre>

<p>Nyní již můžeme rozšířit i konfiguraci Llama Stacku, a to konkrétně do této
podoby:</p>

<pre>
version: '2'
image_name: minimal-viable-llama-stack-configuration
&nbsp;
apis:
  - agents
  - datasetio
  - eval
  - inference
  - post_training
  - safety
  - scoring
  - telemetry
  - tool_runtime
  - vector_io
benchmarks: []
container_image: null
datasets: []
external_providers_dir: null
inference_store:
  db_path: .llama/distributions/ollama/inference_store.db
  type: sqlite
logging: null
metadata_store:
  db_path: .llama/distributions/ollama/registry.db
  namespace: null
  type: sqlite
providers:
  agents:
  - config:
      persistence_store:
        db_path: .llama/distributions/ollama/agents_store.db
        namespace: null
        type: sqlite
      responses_store:
        db_path: .llama/distributions/ollama/responses_store.db
        type: sqlite
    provider_id: meta-reference
    provider_type: inline::meta-reference
  datasetio:
  - config:
      kvstore:
        db_path: .llama/distributions/ollama/huggingface_datasetio.db
        namespace: null
        type: sqlite
    provider_id: huggingface
    provider_type: remote::huggingface
  - config:
      kvstore:
        db_path: .llama/distributions/ollama/localfs_datasetio.db
        namespace: null
        type: sqlite
    provider_id: localfs
    provider_type: inline::localfs
  eval:
  - config:
      kvstore:
        db_path: .llama/distributions/ollama/meta_reference_eval.db
        namespace: null
        type: sqlite
    provider_id: meta-reference
    provider_type: inline::meta-reference
  inference:
    - provider_id: openai
      provider_type: remote::openai
      config:
        api_key: ${env.OPENAI_API_KEY}
  post_training:
  - config:
      checkpoint_format: huggingface
      device: cpu
      distributed_backend: null
    provider_id: huggingface
    provider_type: inline::huggingface
  safety:
  - config:
      excluded_categories: []
    provider_id: llama-guard
    provider_type: inline::llama-guard
  scoring:
  - config: {}
    provider_id: basic
    provider_type: inline::basic
  - config: {}
    provider_id: llm-as-judge
    provider_type: inline::llm-as-judge
  - config:
      openai_api_key: '********'
    provider_id: braintrust
    provider_type: inline::braintrust
  telemetry:
  - config:
      service_name: ''
      sinks: sqlite
      sqlite_db_path: .llama/distributions/ollama/trace_store.db
    provider_id: meta-reference
    provider_type: inline::meta-reference
  tool_runtime:
    - provider_id: model-context-protocol
      provider_type: remote::model-context-protocol
      config: {}
  vector_io:
  - config:
      kvstore:
        db_path: .llama/distributions/ollama/faiss_store.db
        namespace: null
        type: sqlite
    provider_id: faiss
    provider_type: inline::faiss
scoring_fns: []
server:
  auth: null
  host: null
  port: 8321
  quota: null
  tls_cafile: null
  tls_certfile: null
  tls_keyfile: null
shields: []
vector_dbs: []
&nbsp;
models:
  - model_id: gpt-4-turbo
    provider_id: openai
    model_type: llm
    provider_model_id: gpt-4-turbo
</pre>

<p>Jednotlivé části tohoto konfiguračního souboru mají následující význam:</p>

<table>
<tr><th>Sekce</th><th>Stručný popis</th></tr>
<tr><td>apis</td><td>povolená API Llama Stacku, která lze volat z&nbsp;klientů</td></tr>
<tr><td>inference_store</td><td>databáze dotazů a odpovědí LLM serverů</td></tr>
<tr><td>metadata_store</td><td>různé informace o modelech atd.</td></tr>
<tr><td>providers</td><td>konfigurace jednotlivých poskytovatelů podle skupin (my jich zde máme celou řadu)</td></tr>
<tr><td>server</td><td>již jsme používali &ndash; nastavení samotného Llama Stack serveru (port, TLS atd.)</td></tr>
<tr><td>shields</td><td>popíšeme si příště</td></tr>
<tr><td>vector_dbs</td><td>popíšeme si později</td></tr>
<tr><td>models</td><td>konfigurace jednotlivých modelů (typicky LLM modelů atd.)</td></tr>
</table>



<p><a name="k11"></a></p>
<h2 id="k11">11. Pomocné tabulky vytvářené a používané při běhu Llama Stacku</h2>

sqlite3 agents_store.db

SQLite version 3.45.1 2024-01-30 16:01:20
Enter ".help" for usage hints.
sqlite> .tables
kvstore

sqlite> PRAGMA table_info(kvstore);
0|key|TEXT|0||1
1|value|TEXT|0||0
2|expiration|TIMESTAMP|0||0
sqlite> select key from kvstore;
agent:18dc140f-bc78-496a-8cd2-d37a518ee56f
num_infer_iters_in_turn:18dc140f-bc78-496a-8cd2-d37a518ee56f:0171d680-8dcf-4c80-b612-2a0ee40e1a33:e6c3bccc-3174-4ea1-ad9e-227944bfed87
num_infer_iters_in_turn:18dc140f-bc78-496a-8cd2-d37a518ee56f:0171d680-8dcf-4c80-b612-2a0ee40e1a33:f7342a56-bc63-47d3-bf2b-f4946ad34285
session:18dc140f-bc78-496a-8cd2-d37a518ee56f:0171d680-8dcf-4c80-b612-2a0ee40e1a33
session:18dc140f-bc78-496a-8cd2-d37a518ee56f:0171d680-8dcf-4c80-b612-2a0ee40e1a33:e6c3bccc-3174-4ea1-ad9e-227944bfed87
session:18dc140f-bc78-496a-8cd2-d37a518ee56f:0171d680-8dcf-4c80-b612-2a0ee40e1a33:f7342a56-bc63-47d3-bf2b-f4946ad34285

sqlite> select value from kvstore where key='session:18dc140f-bc78-496a-8cd2-d37a518ee56f:0171d680-8dcf-4c80-b612-2a0ee40e1a33';
{"session_id":"0171d680-8dcf-4c80-b612-2a0ee40e1a33","session_name":"830e5c42-9422-4fa3-b0d8-f072e88eb5cb","turns":[],"started_at":"2025-09-30T15:34:59.808900Z","vector_db_id":null,"owner":null,"identifier":"830e5c42-9422-4fa3-b0d8-f072e88eb5cb","type":"session"}
sqlite> select value from kvstore where key='session:18dc140f-bc78-496a-8cd2-d37a518ee56f:0171d680-8dcf-4c80-b612-2a0ee40e1a33:e6c3bccc-3174-4ea1-ad9e-227944bfed87';
{"turn_id":"e6c3bccc-3174-4ea1-ad9e-227944bfed87","session_id":"0171d680-8dcf-4c80-b612-2a0ee40e1a33","input_messages":[{"role":"user","content":"Say Hello in Czech","context":null}],"steps":[{"turn_id":"e6c3bccc-3174-4ea1-ad9e-227944bfed87","step_id":"c2534750-4cea-4f98-a1db-69ce7aa22e2b","started_at":"2025-09-30T15:34:59.867311Z","completed_at":"2025-09-30T15:35:02.306269Z","step_type":"inference","model_response":{"role":"assistant","content":"Hello in Czech is \"Ahoj\" (informal) or \"Dobrý den\" (formal).","stop_reason":"end_of_turn","tool_calls":[]}}],"output_message":{"role":"assistant","content":"Hello in Czech is \"Ahoj\" (informal) or \"Dobrý den\" (formal).","stop_reason":"end_of_turn","tool_calls":[]},"output_attachments":[],"started_at":"2025-09-30T15:34:59.861412Z","completed_at":"2025-09-30T15:35:02.307082Z"}

{
  "turn_id": "e6c3bccc-3174-4ea1-ad9e-227944bfed87",
  "session_id": "0171d680-8dcf-4c80-b612-2a0ee40e1a33",
  "input_messages": [
    {
      "role": "user",
      "content": "Say Hello in Czech",
      "context": null
    }
  ],
  "steps": [
    {
      "turn_id": "e6c3bccc-3174-4ea1-ad9e-227944bfed87",
      "step_id": "c2534750-4cea-4f98-a1db-69ce7aa22e2b",
      "started_at": "2025-09-30T15:34:59.867311Z",
      "completed_at": "2025-09-30T15:35:02.306269Z",
      "step_type": "inference",
      "model_response": {
        "role": "assistant",
        "content": "Hello in Czech is \"Ahoj\" (informal) or \"Dobrý den\" (formal).",
        "stop_reason": "end_of_turn",
        "tool_calls": []
      }
    }
  ],
  "output_message": {
    "role": "assistant",
    "content": "Hello in Czech is \"Ahoj\" (informal) or \"Dobrý den\" (formal).",
    "stop_reason": "end_of_turn",
    "tool_calls": []
  },
  "output_attachments": [],
  "started_at": "2025-09-30T15:34:59.861412Z",
  "completed_at": "2025-09-30T15:35:02.307082Z"
}



<p><a name="k12"></a></p>
<h2 id="k12">12. </h2>



<p><a name="k13"></a></p>
<h2 id="k13">13. </h2>



<p><a name="k14"></a></p>
<h2 id="k14">14. Úplný zdrojový kód klienta, který bude odpovídat na základě dokumentů určených k&nbsp;prohledávání</h2>

<p></p>

<pre>
import uuid
from llama_stack_client import LlamaStackClient

client = LlamaStackClient(base_url="http://localhost:8321")
print(f"Using Llama Stack version {client._version}")

vector_store_name= f"vec_{str(uuid.uuid4())[0:8]}"
print(f"Vector store name: {vector_store_name}")

vector_store = client.vector_stores.create(name=vector_store_name)
vector_store_id = vector_store.id

print(f"Vector store ID: {vector_store_id}")

models = client.models.list()
model_id = models[0].identifier

print(f"Using model {model_id}")

from pathlib import Path

path=Path("05_05_variance.md")
print(f"File path: {path}")

file_create_response = client.files.create(file=path, purpose="assistants")
print(f"File create response: {file_create_response}")

file_ingest_response = client.vector_stores.files.create(
    vector_store_id=vector_store_id,
    file_id=file_create_response.id,
)
print(f"File ingest response: {file_ingest_response}")

MODEL_ID="openai/gpt-4-turbo"

def print_rag_response(response):
    print(f"ID: {response.id}")
    print(f"Status: {response.status}")
    print(f"Model: {response.model}")
    print(f"Created at: {response.created_at}")
    print(f"Output items: {len(response.output)}")
    
    for i, output_item in enumerate(response.output):
        if len(response.output) > 1:
            print(f"\n--- Output Item {i+1} ---")
        print(f"Output type: {output_item.type}")
        
        if output_item.type in ("text", "message"):
            print(f"Response content: {output_item.content[0].text}")
        elif output_item.type == "file_search_call":
            print(f"  Tool Call ID: {output_item.id}")
            print(f"  Tool Status: {output_item.status}")
            # 'queries' is a list, so we join it for clean printing
            print(f"  Queries: {', '.join(output_item.queries)}")
            # Display results if they exist, otherwise note they are empty
            print(f"  Results: {output_item.results if output_item.results else 'None'}")
        else:
            print(f"Response content: {output_item.content}")


response = client.responses.create(
    model=MODEL_ID,
    input="Display class Ovoce",
    tools=[
        {
            "type": "file_search",
            "vector_store_ids": [vector_store_id],
        }
    ]
)

print_rag_response(response)
</pre>



<p><a name="k15"></a></p>
<h2 id="k15">15. </h2>

<pre>
</pre>

https://github.com/sergiou87/open-supaplex/blob/73df867bc9c14790a23b8d66c86388e09b7e5312/resources/README.TXT


<p><a name="k16"></a></p>
<h2 id="k16">16. </h2>

<p></p>

<pre>
Output type: message
Response content: "ZONK" refers to an element in the videogame Supaplex. In the game, a Zonk is depicted as a round rock, and it is a common and typically unpleasant obstacle. Zonks tend to fall downwards whenever possible, for instance, if there is a void directly underneath them. Players must be cautious as a Zonk falling on the character (Murphy) results in an explosion and game-over for the player. Murphy can push Zonks laterally (but not vertically) if there is space for them to move, but can only push one Zonk at a time. Good timing is needed to use Zonks strategically, such as dropping them on top of enemies (like Snik-snaks) to eliminate them through explosions.
</pre>

<pre>
Using Llama Stack version 0.2.22
Vector store name: vec_7cef4383
Vector store ID: vs_1364b1c1-43b6-46f8-8a44-91fa8f8b61fd
Using model granite-embedding-125m
File path: 05_05_variance.md
File create response: File(id='file-a3646e699cce4fca9d81337f013ac151', bytes=18439, created_at=1758745429, expires_at=1790281429, filename='05_05_variance.md', object='file', purpose='assistants')
File ingest response: VectorStoreFile(id='file-a3646e699cce4fca9d81337f013ac151', attributes={}, chunking_strategy=ChunkingStrategyVectorStoreChunkingStrategyAuto(type='auto'), created_at=1758745429, object='vector_store.file', status='completed', usage_bytes=0, vector_store_id='vs_1364b1c1-43b6-46f8-8a44-91fa8f8b61fd', last_error=None)
ID: resp-d0f3e76d-1e79-499a-ba71-4ddef85dedb0
Status: completed
Model: openai/gpt-4-turbo
Created at: 1758745437
Output items: 2

--- Output Item 1 ---
Output type: file_search_call
  Tool Call ID: call_CEgssrYfjL0klzlXkzcYewfI
  Tool Status: completed
  Queries: Class Ovoce
  Results: 
  ...
  ...
  ...

--- Output Item 2 ---
Output type: message
Response content: The class `Ovoce` along with its subclasses `Hruska` (Pear) and `Jablko` (Apple) are defined in a programming example to illustrate the concept of variance in object-oriented programming. Below is the definition of these classes in Java:

```java
class Ovoce {
}

class Hruska extends Ovoce {
    public String toString() {
        return "Hruska";
    }
}

class Jablko extends Ovoce {
    public String toString() {
        return "Jablko";
    }
}
```

This hierarchy involves a base class `Ovoce`, with two derived classes `Hruska` and `Jablko`. The `toString()` method in both subclasses provides a simple way to return the string representation of each fruit. This basic class structure is used to demonstrate type variance and how objects of these types interact within an array specified as type `Ovoce`, addressing concepts such as compile-time and runtime type safety in Java.
</pre>


<p><a name="k17"></a></p>
<h2 id="k17">17. </h2>

<p></p>

<p><a name="k18"></a></p>
<h2 id="k18">18. </h2>

<p></p>

<p><a name="k19"></a></p>
<h2 id="k19">19. Repositář s&nbsp;demonstračními příklady</h2>

<p>Projekty popsané v&nbsp;předchozích kapitolách je možné nalézt
v&nbsp;repositáři <a
href="https://github.com/tisnik/most-popular-python-libs">https://github.com/tisnik/most-popular-python-libs</a>.
Následují odkazy na jednotlivé soubory s&nbsp;jejich stručným popisem:</p>

<h3>První projekt: volání LLM modelu přes stack</h3>

<table>
<tr><th>#<th>Příklad</th><th>Stručný popis</th><th>Adresa příkladu</th></tr></i>
<tr><td>1</td><td>demo1/pyproject.toml</td><td>konfigurační soubor s&nbsp;definicí projektu</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo1/pyproject.toml">https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo1/pyproject.toml</a></td></tr>
<tr><td>2</td><td>demo1/pdm.lock</td><td>soubor se seznamem a haši nainstalovaných balíčků</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo1/pdm.lock">https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo1/pdm.lock</a></td></tr>
<tr><td>3</td><td>demo1/run.yaml</td><td>konfigurace našeho <i>stacku</i></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo1/run.yaml">https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo1/run.yaml</a></td></tr>
<tr><td>4</td><td>demo1/client1.py</td><td>skript v&nbsp;Pythonu s&nbsp;realizovaným klientem</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo1/client1.py">https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo1/client1.py</a></td></tr>
</table>



<h3>Druhý projekt: výpis všech dostupných modelů</h3>

<table>
<tr><th>#<th>Příklad</th><th>Stručný popis</th><th>Adresa příkladu</th></tr></i>
<tr><td>1</td><td>demo2/pyproject.toml</td><td>konfigurační soubor s&nbsp;definicí projektu</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo2/pyproject.toml">https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo2/pyproject.toml</a></td></tr>
<tr><td>2</td><td>demo2/pdm.lock</td><td>soubor se seznamem a haši nainstalovaných balíčků</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo2/pdm.lock">https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo2/pdm.lock</a></td></tr>
<tr><td>3</td><td>demo2/run.yaml</td><td>konfigurace našeho <i>stacku</i></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo2/run.yaml">https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo2/run.yaml</a></td></tr>
<tr><td>4</td><td>demo2/client2.py</td><td>skript v&nbsp;Pythonu s&nbsp;realizovaným klientem</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo2/client2.py">https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo2/client2.py</a></td></tr>
</table>



<h3>Třetí projekt: využití Llama Stacku jako Pythonní knihovny</h3>

<table>
<tr><th>#<th>Příklad</th><th>Stručný popis</th><th>Adresa příkladu</th></tr></i>
<tr><td>1</td><td>demo3/pyproject.toml</td><td>konfigurační soubor s&nbsp;definicí projektu</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo3/pyproject.toml">https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo3/pyproject.toml</a></td></tr>
<tr><td>2</td><td>demo3/pdm.lock</td><td>soubor se seznamem a haši nainstalovaných balíčků</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo3/pdm.lock">https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo3/pdm.lock</a></td></tr>
<tr><td>3</td><td>demo3/run.yaml</td><td>konfigurace našeho <i>stacku</i></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo3/run.yaml">https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo3/run.yaml</a></td></tr>
<tr><td>4</td><td>demo3/client3.py</td><td>skript v&nbsp;Pythonu s&nbsp;realizovaným klientem</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo3/client3.py">https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo3/client3.py</a></td></tr>
</table>



<h3>Čtvrtý projekt: nové rozhraní pro komunikaci s&nbsp;LLM</h3>

<table>
<tr><th>#<th>Příklad</th><th>Stručný popis</th><th>Adresa příkladu</th></tr></i>
<tr><td>1</td><td>demo4/pyproject.toml</td><td>konfigurační soubor s&nbsp;definicí projektu</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo4/pyproject.toml">https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo4/pyproject.toml</a></td></tr>
<tr><td>2</td><td>demo4/run.yaml</td><td>konfigurace našeho <i>stacku</i></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo4/run.yaml">https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo4/run.yaml</a></td></tr>
<tr><td>3</td><td>demo4/client4.py</td><td>skript v&nbsp;Pythonu s&nbsp;realizovaným klientem</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo4/client4.py">https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo4/client4.py</a></td></tr>
</table>



<h3>Pátý projekt: hledání odpovědí z&nbsp;předaných a zpracovaných dokumentů</h3>

<table>
<tr><th>#<th>Příklad</th><th>Stručný popis</th><th>Adresa příkladu</th></tr></i>
<tr><td>1</td><td>demo5/pyproject.toml</td><td>konfigurační soubor s&nbsp;definicí projektu</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo5/pyproject.toml">https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo5/pyproject.toml</a></td></tr>
<tr><td>2</td><td>demo5/run.yaml</td><td>konfigurace našeho <i>stacku</i></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo5/run.yaml">https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo5/run.yaml</a></td></tr>
<tr><td>3</td><td>demo5/client5.py</td><td>skript v&nbsp;Pythonu s&nbsp;realizovaným klientem</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo5/client5.py">https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo5/client5.py</a></td></tr>
<tr><td>4</td><td>demo5/</td><td></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo5/">https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo5/</a></td></tr>
<tr><td>5</td><td>demo5/</td><td></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo5/">https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo5/</a></td></tr>
<tr><td>6</td><td>demo5/</td><td></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo5/">https://github.com/tisnik/most-popular-python-libs/blob/master/llama-stack-demo/demo5/</a></td></tr>
</table>



<p><a name="k20"></a></p>
<h2 id="k20">20. Odkazy na Internetu</h2>

<ol>

<li>llama-stack na GitHubu<br />
<a href="https://github.com/meta-llama/llama-stack">https://github.com/meta-llama/llama-stack</a>
</li>

<li>llama-stack na PyPi<br />
<a href="https://pypi.org/project/llama-stack/">https://pypi.org/project/llama-stack/</a>
</li>

<li>Configuring a "Stack"<br />
<a href="https://llama-stack.readthedocs.io/en/latest/distributions/configuration.html#">https://llama-stack.readthedocs.io/en/latest/distributions/configuration.html#</a>
</li>

<li>Generativní umělá inteligence<br />
<a href="https://cs.wikipedia.org/wiki/Generativn%C3%AD_um%C4%9Bl%C3%A1_inteligence">https://cs.wikipedia.org/wiki/Generativn%C3%AD_um%C4%9Bl%C3%A1_inteligence</a>
</li>

<li>Generative artificial intelligence<br />
<a href="https://en.wikipedia.org/wiki/Generative_artificial_intelligence">https://en.wikipedia.org/wiki/Generative_artificial_intelligence</a>
</li>

<li>Generativní AI a LLM: (R)evoluce v umělé inteligenci?<br />
<a href="https://www.itbiz.cz/clanky/generativni-ai-a-llm-revoluce-v-umele-inteligenci/">https://www.itbiz.cz/clanky/generativni-ai-a-llm-revoluce-v-umele-inteligenci/</a>
</li>

<li>langchain<br />
<a href="https://python.langchain.com/docs/introduction/">https://python.langchain.com/docs/introduction/</a>
</li>

<li>langgraph<br />
<a href="https://github.com/langchain-ai/langgraph">https://github.com/langchain-ai/langgraph</a>
</li>

<li>autogen<br />
<a href="https://github.com/microsoft/autogen">https://github.com/microsoft/autogen</a>
</li>

<li>metaGPT<br />
<a href="https://github.com/geekan/MetaGPT">https://github.com/geekan/MetaGPT</a>
</li>

<li>phidata<br />
<a href="https://github.com/phidatahq/phidata">https://github.com/phidatahq/phidata</a>
</li>

<li>CrewAI<br />
<a href="https://github.com/crewAIInc/crewAI">https://github.com/crewAIInc/crewAI</a>
</li>

<li>pydanticAI<br />
<a href="https://github.com/pydantic/pydantic-ai">https://github.com/pydantic/pydantic-ai</a>
</li>

<li>controlflow<br />
<a href="https://github.com/PrefectHQ/ControlFlow">https://github.com/PrefectHQ/ControlFlow</a>
</li>

<li>langflow<br />
<a href="https://github.com/langflow-ai/langflow">https://github.com/langflow-ai/langflow</a>
</li>

<li>LiteLLM<br />
<a href="https://github.com/BerriAI/litellm">https://github.com/BerriAI/litellm</a>
</li>

<li>Llama Stack<br />
<a href="https://github.com/meta-llama/llama-stack">https://github.com/meta-llama/llama-stack</a>
</li>

<li>uv<br />
<a href="https://docs.astral.sh/uv/">https://docs.astral.sh/uv/</a>
</li>

<li>Python na Root.cz<br />
<a href="https://www.root.cz/n/python/">https://www.root.cz/n/python/</a>
</li>

<li>PDM: moderní správce balíčků a virtuálních prostředí Pythonu<br />
<a href="https://www.root.cz/clanky/pdm-moderni-spravce-balicku-a-virtualnich-prostredi-pythonu/">https://www.root.cz/clanky/pdm-moderni-spravce-balicku-a-virtualnich-prostredi-pythonu/</a>
</li>

<li>YAML<br />
<a href="https://en.wikipedia.org/wiki/YAML">https://en.wikipedia.org/wiki/YAML</a>
</li>

<li>Top 11 LLM API Providers in 2025<br />
<a href="https://www.helicone.ai/blog/llm-api-providers">https://www.helicone.ai/blog/llm-api-providers</a>
</li>

<li>Zpracování dat reprezentovaných ve formátu JSON nástrojem jq<br />
<a href="https://www.root.cz/clanky/zpracovani-dat-reprezentovanych-ve-formatu-json-nastrojem-jq/">https://www.root.cz/clanky/zpracovani-dat-reprezentovanych-ve-formatu-json-nastrojem-jq/</a>
</li>

<li>LLama Stack SDK pro jazyk Python<br />
<a href="https://github.com/meta-llama/llama-stack-client-python">https://github.com/meta-llama/llama-stack-client-python</a>
</li>

<li>LLama Stack SDK pro jazyk Swift<br />
<a href="https://github.com/meta-llama/llama-stack-client-swift/tree/latest-release">https://github.com/meta-llama/llama-stack-client-swift/tree/latest-release</a>
</li>

<li>LLama Stack SDK pro ekosystém Node.js<br />
<a href="https://github.com/meta-llama/llama-stack-client-node">https://github.com/meta-llama/llama-stack-client-node</a>
</li>

<li>LLama Stack SDK pro jazyk Kotlin<br />
<a href="https://github.com/meta-llama/llama-stack-client-kotlin/tree/latest-release">https://github.com/meta-llama/llama-stack-client-kotlin/tree/latest-release</a>
</li>

<li>SQLite<br />
<a href="https://www.sqlite.org/index.html">https://www.sqlite.org/index.html</a>
</li>

<li>Llama Stack examples<br />
<a href="https://github.com/The-AI-Alliance/llama-stack-examples">https://github.com/The-AI-Alliance/llama-stack-examples</a>
</li>

<li>Building RAG-based LLM Applications for Production<br />
<a href="https://www.anyscale.com/blog/a-comprehensive-guide-for-building-rag-based-llm-applications-part-1">https://www.anyscale.com/blog/a-comprehensive-guide-for-building-rag-based-llm-applications-part-1</a>
</li>

<li>Retrieval-augmented generation<br />
<a href="https://medium.com/@mikeshwe_19587/retrieval-augmented-generation-a0b6b6c20c03">https://medium.com/@mikeshwe_19587/retrieval-augmented-generation-a0b6b6c20c03</a>
</li>

<li>Supaplex: README.TXT<br />
<a href="https://github.com/sergiou87/open-supaplex/blob/73df867bc9c14790a23b8d66c86388e09b7e5312/resources/README.TXT">https://github.com/sergiou87/open-supaplex/blob/73df867bc9c14790a23b8d66c86388e09b7e5312/resources/README.TXT</a>
</li>

</ol>



<p></p><p></p>
<p><small>Autor: <a href="https://github.com/tisnik/">Pavel Tišnovský</a> &nbsp; 2025</small></p>
</body>
</html>

