<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
        "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html>
<head>
<title>Tokenizace textu: základní operace při zpracování přirozeného jazyka</title>
<meta name="Author" content="Pavel Tisnovsky" />
<meta name="Generator" content="vim" />
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<style type="text/css">
         body {color:#000000; background:#ffffff;}
         h1  {font-family: arial, helvetica, sans-serif; color:#ffffff; background-color:#c00000; text-align:center; padding-left:1em}
         h2  {font-family: arial, helvetica, sans-serif; color:#ffffff; background-color:#0000c0; padding-left:1em; text-align:left}
         h3  {font-family: arial, helvetica, sans-serif; color:#000000; background-color:#c0c0c0; padding-left:1em; text-align:left}
         h4  {font-family: arial, helvetica, sans-serif; color:#000000; background-color:#e0e0e0; padding-left:1em; text-align:left}
         a   {font-family: arial, helvetica, sans-serif;}
         li  {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify; width:450px;}
         ol  {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify; width:450px;}
         ul  {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify; width:450px;}
         p   {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify;}
         pre {background:#e0e0e0}
</style>
</head>

<body>

<h1>Tokenizace textu: základní operace při zpracování přirozeného jazyka</h1>

<h3>Pavel Tišnovský</h3>

<p></p>

<h1>Úvodník</h1>

<p>V dnešním článku se seznámíme s knihovnou Tiktoken, která umožňuje takzvanou tokenizaci textu. Jedná se o jednu ze základních operací používaných v systémech pro zpracování přirozeného jazyka, což je oblast IT, která se v současnosti bouřlivě rozvijí.</p>



<h2>Obsah</h2>

<p><a href="#k01">1. Tokenizace textu: základní operace při zpracování přirozeného jazyka</a></p>
<p><a href="#k02">2. Převod slov na tokeny</a></p>
<p><a href="#k03">3. Tokenizace složených slov</a></p>
<p><a href="#k04">4. Knihovna <i>tiktoken</i></a></p>
<p><a href="#k05">5. Instalace knihovny <i>tiktoken</i></a></p>
<p><a href="#k06">6. Ukázka tokenizace s&nbsp;využitím kódování <i>cl100k_base</i></a></p>
<p><a href="#k07">7. Interpunkční znaménka</a></p>
<p><a href="#k08">8. Složená slova a kódování <i>cl100k_base</i></a></p>
<p><a href="#k09">9. Složená slova zakódovaná jediným tokenem</a></p>
<p><a href="#k10">10. Kódování mezer</a></p>
<p><a href="#k11">11. Kódování dalších speciálních znaků</a></p>
<p><a href="#k12">12. Zpětný převod tokenů na text</a></p>
<p><a href="#k13">13. Od <i>cl100k_base</i> k&nbsp;dalším enkodérům a modulům</a></p>
<p><a href="#k14">14. Porovnání kódování <i>cl100k_base</i> s&nbsp;dalšími kodéry a moduly</a></p>
<p><a href="#k15">15. Rychlost tokenizace</a></p>
<p><a href="#k16">16. Rychlost zpětného převodu sekvence tokenů na text</a></p>
<p><a href="#k17">17. Další tokenizéry</a></p>
<p><a href="#k18">18. Speciální tokeny</a></p>
<p><a href="#k19">19. Repositář s&nbsp;demonstračními příklady</a></p>
<p><a href="#k20">20. Odkazy na Internetu</a></p>



<p><a name="k01"></a></p>
<h2 id="k01">1. Tokenizace textu: základní operace při zpracování přirozeného jazyka</h2>

<p>V&nbsp;současnosti se obor IT poměrně důrazně zaměřuje na problematiku
zpracování přirozeného jazyka (<i>Natural language processing</i>, <i>NLP</i>),
která byla zpopularizována i mezi širokou veřejností mj.&nbsp;i díky úspěšně
nasazeným rozsáhlým jazykovým modelům (<i>Large Language Model</i>,
<i>LLM</i>), mezi něž patří například GPT (OpenAI), PaLM, Gemini, LLaMA apod.
Při zpracování přirozeného jazyka se používá několik standardních postupů a
algoritmů, s&nbsp;nimiž se postupně seznámíme v&nbsp;navazující sérii článků.
Na samotném začátku zpracování textu, ale i při ukládání &bdquo;znalosti&ldquo;
informací (jazykový korpus), se využívá takzvaná <i>tokenizace</i>.
V&nbsp;dnešním článku se seznámíme se základními principy, na nichž je tato
operace postavena a ukážeme si ji na příkladu knihovny
<strong>tiktoken</strong> (i když dnes existuje celá řada podobně koncipovaných
knihoven, ostatně o některých dalších knihovnách, které jsou používány, se dnes
ještě zmíníme).</p>



<p><a name="k02"></a></p>
<h2 id="k02">2. Převod slov na tokeny</h2>

<p>Tokenizace má sice v&nbsp;informatice různý význam, ale v&nbsp;kontextu
jazykových modelů se tímto slovem označuje převod textu (tedy sekvence znaků)
na tokeny (tedy na sekvenci celočíselných hodnot), přičemž výsledek by měl být
jednoznačný. Typicky také předpokládáme existenci opačné operace,
tj.&nbsp;možnost převodu sekvence tokenů na běžný text. Existuje větší množství
algoritmů pro konstrukci tokenů, ovšem mnoho algoritmů převádí jednodušší slova
nebo velmi často používaná slova (popř.&nbsp;slova s&nbsp;mezerou) na jediný
token. Tento postup uvidíme v&nbsp;praktických příkladech, které využívají
enkodér nazvaný <i>cl100k_base</i>. Tento název je odvozen od faktu, že
obsahuje slovník s&nbsp;přibližně 100000 tokeny reprezentujícími jednotlivá
slova, slova s&nbsp;oddělovači, slovní předpony, přípony, speciální znaky
atd.</p>

<p>Tokenizaci si můžete vyzkoušet na stránce <a
href="https://tiktokenizer.vercel.app/">https://tiktokenizer.vercel.app/</a>.
Pro jednoduchá slova a věty dostaneme:</p>

<a href="https://www.root.cz/obrazek/1118471/"><img src="https://i.iinfo.cz/images/466/tiktoken-1-prev.webp" class="image-1118471" width="370" height="162" data-prev-filename="https://i.iinfo.cz/images/466/tiktoken-1-prev.webp" data-prev-width="370" data-prev-height="162" data-large-filename="https://i.iinfo.cz/images/466/tiktoken-1-large.webp" data-large-width="720" data-large-height="315" alt="&#160;" title="Autor: tisnik, podle licence: &lt;a href=&quot;http://en.wikipedia.org/wiki/Rights_Managed&quot;&gt;Rights Managed&lt;/a&gt;" /></a>
<p><i>Obrázek 1: Tokenizace jednoduché anglické věty.</i></p>



<p><a name="k03"></a></p>
<h2 id="k03">3. Tokenizace složených slov</h2>

<p>Některá delší slova, která jsou složena z&nbsp;prefixu, základu, suffixu
atd., jsou ovšem (pokud se nejedná o velmi často používaná slova), převedena na
několik tokenů, kde typicky jeden z&nbsp;tokenů představuje základ slova a
další pak prefix či suffix. Ovšem u složitějších slov, například chemických
názvů, názvů léků atd., se slova skládají z&nbsp;menších celků (2-4 znaky).
Opět si to můžeme ukázat na příkladu:</p>

<a href="https://www.root.cz/obrazek/1118472/"><img src="https://i.iinfo.cz/images/466/tiktoken-2-prev.webp" class="image-1118472" width="370" height="153" data-prev-filename="https://i.iinfo.cz/images/466/tiktoken-2-prev.webp" data-prev-width="370" data-prev-height="153" data-large-filename="https://i.iinfo.cz/images/466/tiktoken-2-large.webp" data-large-width="720" data-large-height="299" alt="&#160;" title="Autor: tisnik, podle licence: &lt;a href=&quot;http://en.wikipedia.org/wiki/Rights_Managed&quot;&gt;Rights Managed&lt;/a&gt;" /></a>
<p><i>Obrázek 2: Tokenizace věty se složenými slovy.</i></p>

<p><div class="rs-tip-major">Poznámka: barevně jsou zvýrazněny jednotlivé
tokeny, konkrétní hodnoty tokenů pak naleznete v&nbsp;pravé dolní části
okna.</div></p>



<p><a name="k04"></a></p>
<h2 id="k04">4. Knihovna <i>tiktoken</i></h2>

<p>Pro tokenizaci a popř.&nbsp;i pro zpětný převod tokenů na text lze využít
větší množství knihoven. Tyto knihovny se od sebe odlišují jak svou
implementací, tak i tím, zda jsou (či naopak nejsou) součástí větších
programových celků pro realizaci systémů pro zpracování přirozeného jazyka
(dnes typicky založených na LLM). Dnes se budeme primárně zabývat knihovnou
nazvanou <i>tiktoken</i>, jejíž výhodou je fakt, že je velmi snadno použitelná
a lze ji nainstalovat a používat samostatně, bez nutnosti instalace frameworků
pro LLM. Navíc tato knihovna podporuje několik enkodérů textu na tokeny i
několik specifických modelů (například GPT-4 atd.), což je problematika,
k&nbsp;níž se ještě několikrát vrátíme.</p>



<p><a name="k05"></a></p>
<h2 id="k05">5. Instalace knihovny <i>tiktoken</i></h2>

<p>Vzhledem k&nbsp;tomu, že knihovna <i>tiktoken</i> je dostupná na <a
href="https://pypi.org/">PyPI</a>, je její instalace snadná. Buď je možné
provést instalaci pro všechny uživatele:</p>

<pre>
$ <strong>pip3 install tiktoken</strong>
</pre>

<p>Nebo pouze pro aktivního uživatele:</p>

<pre>
$ <strong>pip3 install --user tiktoken</strong>
</pre>

<p>V&nbsp;průběhu instalace se stáhnou i další tranzitivní závislosti, ale není
jich mnoho:</p>

<pre>
Collecting tiktoken
  Downloading tiktoken-0.6.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)
Collecting regex&gt;=2022.1.18 (from tiktoken)
  Downloading regex-2023.12.25-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.9/40.9 kB 839.4 kB/s eta 0:00:00
Collecting requests&gt;=2.26.0 (from tiktoken)
  Downloading requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)
Collecting charset-normalizer&lt;4,&gt;=2 (from requests&gt;=2.26.0-&gt;tiktoken)
  Downloading charset_normalizer-3.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)
Requirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/lib/python3/dist-packages (from requests&gt;=2.26.0-&gt;tiktoken) (2.8)
Requirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/lib/python3/dist-packages (from requests&gt;=2.26.0-&gt;tiktoken) (1.25.8)
Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/lib/python3/dist-packages (from requests&gt;=2.26.0-&gt;tiktoken) (2019.11.28)
Downloading tiktoken-0.6.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.8/1.8 MB 4.9 MB/s eta 0:00:00
Downloading regex-2023.12.25-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (777 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 777.0/777.0 kB 2.7 MB/s eta 0:00:00
Downloading requests-2.31.0-py3-none-any.whl (62 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.6/62.6 kB 2.1 MB/s eta 0:00:00
Downloading charset_normalizer-3.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 141.1/141.1 kB 3.2 MB/s eta 0:00:00
Installing collected packages: regex, charset-normalizer, requests, tiktoken
Successfully installed charset-normalizer-3.3.2 regex-2023.12.25 requests-2.31.0 tiktoken-0.6.0
</pre>

<p>Samotná knihovna <i>tiktoken</i> zabere po instalaci cca 6,5 megabajtu,
přičemž největší soubor tvoří nativní knihovna s&nbsp;realizací algoritmu
tokenizace a zpětného převodu tokenů na text (tato nativní knihovna je
naprogramována v&nbsp;Rustu, výsledkem jsou velmi rychlé realizace všech
operací):</p>

<pre>
~/.local/lib/python3.12/site-packages/tiktoken$ <strong>ls -sh1h</strong>
&nbsp;
total 6,5M
 24K core.py
 20K _educational.py
 12K __init__.py
 16K load.py
 12K model.py
4,0K __pycache__
8,0K py.typed
 12K registry.py
6,4M _tiktoken.cpython-312-x86_64-linux-gnu.so
</pre>



<p><a name="k06"></a></p>
<h2 id="k06">6. Ukázka tokenizace s&nbsp;využitím kódování <i>cl100k_base</i></h2>

<p>Podívejme se nyní na způsob tokenizace jednoduchého textu, konkrétně
klasické zprávy &bdquo;Hello world&ldquo;, na tokeny. Pro tokenizaci bude
využito kódování (resp.&nbsp;možná přesněji řečeno enkodér) <i>cl100k_base</i>.
Toto jméno je odvozeno od faktu, že obsahuje přibližně sto tisíc tokenů
resp.&nbsp;přesněji řečeno mapování sekvence znaků na tokeny (z&nbsp;toho
plyne, že neobsahuje všechna slova, to však nevadí, jak uvidíme dále).</p>

<p>Samotný skript, který tokenizaci provede, je až triviálně jednoduchý, což je
ostatně jedna z&nbsp;dobrých vlastností knihovny <i>tiktoken</i>. Nejprve
získáme potřebný enkodér a poté provedeme tokenizaci zavoláním metody
<strong>encode</strong>. Výsledkem je seznam s&nbsp;tokeny,
tj.&nbsp;s&nbsp;celočíselnými hodnotami:</p>

<pre>
import tiktoken
&nbsp;
encoder = tiktoken.get_encoding("cl100k_base")
&nbsp;
tokens = encoder.encode(<strong>"Hello world"</strong>)
print(tokens)
</pre>

<p>A takto vypadá výsledek:</p>

<pre>
[9906, 1917]
</pre>

<p><div class="rs-tip-major">Poznámka: v&nbsp;tomto případě jsou enkodérem
nalezena obě slova a každé je tedy reprezentováno jedinou celočíselnou
hodnotou. Ne vždy tomu tak však musí být, jak si ostatně ukážeme <a
href="#k07">v&nbsp;navazující kapitole</a>.</div></p>

<p>Na tomto místě je vhodné upozornit na to, že pokud například nahradíme slovo
&bdquo;Hello&ldquo; za &bdquo;hello&ldquo;, bude se z&nbsp;pohledu enkodéru
jednat o odlišné slovo, které bude zakódováno do odlišné sekvence tokenů.
Ostatně si to můžeme velmi snadno ověřit:</p>

<pre>
import tiktoken
&nbsp;
encoder = tiktoken.get_encoding("cl100k_base")
&nbsp;
tokens = encoder.encode(<strong>"Hello world"</strong>)
print(tokens)
&nbsp;
tokens = encoder.encode(<strong>"hello world"</strong>)
print(tokens)
&nbsp;
tokens = encoder.encode(<strong>"hello World"</strong>)
print(tokens)
&nbsp;
tokens = encoder.encode(<strong>"Hello World"</strong>)
print(tokens)
</pre>

<p>Výsledky naznačují, jaké tokeny odpovídají slovům &bdquo;Hello&ldquo;,
&bdquo;hello&ldquo;, &bdquo;World&ldquo; i &bdquo;world&ldquo;:</p>

<pre>
[9906, 1917]
[15339, 1917]
[15339, 4435]
[9906, 4435]
</pre>

<p><div class="rs-tip-major">Poznámka: první spuštění bude trvat déle, neboť si
knihovna <i>Tiktoken</i> musí stáhnout i specifikovaný enkodér.</div></p>



<p><a name="k07"></a></p>
<h2 id="k07">7. Interpunkční znaménka</h2>

<p>Ve skutečnosti má být zpráva &bdquo;Hello world&ldquo; správně zapsána jako
&bdquo;Hello, world!&ldquo;. To nás vede k&nbsp;problematice kódování
interpunkčních znamének. Zkusme si tedy nechat tokenizovat zprávu i
s&nbsp;oběma interpunkčními znaménky:</p>

<pre>
import tiktoken
&nbsp;
encoder = tiktoken.get_encoding("cl100k_base")
&nbsp;
tokens = encoder.encode(<strong>"Hello, world!"</strong>)
print(tokens)
</pre>

<p>Výsledkem v&nbsp;tomto případě již nebude sekvence dvou tokenů, ale tokenů
čtyř, což znamená, že pokud sekvenci tokenů použijeme v&nbsp;systému pro
zpracování přirozeného jazyka, může jazykový model &bdquo;rozumět&ldquo; i
těmto důležitým znakům:</p>

<pre>
[9906, 11, 1917, 0]
</pre>

<p>Jen pro zajímavost &ndash; poslední token má hodnotu 0. Jedná se o token
reprezentující samotný vykřičník, o čemž se můžeme velmi snadno přesvědčit:</p>

<pre>
import tiktoken
&nbsp;
encoder = tiktoken.get_encoding("cl100k_base")
&nbsp;
tokens = encoder.encode(<strong>"!"</strong>)
print(tokens)
</pre>

<p>Výsledkem bude v&nbsp;tomto případě sekvence obsahující jediný token
s&nbsp;hodnotou 0:</p>

<pre>
[0]
</pre>

<p><div class="rs-tip-major">Poznámka: token číslo 1 odpovídá
uvozovce.</div></p>



<p><a name="k08"></a></p>
<h2 id="k08">8. Složená slova a kódování <i>cl100k_base</i></h2>

<p>Mnohá jednoduchá anglická slova jsou transformována do jediného tokenu.
Ovšem u slov delších popř.&nbsp;u slov, která obsahují často používaný základ,
je tomu jinak. Taková slova jsou složena z&nbsp;tokenů odpovídajících slovům
(či části slov), z&nbsp;nichž jsou složena. Opět si to ukažme, a to například
na slovech &bdquo;black&ldquo;, &bdquo;blue&ldquo;, &bdquo;bird&ldquo;,
&bdquo;blackbird&ldquo; a &bdquo;bluebird&ldquo;:</p>

<pre>
import tiktoken
&nbsp;
encoder = tiktoken.get_encoding("cl100k_base")
&nbsp;
compound_words = (
        "bird",
        "black",
        "blue",
        "blackbird",
        "bluebird",
)
&nbsp;
for word in compound_words:
    tokens = encoder.encode(word)
    print(f"{word:12}", tokens)
</pre>

<p>Výsledek ukazuje, jak slova &bdquo;blackbird&ldquo; a &bdquo;bluebird&ldquo;
vznikla složením jednodušších slov:</p>

<pre>
bird         [23414]
black        [11708]
blue         [12481]
blackbird    [11708, 23414]
bluebird     [12481, 23414]
</pre>

<p>Další příklad, tentokráte slov se stejným prefixem:</p>

<pre>
import tiktoken
&nbsp;
encoder = tiktoken.get_encoding("cl100k_base")
&nbsp;
compound_words = (
        "inter",
        "intermedial",
        "intermediate",
        "intermediaries",
        "intermediation",
)
&nbsp;
for word in compound_words:
    tokens = encoder.encode(word)
    print(f"{word:14}", tokens)
</pre>

<p>U posledních dvou slov prostřední token 4503 znamená &bdquo;medi&ldquo;:</p>

<pre>
inter          [2295]
intermedial    [2295, 2106, 532]
intermediate   [2295, 14978]
intermediaries [2295, 4503, 5548]
intermediation [2295, 4503, 367]
</pre>

<p>Známé dlouhé slovo je &bdquo;internationalization&ldquo;, ale zkusme si
tokenizovat ještě delší slovo &bdquo;counterrevolutionaries&ldquo;:</p>

<pre>
import tiktoken
&nbsp;
encoder = tiktoken.get_encoding("cl100k_base")
&nbsp;
compound_words = (
        "unprofessionally",
        "internationalization",
        "counterrevolutionaries",
)
&nbsp;
for word in compound_words:
    tokens = encoder.encode(word)
    print(f"{word:22}", tokens)
</pre>

<p>Kupodivu jsou tato slova stále reprezentována relativně malým množstvím
tokenů:</p>

<pre>
unprofessionally       [359, 97235, 750]
internationalization   [98697, 2065]
counterrevolutionaries [8456, 96822, 5548]
</pre>

<p><div class="rs-tip-major">Poznámka: existují i delší anglická slova,
například názvy chemických sloučenin atd. Mé oblíbené je
&bdquo;hexakosioihexekontahexaphobia&ldquo; (strach z&nbsp;čísla 666), které
není tokenizováno na [666] ale na, protože každý token zde představuje dvojici
či trojici znaků:</div></p>

<pre>
[17757, 587, 437, 822, 7141, 327, 1247, 546, 1494, 327, 1366, 41163]
</pre>



<p><a name="k09"></a></p>
<h2 id="k09">9. Složená slova zakódovaná jediným tokenem</h2>

<p>Některá složená slova jsou používána tak často, že jsou reprezentována
jediným tokenem. Příkladem takového slova je (v&nbsp;námi používaném enkodéru!)
&bdquo;firefox&ldquo; zatímco slovo &bdquo;fireplace&ldquo; je používáno méně
často a proto je zakódováno dvojicí tokenů. Ostatně si to můžeme velmi snadno
ověřit spuštěním následujícího skriptu:</p>

<pre>
import tiktoken
&nbsp;
encoder = tiktoken.get_encoding("cl100k_base")
&nbsp;
compound_words = (
        "fire",
        "fox",
        "place",
        "fireplace",
        "firefox",
)
&nbsp;
for word in compound_words:
    tokens = encoder.encode(word)
    print(f"{word:12}", tokens)
</pre>

<p>Slova &bdquo;fire&ldquo;, &bdquo;fox&ldquo; a &bdquo;place&ldquo; jsou
reprezentována jediným tokenem, zatímco slovo &bdquo;fireplace&ldquo; vzniklo
(podle očekávání) spojením &bdquo;fire+place&ldquo;. Ovšem
&bdquo;firefox&ldquo; má vlastní token:</p>

<pre>
fire         [11029]
fox          [15361]
place        [2050]
fireplace    [11029, 2050]
firefox      [99012]
</pre>



<p><a name="k10"></a></p>
<h2 id="k10">10. Kódování mezer</h2>

<p>Většinou se setkáme i s&nbsp;tím, že existují vyhrazené tokeny pro sekvence
mezer, které v&nbsp;automaticky zpracovávaných textech taktéž často nalezneme
(i když se mnohdy tyto mezery odstraňují při preprocessingu). Jedna mezera mezi
slovy je součástí tokenů slov, ale více mezer již potřebuje svůj vlastní token
(či tokeny, pokud je mezer opravdu hodně). Opět si to otestujme, a to na známém
příkladu textu &bdquo;Hello world&ldquo;, do něhož budeme vkládat mezery mezi
obě slova:</p>

<pre>
import tiktoken
&nbsp;
encoder = tiktoken.get_encoding("cl100k_base")
&nbsp;
spaces = (
        "Hello world",
        "Hello  world",
        "Hello   world",
        "Hello    world",
        "Hello     world",
)
&nbsp;
for word in spaces:
    tokens = encoder.encode(word)
    print(f"{word:16}", tokens)
</pre>

<p>Z&nbsp;výsledků je patrné, že větší množství mezer je zakódováno jediným
tokenem (samozřejmě s&nbsp;nějakým limitem):</p>

<pre>
Hello world      [9906, 1917]
Hello  world     [9906, 220, 1917]
Hello   world    [9906, 256, 1917]
Hello    world   [9906, 262, 1917]
Hello     world  [9906, 257, 1917]
</pre>

<p>Dosažení limitu:</p>

<pre>
Hello                                                                                         world
</pre>

<p>Nyní bude tokenizace odlišná:</p>

<pre>
[9906, 5351, 5218, 1917]
</pre>

<p><div class="rs-tip-major">Poznámka: mezera mezi slovy je ve skutečnosti
součástí druhého slova. Viz rozdíl mezi tokeny pro slovo &bdquo;world&ldquo; a
&bdquo; world&ldquo;:</div></p>

<pre>
hello            [15339]
world            [14957]
 world           [1917]
hello world      [15339, 1917]  &lt;- zde se používá token slova " world" s mezerou
</pre>



<p><a name="k11"></a></p>
<h2 id="k11">11. Kódování dalších speciálních znaků</h2>

<p>Enkodér <i>cl100k_base</i> je primárně určen pro anglické texty, ale měl by
zvládnout i použití různých speciálních znaků. Vyzkoušejme si tedy poněkud
extrémní případ (který je ovšem taktéž nutné v&nbsp;praxi řešit), a to
konkrétně tokenizaci části zdrojového kódu. Výsledek pravděpodobně nebude
úsporný, ale minimálně by měl být korektní:</p>

<pre>
import tiktoken
&nbsp;
encoder = tiktoken.get_encoding("cl100k_base")
&nbsp;
source_code = '''
    """Ackermannova funkce."""
    if m == 0:
        return n + 1
    if n == 0:
        return A(m - 1, 1)
    return A(m - 1, A(m, n - 1))
'''
&nbsp;
tokens = encoder.encode(source_code)
print("Tokens: ", len(tokens))
print(tokens)
</pre>

<p>Získáme sekvenci 57 tokenů, což je pro tak malý vstup poměrně vysoká hodnota
naznačující, že tokenizér není pro tento vstup určen:</p>

<pre>
Tokens:  57
[198, 262, 4304, 56659, 92550, 12949, 69392, 346,
14781, 262, 422, 296, 624, 220, 15, 512, 286, 471,
308, 489, 220, 16, 198, 262, 422, 308, 624, 220,
15, 512, 286, 471, 362, 1278, 482, 220, 16, 11,
220, 16, 340, 262, 471, 362, 1278, 482, 220, 16,
11, 362, 1278, 11, 308, 482, 220, 16, 1192]
</pre>

<p><div class="rs-tip-major">Poznámka: v&nbsp;případě, že je token uložen ve
čtyřech bajtech, je délka tokenizovaného textu rovna 57&times;4=228 bajtům, což
je více, než původní zdrojový text. U anglických textů je tomu přesně
naopak.</div></p>



<p><a name="k12"></a></p>
<h2 id="k12">12. Zpětný převod tokenů na text</h2>

<p>Knihovna <i>Tiktoken</i> umožňuje provádět i zpětný převod tokenů na text.
Může se přitom jednat o libovolnou sekvenci tokenů &ndash; ovšem ne vždy je
(pochopitelně) výsledkem čitelný a gramaticky správný text. Ukažme si tedy
jednoduchou de-tokenizaci:</p>

<pre>
import tiktoken
&nbsp;
encoder = tiktoken.get_encoding("cl100k_base")
&nbsp;
tokens = [9906, 1917]
text = encoder.decode(tokens)
print(text)
&nbsp;
tokens = [9906, 11, 1917, 0]
text = encoder.decode(tokens)
print(text)
&nbsp;
tokens = [9906, 11, 1917, 0, 0, 0, 0]
text = encoder.decode(tokens)
print(text)
&nbsp;
tokens = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
text = encoder.decode(tokens)
print(text)
&nbsp;
tokens = [1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010]
text = encoder.decode(tokens)
print(text)
</pre>

<p>Výsledky vypsané po spuštění tohoto skriptu:</p>

<pre>
Hello world
Hello, world!
Hello, world!!!!
!"#$%&amp;'()*+
indowlementpectash[i use.Fpec adoveception
</pre>

<p>Poslední dva řádky ukazují obsah tokenů 0-10 (běžné znaky, které
v&nbsp;textu nalezneme) a 1000 až 1010 (což jsou části slov, takže výsledek
nedává smysl).</p>



<p><a name="k13"></a></p>
<h2 id="k13">13. Od <i>cl100k_base</i> k&nbsp;dalším enkodérům a modulům</h2>

<p><i>cl100k_base</i> je jen jedním z&nbsp;enkodérů umožňujících tokenizaci
textu. Těchto enkodérů existuje více a některé jsou ve výchozím nastavení
používány určitými modely. <i>cl100k_base</i> je používán v&nbsp;modelech
GPT-3.4 i GPT-4. Naproti tomu modely DaVinci a Babbage (pro větší zmatek se
tyto modely navíc číslují/verzují) používají buď taktéž <i>cl100k_base</i> nebo
<i>p50k_base</i> či <i>r50k_base</i>, na základě zvolené konfigurace. Jak
uvidíme v&nbsp;dalším článku, může být využití dekodérů s&nbsp;menším celkovým
množstvím tokenů v&nbsp;některých případech výhodné (ostatně i proto se také
nepoužívají enkodéry s&nbsp;naprosto všemi anglickými slovy reprezentovanými
unikátními tokeny, i když je to teoreticky možné), to však již poněkud
předbíháme.</p>



<p><a name="k14"></a></p>
<h2 id="k14">14. Porovnání kódování <i>cl100k_base</i> s&nbsp;dalšími kodéry a moduly</h2>

<p>Jednotlivé kodéry si pochopitelně můžeme mezi sebou snadno porovnat.
V&nbsp;následujícím skriptu, který byl inspirován přímo dokumentací ke knihovně
<i>Tiktoken</i>, je tokenizována věta a následně slovo &bdquo;firefox&ldquo;, a
to s&nbsp;využitím různých &bdquo;slovníků tokenů&ldquo;. Ten nejčastěji
používaný <i>cl100k_base</i> již známe (obsahuje cca 100000 tokenů pro slova i
jejich části), ale zajímavé bude zjistit, jak vypadá tokenizace v&nbsp;případě
použití <i>r50k_base</i> a <i>p50k_base</i> s&nbsp;menším množstvím tokenů ve
slovníku. Lze předpokládat, že pro menší slovníky budou některá slova rozdělena
na více tokenů:</p>

<pre>
import tiktoken
&nbsp;
def <strong>compare_encodings</strong>(example_string: str) -&gt; None:
    print(f'\nExample string: "{example_string}"')
&nbsp;
    for encoding_name in ["r50k_base", "p50k_base", "cl100k_base"]:
        encoding = tiktoken.get_encoding(encoding_name)
        tokens = encoding.encode(example_string)
        num_tokens = len(tokens)
        bytes = [encoding.decode_single_token_bytes(token) for token in tokens]
&nbsp;
        print()
        print(f"{encoding_name}: {num_tokens} tokens")
        print(f"token integers: {tokens}")
        print(f"token bytes: {bytes}")
    print()
&nbsp;
&nbsp;
compare_encodings("The quick brown fox jumps over the lazy dog.")
&nbsp;
compare_encodings("firefox")
</pre>

<p>Výsledky pro anglickou větu vypadají stejně (co slovo, to token, následuje
token pro tečku), ovšem hodnoty některých tokenů se od sebe odlišují:</p>

<pre>
r50k_base: 10 tokens
token integers: [464, 2068, 7586, 21831, 18045, 625, 262, 16931, 3290, 13]
token bytes: [b'The', b' quick', b' brown', b' fox', b' jumps', b' over', b' the', b' lazy', b' dog', b'.']
&nbsp;
p50k_base: 10 tokens
token integers: [464, 2068, 7586, 21831, 18045, 625, 262, 16931, 3290, 13]
token bytes: [b'The', b' quick', b' brown', b' fox', b' jumps', b' over', b' the', b' lazy', b' dog', b'.']
&nbsp;
cl100k_base: 10 tokens
token integers: [791, 4062, 14198, 39935, 35308, 927, 279, 16053, 5679, 13]
token bytes: [b'The', b' quick', b' brown', b' fox', b' jumps', b' over', b' the', b' lazy', b' dog', b'.']
</pre>

<p>U slova &bdquo;firefox&ldquo; je však patrné, že toto slovo jako celek
existuje pouze v&nbsp;<i>cl100k_base</i>, zatímco v&nbsp;případě menších
slovníků muselo být slovo reprezentováno dvěma tokeny pro &bdquo;fire&ldquo; a
&bdquo;fox&ldquo; (bez mezery):</p>

<pre>
Example string: "firefox"
&nbsp;
r50k_base: 2 tokens
token integers: [6495, 12792]
token bytes: [b'fire', b'fox']
&nbsp;
p50k_base: 2 tokens
token integers: [6495, 12792]
token bytes: [b'fire', b'fox']
&nbsp;
cl100k_base: 1 tokens
token integers: [99012]
token bytes: [b'firefox']
</pre>



<p><a name="k15"></a></p>
<h2 id="k15">15. Rychlost tokenizace</h2>

<p>V&nbsp;některých oblastech je důležitá i rychlost tokenizace, i když
v&nbsp;případě jazykových modelů jsou časově mnohem náročnější odlišné operace.
Knihovna <i>Tiktoken</i> je z&nbsp;větší části naprogramována v&nbsp;Rustu a
tak by rychlost tokenizace či zpětné tokenizace měla být poměrně vysoká (záleží
ovšem na vnitřní struktuře slovníku tokenů). Ovšem i tento předpoklad si můžeme
ověřit. Budeme opakovaně tokenizovat větu, která se převede na deset tokenů.
Celkem tedy provedeme tokenizaci textu s&nbsp;výsledkem 1 milion tokenů a
budeme přitom měřit dosažený čas:</p>

<pre>
from time import perf_counter
import tiktoken
&nbsp;
encoder = tiktoken.get_encoding("cl100k_base")
&nbsp;
start_time = perf_counter()
&nbsp;
token_count = 0
for i in range(100000):
    tokens = encoder.encode("The quick brown fox jumps over the lazy dog.")
    token_count += len(tokens)
&nbsp;
end_time = perf_counter()
&nbsp;
seconds = end_time-start_time
&nbsp;
print("Tokens:", token_count)
print("Time:", seconds, "seconds")
&nbsp;
speed = int(token_count / seconds)
print(speed, "tokens per second")
</pre>



<p><a name="k16"></a></p>
<h2 id="k16">16. Rychlost zpětného převodu sekvence tokenů na text</h2>

<pre>
from time import perf_counter
import tiktoken
&nbsp;
encoder = tiktoken.get_encoding("cl100k_base")
&nbsp;
start_time = perf_counter()
&nbsp;
tokens = encoder.encode("The quick brown fox jumps over the lazy dog.")
&nbsp;
characters_count = 0
for i in range(100000):
    text = encoder.decode(tokens)
    characters_count += len(text)
&nbsp;
end_time = perf_counter()
&nbsp;
seconds = end_time-start_time
&nbsp;
print("Characters:", characters_count)
print("Time:", seconds, "seconds")
&nbsp;
speed = int(characters_count / seconds)
print(speed, "characters per second")
</pre>



<p><a name="k17"></a></p>
<h2 id="k17">17. Další tokenizéry</h2>

<pre>
from transformers import BertTokenizer
tokenizer = BertTokenizer.from_pretrained("cesta")
text = "Hello, world!"
encoding = tokenizer.encode(text)
print(encoding)
print(tokenizer.convert_ids_to_tokens(encoding))
</pre>



<p><a name="k18"></a></p>
<h2 id="k18">18. Speciální tokeny</h2>



<p><a name="k19"></a></p>
<h2 id="k19">19. Repositář s&nbsp;demonstračními příklady</h2>

<p>Zdrojové kódy všech popsaných demonstračních příkladů určených pro
programovací jazyk Python 3 a knihovnu <i>tiktoken</i> byly uloženy do Git
repositáře dostupného na adrese <a
href="https://github.com/tisnik/most-popular-python-libs">https://github.com/tisnik/most-popular-python-libs</a>:</p>

<table>
<tr><th> #</th><th>Demonstrační příklad</th><th>Stručný popis příkladu</th><th>Cesta</th></tr>
<tr><td> 1</td><td>tokenize_hello_world_1.py</td><td>tokenizace textu &bdquo;Hello world&ldquo;</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/tiktoken/tokenize_hello_world_1.py">https://github.com/tisnik/most-popular-python-libs/blob/master/tiktoken/tokenize_hello_world_1.py</a></td></tr>
<tr><td> 2</td><td>tokenize_hello_world_2.py</td><td>tokenizace textu &bdquo;Hello world&ldquo; při změně velikosti písmen</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/tiktoken/tokenize_hello_world_2.py">https://github.com/tisnik/most-popular-python-libs/blob/master/tiktoken/tokenize_hello_world_2.py</a></td></tr>
<tr><td> 3</td><td>tokenize_hello_world_3.py</td><td>tokenizace textu &bdquo;Hello, world!&ldquo; (i s&nbsp;interpunkčními znaménky)</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/tiktoken/tokenize_hello_world_3.py">https://github.com/tisnik/most-popular-python-libs/blob/master/tiktoken/tokenize_hello_world_3.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td> 4</td><td>exclamation_mark.py</td><td>tokenizace jediného znaku (vykřičníku)</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/tiktoken/exclamation_mark.py">https://github.com/tisnik/most-popular-python-libs/blob/master/tiktoken/exclamation_mark.py</a></td></tr>
<tr><td> 5</td><td>compound_words_1.py</td><td>tokenizace složených slov</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/tiktoken/compound_words_1.py">https://github.com/tisnik/most-popular-python-libs/blob/master/tiktoken/compound_words_1.py</a></td></tr>
<tr><td> 6</td><td>compound_words_2.py</td><td>tokenizace složených slov</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/tiktoken/compound_words_2.py">https://github.com/tisnik/most-popular-python-libs/blob/master/tiktoken/compound_words_2.py</a></td></tr>
<tr><td> 7</td><td>compound_words_3.py</td><td>tokenizace složených slov</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/tiktoken/compound_words_3.py">https://github.com/tisnik/most-popular-python-libs/blob/master/tiktoken/compound_words_3.py</a></td></tr>
<tr><td> 8</td><td>compound_words_4.py</td><td>složená slova zakódovaná jediným tokenem</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/tiktoken/compound_words_4.py">https://github.com/tisnik/most-popular-python-libs/blob/master/tiktoken/compound_words_4.py</a></td></tr>
<tr><td> 9</td><td>spaces_1.py</td><td>tokenizace mezer</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/tiktoken/spaces_1.py">https://github.com/tisnik/most-popular-python-libs/blob/master/tiktoken/spaces_1.py</a></td></tr>
<tr><td>10</td><td>spaces_2.py</td><td>tokenizace mezer</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/tiktoken/spaces_2.py">https://github.com/tisnik/most-popular-python-libs/blob/master/tiktoken/spaces_2.py</a></td></tr>
<tr><td>11</td><td>special_characters.py</td><td>tokenizace speciálních znaků</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/tiktoken/special_characters.py">https://github.com/tisnik/most-popular-python-libs/blob/master/tiktoken/special_characters.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>12</td><td>tokens_to_text.py</td><td>převod tokenů zpět na text</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/tiktoken/tokens_to_text.py">https://github.com/tisnik/most-popular-python-libs/blob/master/tiktoken/tokens_to_text.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>13</td><td>speed_tokenization.py</td><td>změření rychlosti tokenizace</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/tiktoken/speed_tokenization.py">https://github.com/tisnik/most-popular-python-libs/blob/master/tiktoken/speed_tokenization.py</a></td></tr>
<tr><td>14</td><td>speed_to_text.py</td><td>změření rychlosti generování textu z&nbsp;tokenů</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/tiktoken/speed_to_text.py">https://github.com/tisnik/most-popular-python-libs/blob/master/tiktoken/speed_to_text.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>15</td><td>different_encoders.py</td><td>použití odlišných enkodérů</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/tiktoken/different_encoders.py">https://github.com/tisnik/most-popular-python-libs/blob/master/tiktoken/different_encoders.py</a></td></tr>
</table>



<p><a name="k20"></a></p>
<h2 id="k20">20. Odkazy na Internetu</h2>

<ol>

<li>tiktoken na GitHubu<br />
<a href="https://github.com/openai/tiktoken">https://github.com/openai/tiktoken</a>
</li>

<li>tiktoken na PyPi<br />
<a href="https://pypi.org/project/tiktoken/">https://pypi.org/project/tiktoken/</a>
</li>

<li>Byte pair encoding (Wikipedie)<br />
<a href="https://en.wikipedia.org/wiki/Byte_pair_encoding">https://en.wikipedia.org/wiki/Byte_pair_encoding</a>
</li>

<li>A Beginner’s Guide to Tokens, Vectors, and Embeddings in NLP<br />
<a href="https://medium.com/@saschametzger/what-are-tokens-vectors-and-embeddings-how-do-you-create-them-e2a3e698e037">https://medium.com/@saschametzger/what-are-tokens-vectors-and-embeddings-how-do-you-create-them-e2a3e698e037</a>
</li>

<li>5 Simple Ways to Tokenize Text in Python<br />
<a href="https://towardsdatascience.com/5-simple-ways-to-tokenize-text-in-python-92c6804edfc4">https://towardsdatascience.com/5-simple-ways-to-tokenize-text-in-python-92c6804edfc4</a>
</li>

<li>How to count tokens with Tiktoken<br />
<a href="https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken">https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken</a>
</li>

<li>An Explanatory Guide to BERT Tokenizer<br />
<a href="https://www.analyticsvidhya.com/blog/2021/09/an-explanatory-guide-to-bert-tokenizer/">https://www.analyticsvidhya.com/blog/2021/09/an-explanatory-guide-to-bert-tokenizer/</a>
</li>

<li>Tiktokenizer<br />
<a href="https://tiktokenizer.vercel.app/">https://tiktokenizer.vercel.app/</a>
</li>

<li>Mastering BERT Tokenization and Encoding<br />
<a href="https://albertauyeung.github.io/2020/06/19/bert-tokenization.html">https://albertauyeung.github.io/2020/06/19/bert-tokenization.html</a>
</li>

<li>The amazing power of word vectors<br />
<a href="https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/">https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/</a>
</li>

<li>Which embedding tokenizer should I use?<br />
<a href="https://community.openai.com/t/which-embedding-tokenizer-should-i-use/82483">https://community.openai.com/t/which-embedding-tokenizer-should-i-use/82483</a>
</li>

<li>Getting Started With Embeddings<br />
<a href="https://huggingface.co/blog/getting-started-with-embeddings">https://huggingface.co/blog/getting-started-with-embeddings</a>
</li>

<li>Lexical analysis (Wikipedia)<br />
<a href="https://en.wikipedia.org/wiki/Lexical_analysis#Token">https://en.wikipedia.org/wiki/Lexical_analysis#Token</a>
</li>

<li>Lexikální analýza (Wikipedia)<br />
<a href="https://cs.wikipedia.org/wiki/Lexik%C3%A1ln%C3%AD_anal%C3%BDza">https://cs.wikipedia.org/wiki/Lexik%C3%A1ln%C3%AD_anal%C3%BDza</a>
</li>

<li>Jazykový korpus<br />
<a href="https://cs.wikipedia.org/wiki/Jazykov%C3%BD_korpus">https://cs.wikipedia.org/wiki/Jazykov%C3%BD_korpus</a>
</li>

<li>AP8, IN8 Regulární jazyky<br />
<a href="http://statnice.dqd.cz/home:inf:ap8">http://statnice.dqd.cz/home:inf:ap8</a>
</li>

<li>AP9, IN9 Konečné automaty<br />
<a href="http://statnice.dqd.cz/home:inf:ap9">http://statnice.dqd.cz/home:inf:ap9</a>
</li>

<li>AP10, IN10 Bezkontextové jazyky<br />
<a href="http://statnice.dqd.cz/home:inf:ap10">http://statnice.dqd.cz/home:inf:ap10</a>
</li>

<li>AP11, IN11 Zásobníkové automaty, Syntaktická analýza<br />
<a href="http://statnice.dqd.cz/home:inf:ap11">http://statnice.dqd.cz/home:inf:ap11</a>
</li>

<li>PaLM 2 (Pathways Language Model)<br />
<a href="https://ai.google/discover/palm2/">https://ai.google/discover/palm2/</a>
</li>

<li>Gemini<br />
<a href="https://deepmind.google/technologies/gemini/#introduction">https://deepmind.google/technologies/gemini/#introduction</a>
</li>

<li>LLaMA (Large Language Model Meta AI)<br />
<a href="https://en.wikipedia.org/wiki/LLaMA">https://en.wikipedia.org/wiki/LLaMA</a>
</li>

<li>GPT-4<br />
<a href="https://openai.com/gpt-4">https://openai.com/gpt-4</a>
</li>

</ol>



<p></p><p></p>
<p><small>Autor: <a href="http://www.fit.vutbr.cz/~tisnovpa">Pavel Tišnovský</a> &nbsp; 2024</small></p>
</body>
</html>

