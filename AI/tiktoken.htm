<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
        "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html>
<head>
<title>Tokenizace textu: základní operace při zpracování přirozeného jazyka</title>
<meta name="Author" content="Pavel Tisnovsky" />
<meta name="Generator" content="vim" />
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<style type="text/css">
         body {color:#000000; background:#ffffff;}
         h1  {font-family: arial, helvetica, sans-serif; color:#ffffff; background-color:#c00000; text-align:center; padding-left:1em}
         h2  {font-family: arial, helvetica, sans-serif; color:#ffffff; background-color:#0000c0; padding-left:1em; text-align:left}
         h3  {font-family: arial, helvetica, sans-serif; color:#000000; background-color:#c0c0c0; padding-left:1em; text-align:left}
         h4  {font-family: arial, helvetica, sans-serif; color:#000000; background-color:#e0e0e0; padding-left:1em; text-align:left}
         a   {font-family: arial, helvetica, sans-serif;}
         li  {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify; width:450px;}
         ol  {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify; width:450px;}
         ul  {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify; width:450px;}
         p   {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify;}
         pre {background:#e0e0e0}
</style>
</head>

<body>

<h1>Tokenizace textu: základní operace při zpracování přirozeného jazyka</h1>

<h3>Pavel Tišnovský</h3>

<p></p>

<h1>Úvodník</h1>

<p></p>



<h2>Obsah</h2>

<p><a href="#k01">1. Tokenizace textu: základní operace při zpracování přirozeného jazyka</a></p>
<p><a href="#k02">*** 2. Převod slov na tokeny</a></p>
<p><a href="#k03">*** 3. Tokenizace složených slov</a></p>
<p><a href="#k04">4. Knihovna <i>tiktoken</i></a></p>
<p><a href="#k05">5. Instalace knihovny <i>tiktoken</i></a></p>
<p><a href="#k06">6. Ukázka tokenizace s&nbsp;využitím kódování <i>c100k_base</i></a></p>
<p><a href="#k07">7. Interpunkční znaménka</a></p>
<p><a href="#k08">*** 8. Složená slova a kódování <i>c100k_base</i></a></p>
<p><a href="#k09">*** 9. Kódování mezer</a></p>
<p><a href="#k10">*** 10. Kódování dalších speciálních znaků</a></p>
<p><a href="#k11">*** 11. Zpětný převod tokenů na text</a></p>
<p><a href="#k12">*** 12. Od <i>cl100k_base</i> k&nbsp;dalším enkodérům a modulům</a></p>
<p><a href="#k13">*** 13. Porovnání kódování <i>cl100k_base</i> s&nbsp;dalšími kodéry a moduly</a></p>
<p><a href="#k14">*** 14. Rychlost tokenizace</a></p>
<p><a href="#k15">*** 15. Rychlost zpětného převodu sekvence tokenů na text</a></p>
<p><a href="#k14">*** 14. </a></p>
<p><a href="#k15">*** 15. </a></p>
<p><a href="#k16">*** 16. </a></p>
<p><a href="#k17">*** 17. </a></p>
<p><a href="#k18">*** 18. </a></p>
<p><a href="#k19">*** 19. Repositář s&nbsp;demonstračními příklady</a></p>
<p><a href="#k20">20. Odkazy na Internetu</a></p>



<p><a name="k01"></a></p>
<h2 id="k01">1. Tokenizace textu: základní operace při zpracování přirozeného jazyka</h2>

<p>V&nbsp;současnosti se obor IT poměrně důrazně zaměřuje na problematiku
zpracování přirozeného jazyka (<i>Natural language processing</i>, <i>NLP</i>),
která byla zpopularizována i mezi širokou veřejností mj.&nbsp;i díky úspěšně
nasazeným rozsáhlým jazykovým modelům (<i>Large Language Model</i>,
<i>LLM</i>), mezi něž patří například GPT (OpenAI), PaLM, Gemini, LLaMA apod.
Při zpracování přirozeného jazyka se používá několik standardních postupů a
algoritmů, s&nbsp;nimiž se postupně seznámíme v&nbsp;navazující sérii článků.
Na samotném začátku zpracování textu, ale i při ukládání &bdquo;znalosti&ldquo;
informací (jazykový korpus), se využívá takzvaná <i>tokenizace</i>.
V&nbsp;dnešním článku se seznámíme se základními principy, na nichž je tato
operace postavena a ukážeme si ji na příkladu knihovny
<strong>tiktoken</strong> (i když dnes existuje celá řada podobně koncipovaných
knihoven, ostatně o některých dalších knihovnách, které jsou používány, se dnes
ještě zmíníme).</p>



<p><a name="k02"></a></p>
<h2 id="k02">2. Převod slov na tokeny</h2>



<p><a name="k03"></a></p>
<h2 id="k03">3. Tokenizace složených slov</h2>



<p><a name="k04"></a></p>
<h2 id="k04">4. Knihovna <i>tiktoken</i></h2>

<p>Pro tokenizaci a popř.&nbsp;i pro zpětný převod tokenů na text lze využít
větší množství knihoven. Tyto knihovny se od sebe odlišují jak svou
implementací, tak i tím, zda jsou (či naopak nejsou) součástí větších
programových celků pro realizaci systémů pro zpracování přirozeného jazyka
(dnes typicky založených na LLM). Dnes se budeme primárně zabývat knihovnou
nazvanou <i>tiktoken</i>, jejíž výhodou je fakt, že je velmi snadno použitelná
a lze ji nainstalovat a používat samostatně, bez nutnosti instalace frameworků
pro LLM. Navíc tato knihovna podporuje několik enkodérů textu na tokeny i
několik specifických modelů (například GPT-4 atd.), což je problematika,
k&nbsp;níž se ještě několikrát vrátíme.</p>



<p><a name="k05"></a></p>
<h2 id="k05">5. Instalace knihovny <i>tiktoken</i></h2>

<p>Vzhledem k&nbsp;tomu, že knihovna <i>tiktoken</i> je dostupná na <a
href="https://pypi.org/">PyPI</a>, je její instalace snadná. Buď je možné
provést instalaci pro všechny uživatele:</p>

<pre>
$ <strong>pip3 install tiktoken</strong>
</pre>

<p>Nebo pouze pro aktivního uživatele:</p>

<pre>
$ <strong>pip3 install --user tiktoken</strong>
</pre>

<p>V&nbsp;průběhu instalace se stáhnou i další tranzitivní závislosti, ale není
jich mnoho:</p>

<pre>
Collecting tiktoken
  Downloading tiktoken-0.6.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)
Collecting regex&gt;=2022.1.18 (from tiktoken)
  Downloading regex-2023.12.25-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.9/40.9 kB 839.4 kB/s eta 0:00:00
Collecting requests&gt;=2.26.0 (from tiktoken)
  Downloading requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)
Collecting charset-normalizer&lt;4,&gt;=2 (from requests&gt;=2.26.0-&gt;tiktoken)
  Downloading charset_normalizer-3.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)
Requirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/lib/python3/dist-packages (from requests&gt;=2.26.0-&gt;tiktoken) (2.8)
Requirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/lib/python3/dist-packages (from requests&gt;=2.26.0-&gt;tiktoken) (1.25.8)
Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/lib/python3/dist-packages (from requests&gt;=2.26.0-&gt;tiktoken) (2019.11.28)
Downloading tiktoken-0.6.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.8/1.8 MB 4.9 MB/s eta 0:00:00
Downloading regex-2023.12.25-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (777 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 777.0/777.0 kB 2.7 MB/s eta 0:00:00
Downloading requests-2.31.0-py3-none-any.whl (62 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.6/62.6 kB 2.1 MB/s eta 0:00:00
Downloading charset_normalizer-3.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 141.1/141.1 kB 3.2 MB/s eta 0:00:00
Installing collected packages: regex, charset-normalizer, requests, tiktoken
Successfully installed charset-normalizer-3.3.2 regex-2023.12.25 requests-2.31.0 tiktoken-0.6.0
</pre>

<p>Samotná knihovna <i>tiktoken</i> zabere po instalaci cca 6,5 megabajtu,
přičemž největší soubor tvoří nativní knihovna s&nbsp;realizací algoritmu
tokenizace a zpětného převodu tokenů na text (tato nativní knihovna je
naprogramována v&nbsp;Rustu, výsledkem jsou velmi rychlé realizace všech
operací):</p>

<pre>
~/.local/lib/python3.12/site-packages/tiktoken$ <strong>ls -sh1h</strong>
&nbsp;
total 6,5M
 24K core.py
 20K _educational.py
 12K __init__.py
 16K load.py
 12K model.py
4,0K __pycache__
8,0K py.typed
 12K registry.py
6,4M _tiktoken.cpython-312-x86_64-linux-gnu.so
</pre>



<p><a name="k06"></a></p>
<h2 id="k06">6. Ukázka tokenizace s&nbsp;využitím kódování <i>c100k_base</i></h2>

<p>Podívejme se nyní na způsob tokenizace jednoduchého textu, konkrétně
klasické zprávy &bdquo;Hello world&ldquo;, na tokeny. Pro tokenizaci bude
využito kódování (resp.&nbsp;možná přesněji řečeno enkodér) <i>c100k_base</i>.
Toto jméno je odvozeno od faktu, že obsahuje přibližně sto tisíc tokenů
resp.&nbsp;přesněji řečeno mapování sekvence znaků na tokeny (z&nbsp;toho
plyne, že neobsahuje všechna slova, to však nevadí, jak uvidíme dále).</p>

<p>Samotný skript, který tokenizaci provede, je až triviálně jednoduchý, což je
ostatně jedna z&nbsp;dobrých vlastností knihovny <i>tiktoken</i>. Nejprve
získáme potřebný enkodér a poté provedeme tokenizaci zavoláním metody
<strong>encode</strong>. Výsledkem je seznam s&nbsp;tokeny,
tj.&nbsp;s&nbsp;celočíselnými hodnotami:</p>

<pre>
import tiktoken
&nbsp;
encoder = tiktoken.get_encoding("cl100k_base")
&nbsp;
tokens = encoder.encode(<strong>"Hello world"</strong>)
print(tokens)
</pre>

<p>A takto vypadá výsledek:</p>

<pre>
[9906, 1917]
</pre>

<p><div class="rs-tip-major">Poznámka: v&nbsp;tomto případě jsou enkodérem
nalezena obě slova a každé je tedy reprezentováno jedinou celočíselnou
hodnotou. Ne vždy tomu tak však musí být, jak si ostatně ukážeme <a
href="#k07">v&nbsp;navazující kapitole</a>.</div></p>

<p>Na tomto místě je vhodné upozornit na to, že pokud například nahradíme slovo
&bdquo;Hello&ldquo; za &bdquo;hello&ldquo;, bude se z&nbsp;pohledu enkodéru
jednat o odlišné slovo, které bude zakódováno do odlišné sekvence tokenů.
Ostatně si to můžeme velmi snadno ověřit:</p>

<pre>
import tiktoken
&nbsp;
encoder = tiktoken.get_encoding("cl100k_base")
&nbsp;
tokens = encoder.encode(<strong>"Hello world"</strong>)
print(tokens)
&nbsp;
tokens = encoder.encode(<strong>"hello world"</strong>)
print(tokens)
&nbsp;
tokens = encoder.encode(<strong>"hello World"</strong>)
print(tokens)
&nbsp;
tokens = encoder.encode(<strong>"Hello World"</strong>)
print(tokens)
</pre>

<p>Výsledky naznačují, jaké tokeny odpovídají slovům &bdquo;Hello&ldquo;,
&bdquo;hello&ldquo;, &bdquo;World&ldquo; i &bdquo;world&ldquo;:</p>

<pre>
[9906, 1917]
[15339, 1917]
[15339, 4435]
[9906, 4435]
</pre>



<p><a name="k07"></a></p>
<h2 id="k07">7. Interpunkční znaménka</h2>

<p>Ve skutečnosti má být zpráva &bdquo;Hello world&ldquo; správně zapsána jako
&bdquo;Hello, world!&ldquo;. To nás vede k&nbsp;problematice kódování
interpunkčních znamének. Zkusme si tedy nechat tokenizovat zprávu i
s&nbsp;oběma interpunkčními znaménky:</p>

<pre>
import tiktoken
&nbsp;
encoder = tiktoken.get_encoding("cl100k_base")
&nbsp;
tokens = encoder.encode(<strong>"Hello, world!"</strong>)
print(tokens)
</pre>

<p>Výsledkem v&nbsp;tomto případě již nebude sekvence dvou tokenů, ale tokenů
čtyř, což znamená, že pokud sekvenci tokenů použijeme v&nbsp;systému pro
zpracování přirozeného jazyka, může jazykový model &bdquo;rozumět&ldquo; i
těmto důležitým znakům:</p>

<pre>
[9906, 11, 1917, 0]
</pre>

<p>Jen pro zajímavost &ndash; poslední token má hodnotu 0. Jedná se o token
reprezentující samotný vykřičník, o čemž se můžeme velmi snadno přesvědčit:</p>

<pre>
import tiktoken
&nbsp;
encoder = tiktoken.get_encoding("cl100k_base")
&nbsp;
tokens = encoder.encode(<strong>"!"</strong>)
print(tokens)
</pre>

<p>Výsledkem bude v&nbsp;tomto případě sekvence obsahující jediný token
s&nbsp;hodnotou 0:</p>

<pre>
[0]
</pre>

<p><div class="rs-tip-major">Poznámka: token číslo 1 odpovídá
uvozovce.</div></p>



<p><a name="k08"></a></p>
<h2 id="k08">8. Složená slova a kódování <i>c100k_base</i></h2>

<p>Mnohá jednoduchá anglická slova jsou transformována do jediného tokenu. Ovšem u slov delších popř.&nbsp;u slov, které obsahují často používaný základ, je tomu jinak.</p>



<p><a name="k09"></a></p>
<h2 id="k09">9. Kódování mezer</h2>



<p><a name="k10"></a></p>
<h2 id="k10">10. Kódování dalších speciálních znaků</h2>



<p><a name="k11"></a></p>
<h2 id="k11">11. Zpětný převod tokenů na text</h2>



<p><a name="k12"></a></p>
<h2 id="k12">12. Od <i>cl100k_base</i> k&nbsp;dalším enkodérům a modulům</h2>



<p><a name="k13"></a></p>
<h2 id="k13">13. Porovnání kódování <i>cl100k_base</i> s&nbsp;dalšími kodéry a moduly</h2>



<p><a name="k14"></a></p>
<h2 id="k14">14. Rychlost tokenizace</h2>



<p><a name="k15"></a></p>
<h2 id="k15">15. Rychlost zpětného převodu sekvence tokenů na text</h2>



<p><a name="k16"></a></p>
<h2 id="k16">16. </h2>

from transformers import BertTokenizer
tokenizer = BertTokenizer.from_pretrained(//path to tokenizers)
sample = 'where is Himalayas in the world map?'
encoding = tokenizer.encode(sample)
print(encoding)
print(tokenizer.convert_ids_to_tokens(encoding))


<p><a name="k17"></a></p>
<h2 id="k17">17. </h2>



<p><a name="k18"></a></p>
<h2 id="k18">18. </h2>



<p><a name="k19"></a></p>
<h2 id="k19">19. Repositář s&nbsp;demonstračními příklady</h2>

<p>Zdrojové kódy všech popsaných demonstračních příkladů určených pro
programovací jazyk Python 3 a knihovnu <i>tiktoken</i> byly uloženy do Git
repositáře dostupného na adrese <a
href="https://github.com/tisnik/most-popular-python-libs">https://github.com/tisnik/most-popular-python-libs</a>:</p>

<table>
<tr><th> #</th><th>Demonstrační příklad</th><th>Stručný popis příkladu</th><th>Cesta</th></tr>
<tr><td> 1</td><td>tokenize_hello_world_1.py</td><td></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/tiktoken/tokenize_hello_world_1.py">https://github.com/tisnik/most-popular-python-libs/blob/master/tiktoken/tokenize_hello_world_1.py</a></td></tr>
<tr><td> 2</td><td>tokenize_hello_world_2.py</td><td></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/tiktoken/tokenize_hello_world_2.py">https://github.com/tisnik/most-popular-python-libs/blob/master/tiktoken/tokenize_hello_world_2.py</a></td></tr>
<tr><td> 3</td><td>tokenize_hello_world_3.py</td><td></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/tiktoken/tokenize_hello_world_3.py">https://github.com/tisnik/most-popular-python-libs/blob/master/tiktoken/tokenize_hello_world_3.py</a></td></tr>
<tr><td> 4</td><td></td><td></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/tiktoken/">https://github.com/tisnik/most-popular-python-libs/blob/master/tiktoken/</a></td></tr>
<tr><td> 5</td><td></td><td></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/tiktoken/">https://github.com/tisnik/most-popular-python-libs/blob/master/tiktoken/</a></td></tr>
<tr><td> 6</td><td></td><td></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/tiktoken/">https://github.com/tisnik/most-popular-python-libs/blob/master/tiktoken/</a></td></tr>
<tr><td> 7</td><td></td><td></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/tiktoken/">https://github.com/tisnik/most-popular-python-libs/blob/master/tiktoken/</a></td></tr>
<tr><td> 8</td><td></td><td></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/tiktoken/">https://github.com/tisnik/most-popular-python-libs/blob/master/tiktoken/</a></td></tr>
<tr><td> 9</td><td></td><td></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/tiktoken/">https://github.com/tisnik/most-popular-python-libs/blob/master/tiktoken/</a></td></tr>
<tr><td>10</td><td></td><td></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/tiktoken/">https://github.com/tisnik/most-popular-python-libs/blob/master/tiktoken/</a></td></tr>
</table>



<p><a name="k20"></a></p>
<h2 id="k20">20. Odkazy na Internetu</h2>

<ol>

<li>tiktoken na GitHubu<br />
<a href="https://github.com/openai/tiktoken">https://github.com/openai/tiktoken</a>
</li>

<li>tiktoken na PyPi<br />
<a href="https://pypi.org/project/tiktoken/">https://pypi.org/project/tiktoken/</a>
</li>

<li>Byte pair encoding (Wikipedie)<br />
<a href="https://en.wikipedia.org/wiki/Byte_pair_encoding">https://en.wikipedia.org/wiki/Byte_pair_encoding</a>
</li>

<li>A Beginner’s Guide to Tokens, Vectors, and Embeddings in NLP<br />
<a href="https://medium.com/@saschametzger/what-are-tokens-vectors-and-embeddings-how-do-you-create-them-e2a3e698e037">https://medium.com/@saschametzger/what-are-tokens-vectors-and-embeddings-how-do-you-create-them-e2a3e698e037</a>
</li>

<li>5 Simple Ways to Tokenize Text in Python<br />
<a href="https://towardsdatascience.com/5-simple-ways-to-tokenize-text-in-python-92c6804edfc4">https://towardsdatascience.com/5-simple-ways-to-tokenize-text-in-python-92c6804edfc4</a>
</li>

<li>How to count tokens with Tiktoken<br />
<a href="https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken">https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken</a>
</li>

<li>An Explanatory Guide to BERT Tokenizer<br />
<a href="https://www.analyticsvidhya.com/blog/2021/09/an-explanatory-guide-to-bert-tokenizer/">https://www.analyticsvidhya.com/blog/2021/09/an-explanatory-guide-to-bert-tokenizer/</a>
</li>

<li>Tiktokenizer<br />
<a href="https://tiktokenizer.vercel.app/">https://tiktokenizer.vercel.app/</a>
</li>

<li>Mastering BERT Tokenization and Encoding<br />
<a href="https://albertauyeung.github.io/2020/06/19/bert-tokenization.html">https://albertauyeung.github.io/2020/06/19/bert-tokenization.html</a>
</li>

<li>The amazing power of word vectors<br />
<a href="https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/">https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/</a>
</li>

<li>Which embedding tokenizer should I use?<br />
<a href="https://community.openai.com/t/which-embedding-tokenizer-should-i-use/82483">https://community.openai.com/t/which-embedding-tokenizer-should-i-use/82483</a>
</li>

<li>Getting Started With Embeddings<br />
<a href="https://huggingface.co/blog/getting-started-with-embeddings">https://huggingface.co/blog/getting-started-with-embeddings</a>
</li>

<li>Lexical analysis (Wikipedia)<br />
<a href="https://en.wikipedia.org/wiki/Lexical_analysis#Token">https://en.wikipedia.org/wiki/Lexical_analysis#Token</a>
</li>

<li>Lexikální analýza (Wikipedia)<br />
<a href="https://cs.wikipedia.org/wiki/Lexik%C3%A1ln%C3%AD_anal%C3%BDza">https://cs.wikipedia.org/wiki/Lexik%C3%A1ln%C3%AD_anal%C3%BDza</a>
</li>

<li>Jazykový korpus<br />
<a href="https://cs.wikipedia.org/wiki/Jazykov%C3%BD_korpus">https://cs.wikipedia.org/wiki/Jazykov%C3%BD_korpus</a>
</li>

<li>AP8, IN8 Regulární jazyky<br />
<a href="http://statnice.dqd.cz/home:inf:ap8">http://statnice.dqd.cz/home:inf:ap8</a>
</li>

<li>AP9, IN9 Konečné automaty<br />
<a href="http://statnice.dqd.cz/home:inf:ap9">http://statnice.dqd.cz/home:inf:ap9</a>
</li>

<li>AP10, IN10 Bezkontextové jazyky<br />
<a href="http://statnice.dqd.cz/home:inf:ap10">http://statnice.dqd.cz/home:inf:ap10</a>
</li>

<li>AP11, IN11 Zásobníkové automaty, Syntaktická analýza<br />
<a href="http://statnice.dqd.cz/home:inf:ap11">http://statnice.dqd.cz/home:inf:ap11</a>
</li>

<li>PaLM 2 (Pathways Language Model)<br />
<a href="https://ai.google/discover/palm2/">https://ai.google/discover/palm2/</a>
</li>

<li>Gemini<br />
<a href="https://deepmind.google/technologies/gemini/#introduction">https://deepmind.google/technologies/gemini/#introduction</a>
</li>

<li>LLaMA (Large Language Model Meta AI)<br />
<a href="https://en.wikipedia.org/wiki/LLaMA">https://en.wikipedia.org/wiki/LLaMA</a>
</li>

<li>GPT-4<br />
<a href="https://openai.com/gpt-4">https://openai.com/gpt-4</a>
</li>

</ol>



<p></p><p></p>
<p><small>Autor: <a href="http://www.fit.vutbr.cz/~tisnovpa">Pavel Tišnovský</a> &nbsp; 2024</small></p>
</body>
</html>

