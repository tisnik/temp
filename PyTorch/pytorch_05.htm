<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
        "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html>
<head>
<title>Realizace neuronových sítí s využitím knihovny PyTorch (2. část)</title>
<meta name="Author" content="Pavel Tisnovsky" />
<meta name="Generator" content="vim" />
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<style type="text/css">
         body {color:#000000; background:#ffffff;}
         h1  {font-family: arial, helvetica, sans-serif; color:#ffffff; background-color:#c00000; text-align:center; padding-left:1em}
         h2  {font-family: arial, helvetica, sans-serif; color:#ffffff; background-color:#0000c0; padding-left:1em; text-align:left}
         h3  {font-family: arial, helvetica, sans-serif; color:#000000; background-color:#c0c0c0; padding-left:1em; text-align:left}
         h4  {font-family: arial, helvetica, sans-serif; color:#000000; background-color:#e0e0e0; padding-left:1em; text-align:left}
         a   {font-family: arial, helvetica, sans-serif;}
         li  {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify; width:450px;}
         ol  {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify; width:450px;}
         ul  {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify; width:450px;}
         p   {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify;}
         pre {background:#e0e0e0}
</style>
</head>

<body>

<h1>Realizace neuronových sítí s využitím knihovny PyTorch (2. část)</h1>

<h3>Pavel Tišnovský</h3>

<p></p>

<h1>Úvodník</h1>

<p>Dnešní článek o práci s neuronovými sítěmi v knihovně PyTorch je zaměřen prakticky. Ukážeme si v něm tvorbu a konfiguraci neuronové sítě, natrénování této sítě, její otestování, vizualizaci získaných výsledků atd. Taktéž se zmíníme o problému přeučení a nedoučení. </p>



<h2>Obsah</h2>

<p><a href="#k01">1. Deklarace třídy představující neuronovou síť</a></p>
<p><a href="#k02">2. Definice vrstev neuronové sítě</a></p>
<p><a href="#k03">3. Pomocný modul pro přípravu trénovacích a testovacích dat</a></p>
<p><a href="#k04">4. Trénink neuronové sítě</a></p>
<p><a href="#k05">5. Třetí demonstrační příklad: trénink neuronové sítě</a></p>
<p><a href="#k06">6. Vizualizace výsledků tréninku</a></p>
<p><a href="#k07">7. Čtvrtý demonstrační příklad: trénink neuronové sítě se zobrazením kvality tréninku</a></p>
<p><a href="#k08">8. Přidání dalších vrstev do neuronové sítě</a></p>
<p><a href="#k09">9. Pátý demonstrační příklad: neuronová síť s&nbsp;jednou skrytou vrstvou</a></p>
<p><a href="#k10">10. Riziko nedoučení složitější neuronové sítě</a></p>
<p><a href="#k11">11. Šestý demonstrační příklad: neuronová síť s&nbsp;více skrytými vrstvami, která nebude dotrénována</a></p>
<p><a href="#k12">12. Sedmý demonstrační příklad: vliv parametru <strong>learning_rate</strong> na rychlosti naučení sítě</a></p>
<p><a href="#k13">13. Porovnání účelové funkce všech tří doposud použitých neuronových sítí</a></p>
<p><a href="#k14">14. Ověření kvality neuronové sítě</a></p>
<p><a href="#k15">15. Osmý demonstrační příklad: výpočet kvality neuronové sítě</a></p>
<p><a href="#k16">16. Vizualizace predikce neuronové sítě: body patřící do první nebo druhé skupiny</a></p>
<p><a href="#k17">17. Devátý demonstrační příklad: vizualizace predikce neuronové sítě</a></p>
<p><a href="#k18">18. Výsledky získané devátým demonstračním příkladem</a></p>
<p><a href="#k19">19. Repositář s&nbsp;demonstračními příklady</a></p>
<p><a href="#k20">20. Odkazy na Internetu</a></p>



<p><a name="k01"></a></p>
<h2 id="k01">1. Deklarace třídy představující neuronovou síť</h2>

<p>Dnešní článek je, částečně na rozdíl od <a
href="https://www.root.cz/clanky/realizace-neuronovych-siti-s-vyuzitim-knihovny-pytorch/">článku
předchozího</a>, zaměřen prakticky. Ukážeme si v&nbsp;něm tvorbu a konfiguraci
neuronové sítě, natrénování této sítě, její otestování, vizualizaci výsledků
atd. Taktéž se zmíníme o problému přeučení a nedoučení. Celý postup si ukážeme
krok za krokem a z&nbsp;tohoto důvodu je dnešní první příklad triviální. Je
v&nbsp;něm ukázáno, jakým způsobem se s&nbsp;využitím knihovny PyTorch
deklaruje třída představující neuronovou síť. V&nbsp;té nejjednodušší podobě
postačuje nadeklarovat novou třídu odvozenou od třídy
<strong>torch.nn.Module</strong>:</p>

<pre>
from torch import nn
&nbsp;
&nbsp;
class <strong>NeuralNetwork</strong>(nn.Module):
    <i>"""Třída reprezentující neuronovou síť."""</i>
&nbsp;
    def <strong>__init__</strong>(self):
        super().__init__()
&nbsp;
&nbsp;
<i># konstrukce neuronové sítě</i>
nn1 = NeuralNetwork()
&nbsp;
<i># výpis základních informací o neuronové síti</i>
print(nn1)
</pre>

<p>Po spuštění tohoto skriptu se zobrazí základní informace o této síti:</p>

<pre>
NeuralNetwork()
</pre>

<p><div class="rs-tip-major">Poznámka: ve skutečnosti se prozatím o skutečnou
síť nejedná; viz další text.</div></p>



<p><a name="k02"></a></p>
<h2 id="k02">2. Definice vrstev neuronové sítě</h2>

<p>Minule jsme si řekli, že umělé neuronové sítě se typicky skládají
z&nbsp;vrstev neuronů. Velmi jednoduchá síť může vypadat následovně:</p>

<img src="https://i.iinfo.cz/images/329/torch-nn1-5.png" class="image-312265" alt="&#160;" width="600" height="400" />
<p><i>Obrázek 1: Uspořádání neuronů do vrstev ve feed-forward síti.</i></p>

<p>V&nbsp;PyTorchi je přidání nové vrstvy (resp.&nbsp;přesněji řečeno propojení
mezi vrstvami) relativně snadné. Ukažme si tu nejjednodušší konfiguraci sítě,
tj.&nbsp;síť, která bude mít <strong>input_dim</strong> vstupů a
<strong>output_dim</strong> výstupů. Mezi vstupy a výstupy se provádí výpočet
běžné afinní (lineární) transformace a v&nbsp;metodě <strong>forward</strong>
aplikace aktivační funkce. Tím je vlastně realizována funkce umělého neuronu
&ndash; váhování vstupů a přičtení biasu. Interně je tato část realizována
maticí <strong>input_dim&times;output_dim</strong> vah a vektorem s&nbsp;biasy.
Výpočet s&nbsp;využitím aktivační funkce je deklarován v&nbsp;metodě
<strong>forward</strong>. Tento výpočet je prováděn se všemi vstupy a výsledek
je poslán na výstup neuronové sítě:</p>

<pre>
from torch import nn
&nbsp;
&nbsp;
class <strong>NeuralNetwork</strong>(nn.Module):
    <i>"""Třída reprezentující neuronovou síť."""</i>
&nbsp;
    def <strong>__init__</strong>(self, input_dim, output_dim):
        super().__init__()
        <i># vrstvy neuronové sítě</i>
        self.layer_1 = nn.Linear(input_dim, output_dim)

&nbsp;
    def <strong>forward</strong>(self, x):
        <i># propagace hodnot přes neuronovou síť</i>
        x = nn.functional.sigmoid(self.layer_1(x))
        return x
&nbsp;
&nbsp;
<i># konfigurace vrstev neuronové sítě</i>
input_dim = 2
output_dim = 1
&nbsp;
<i># konstrukce neuronové sítě</i>
nn2 = NeuralNetwork(input_dim, output_dim)
&nbsp;
<i># výpis základních informací o neuronové síti</i>
print(nn2)
&nbsp;
<i># výpis informace o vrstvě neuronové sítě</i>
print(nn2.layer_1)
&nbsp;
<i># výpis informace o transformační matici a biasech</i>
print(nn2.layer_1.weight)
print(nn2.layer_1.bias)
</pre>

<p>Po spuštění se nejdříve vypíše informace o celé neuronové síti:</p>

<pre>
NeuralNetwork(
  (layer_1): Linear(in_features=2, out_features=1, bias=True)
)
</pre>

<p>Dále se vypíše informace o její jediné vrstvě:</p>

<pre>
Linear(in_features=2, out_features=1, bias=True)
</pre>

<p>A na závěr obsah transformační matice (2&times;1) a vektor biasů (má jediný
bias). Ve výchozím nastavení jsou tyto hodnoty náhodné:</p>

<pre>
Parameter containing:
tensor([[0.2180, 0.6056]], requires_grad=True)
&nbsp;
Parameter containing:
tensor([0.2079], requires_grad=True)
</pre>

<p>Tato síť tedy pro vstupy [x1, x2] provádí výpočet:</p>

<pre>
y = x1 * 0,2180 + x2 * 0,6056 + 0,2079
</pre>

<p>Což je triviální lineární transformace z&nbsp;roviny na přímku.</p>

<p><div class="rs-tip-major">Poznámka: jak přesně pracuje funkce
<strong>nn.Linear</strong> si ukážeme příště.</div></p>



<p><a name="k03"></a></p>
<h2 id="k03">3. Pomocný modul pro přípravu trénovacích a testovacích dat</h2>

<p>Pro neuronové sítě, které budeme tvořit v&nbsp;rámci dalších kapitol, je
nutné připravit trénovací a testovací data. Základy jsme si již ukázali minule.
Připomeňme si, že jsme k&nbsp;tomuto účelu použili funkci
<strong>sklearn.datasets.make_circles</strong> určenou pro vygenerování sady
bodů v&nbsp;rovině, přičemž každý bod patří buď do vnější nebo vnitřní kružnice
(každý bod má nastaven příslušný <i>label</i>). Následně se sada bodů rozdělí
na trénovací data a testovací data &ndash; tyto skupiny bodů se nepřekrývají a
jsou ze vstupu vybrány náhodně.</p>

<a href="https://www.root.cz/obrazek/1187583/"><img src="https://i.iinfo.cz/images/654/pytorch-nn-1-12-prev.png" class="image-1187583" width="370" height="185" data-prev-filename="https://i.iinfo.cz/images/654/pytorch-nn-1-12-prev.png" data-prev-filename-webp="https://i.iinfo.cz/images/654/pytorch-nn-1-12-prev.webp" data-prev-width="370" data-prev-height="185" data-large-filename="https://i.iinfo.cz/images/654/pytorch-nn-1-12-large.png" data-large-filename-webp="https://i.iinfo.cz/images/654/pytorch-nn-1-12-large.webp" data-large-width="720" data-large-height="360" alt="pytorch-nn-1" data-description="&#160;" title="Autor: tisnik, podle licence: &lt;a href=&quot;http://en.wikipedia.org/wiki/Rights_Managed&quot;&gt;Rights Managed&lt;/a&gt;" /></a>
<p><i>Obrázek 2: Trénovací a testovací data získaná rozdělením bodů získaných

funkcí <strong>make_circles</strong>.</i></p>
<p>Skript byl upraven do takové podoby, aby trénovací a testovací data vracel
z&nbsp;funkce <strong>compute_train_and_test_data</strong>, což nám později
usnadní použití tohoto modulu:</p>

<pre>
from sklearn.datasets import make_circles
from sklearn.model_selection import train_test_split
import torch
import numpy as np
from torch.utils.data import Dataset, DataLoader
&nbsp;
&nbsp;
<i># konverze původních dat z NumPy do tenzorů</i>
class <strong>Data</strong>(Dataset):
    def <strong>__init__</strong>(self, X, y):
        self.X = torch.from_numpy(X.astype(np.float32))
        self.y = torch.from_numpy(y.astype(np.float32))
        self.len = self.X.shape[0]
&nbsp;
    def <strong>__getitem__</strong>(self, index):
        return self.X[index], self.y[index]
&nbsp;
    def <strong>__len__</strong>(self):
        return self.len
&nbsp;
&nbsp;
def <strong>compute_train_and_test_data</strong>(
    n_samples=2000, factor=0.5, noise=0.05, test_size=1 / 3
):
    samples, labels = make_circles(n_samples=n_samples, factor=factor, noise=noise)
&nbsp;
    <i># rozdělení na trénovací a testovací množinu</i>
    X_train, X_test, y_train, y_test = train_test_split(
        samples, labels, test_size=test_size, random_state=26
    )
&nbsp;
    <i># trénovací a testovací sada</i>
&nbsp;
    <i># trénovací sada</i>
    train_data = Data(X_train, y_train)
&nbsp;
    <i># testovací sada</i>
    test_data = Data(X_test, y_test)
&nbsp;
    return train_data, test_data
</pre>

<p>Použití tohoto modulu je snadné:</p>

<pre>
<i># trénovací a testovací data</i>
from compute_train_and_test_data import compute_train_and_test_data
&nbsp;
train_data, test_data = <strong>compute_train_and_test_data()</strong>
&nbsp;
print(train_data.X)
print(train_data.y)
&nbsp;
print()
print(test_data.X)
print(test_data.y)
</pre>

<p>Po spuštění se vypíše matice <strong>X</strong> a vektor <strong>y</strong>
pro trénovací i testovací data:</p>

<pre>
tensor([[ 0.4599,  0.0053],
        [ 0.7305,  0.6499],
        [ 0.2827,  0.4137],
        ...,
        [-0.4188, -0.3083],
        [ 0.0909, -0.5549],
        [-0.4897,  0.2897]])
tensor([1., 0., 1.,  ..., 1., 1., 1.])
&nbsp;
tensor([[-0.2496,  0.9122],
        [ 0.4308,  0.2262],
        [ 0.5084,  0.2717],
        ...,
        [ 0.4003,  0.3459],
        [-0.3190, -0.9857],
        [-0.3120,  0.3337]])
tensor([0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,
        ...
        ...
        ...
</pre>



<p><a name="k04"></a></p>
<h2 id="k04">4. Trénink neuronové sítě</h2>

<p>Nejzajímavější je vlastní proces tréninku (učení) neuronové sítě. Ten může
probíhat několika způsoby, ovšem nejčastější je učení založené na tom, že na
vstup sítě přivedeme data, u nichž dopředu známe očekávaný výsledek (takzvaná
trénovací data). Neuronová síť pro tato vstupní data provede svůj odhad
(z&nbsp;počátku náhodný, v&nbsp;dalším kroku velmi špatný, potom lepší atd.) a
na základě rozdílů mezi odhadem sítě a očekávaným výsledkem se více či méně
sofistikovanými algoritmy nepatrně pozmění váhy <strong>w<sub>i</sub></strong>
na vstupech do neuronů (včetně biasu, tedy <strong>w<sub>0</sub></strong>).</p>

<img src="https://i.iinfo.cz/images/329/torch-nn1-2.png" class="image-312262" alt="&#160;" width="465" height="145" />
<p><i>Obrázek 3: Idealizovaný model neuronu s&nbsp;biasem. V&nbsp;průběhu
tréninku se upravují váhy <strong>w<sub>i</sub></strong>.</i></p>

<p>Konkrétní míra změn váhy na vstupech neuronů v&nbsp;jednotlivých vrstvách
sítě je globálně řízena dalším parametrem či několika parametry, z&nbsp;nichž
ten nejdůležitější ovlivňuje rychlost učení. Ta by neměla být příliš nízká (to
ovšem vyžaduje objemná trénovací data nebo jejich opakování), ale ani příliš
vysoká, protože by síť mohla &bdquo;oscilovat&ldquo; mezi několika
neoptimálními stavy. Základní algoritmus učení neuronové sítě se jmenuje
<i>backpropagation</i>, protože se váhy skutečně mění v&nbsp;opačném směru
&ndash; od výstupů (na něž se přivede vypočtená chyba) ke vstupům. Asi nejlépe
je tento koncept popsán v&nbsp;článku dostupném na adrese <a
href="https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/">https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/</a>,
tuto část za nás však vykoná knihovna <i>PyTorch</i> ve fázi učení zcela
automaticky.</p>



<p><a name="k05"></a></p>
<h2 id="k05">5. Třetí demonstrační příklad: trénink neuronové sítě</h2>

<p>Trénink neuronové sítě je ukázán v&nbsp;dnešním třetím demonstračním
příkladu. Ten je rozdělen do čtyř částí. První část již známe &ndash; ta slouží
pro konstrukci neuronové sítě, která je v&nbsp;našem případě reprezentována
objektem typu <strong>NeuralNetwork</strong>. Ve druhé části získáme trénovací
data (prozatím skutečně jen trénovací data) z&nbsp;modulu
<strong>compute_train_and_test_data</strong> popsaného <a href="#k03">ve třetí
kapitole</a>. V&nbsp;části třetí je provedeno vlastní učení (trénink) neuronové
sítě s&nbsp;využitím trénovacích dat. A konečně v&nbsp;části čtvrté si necháme
vypsat hodnoty vypočtené takzvanou <i>účelovou funkcí</i> (<i>loss
function</i>), jejíž hodnoty reprezentují rozdíly mezi očekávaným výsledkem
(ten při tréninku pochopitelně musíme znát) a odhadem neuronové sítě.
V&nbsp;ideálním případě by měly tyto hodnoty postupně klesat, protože síť by
měla v&nbsp;procesu učení zlepšovat své odhady:</p>

<pre>
from torch import nn
from torch import optim
from torch.utils.data import DataLoader
&nbsp;
&nbsp;
class <strong>NeuralNetwork</strong>(nn.Module):
    <i>"""Třída reprezentující neuronovou síť."""</i>
&nbsp;
    def <strong>__init__</strong>(self, input_dim, output_dim):
        super().__init__()
        <i># vrstvy neuronové sítě</i>
        self.layer_1 = nn.Linear(input_dim, output_dim)
&nbsp;
    def <strong>forward</strong>(self, x):
        <i># propagace hodnot přes neuronvou síť</i>
        x = nn.functional.sigmoid(self.layer_1(x))
        return x
&nbsp;
&nbsp;
<i># konfigurace vrstev neuronové sítě</i>
input_dim = 2
output_dim = 1
&nbsp;
<i># konstrukce neuronové sítě</i>
nn3 = NeuralNetwork(input_dim, output_dim)
&nbsp;
<i># výpis základních informací o neuronové síti</i>
print(nn3)
&nbsp;
&nbsp;
<i># příprava na trénink neuronové sítě</i>
learning_rate = 0.1
loss_fn = nn.BCELoss()
&nbsp;
optimizer = optim.SGD(nn3.parameters(), lr=learning_rate)
&nbsp;
<i># trénovací data</i>
from compute_train_and_test_data import compute_train_and_test_data
&nbsp;
train_data, _ = compute_train_and_test_data()
&nbsp;
<i># zpracovat trénovací data</i>
batch_size = 64
train_dataloader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)
print("Batches: ", len(train_dataloader))
&nbsp;
<i># vlastní trénink</i>
print("Training started")
num_epochs = 100
loss_values = []
for epoch in range(num_epochs):
    print(f"    Epoch {epoch}: ", end="")
    last_lost_value = None
    for X, y in train_dataloader:
        optimizer.zero_grad()
&nbsp;
        <i># dopředný tok + zpětný tok + optimalizace</i>
        pred = nn3(X)
&nbsp;
        <i># výpočet účelové funkce</i>
        loss = loss_fn(pred, y.unsqueeze(-1))
        loss_values.append(loss.item())
        loss.backward()
        optimizer.step()
        last_lost_value = loss.item()
        print(".", end="")
    print(last_lost_value)
&nbsp;
print("Training completed")
&nbsp;
print("Loss values")
for i, loss_value in enumerate(loss_values):
    print(i, loss_value)
</pre>

<p><div class="rs-tip-major">Poznámka: přesné významy všech prováděných kroků
si podrobněji vysvětlíme příště. Nyní je nutné pouze znát celkovou strukturu
skriptu.</div></p>

<p>Výsledky ukazují jednotlivé prováděné kroky:</p>

<p>Konstrukce neuronové sítě:</p>

<pre>
NeuralNetwork(
  (layer_1): Linear(in_features=2, out_features=1, bias=True)
)
</pre>

<p>Rozdělená trénovací data:</p>

<pre>
Batches:  21
</pre>

<p>Trénink neuronové sítě:</p>

<pre>
Training started
    Epoch 0: .....................0.7136642932891846
    Epoch 1: .....................0.7086359858512878
    Epoch 2: .....................0.7138426899909973
    Epoch 3: .....................0.71360182762146
    Epoch 4: .....................0.7031393051147461
    Epoch 5: .....................0.6998087763786316
    Epoch 6: .....................0.685001790523529
    Epoch 7: .....................0.701408326625824
    Epoch 8: .....................0.6959707140922546
    Epoch 9: .....................0.6810523271560669
    Epoch 10: .....................0.6941555738449097
    ...
    ...
    ...
    Epoch 87: .....................0.6884103417396545
    Epoch 88: .....................0.698021650314331
    Epoch 89: .....................0.6896729469299316
    Epoch 90: .....................0.691299557685852
    Epoch 91: .....................0.6931966543197632
    Epoch 92: .....................0.6909589171409607
    Epoch 93: .....................0.6942797899246216
    Epoch 94: .....................0.6978365182876587
    Epoch 95: .....................0.6871546506881714
    Epoch 96: .....................0.6888548135757446
    Epoch 97: .....................0.6873869299888611
    Epoch 98: .....................0.6864196062088013
    Epoch 99: .....................0.6999951601028442
Training completed
</pre>

<p>Hodnoty účelové funkce v&nbsp;průběhu tréninku:</p>

<pre>
Loss values
0 0.7395060062408447
1 0.7825191020965576
2 0.7868967652320862
3 0.786351203918457
4 0.7562863826751709
5 0.7235647439956665
6 0.742216944694519
7 0.6739267110824585
8 0.7016011476516724
9 0.7245702147483826
10 0.664133608341217
...
...
...
82 0.697323203086853
83 0.6908646821975708
84 0.6865593791007996
85 0.6937276124954224
86 0.7040309309959412
87 0.6995502710342407
88 0.7057774066925049
</pre>



<p><a name="k06"></a></p>
<h2 id="k06">6. Vizualizace výsledků tréninku</h2>

<p>Tabulka s&nbsp;hodnotami účelové funkce nám může prozradit, jak rychle (a
zda vůbec) se síť dokáže učit. Počáteční hodnota této funkce může být vysoká
(resp.&nbsp;přesněji řečeno blízká jedničce), protože na začátku učení jsou
váhy jednotlivých neuronů nastaveny na náhodnou hodnotu. Ovšem v&nbsp;průběhu
tréninku se na vstup sítě zapisují trénovací hodnoty, na výstupu jsou zjištěny
rozdíly mezi očekávaným výsledkem a výsledkem, který poskytne síť a podle
rozdílu těchto hodnot jsou postupně (od poslední vrstvy) váhy neuronů
upravovány tak, aby byl další výsledek (v&nbsp;ideálním případě) lepší.</p>

<p>Neuronová síť by se tedy měla postupně zlepšovat a hodnota účelové funkce by
měla postupně klesat směrem k&nbsp;nule. Je zde mnoho proměnných, které tento
postup ovlivňují a postupně se s&nbsp;nimi seznámíme v&nbsp;rámci dalšího
textu. Základem ovšem bude vizualizace účelové funkce, tj.&nbsp;změny její
hodnoty v&nbsp;čase. Pokud nebude hodnota klesat ani po několika stech či
tisících kroků, bude nutné změnit parametry sítě (počet vrstev, počet neuronů,
typ aktivačních funkcí atd.).</p>



<p><a name="k07"></a></p>
<h2 id="k07">7. Čtvrtý demonstrační příklad: trénink neuronové sítě se zobrazením kvality tréninku</h2>

<p>Čtvrtý demonstrační příklad se do značné míry podobá příkladu třetímu, ovšem
s&nbsp;tím rozdílem, že výsledky průběhu tréninku jsou zobrazeny formou grafu
(poslední, pátá část skriptu). Pro vizualizaci opět použijeme, podobně jako
v&nbsp;předchozím článku, knihovnu Matplotlib. Povšimněte si, že Matplotlib
dokáže pracovat s&nbsp;n-dimenzionálními poli knihovny NumPy, ale nikoli už
z&nbsp;tenzory knihovny PyTorch:</p>

<pre>
from torch import nn
from torch import optim
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import numpy as np
&nbsp;
&nbsp;
class <strong>NeuralNetwork</strong>(nn.Module):
    <strong>"""Třída reprezentující neuronovou síť."""</strong>
&nbsp;
    def <strong>__init__</strong>(self, input_dim, output_dim):
        super().__init__()
        <i># vrstvy neuronové sítě</i>
        self.layer_1 = nn.Linear(input_dim, output_dim)
&nbsp;
    def <strong>forward</strong>(self, x):
        <i># propagace hodnot přes neuronvou síť</i>
        x = nn.functional.sigmoid(self.layer_1(x))
        return x
&nbsp;
&nbsp;
<i># konfigurace vrstev neuronové sítě</i>
input_dim = 2
output_dim = 1
&nbsp;
<i># konstrukce neuronové sítě</i>
nn4 = NeuralNetwork(input_dim, output_dim)
&nbsp;
<i># výpis základních informací o neuronové síti</i>
print(nn4)
&nbsp;
&nbsp;
<i># příprava na trénink neuronové sítě</i>
learning_rate = 0.1
loss_fn = nn.BCELoss()
&nbsp;
optimizer = optim.SGD(nn4.parameters(), lr=learning_rate)
&nbsp;
<i># trénovací data</i>
from compute_train_and_test_data import compute_train_and_test_data
&nbsp;
train_data, _ = compute_train_and_test_data()
&nbsp;
<i># zpracovat trénovací data</i>
batch_size = 64
train_dataloader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)
print("Batches: ", len(train_dataloader))
&nbsp;
<i># vlastní trénink</i>
print("Training started")
num_epochs = 100
loss_values = []
for epoch in range(num_epochs):
    print(f"    Epoch {epoch}: ", end="")
    last_lost_value = None
    for X, y in train_dataloader:
        optimizer.zero_grad()
&nbsp;
        <i># dopředný tok + zpětný tok + optimalizace</i>
        pred = nn4(X)
&nbsp;
        <i># výpočet účelové funkce</i>
        loss = loss_fn(pred, y.unsqueeze(-1))
        loss_values.append(loss.item())
        loss.backward()
        optimizer.step()
        last_lost_value = loss.item()
        print(".", end="")
    print(last_lost_value)
&nbsp;
print("Training completed")
&nbsp;
step = range(len(loss_values))
&nbsp;
<i># příprava na vykreslení grafu</i>
fig, ax = plt.subplots(figsize=(6.4, 4.8))
plt.plot(step, np.array(loss_values))
plt.title("Průběh tréninku neuronové sítě")
plt.xlabel("Epocha")
plt.ylabel("Účelová funkce")
&nbsp;
<i># uložení do souboru</i>
plt.savefig("nn_4.png")
&nbsp;
<i># vykreslení grafu</i>
plt.show()
</pre>

<p>Ze zobrazeného grafu je patrné, že naše neuronová síť je velmi
<i>mizerná</i> a vlastně se nedokáže skutečně natrénovat (protože vstupní data
mají složité interní vztahy, složitější, než lze naučit dva neurony):</p>

<img src="https://i.iinfo.cz/images/37/pytorch-nn-2-1.png" class="image-1188087" width="640" height="480" alt="PyTorch NN" data-description="&#160;" title="Autor: tisnik, podle licence: &lt;a href=&quot;http://en.wikipedia.org/wiki/Rights_Managed&quot;&gt;Rights Managed&lt;/a&gt;" />
<p><i>Obrázek 4: Průběh učení neuronové sítě bez skryté vrstvy.</i></p>



<p><a name="k08"></a></p>
<h2 id="k08">8. Přidání dalších vrstev do neuronové sítě</h2>

<p>Mohlo by se zdát, že větší neuronová síť (tedy více neuronů) může znamenat,
že dostaneme i kvalitnější model a tím pádem i lepší výsledky. Ovšem
v&nbsp;praxi můžeme narazit na úplný opak, a to ve chvíli, kdy se neuronová síť
nedokáže správně zaučit &ndash; datová sada je buď příliš malá, nebo je neuronů
tolik, že <i>gradient</i> při změně jejich vah je příliš malý nebo dokonce
nulový. Ovšem zaměřme se nejdříve na případ, kdy vyšší počet vrstev a neuronů
v&nbsp;těchto vrstvách má dobrý vliv na kvalitu modelu. Do nové verze naší
neuronové sítě přidáme další vrstvu. Již se nebude jednat o vrstvu vstupní ani
výstupní, ale o takzvanou skrytou vrstvu:</p>

<pre>
self.layer_1 = nn.Linear(input_dim, hidden_dim)
self.layer_2 = nn.Linear(hidden_dim, output_dim)
</pre>

<p>Konfigurace vrstev:</p>

<pre>
input_dim = 2
hidden_dim = 10
output_dim = 1
</pre>

<p>Na vstupu neuronové sítě jsou dva vstupy, což odpovídá dvojicím souřadnic
bodů [x, y].  Výsledkem (výstupem) je jediná hodnota, která bude naznačovat, do
které skupiny body patří. A skrytá vrstva bude mít deset neuronů. To je již
poměrně komplikovaná síť 2&rarr;10&rarr;1 neurony, která by mohla mít lepší
učící schopnosti, než první verze sítě.</p>

<p>Mimochodem: pochopitelně se změní i výpočet prováděný neuronovou sítí. Ten
nyní může vypadat takto (v&nbsp;každé vrstvě použijeme odlišné aktivační
funkce):</p>

<pre>
def <strong>forward</strong>(self, x):
    <i># propagace hodnot přes neuronvou síť</i>
    x = torch.nn.functional.relu(self.layer_1(x))
    x = torch.nn.functional.sigmoid(self.layer_2(x))
    return x
</pre>



<p><a name="k09"></a></p>
<h2 id="k09">9. Pátý demonstrační příklad: neuronová síť s&nbsp;jednou skrytou vrstvou</h2>

<p>V&nbsp;pátém demonstračním příkladu použijeme pro trénink a vyhodnocení
neuronové sítě síť s&nbsp;jednou skrytou vrstvou. Konfigurace sítě přesně
odpovídá popisu <a href="#k08">z&nbsp;osmé kapitoly</a>:</p>

<pre>
import torch
from torch import nn
from torch import optim
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import numpy as np
&nbsp;
&nbsp;
class <strong>NeuralNetwork</strong>(nn.Module):
    <i>"""Třída reprezentující neuronovou síť."""</i>
&nbsp;
    def <strong>__init__</strong>(self, input_dim, hidden_dim, output_dim):
        super().__init__()
        <i># vrstvy neuronové sítě</i>
        self.layer_1 = nn.Linear(input_dim, hidden_dim)
        self.layer_2 = nn.Linear(hidden_dim, output_dim)
&nbsp;
    def <strong>forward</strong>(self, x):
        <i># propagace hodnot přes neuronvou síť</i>
        x = torch.nn.functional.relu(self.layer_1(x))
        x = torch.nn.functional.sigmoid(self.layer_2(x))
        return x
&nbsp;
&nbsp;
<i># konfigurace vrstev neuronové sítě</i>
input_dim = 2
hidden_dim = 10
output_dim = 1
&nbsp;
<i># konstrukce neuronové sítě</i>
nn5 = NeuralNetwork(input_dim, hidden_dim, output_dim)
&nbsp;
<i># výpis základních informací o neuronové síti</i>
print(nn5)
&nbsp;
&nbsp;
<i># příprava na trénink neuronové sítě</i>
learning_rate = 0.1
loss_fn = nn.BCELoss()
&nbsp;
optimizer = optim.SGD(nn5.parameters(), lr=learning_rate)
&nbsp;
<i># trénovací data</i>
from compute_train_and_test_data import compute_train_and_test_data
&nbsp;
train_data, _ = compute_train_and_test_data()
&nbsp;
<i># zpracovat trénovací data</i>
batch_size = 64
train_dataloader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)
print("Batches: ", len(train_dataloader))
&nbsp;
<i># vlastní trénink</i>
print("Training started")
num_epochs = 100
loss_values = []
for epoch in range(num_epochs):
    print(f"    Epoch {epoch}: ", end="")
    last_lost_value = None
    for X, y in train_dataloader:
        optimizer.zero_grad()
&nbsp;
        <i># dopředný tok + zpětný tok + optimalizace</i>
        pred = nn5(X)
&nbsp;
        <i># výpočet účelové funkce</i>
        loss = loss_fn(pred, y.unsqueeze(-1))
        loss_values.append(loss.item())
        loss.backward()
        optimizer.step()
        last_lost_value = loss.item()
        print(".", end="")
    print(last_lost_value)
&nbsp;
print("Training completed")
&nbsp;
step = range(len(loss_values))
&nbsp;
<i># příprava na vykreslení grafu</i>
fig, ax = plt.subplots(figsize=(6.4, 4.8))
plt.plot(step, np.array(loss_values))
plt.title("Průběh tréninku neuronové sítě")
plt.xlabel("Epocha")
plt.ylabel("Účelová funkce")
&nbsp;
<i># uložení do souboru</i>
plt.savefig("nn_5.png")
&nbsp;
<i># vykreslení grafu</i>
plt.show()
</pre>

<img src="https://i.iinfo.cz/images/37/pytorch-nn-2-2.png" class="image-1188090" width="640" height="480" alt="PyTorch NN" data-description="&#160;" title="Autor: tisnik, podle licence: &lt;a href=&quot;http://en.wikipedia.org/wiki/Rights_Managed&quot;&gt;Rights Managed&lt;/a&gt;" />
<p><i>Obrázek 5: Průběh učení neuronové sítě s&nbsp;jednou skrytou
vrstvou.</i></p>



<p><a name="k10"></a></p>
<h2 id="k10">10. Riziko nedoučení složitější neuronové sítě</h2>

<p>Při porovnání průběhů účelové funkce pro neuronovou síť bez skryté vrstvy a
pro sít s&nbsp;jednou vrstvou jsou výsledky zřejmé: první síť se nedokáže učit,
kdežto druhá ano:</p>

<img src="https://i.iinfo.cz/images/37/pytorch-nn-2-3.png" class="image-1188093" width="640" height="480" alt="PyTorch NN" data-description="&#160;" title="Autor: tisnik, podle licence: &lt;a href=&quot;http://en.wikipedia.org/wiki/Rights_Managed&quot;&gt;Rights Managed&lt;/a&gt;" />
<p><i>Obrázek 6: Průběh učení neuronové sítě bez skryté vrstvy a sítě
s&nbsp;jednou skrytou vrstvou.</i></p>

<p>Z&nbsp;toho by mohl plynout chybný závěr, že čím více vrstev (a čím více
neuronů) bude neuronová síť obsahovat, tím bude kvalitnější a bude se i lépe
učit. V&nbsp;praxi tomu tak není, protože se zde projevuje další proměnná
&ndash; <i>learning rate</i>. Tu potřebujeme mít dostatečně velkou na to, aby
se síť s&nbsp;každými dalšími trénovacími daty učila, ale na druhou stranu
dostatečně malou, aby nedocházelo k&nbsp;oscilacím. Navíc u rozsáhlých sítí
dochází k&nbsp;efektu nazvanému <i>vanishing gradient</i>, kterému se budeme
podrobněji věnovat příště. Ovšem závěr je (pro tuto chvíli) následující:
neuronová síť má být dostatečně rozsáhlá, aby se naučila řešit náš problém, ale
ne rozsáhlejší.</p>

<p>Mimochodem &ndash; obrázek 6 byl vypočten tímto skriptem, který v&nbsp;sobě
spojuje předchozí dva příklady (a několik zjednodušení):</p>

<pre>
import torch
from torch import nn
from torch import optim
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import numpy as np
&nbsp;
&nbsp;
class <strong>NeuralNetwork</strong>4(nn.Module):
    <i>"""Třída reprezentující neuronovou síť."""</i>
&nbsp;
    def <strong>__init__</strong>(self, input_dim, output_dim):
        super().__init__()
        <i># vrstvy neuronové sítě</i>
        self.layer_1 = nn.Linear(input_dim, output_dim)
&nbsp;
    def <strong>forward</strong>(self, x):
        <i># propagace hodnot přes neuronovou síť</i>
        x = nn.functional.sigmoid(self.layer_1(x))
        return x
&nbsp;
&nbsp;
class <strong>NeuralNetwork</strong>5(nn.Module):
    <i>"""Třída reprezentující neuronovou síť."""</i>
&nbsp;
    def <strong>__init__</strong>(self, input_dim, hidden_dim, output_dim):
        super().__init__()
        <i># vrstvy neuronové sítě</i>
        self.layer_1 = nn.Linear(input_dim, hidden_dim)
        self.layer_2 = nn.Linear(hidden_dim, output_dim)
&nbsp;
    def <strong>forward</strong>(self, x):
        <i># propagace hodnot přes neuronovou síť</i>
        x = torch.nn.functional.relu(self.layer_1(x))
        x = torch.nn.functional.sigmoid(self.layer_2(x))
        return x
&nbsp;
&nbsp;
<i># konstrukce dvou neuronových sítí</i>
nn4 = NeuralNetwork4(2, 1)
nn5 = NeuralNetwork5(2, 10, 1)
&nbsp;
<i># příprava na trénink neuronové sítě</i>
learning_rate = 0.1
loss_fn = nn.BCELoss()
&nbsp;
optimizer4 = optim.SGD(nn4.parameters(), lr=learning_rate)
optimizer5 = optim.SGD(nn5.parameters(), lr=learning_rate)
&nbsp;
<i># trénovací data</i>
from compute_train_and_test_data import compute_train_and_test_data
&nbsp;
train_data, _ = compute_train_and_test_data()
&nbsp;
<i># zpracovat trénovací data</i>
batch_size = 64
train_dataloader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)
print("Batches: ", len(train_dataloader))
&nbsp;
<i># vlastní trénink</i>
print("Training started")
num_epochs = 100
loss_values_4 = []
loss_values_5 = []
for epoch in range(num_epochs):
    print(f"    Epoch {epoch}: ", end="")
    for X, y in train_dataloader:
        optimizer4.zero_grad()
&nbsp;
        <i># dopředný tok + zpětný tok + optimalizace</i>
        pred = nn4(X)
&nbsp;
        <i># výpočet účelové funkce</i>
        loss = loss_fn(pred, y.unsqueeze(-1))
        loss_values_4.append(loss.item())
        loss.backward()
        optimizer4.step()
&nbsp;
        optimizer5.zero_grad()
&nbsp;
        <i># dopředný tok + zpětný tok + optimalizace</i>
        pred = nn5(X)
&nbsp;
        <i># výpočet účelové funkce</i>
        loss = loss_fn(pred, y.unsqueeze(-1))
        loss_values_5.append(loss.item())
        loss.backward()
        optimizer5.step()
        print(".", end="")
    print()
&nbsp;
print("Training completed")
&nbsp;
step = range(len(loss_values_4))
&nbsp;
<i># příprava na vykreslení grafu</i>
fig, ax = plt.subplots(figsize=(6.4, 4.8))
plt.plot(step, np.array(loss_values_4), label="nn4")
plt.plot(step, np.array(loss_values_5), label="nn5")
plt.title("Průběh tréninku neuronové sítě")
plt.xlabel("Epocha")
plt.ylabel("Účelová funkce")
plt.legend(loc="best")
&nbsp;
<i># uložení do souboru</i>
plt.savefig("nn_4_5.png")
&nbsp;
<i># vykreslení grafu</i>
plt.show()
</pre>



<p><a name="k11"></a></p>
<h2 id="k11">11. Šestý demonstrační příklad: neuronová síť s&nbsp;více skrytými vrstvami, která nebude dotrénována</h2>

<p>V&nbsp;dalším demonstračním příkladu je vytvořena sít s&nbsp;mnoha vrstvami,
které způsobí nedoučení sítě, protože máme málo trénovacích dat a především
hodnota <i>learning date</i> je nastavena velmi nízko:</p>

<pre>
    def __init__(self, input_dim, hidden_dim, output_dim):
        super().__init__()
        <i># vrstvy neuronové sítě</i>
        self.layer_1 = nn.Linear(input_dim, hidden_dim)
        self.layer_2 = nn.Linear(hidden_dim, hidden_dim)
        self.layer_3 = nn.Linear(hidden_dim, hidden_dim)
        self.layer_4 = nn.Linear(hidden_dim, output_dim)
</pre>

<p>Nastavení parametrů učení:</p>

<pre>
<i># příprava na trénink neuronové sítě</i>
learning_rate = 0.1
loss_fn = nn.BCELoss()
&nbsp;
optimizer = optim.SGD(nn6.parameters(), lr=learning_rate)
</pre>

<p>Tato mnohem komplikovanější síť bude ve skutečnosti prakticky stejně
nekvalitní, jako zcela první síť bez skrytých vrstev:</p>

<img src="https://i.iinfo.cz/images/37/pytorch-nn-2-4.png" class="image-1188096" width="640" height="480" alt="PyTorch NN" data-description="&#160;" title="Autor: tisnik, podle licence: &lt;a href=&quot;http://en.wikipedia.org/wiki/Rights_Managed&quot;&gt;Rights Managed&lt;/a&gt;" />
<p><i>Obrázek 7: Účelová funkce s&nbsp;větším množstvím skrytých vrstev, které
ve výsledku vedou k&nbsp;jejímu nedoučení.</i></p>

<p>Zdrojový kód tohoto příkladu vypadá následovně:</p>

<pre>
import torch
from torch import nn
from torch import optim
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import numpy as np
&nbsp;
&nbsp;
class <strong>NeuralNetwork</strong>(nn.Module):
    <i>"""Třída reprezentující neuronovou síť."""</i>
&nbsp;
    def __init__(self, input_dim, hidden_dim, output_dim):
        super().__init__()
        <i># vrstvy neuronové sítě</i>
        self.layer_1 = nn.Linear(input_dim, hidden_dim)
        self.layer_2 = nn.Linear(hidden_dim, hidden_dim)
        self.layer_3 = nn.Linear(hidden_dim, hidden_dim)
        self.layer_4 = nn.Linear(hidden_dim, output_dim)
&nbsp;
    def forward(self, x):
        <i># propagace hodnot přes neuronvou síť</i>
        x = torch.nn.functional.relu(self.layer_1(x))
        x = torch.nn.functional.sigmoid(self.layer_2(x))
        x = torch.nn.functional.sigmoid(self.layer_3(x))
        x = torch.nn.functional.sigmoid(self.layer_4(x))
        return x
&nbsp;
&nbsp;
<i># konfigurace vrstev neuronové sítě</i>
input_dim = 2
hidden_dim = 10
output_dim = 1
&nbsp;
<i># konstrukce neuronové sítě</i>
nn6 = NeuralNetwork(input_dim, hidden_dim, output_dim)
&nbsp;
<i># výpis základních informací o neuronové síti</i>
print(nn6)
&nbsp;
&nbsp;
<i># příprava na trénink neuronové sítě</i>
learning_rate = 0.1
loss_fn = nn.BCELoss()
&nbsp;
optimizer = optim.SGD(nn6.parameters(), lr=learning_rate)
&nbsp;
<i># trénovací data</i>
from compute_train_and_test_data import compute_train_and_test_data
&nbsp;
train_data, _ = compute_train_and_test_data()
&nbsp;
<i># zpracovat trénovací data</i>
batch_size = 64
train_dataloader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)
print("Batches: ", len(train_dataloader))
&nbsp;
<i># vlastní trénink</i>
print("Training started")
num_epochs = 100
loss_values = []
for epoch in range(num_epochs):
    print(f"    Epoch {epoch}: ", end="")
    last_lost_value = None
    for X, y in train_dataloader:
        optimizer.zero_grad()
&nbsp;
        <i># dopředný tok + zpětný tok + optimalizace</i>
        pred = nn6(X)
&nbsp;
        <i># výpočet účelové funkce</i>
        loss = loss_fn(pred, y.unsqueeze(-1))
        loss_values.append(loss.item())
        loss.backward()
        optimizer.step()
        last_lost_value = loss.item()
        print(".", end="")
    print(last_lost_value)
&nbsp;
print("Training completed")
&nbsp;
step = range(len(loss_values))
&nbsp;
<i># příprava na vykreslení grafu</i>
fig, ax = plt.subplots(figsize=(6.4, 4.8))
plt.plot(step, np.array(loss_values))
plt.title("Průběh tréninku neuronové sítě")
plt.xlabel("Epocha")
plt.ylabel("Účelová funkce")
&nbsp;
<i># uložení do souboru</i>
plt.savefig("nn_6.png")
&nbsp;
<i># vykreslení grafu</i>
plt.show()
</pre>



<p><a name="k12"></a></p>
<h2 id="k12">12. Sedmý demonstrační příklad: vliv parametru <strong>learning_rate</strong> na rychlosti naučení sítě</h2>

<p>Zkusme nyní zjistit, co se stane ve chvíli, kdy sice stále budeme používat
neuronovou síť s&nbsp;více skrytými vrstvami, ovšem zvýšíme hodnotu parametru
<i>learning rate</i>.</p>

<p>Konfigurace neuronové sítě:</p>

<pre>
    def __init__(self, input_dim, hidden_dim, output_dim):
        super().__init__()
        <i># vrstvy neuronové sítě</i>
        self.layer_1 = nn.Linear(input_dim, hidden_dim)
        self.layer_2 = nn.Linear(hidden_dim, hidden_dim)
        self.layer_3 = nn.Linear(hidden_dim, hidden_dim)
        self.layer_4 = nn.Linear(hidden_dim, output_dim)
</pre>

<p>Parametry učení neuronové sítě:</p>

<pre>
<i># příprava na trénink neuronové sítě</i>
learning_rate = 0.5
loss_fn = nn.BCELoss()
&nbsp;
optimizer = optim.SGD(nn7.parameters(), lr=learning_rate)
</pre>

<p>Tato síť se bude učit rychleji a nedojde k&nbsp;efektu <i>vanishing
gradient</i>, takže průběh účelové funkce bude téměř dokonalý:</p>

<img src="https://i.iinfo.cz/images/37/pytorch-nn-2-5.png" class="image-1188099" width="640" height="480" alt="PyTorch NN" data-description="&#160;" title="Autor: tisnik, podle licence: &lt;a href=&quot;http://en.wikipedia.org/wiki/Rights_Managed&quot;&gt;Rights Managed&lt;/a&gt;" />
<p><i>Obrázek 8: Účelová funkce pro neuronovou síť s&nbsp;více skrytými
vrstvami a zvýšenou hodnotou <strong>learning rate</strong>.</i></p>

<p>Celý zdrojový kód příkladu:</p>

<pre>
import torch
from torch import nn
from torch import optim
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import numpy as np
&nbsp;
&nbsp;
class <strong>NeuralNetwork</strong>(nn.Module):
    <i>"""Třída reprezentující neuronovou síť."""</i>
&nbsp;
    def <strong>__init__</strong>(self, input_dim, hidden_dim, output_dim):
        super().__init__()
        <i># vrstvy neuronové sítě</i>
        self.layer_1 = nn.Linear(input_dim, hidden_dim)
        self.layer_2 = nn.Linear(hidden_dim, hidden_dim)
        self.layer_3 = nn.Linear(hidden_dim, hidden_dim)
        self.layer_4 = nn.Linear(hidden_dim, output_dim)
&nbsp;
    def <strong>forward</strong>(self, x):
        <i># propagace hodnot přes neuronvou síť</i>
        x = torch.nn.functional.relu(self.layer_1(x))
        x = torch.nn.functional.sigmoid(self.layer_2(x))
        x = torch.nn.functional.sigmoid(self.layer_3(x))
        x = torch.nn.functional.sigmoid(self.layer_4(x))
        return x
&nbsp;
&nbsp;
<i># konfigurace vrstev neuronové sítě</i>
input_dim = 2
hidden_dim = 10
output_dim = 1
&nbsp;
<i># konstrukce neuronové sítě</i>
nn7 = NeuralNetwork(input_dim, hidden_dim, output_dim)
&nbsp;
<i># výpis základních informací o neuronové síti</i>
print(nn7)
&nbsp;
&nbsp;
<i># příprava na trénink neuronové sítě</i>
learning_rate = 0.5
loss_fn = nn.BCELoss()
&nbsp;
optimizer = optim.SGD(nn7.parameters(), lr=learning_rate)
&nbsp;
<i># trénovací data</i>
from compute_train_and_test_data import compute_train_and_test_data
&nbsp;
train_data, _ = compute_train_and_test_data()
&nbsp;
<i># zpracovat trénovací data</i>
batch_size = 64
train_dataloader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)
print("Batches: ", len(train_dataloader))
&nbsp;
<i># vlastní trénink</i>
print("Training started")
num_epochs = 100
loss_values = []
for epoch in range(num_epochs):
    print(f"    Epoch {epoch}: ", end="")
    last_lost_value = None
    for X, y in train_dataloader:
        optimizer.zero_grad()
&nbsp;
        <i># dopředný tok + zpětný tok + optimalizace</i>
        pred = nn7(X)
&nbsp;
        <i># výpočet účelové funkce</i>
        loss = loss_fn(pred, y.unsqueeze(-1))
        loss_values.append(loss.item())
        loss.backward()
        optimizer.step()
        last_lost_value = loss.item()
        print(".", end="")
    print(last_lost_value)
&nbsp;
print("Training completed")
&nbsp;
step = range(len(loss_values))
&nbsp;
<i># příprava na vykreslení grafu</i>
fig, ax = plt.subplots(figsize=(6.4, 4.8))
plt.plot(step, np.array(loss_values))
plt.title("Průběh tréninku neuronové sítě")
plt.xlabel("Epocha")
plt.ylabel("Účelová funkce")
&nbsp;
<i># uložení do souboru</i>
plt.savefig("nn_7.png")
&nbsp;
<i># vykreslení grafu</i>
plt.show()
</pre>



<p><a name="k13"></a></p>
<h2 id="k13">13. Porovnání účelové funkce všech tří doposud použitých neuronových sítí</h2>

<p>Zajímavé bude porovnání účelové funkce získané při tréninku všech tří
neuronových sítí, které jsme si popsali v&nbsp;předchozích kapitolách. Jedná se
postupně o síť bez skrytých vrstev, síť s&nbsp;jednou skrytou vrstvou (10
neuronů) a parametrem <i>learning rate</i> nastaveným na hodnotu 0,1 a konečně
o síť se třemi skrytými vrstvami (10&rarr;10&rarr;10 neuronů), ovšem
s&nbsp;parametrem <i>learning rate</i> nastaveným na hodnotu 0,5 (rychlejší
učení, menší pravděpodobnost vanishing gradientu):</p>

<img src="https://i.iinfo.cz/images/37/pytorch-nn-2-6.png" class="image-1188102" width="640" height="480" alt="PyTorch NN" data-description="&#160;" title="Autor: tisnik, podle licence: &lt;a href=&quot;http://en.wikipedia.org/wiki/Rights_Managed&quot;&gt;Rights Managed&lt;/a&gt;" />
<p><i>Obrázek 9: Porovnání účelové funkce všech tří doposud použitých neuronových sítí.</i></p>

<p>Z&nbsp;tohoto průběhu je patrné, že poslední síť s&nbsp;mnoha skrytými
vrstvami bude ve výsledku kvalitnější, ovšem musíme zajistit dostatečně dlouhý
trénink (množství trénovacích dat atd.), protože je síť zpočátku tréninku
horší, než jednodušší sítě. Až později dojde k&nbsp;žádoucímu zlomu a poklesu
hodnoty účelové funkce. V&nbsp;tomto okamžiku by se mohlo s&nbsp;tréninkem
skončit, aby se ušetřily zdroje a zabránilo se případnému přetrénování. Naproti
tomu první síť je tak primitivní, že ani delší trénink její schopnosti
nezlepší:</p>

<img src="https://i.iinfo.cz/images/37/pytorch-nn-2-7.png" class="image-1188105" width="640" height="480" alt="PyTorch NN" data-description="&#160;" title="Autor: tisnik, podle licence: &lt;a href=&quot;http://en.wikipedia.org/wiki/Rights_Managed&quot;&gt;Rights Managed&lt;/a&gt;" />
<p><i>Obrázek 10: Porovnání v&nbsp;případě, že se provede dvojnásobek tréninku.</i></p>



<p><a name="k14"></a></p>
<h2 id="k14">14. Ověření kvality neuronové sítě</h2>

<p>Zjištění hodnot účelové funkce je sice užitečné (už jen proto, že naznačuje,
kdy je vhodné trénink ukončit), ale stále nám nic neříká o tom, jak kvalitně
dokáže neuronová síť predikovat výsledky na základě zadaných vstupů &ndash; což
je vlastně jediný účel, proč jsme neuronovou síť vytvořili. Z&nbsp;tohoto
důvodu musíme provést test sítě, a to s&nbsp;využitím testovacích (validačních)
dat. Tato data by se neměla překrývat s&nbsp;trénovacími daty, protože nás
příliš nezajímá, že se síť dokáže naučit nazpaměť předané výsledky (na to by
stačila triviální implementace cache), ale spíše její schopnosti
generalizace.</p>

<p>Do sítě tedy budeme posílat testovací data a porovnávat výsledky sítě
s&nbsp;očekávanými výsledky (ty známe). Povšimněte si způsobu převodu reálného
čísla na výstupu sítě (hodnoty v&nbsp;rozsahu 0 až 1 &ndash; jedná se o obor
hodnot aktivační funkce <i>sigmoid</i>) na binární hodnotu říkající, zda
vstupní bod [X1, X2] náleží do prvního nebo do druhého mezikruží. Zbylé
programové řádky pouze slouží pro výpočet přesnosti v&nbsp;rozsahu 0 až
100%:</p>

<pre>
test_dataloader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=True)
&nbsp;
correct = 0
total = 0
&nbsp;
import itertools
&nbsp;
with torch.no_grad():
    for X, y in test_dataloader:
        outputs = nn8(X)
        predicted = np.where(outputs.numpy() &lt; 0.5, 0, 1)
        predicted = list(itertools.chain(*predicted))
        total += y.size(0)
        correct += (predicted == y.numpy()).sum().item()
&nbsp;
print(f"Accuracy: {100 * correct // total}%")
</pre>

<p><div class="rs-tip-major">Poznámka: přesný popis toho, jak se získává a
zpracovává predikce sítě, bude uveden příště.</div></p>



<p><a name="k15"></a></p>
<h2 id="k15">15. Osmý demonstrační příklad: výpočet kvality neuronové sítě</h2>

<p>V&nbsp;pořadí již osmém demonstračním příkladu provedeme výpočet
resp.&nbsp;přesněji řečeno ověření kvality neuronové sítě. Máme
k&nbsp;dispozici testovací data odlišná od trénovacích dat. Necháme tedy síť
předpovědět, které body z&nbsp;testovacích dat patří do první skupiny
(mezikruží) a které do skupiny druhé. Poté již jednoduchou statistikou zjistíme
míru přesnosti neuronové sítě:</p>

<pre>
import torch
from torch import nn
from torch import optim
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import numpy as np
&nbsp;
&nbsp;
class <strong>NeuralNetwork</strong>(nn.Module):
    <i>"""Třída reprezentující neuronovou síť."""</i>
&nbsp;
    def <strong>__init__</strong>(self, input_dim, hidden_dim, output_dim):
        super().__init__()
        <i># vrstvy neuronové sítě</i>
        self.layer_1 = nn.Linear(input_dim, hidden_dim)
        self.layer_2 = nn.Linear(hidden_dim, hidden_dim)
        self.layer_3 = nn.Linear(hidden_dim, output_dim)
&nbsp;
    def <strong>forward</strong>(self, x):
        <i># propagace hodnot přes neuronvou síť</i>
        x = torch.nn.functional.relu(self.layer_1(x))
        x = torch.nn.functional.sigmoid(self.layer_2(x))
        x = torch.nn.functional.sigmoid(self.layer_3(x))
        return x
&nbsp;
&nbsp;
<i># konfigurace vrstev neuronové sítě</i>
input_dim = 2
hidden_dim = 10
output_dim = 1
&nbsp;
<i># konstrukce neuronové sítě</i>
nn8 = NeuralNetwork(input_dim, hidden_dim, output_dim)
&nbsp;
<i># výpis základních informací o neuronové síti</i>
print(nn8)
&nbsp;
&nbsp;
<i># příprava na trénink neuronové sítě</i>
learning_rate = 0.1
loss_fn = nn.BCELoss()
&nbsp;
optimizer = optim.SGD(nn8.parameters(), lr=learning_rate)
&nbsp;
<i># trénovací data</i>
from compute_train_and_test_data import compute_train_and_test_data
&nbsp;
train_data, test_data = compute_train_and_test_data()
&nbsp;
<i># zpracovat trénovací data</i>
batch_size = 64
train_dataloader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)
print("Batches: ", len(train_dataloader))
&nbsp;
<i># vlastní trénink</i>
print("Training started")
num_epochs = 20
loss_values = []
for epoch in range(num_epochs):
    print(f"    Epoch {epoch}: ", end="")
    last_lost_value = None
    for X, y in train_dataloader:
        optimizer.zero_grad()
&nbsp;
        <i># dopředný tok + zpětný tok + optimalizace</i>
        pred = nn8(X)
&nbsp;
        <i># výpočet účelové funkce</i>
        loss = loss_fn(pred, y.unsqueeze(-1))
        loss_values.append(loss.item())
        loss.backward()
        optimizer.step()
        last_lost_value = loss.item()
        print(".", end="")
    print(last_lost_value)
&nbsp;
print("Training completed")
&nbsp;
test_dataloader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=True)
&nbsp;
correct = 0
total = 0
&nbsp;
import itertools
&nbsp;
with torch.no_grad():
    for X, y in test_dataloader:
        outputs = nn8(X)
        predicted = np.where(outputs.numpy() &lt; 0.5, 0, 1)
        predicted = list(itertools.chain(*predicted))
        total += y.size(0)
        correct += (predicted == y.numpy()).sum().item()
&nbsp;
print(f"Accuracy: {100 * correct // total}%")
</pre>

<p>Výsledky (nejdůležitější je poslední řádek):</p>

<pre>
NeuralNetwork(
  (layer_1): Linear(in_features=2, out_features=10, bias=True)
  (layer_2): Linear(in_features=10, out_features=10, bias=True)
  (layer_3): Linear(in_features=10, out_features=1, bias=True)
)
Batches:  21
Training started
    Epoch 0: .....................0.6994423866271973
    Epoch 1: .....................0.6892452836036682
    Epoch 2: .....................0.6878048181533813
    Epoch 3: .....................0.6901800632476807
    Epoch 4: .....................0.6891487240791321
    Epoch 5: .....................0.6901738047599792
    Epoch 6: .....................0.6905770897865295
    Epoch 7: .....................0.6840065121650696
    Epoch 8: .....................0.6886505484580994
    Epoch 9: .....................0.6881945133209229
    Epoch 10: .....................0.6873928904533386
    ...
    ...
    ...
    Epoch 90: .....................0.030736887827515602
    Epoch 91: .....................0.033318083733320236
    Epoch 92: .....................0.026814712211489677
    Epoch 93: .....................0.02321838028728962
    Epoch 94: .....................0.023601409047842026
    Epoch 95: .....................0.026054512709379196
    Epoch 96: .....................0.03182302415370941
    Epoch 97: .....................0.021505670621991158
    Epoch 98: .....................0.02354506216943264
    Epoch 99: .....................0.024997740983963013
Training completed
Accuracy: 100%
</pre>



<p><a name="k16"></a></p>
<h2 id="k16">16. Vizualizace predikce neuronové sítě: body patřící do první nebo druhé skupiny</h2>

<p>Výsledek, který jsme získali skriptem uvedeným v&nbsp;deváté kapitole, nám
pouze jednorozměrným číslem určuje míru úspěšnosti neuronové sítě při
predikcích. Ovšem v&nbsp;praxi je mnohem lepší si výsledky vhodným způsobem
vizualizovat, protože jen tak lze zjistit, v&nbsp;jakých oblastech dává síť
nekorektní odpovědi atd. V&nbsp;našem konkrétním případě je vizualizace až
triviálně snadná, protože na vstupu jsou body v&nbsp;rovině a výstupem sítě je
hodnota říkající, zda bod patří do prvního či druhého mezikruží. To znamená, že
si můžeme vykreslit vstupní body do grafu a obarvit je podle toho, jestli síť
odpověděla 1 nebo 2 (ve skutečnosti je odpovědí reálné číslo z&nbsp;rozsahu 0
až 1, to je však snadno převoditelné na binární hodnotu). Do skriptu doplníme
tento kód, který zajistí vizualizaci:</p>

<pre>
import itertools
&nbsp;
all_predicts = []
x_coords = []
y_coords = []
&nbsp;
with torch.no_grad():
    for X, y in test_dataloader:
        outputs = nn9(X)
        predicted = np.where(outputs.numpy() &lt; 0.5, 0, 1)
        predicted = list(itertools.chain(*predicted))
        all_predicts += predicted
        coords = X.tolist()
        for coord in coords:
            x_coords.append(coord[0])
            y_coords.append(coord[1])
&nbsp;
<i># velikost obrázku s grafem</i>
plt.subplots(figsize=(6.4, 6.4))
&nbsp;
<i># vizualizace</i>
plt.scatter(<strong>x_coords</strong>, <strong>y_coords</strong>, s=1.5, c=<strong>all_predicts</strong>, cmap=plt.cm.Set1)
&nbsp;
<i># popisek grafu</i>
plt.title("Predikované výsledky")
&nbsp;
<i># uložení grafu do souboru</i>
plt.savefig("nn_9.png")
&nbsp;
<i># vykreslení na obrazovku</i>
plt.show()
</pre>



<p><a name="k17"></a></p>
<h2 id="k17">17. Devátý demonstrační příklad: vizualizace predikce neuronové sítě</h2>

<p>Postup popsaný <a href="#k16">v&nbsp;šestnácté kapitole</a> je součástí
dnešního posledního demonstračního příkladu, jehož zdrojový kód vypadá
následovně:</p>

<pre>
import torch
from torch import nn
from torch import optim
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import numpy as np
&nbsp;
<i># budeme provádět vykreslování de facto standardní knihovnou Matplotlib</i>
import matplotlib.pyplot as plt
&nbsp;
&nbsp;
class <strong>NeuralNetwork</strong>(nn.Module):
    <i>"""Třída reprezentující neuronovou síť."""</i>
&nbsp;
    def <strong>__init__</strong>(self, input_dim, hidden_dim, output_dim):
        super().__init__()
        <i># vrstvy neuronové sítě</i>
        self.layer_1 = nn.Linear(input_dim, hidden_dim)
        self.layer_2 = nn.Linear(hidden_dim, hidden_dim)
        self.layer_3 = nn.Linear(hidden_dim, output_dim)
&nbsp;
    def <strong>forward</strong>(self, x):
        <i># propagace hodnot přes neuronvou síť</i>
        x = torch.nn.functional.relu(self.layer_1(x))
        x = torch.nn.functional.sigmoid(self.layer_2(x))
        x = torch.nn.functional.sigmoid(self.layer_3(x))
        return x
&nbsp;
&nbsp;
<i># konfigurace vrstev neuronové sítě</i>
input_dim = 2
hidden_dim = 10
output_dim = 1
&nbsp;
<i># konstrukce neuronové sítě</i>
nn9 = NeuralNetwork(input_dim, hidden_dim, output_dim)
&nbsp;
<i># výpis základních informací o neuronové síti</i>
print(nn9)
&nbsp;
&nbsp;
<i># příprava na trénink neuronové sítě</i>
learning_rate = 0.1
loss_fn = nn.BCELoss()
&nbsp;
optimizer = optim.SGD(nn9.parameters(), lr=learning_rate)
&nbsp;
<i># trénovací data</i>
from compute_train_and_test_data import compute_train_and_test_data
&nbsp;
train_data, test_data = compute_train_and_test_data()
&nbsp;
<i># zpracovat trénovací data</i>
batch_size = 64
train_dataloader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)
print("Batches: ", len(train_dataloader))
&nbsp;
<i># vlastní trénink</i>
print("Training started")
num_epochs = 200
loss_values = []
for epoch in range(num_epochs):
    print(f"    Epoch {epoch}: ", end="")
    last_lost_value = None
    for X, y in train_dataloader:
        optimizer.zero_grad()
&nbsp;
        <i># dopředný tok + zpětný tok + optimalizace</i>
        pred = nn9(X)
&nbsp;
        <i># výpočet účelové funkce</i>
        loss = loss_fn(pred, y.unsqueeze(-1))
        loss_values.append(loss.item())
        loss.backward()
        optimizer.step()
        last_lost_value = loss.item()
        print(".", end="")
    print(last_lost_value)
&nbsp;
print("Training completed")
&nbsp;
test_dataloader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=True)
&nbsp;
import itertools
&nbsp;
all_predicts = []
x_coords = []
y_coords = []
&nbsp;
with torch.no_grad():
    for X, y in test_dataloader:
        outputs = nn9(X)
        predicted = np.where(outputs.numpy() &lt; 0.5, 0, 1)
        predicted = list(itertools.chain(*predicted))
        all_predicts += predicted
        coords = X.tolist()
        for coord in coords:
            x_coords.append(coord[0])
            y_coords.append(coord[1])
&nbsp;
<i># velikost obrázku s grafem</i>
plt.subplots(figsize=(6.4, 6.4))
&nbsp;
<i># vizualizace</i>
plt.scatter(x_coords, y_coords, s=1.5, c=all_predicts, cmap=plt.cm.Set1)
&nbsp;
<i># popisek grafu</i>
plt.title("Predikované výsledky")
&nbsp;
<i># uložení grafu do souboru</i>
plt.savefig("nn_9.png")
&nbsp;
<i># vykreslení na obrazovku</i>
plt.show()
</pre>



<p><a name="k18"></a></p>
<h2 id="k18">18. Výsledky získané devátým demonstračním příkladem</h2>

<p>Okomentované výsledky získané devátým demonstračním příkladem pro různě
nastavené neuronové sítě:</p>

<img src="https://i.iinfo.cz/images/37/pytorch-nn-2-8.png" class="image-1188108" width="640" height="640" alt="PyTorch NN" data-description="&#160;" title="Autor: tisnik, podle licence: &lt;a href=&quot;http://en.wikipedia.org/wiki/Rights_Managed&quot;&gt;Rights Managed&lt;/a&gt;" />
<p><i>Obrázek 11: Korektně naučená síť &ndash; každý bod je ohodnocen
správně.</i></p>

<img src="https://i.iinfo.cz/images/37/pytorch-nn-2-9.png" class="image-1188111" width="640" height="640" alt="PyTorch NN" data-description="&#160;" title="Autor: tisnik, podle licence: &lt;a href=&quot;http://en.wikipedia.org/wiki/Rights_Managed&quot;&gt;Rights Managed&lt;/a&gt;" />
<p><i>Obrázek 12: Nekorektně naučená síť &ndash; největší chyby vidíme ve
spodní části grafu.</i></p>

<img src="https://i.iinfo.cz/images/37/pytorch-nn-2-10.png" class="image-1188114" width="640" height="640" alt="PyTorch NN" data-description="&#160;" title="Autor: tisnik, podle licence: &lt;a href=&quot;http://en.wikipedia.org/wiki/Rights_Managed&quot;&gt;Rights Managed&lt;/a&gt;" />
<p><i>Obrázek 13: Korektně naučená síť &ndash; každý bod je ohodnocen
správně (větší množství testovacích dat).</i></p>

<img src="https://i.iinfo.cz/images/37/pytorch-nn-2-11.png" class="image-1188117" width="640" height="640" alt="PyTorch NN" data-description="&#160;" title="Autor: tisnik, podle licence: &lt;a href=&quot;http://en.wikipedia.org/wiki/Rights_Managed&quot;&gt;Rights Managed&lt;/a&gt;" />
<p><i>Obrázek 14: Nekorektně naučená síť; tentokrát je problematický první
kvadrant.</i></p>

<img src="https://i.iinfo.cz/images/37/pytorch-nn-2-12.png" class="image-1188120" width="640" height="640" alt="PyTorch NN" data-description="&#160;" title="Autor: tisnik, podle licence: &lt;a href=&quot;http://en.wikipedia.org/wiki/Rights_Managed&quot;&gt;Rights Managed&lt;/a&gt;" />
<p><i>Obrázek 15: Korektně naučená síť v&nbsp;případě, že se mezikruží
dotýkají.</i></p>



<p><a name="k19"></a></p>
<h2 id="k19">19. Repositář s&nbsp;demonstračními příklady</h2>

<p>Všechny demonstrační příklady využívající knihovnu PyTorch lze nalézt
v&nbsp;repositáři <a
href="https://github.com/tisnik/most-popular-python-libs">https://github.com/tisnik/most-popular-python-libs</a>.
Následují odkazy na jednotlivé příklady:</p>

<table>
<tr><th>#<th>Příklad</th><th>Stručný popis</th><th>Adresa příkladu</th></tr></i>
<tr><td> 1</td><td>tensor_constructor_scalar_1.py</td><td>konstrukce tenzoru nultého a prvního řádu</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_constructor_scalar_1.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_constructor_scalar_1.py</a></td></tr>
<tr><td> 2</td><td>tensor_constructor_scalar_2.py</td><td>inicializace tenzoru prvního řádu s&nbsp;jedním prvkem</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_constructor_scalar_2.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_constructor_scalar_2.py</a></td></tr>
<tr><td> 3</td><td>tensor_constructor_vector_1.py</td><td>konstrukce tenzoru prvního řádu (tříprvkový vektor)</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_constructor_vector_1.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_constructor_vector_1.py</a></td></tr>
<tr><td> 4</td><td>tensor_constructor_vector_2.py</td><td>konstrukce tenzoru prvního řádu s&nbsp;inicializací prvků</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_constructor_vector_2.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_constructor_vector_2.py</a></td></tr>
<tr><td> 5</td><td>tensor_constructor_vector_3.py</td><td>konstrukce tenzoru prvního řádu s&nbsp;využitím generátoru <strong>range</strong></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_constructor_vector_3.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_constructor_vector_3.py</a></td></tr>
<tr><td> 6</td><td>tensor_constructor_matrix_1.py</td><td>vytvoření a inicializace tenzoru druhého řádu, který může být reprezentován maticí</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_constructor_matrix_1.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_constructor_matrix_1.py</a></td></tr>
<tr><td> 7</td><td>tensor_constructor_matrix_2.py</td><td>inicializace prvků matice</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_constructor_matrix_2.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_constructor_matrix_2.py</a></td></tr>
<tr><td> 8</td><td>tensor_constructor_3D_1.py</td><td>tenzor třetího řádu reprezentovaný &bdquo;3D maticí&ldquo;</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_constructor_3D_1.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_constructor_3D_1.py</a></td></tr>
<tr><td> 9</td><td>tensor_constructor_3D_2.py</td><td>tenzor třetího řádu reprezentovaný &bdquo;3D maticí&ldquo; (jiná forma inicializace)</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_constructor_3D_2.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_constructor_3D_2.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>10</td><td>tensor_constructor_scalar_zero.py</td><td>vynulování prvků tenzoru nultého řádu</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_constructor_scalar_zero.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_constructor_scalar_zero.py</a></td></tr>
<tr><td>11</td><td>tensor_constructor_vector_zero.py</td><td>vynulování prvků tenzoru prvního řádu</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_constructor_vector_zero.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_constructor_vector_zero.py</a></td></tr>
<tr><td>12</td><td>tensor_constructor_matrix_zero.py</td><td>vynulování prvků tenzoru druhého řádu</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_constructor_matrix_zero.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_constructor_matrix_zero.py</a></td></tr>
<tr><td>13</td><td>tensor_constructor_3D_zero.py</td><td>vynulování prvků tenzoru třetího řádu</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_constructor_3D_zero.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_constructor_3D_zero.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>14</td><td>tensor_zeros_shape.py</td><td>použití konstruktoru <strong>zeros</strong> pro tenzory různých řádů a tvarů</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_zeros_shape.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_zeros_shape.py</a></td></tr>
<tr><td>15</td><td>tensor_ones_shape.py</td><td>použití konstruktoru <strong>ones</strong> pro tenzory různých řádů a tvarů</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_ones_shape.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_ones_shape.py</a></td></tr>
<tr><td>16</td><td>tensor_eye.py</td><td>konstrukce jednotkové matice</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_eye.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_eye.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>17</td><td>tensor_range.py</td><td>využití konstruktoru <strong>range</strong></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_range.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_range.py</a></td></tr>
<tr><td>18</td><td>tensor_arange.py</td><td>využití konstruktoru <strong>arange</strong></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_arange.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_arange.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>19</td><td>tensor_shape.py</td><td>zjištění tvaru tenzoru</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_shape.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_shape.py</a></td></tr>
<tr><td>20</td><td>tensor_zeros_shape.py</td><td>zjištění tvaru tenzoru vytvořeného konstruktorem <strong>zeros</strong></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_zeros_shape.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_zeros_shape.py</a></td></tr>
<tr><td>21</td><td>tensor_ones_shape.py</td><td>zjištění tvaru tenzoru vytvořeného konstruktorem <strong>ones</strong></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_ones_shape.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_ones_shape.py</a></td></tr>
<tr><td>22</td><td>tensor_read_dtype.py</td><td>zjištění, jakého typu jsou prvky tenzoru</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_read_dtype.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_read_dtype.py</a></td></tr>
<tr><td>23</td><td>tensor_set_dtype.py</td><td>nastavení či změna typu prvků tenzoru</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_set_dtype.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_set_dtype.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>24</td><td>tensor_storage_1.py</td><td>získání datové struktury se zdrojem dat pro tenzor</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_storage_1.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_storage_1.py</a></td></tr>
<tr><td>25</td><td>tensor_storage_2.py</td><td>získání datové struktury se zdrojem dat pro tenzor</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_storage_2.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_storage_2.py</a></td></tr>
<tr><td>26</td><td>tensor_storage_3.py</td><td>získání datové struktury se zdrojem dat pro tenzor</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_storage_3.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_storage_3.py</a></td></tr>
<tr><td>27</td><td>tensor_storage_casts.py</td><td>přetypování datové struktury se zdrojem dat pro tenzor</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_storage_casts.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_storage_casts.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>28</td><td>tensor_slice_operation_1.py</td><td>konstrukce řezu z&nbsp;tenzoru prvního řádu</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_slice_operation_1.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_slice_operation_1.py</a></td></tr>
<tr><td>29</td><td>tensor_slice_operation_2.py</td><td>konstrukce řezu z&nbsp;tenzoru druhého řádu (přes řádky a sloupce)</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_slice_operation_2.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_slice_operation_2.py</a></td></tr>
<tr><td>30</td><td>tensor_slice_operation_3.py</td><td>konstrukce řezu s&nbsp;jeho následnou modifikací</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_slice_operation_3.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_slice_operation_3.py</a></td></tr>
<tr><td>31</td><td>tensor_slice_operation_4.py</td><td>konstrukce řezu s&nbsp;jeho následnou modifikací (odlišné operace od předchozího příkladu)</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_slice_operation_4.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_slice_operation_4.py</a></td></tr>
<tr><td>32</td><td>tensor_is_slice.py</td><td>test základních vlastností řezů</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_is_slice.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_is_slice.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>33</td><td>tensor_stride_1.py</td><td>význam atributů <strong>stride</strong> a <strong>storage_offset</strong></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_stride_1.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_stride_1.py</a></td></tr>
<tr><td>34</td><td>tensor_stride_2.py</td><td>význam atributů <strong>stride</strong> a <strong>storage_offset</strong></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_stride_2.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_stride_2.py</a></td></tr>
<tr><td>35</td><td>tensor_stride_3.py</td><td>význam atributů <strong>stride</strong> a <strong>storage_offset</strong></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_stride_3.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_stride_3.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>36</td><td>tensor_reshape.py</td><td>změna tvaru tenzoru operací <strong>reshape</strong></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_reshape.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_reshape.py</a></td></tr>
<tr><td>37</td><td>tensor_reshape_2.py</td><td>změna tvaru tenzoru operací <strong>reshape</strong></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_reshape_2.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_reshape_2.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>38</td><td>tensor_narrow_operation_1.py</td><td>operace nad celým tenzorem typu <i>narrow</i>, první ukázka</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_narrow_operation_1.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_narrow_operation_1.py</a></td></tr>
<tr><td>39</td><td>tensor_narrow_operation_1_B.py</td><td>operace nad celým tenzorem typu <i>narrow</i>, první ukázka přepsaná do volání metody</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_narrow_operation_1_B.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_narrow_operation_1_B.py</a></td></tr>
<tr><td>40</td><td>tensor_narrow_operation_2.py</td><td>operace nad celým tenzorem typu <i>narrow</i>, druhá ukázka</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_narrow_operation_2.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_narrow_operation_2.py</a></td></tr>
<tr><td>41</td><td>tensor_narrow_operation_2_B.py</td><td>operace nad celým tenzorem typu <i>narrow</i>, druhá ukázka přepsaná do volání metody</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_narrow_operation_2_B.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_narrow_operation_2_B.py</a></td></tr>
<tr><td>42</td><td>tensor_narrow_operation_3.py</td><td>operace nad celým tenzorem typu <i>narrow</i>, třetí ukázka</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_narrow_operation_3.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_narrow_operation_3.py</a></td></tr>
<tr><td>43</td><td>tensor_narrow_operation_3_B.py</td><td>operace nad celým tenzorem typu <i>narrow</i>, třetí ukázka přepsaná do volání metody</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_narrow_operation_3_B.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_narrow_operation_3_B.py</a></td></tr>
<tr><td>44</td><td>tensor_narrow_operation_4.py</td><td>přepis původní matice přes pohled na ni (<i>view</i>)</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_narrow_operation_4.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_narrow_operation_4.py</a></td></tr>
<tr><td>45</td><td>tensor_narrow_operation_5.py</td><td>přepis původní matice přes pohled na ni (<i>view</i>)<i>narrow</i>, třetí ukázka</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_narrow_operation_5.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_narrow_operation_5.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>46</td><td>tensor_operator_add.py</td><td>součet dvou tenzorů prvek po prvku</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_operator_add.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_operator_add.py</a></td></tr>
<tr><td>47</td><td>tensor_operator_sub.py</td><td>rozdíl dvou tenzorů prvek po prvku</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_operator_sub.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_operator_sub.py</a></td></tr>
<tr><td>48</td><td>tensor_operator_mul.py</td><td>součin dvou tenzorů prvek po prvku</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_operator_mul.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_operator_mul.py</a></td></tr>
<tr><td>49</td><td>tensor_operator_div.py</td><td>podíl dvou tenzorů prvek po prvku</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_operator_div.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_operator_div.py</a></td></tr>
<tr><td>50</td><td>tensor_dot_product.py</td><td>skalární součin dvou tenzorů prvního řádu</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_dot_product.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_dot_product.py</a></td></tr>
<tr><td>50</td><td>tensor_operator_matmul.py</td><td>maticové násobení (dvou tenzorů druhého řádu)</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_operator_matmul.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_operator_matmul.py</a></td></tr>
<tr><td>51</td><td>tensor_operator_matmul_2.py</td><td>maticové násobení (dvou tenzorů druhého řádu)</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_operator_matmul_2.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_operator_matmul_2.py</a></td></tr>
<tr><td>52</td><td>tensor_operator_matmul_3.py</td><td>maticové násobení v&nbsp;případě nekompatibilních tvarů matic</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_operator_matmul_3.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_operator_matmul_3.py</a></td></tr>
<tr><td>53</td><td>tensor_operator_matmul_4.py</td><td>maticové násobení s&nbsp;broadcastingem</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_operator_matmul_4.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_operator_matmul_4.py</a></td></tr>
<tr><td>54</td><td>tensor_operator_matmul_5.py</td><td>násobení vektoru a matice</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_operator_matmul_5.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_operator_matmul_5.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>55</td><td>tensor_broadcast_1.py</td><td>operace <i>broadcast</i> (součin každého prvku tenzoru se skalárem)</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_broadcast_1.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_broadcast_1.py</a></td></tr>
<tr><td>56</td><td>tensor_broadcast_2.py</td><td>operace <i>broadcast</i> (součin tenzoru druhého řádu s vektorem)</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_broadcast_2.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_broadcast_2.py</a></td></tr>
<tr><td>57</td><td>tensor_broadcast_3.py</td><td>operace <i>broadcast</i> (součin vektoru s&nbsp;tenzorem druhého řádu)</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_broadcast_3.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_broadcast_3.py</a></td></tr>
<tr><td>58</td><td>tensor_broadcast_4.py</td><td>operace <i>broadcast</i> (součet tenzorů druhého a třetího řádu)</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_broadcast_4.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_broadcast_4.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>59</td><td>tensor_sparse.py</td><td>konstrukce řídkého tenzoru</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_sparse.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_sparse.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>60</td><td>activation_function_relu_.py</td><td>aktivační funkce ReLU vypočtená knihovnou NumPy</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/activation_function_relu_numpy.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/activation_function_relu_numpy.py</a></td></tr>
<tr><td>61</td><td>activation_function_relu_pytorch.py</td><td>aktivační funkce ReLU vypočtená knihovnou PyTorch</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/activation_function_relu_pytorch.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/activation_function_relu_pytorch.py</a></td></tr>
<tr><td>62</td><td>activation_function_sigmoid_.py</td><td>aktivační funkce sigmoid vypočtená knihovnou NumPy</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/activation_function_sigmoid_numpy.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/activation_function_sigmoid_numpy.py</a></td></tr>
<tr><td>63</td><td>activation_function_sigmoid_pytorch.py</td><td>aktivační funkce sigmoid vypočtená knihovnou PyTorch</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/activation_function_sigmoid_pytorch.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/activation_function_sigmoid_pytorch.py</a></td></tr>
<tr><td>64</td><td>activation_function_tanh_.py</td><td>aktivační funkce tanh vypočtená knihovnou NumPy</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/activation_function_tanh_numpy.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/activation_function_tanh_numpy.py</a></td></tr>
<tr><td>65</td><td>activation_function_tanh_pytorch.py</td><td>aktivační funkce tanh vypočtená knihovnou PyTorch</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/activation_function_tanh_pytorch.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/activation_function_tanh_pytorch.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>66</td><td>make_circles.py</td><td>vygenerování dat pro neuronovou síť funkcí <strong>make_circles</strong></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/make_circles.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/make_circles.py</a></td></tr>
<tr><td>67</td><td>make_circles_labels.py</td><td>vizualizace dat společně s&nbsp;jejich skupinou (ohodnocením)</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/make_circles_labels.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/make_circles_labels.py</a></td></tr>
<tr><td>68</td><td>make_more_noise_circles.py</td><td>získání náhodnějších dat funkcí <strong>make_circles</strong></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/make_more_noise_circles.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/make_more_noise_circles.py</a></td></tr>
<tr><td>69</td><td>make_data_set.py</td><td>náhodné rozdělení datové sady funkcí <strong>train_test_split</strong></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/make_data_set.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/make_data_set.py</a></td></tr>
<tr><td>70</td><td>prepare_for_training.py</td><td>konverze původních dat z&nbsp;n-dimenzionálních polí do tenzorů</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/prepare_for_training.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/prepare_for_training.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>71</td><td>compute_train_and_test_data.py</td><td>výpočet trénovacích a testovacích dat pro neuronové sítě</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/compute_train_and_test_data.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/compute_train_and_test_data.py</a></td></tr>
<tr><td>72</td><td>print_train_and_test_data.py</td><td>tisk dat získaných předchozím skriptem</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/print_train_and_test_data.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/print_train_and_test_data.py</a></td></tr>
<tr><td>73</td><td>nn_01.py</td><td>deklarace třídy představující neuronovou síť</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/nn_01.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/nn_01.py</a></td></tr>
<tr><td>74</td><td>nn_02.py</td><td>definice vrstev neuronové sítě</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/nn_02.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/nn_02.py</a></td></tr>
<tr><td>75</td><td>nn_03.py</td><td>trénink neuronové sítě</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/nn_03.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/nn_03.py</a></td></tr>
<tr><td>76</td><td>nn_04.py</td><td>trénink neuronové sítě se zobrazením kvality tréninku</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/nn_04.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/nn_04.py</a></td></tr>
<tr><td>77</td><td>nn_05.py</td><td>neuronová síť s&nbsp;jednou skrytou vrstvou</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/nn_05.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/nn_05.py</a></td></tr>
<tr><td>78</td><td>nn_06.py</td><td>neuronová síť s&nbsp;více skrytými vrstvami, která nebude dotrénována</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/nn_06.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/nn_06.py</a></td></tr>
<tr><td>79</td><td>nn_07.py</td><td>vliv parametru <strong>learning_rate</strong> na rychlosti naučení sítě</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/nn_07.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/nn_07.py</a></td></tr>
<tr><td>80</td><td>nn_08.py</td><td>výpočet kvality neuronové sítě</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/nn_08.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/nn_08.py</a></td></tr>
<tr><td>81</td><td>nn_09.py</td><td>vizualizace predikce neuronové sítě</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/nn_09.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/nn_09.py</a></td></tr>
</table>



<p><a name="k20"></a></p>
<h2 id="k20">20. Odkazy na Internetu</h2>

<ol>

<li>Seriál Programovací jazyk Lua na Rootu:<br />
<a href="https://www.root.cz/serialy/programovaci-jazyk-lua/">https://www.root.cz/serialy/programovaci-jazyk-lua/</a>
</li>

<li>PDM: moderní správce balíčků a virtuálních prostředí Pythonu:<br />
<a href="https://www.root.cz/clanky/pdm-moderni-spravce-balicku-a-virtualnich-prostredi-pythonu/">https://www.root.cz/clanky/pdm-moderni-spravce-balicku-a-virtualnich-prostredi-pythonu/</a>
</li>

<li>PyTorch Tutorial: Building a Simple Neural Network From Scratch<br />
<a href="https://www.datacamp.com/tutorial/pytorch-tutorial-building-a-simple-neural-network-from-scratch">https://www.datacamp.com/tutorial/pytorch-tutorial-building-a-simple-neural-network-from-scratch</a>
</li>

<li>Interní reprezentace numerických hodnot: od skutečného počítačového pravěku po IEEE 754–2008:<br />
<a href="https://www.root.cz/clanky/interni-reprezentace-numerickych-hodnot-od-skutecneho-pocitacoveho-praveku-po-ieee-754-2008/">https://www.root.cz/clanky/interni-reprezentace-numerickych-hodnot-od-skutecneho-pocitacoveho-praveku-po-ieee-754-2008/</a>
</li>

<li>Interní reprezentace numerických hodnot: od skutečného počítačového pravěku po IEEE 754–2008 (dokončení):<br />
<a href="https://www.root.cz/clanky/interni-reprezentace-numerickych-hodnot-od-skutecneho-pocitacoveho-praveku-po-ieee-754-2008-dokonceni/">https://www.root.cz/clanky/interni-reprezentace-numerickych-hodnot-od-skutecneho-pocitacoveho-praveku-po-ieee-754-2008-dokonceni/</a>
</li>

<li>Brain Floating Point &ndash; nový formát uložení čísel pro strojové učení a chytrá čidla:<br />
<a href="https://www.root.cz/clanky/brain-floating-point-ndash-novy-format-ulozeni-cisel-pro-strojove-uceni-a-chytra-cidla/">https://www.root.cz/clanky/brain-floating-point-ndash-novy-format-ulozeni-cisel-pro-strojove-uceni-a-chytra-cidla/</a>
</li>

<li>Stránky projektu PyTorch:<br />
<a href="https://pytorch.org/">https://pytorch.org/</a>
</li>

<li>Informace o instalaci PyTorche:<br />
<a href="https://pytorch.org/get-started/locally/">https://pytorch.org/get-started/locally/</a>
</li>

<li>Tenzor (Wikipedia):<br />
<a href="https://cs.wikipedia.org/wiki/Tenzor">https://cs.wikipedia.org/wiki/Tenzor</a>
</li>

<li>Introduction to Tensors:<br />
<a href="https://www.youtube.com/watch?v=uaQeXi4E7gA">https://www.youtube.com/watch?v=uaQeXi4E7gA</a>
</li>

<li>Introduction to Tensors: Transformation Rules:<br />
<a href="https://www.youtube.com/watch?v=j6DazQDbEhQ">https://www.youtube.com/watch?v=j6DazQDbEhQ</a>
</li>

<li>Tensor Attributes:<br />
<a href="https://pytorch.org/docs/stable/tensor_attributes.html">https://pytorch.org/docs/stable/tensor_attributes.html</a>
</li>

<li>Tensors Explained Intuitively: Covariant, Contravariant, Rank :<br />
<a href="https://www.youtube.com/watch?v=CliW7kSxxWU">https://www.youtube.com/watch?v=CliW7kSxxWU</a>
</li>

<li>What is the relationship between PyTorch and Torch?:<br />
<a href="https://stackoverflow.com/questions/44371560/what-is-the-relationship-between-pytorch-and-torch">https://stackoverflow.com/questions/44371560/what-is-the-relationship-between-pytorch-and-torch</a>
</li>

<li>What is a tensor anyway?? (from a mathematician):<br />
<a href="https://www.youtube.com/watch?v=K7f2pCQ3p3U">https://www.youtube.com/watch?v=K7f2pCQ3p3U</a>
</li>

<li>Visualization of tensors - part 1 :<br />
<a href="https://www.youtube.com/watch?v=YxXyN2ifK8A">https://www.youtube.com/watch?v=YxXyN2ifK8A</a>
</li>

<li>Visualization of tensors - part 2A:<br />
<a href="https://www.youtube.com/watch?v=A95jdIuUUW0">https://www.youtube.com/watch?v=A95jdIuUUW0</a>
</li>

<li>Visualization of tensors - part 2B:<br />
<a href="https://www.youtube.com/watch?v=A95jdIuUUW0">https://www.youtube.com/watch?v=A95jdIuUUW0</a>
</li>

<li>What the HECK is a Tensor?!?:<br />
<a href="https://www.youtube.com/watch?v=bpG3gqDM80w">https://www.youtube.com/watch?v=bpG3gqDM80w</a>
</li>

<li>Stránka projektu Torch<br />
<a href="http://torch.ch/">http://torch.ch/</a>
</li>

<li>Torch na GitHubu (několik repositářů)<br />
<a href="https://github.com/torch">https://github.com/torch</a>
</li>

<li>Torch (machine learning), Wikipedia<br />
<a href="https://en.wikipedia.org/wiki/Torch_%28machine_learning%29">https://en.wikipedia.org/wiki/Torch_%28machine_learning%29</a>
</li>

<li>Torch Package Reference Manual<br />
<a href="https://github.com/torch/torch7/blob/master/README.md">https://github.com/torch/torch7/blob/master/README.md</a>
</li>

<li>Torch Cheatsheet<br />
<a href="https://github.com/torch/torch7/wiki/Cheatsheet">https://github.com/torch/torch7/wiki/Cheatsheet</a>
</li>

<li>An Introduction to Tensors<br />
<a href="https://math.stackexchange.com/questions/10282/an-introduction-to-tensors">https://math.stackexchange.com/questions/10282/an-introduction-to-tensors</a>
</li>

<li>Differences between a matrix and a tensor<br />
<a href="https://math.stackexchange.com/questions/412423/differences-between-a-matrix-and-a-tensor">https://math.stackexchange.com/questions/412423/differences-between-a-matrix-and-a-tensor</a>
</li>

<li>Qualitatively, what is the difference between a matrix and a tensor?<br />
<a href="https://math.stackexchange.com/questions/1444412/qualitatively-what-is-the-difference-between-a-matrix-and-a-tensor?">https://math.stackexchange.com/questions/1444412/qualitatively-what-is-the-difference-between-a-matrix-and-a-tensor?</a>
</li>

<li>Tensors for Neural Networks, Clearly Explained!!!:<br />
<a href="https://www.youtube.com/watch?v=L35fFDpwIM4">https://www.youtube.com/watch?v=L35fFDpwIM4</a>
</li>

<li>Tensor Processing Unit:<br />
<a href="https://en.wikipedia.org/wiki/Tensor_Processing_Unit">https://en.wikipedia.org/wiki/Tensor_Processing_Unit</a>
</li>

<li>Třída Storage:<br />
<a href="http://docs.pytorch.wiki/en/storage.html">http://docs.pytorch.wiki/en/storage.html</a>
</li>

<li>Funkce torch.dot<br />
<a href="https://pytorch.org/docs/stable/generated/torch.dot.html#torch.dot">https://pytorch.org/docs/stable/generated/torch.dot.html#torch.dot</a>
</li>

<li>Funkce torch.narrow<br />
<a href="https://pytorch.org/docs/stable/generated/torch.narrow.html">https://pytorch.org/docs/stable/generated/torch.narrow.html</a>
</li>

<li>Funkce torch.matmul<br />
<a href="https://pytorch.org/docs/stable/generated/torch.matmul.html">https://pytorch.org/docs/stable/generated/torch.matmul.html</a>
</li>

<li>Funkce torch.reshape<br />
<a href="https://pytorch.org/docs/stable/generated/torch.reshape.html">https://pytorch.org/docs/stable/generated/torch.reshape.html</a>
</li>

<li>Funkce torch.arange<br />
<a href="https://pytorch.org/docs/stable/generated/torch.arange.html">https://pytorch.org/docs/stable/generated/torch.arange.html</a>
</li>

<li>Funkce torch.range<br />
<a href="https://pytorch.org/docs/stable/generated/torch.range.html">https://pytorch.org/docs/stable/generated/torch.range.html</a>
</li>

<li>Třída torch.Tensor<br />
<a href="https://pytorch.org/docs/stable/tensors.html">https://pytorch.org/docs/stable/tensors.html</a>
</li>

<li>Atributy tenzorů<br />
<a href="https://pytorch.org/docs/stable/tensor_attributes.html">https://pytorch.org/docs/stable/tensor_attributes.html</a>
</li>

<li>Pohledy vytvořené nad tenzory<br />
<a href="https://pytorch.org/docs/stable/tensor_view.html">https://pytorch.org/docs/stable/tensor_view.html</a>
</li>

<li>Broadcasting v&nbsp;knihovně <br />
<a href="https://.org/doc/stable/user/basics.broadcasting.html">https://numpy.org/doc/stable/user/basics.broadcasting.html</a>
</li>

<li>Broadcasting semantics (v&nbsp;knihovně PyTorch)<br />
<a href="https://pytorch.org/docs/stable/notes/broadcasting.html">https://pytorch.org/docs/stable/notes/broadcasting.html</a>
</li>

<li>Dot Product In Physics: What Is The Physical Meaning of It?<br />
<a href="https://profoundphysics.com/dot-product-in-physics-what-is-the-physical-meaning-of-it/">https://profoundphysics.com/dot-product-in-physics-what-is-the-physical-meaning-of-it/</a>
</li>

<li>scikit-learn: Getting Started<br />
<a href="https://scikit-learn.org/stable/getting_started.html">https://scikit-learn.org/stable/getting_started.html</a>
</li>

<li>Support Vector Machines<br />
<a href="https://scikit-learn.org/stable/modules/svm.html">https://scikit-learn.org/stable/modules/svm.html</a>
</li>

<li>Use Deep Learning to Detect Programming Languages<br />
<a href="http://searene.me/2017/11/26/use-neural-networks-to-detect-programming-languages/">http://searene.me/2017/11/26/use-neural-networks-to-detect-programming-languages/</a>
</li>

<li>Data pro neuronové sítě<br />
<a href="http://archive.ics.uci.edu/ml/index.php">http://archive.ics.uci.edu/ml/index.php</a>
</li>

<li>Feedforward neural network<br />
<a href="https://en.wikipedia.org/wiki/Feedforward_neural_network">https://en.wikipedia.org/wiki/Feedforward_neural_network</a>
</li>

<li>Biologické algoritmy (4) - Neuronové sítě<br />
<a href="https://www.root.cz/clanky/biologicke-algoritmy-4-neuronove-site/">https://www.root.cz/clanky/biologicke-algoritmy-4-neuronove-site/</a>
</li>

<li>Biologické algoritmy (5) - Neuronové sítě<br />
<a href="https://www.root.cz/clanky/biologicke-algoritmy-5-neuronove-site/">https://www.root.cz/clanky/biologicke-algoritmy-5-neuronove-site/</a>
</li>

<li>Umělá neuronová síť (Wikipedia)<br />
<a href="https://cs.wikipedia.org/wiki/Um%C4%9Bl%C3%A1_neuronov%C3%A1_s%C3%AD%C5%A5">https://cs.wikipedia.org/wiki/Um%C4%9Bl%C3%A1_neuronov%C3%A1_s%C3%AD%C5%A5</a>
</li>

<li>AI vs Machine Learning (Youtube)<br />
<a href="https://www.youtube.com/watch?v=4RixMPF4xis">https://www.youtube.com/watch?v=4RixMPF4xis</a>
</li>

<li>Machine Learning | What Is Machine Learning? | Introduction To Machine Learning | 2024 | Simplilearn (Youtube)<br />
<a href="https://www.youtube.com/watch?v=ukzFI9rgwfU">https://www.youtube.com/watch?v=ukzFI9rgwfU</a>
</li>

<li>A Gentle Introduction to Machine Learning (Youtube)<br />
<a href="https://www.youtube.com/watch?v=Gv9_4yMHFhI">https://www.youtube.com/watch?v=Gv9_4yMHFhI</a>
</li>

<li>Machine Learning vs Deep Learning<br />
<a href="https://www.youtube.com/watch?v=q6kJ71tEYqM">https://www.youtube.com/watch?v=q6kJ71tEYqM</a>
</li>

<li>Umělá inteligence (slajdy)<br />
<a href="https://slideplayer.cz/slide/12119218/">https://slideplayer.cz/slide/12119218/</a>
</li>

<li>Úvod do umělé inteligence<br />
<a href="https://slideplayer.cz/slide/2505525/">https://slideplayer.cz/slide/2505525/</a>
</li>

<li>Umělá inteligence I / Artificial Intelligence I<br />
<a href="https://ktiml.mff.cuni.cz/~bartak/ui/">https://ktiml.mff.cuni.cz/~bartak/ui/</a>
</li>

</ol>



<p></p><p></p>
<p><small>Autor: <a href="http://www.fit.vutbr.cz/~tisnovpa">Pavel Tišnovský</a> &nbsp; 2024</small></p>
</body>
</html>

