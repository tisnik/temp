<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
        "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html>
<head>
<title>Realizace neuronových sítí s využitím knihovny PyTorch</title>
<meta name="Author" content="Pavel Tisnovsky" />
<meta name="Generator" content="vim" />
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<style type="text/css">
         body {color:#000000; background:#ffffff;}
         h1  {font-family: arial, helvetica, sans-serif; color:#ffffff; background-color:#c00000; text-align:center; padding-left:1em}
         h2  {font-family: arial, helvetica, sans-serif; color:#ffffff; background-color:#0000c0; padding-left:1em; text-align:left}
         h3  {font-family: arial, helvetica, sans-serif; color:#000000; background-color:#c0c0c0; padding-left:1em; text-align:left}
         h4  {font-family: arial, helvetica, sans-serif; color:#000000; background-color:#e0e0e0; padding-left:1em; text-align:left}
         a   {font-family: arial, helvetica, sans-serif;}
         li  {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify; width:450px;}
         ol  {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify; width:450px;}
         ul  {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify; width:450px;}
         p   {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify;}
         pre {background:#e0e0e0}
</style>
</head>

<body>

<h1>Realizace neuronových sítí s využitím knihovny PyTorch</h1>

<h3>Pavel Tišnovský</h3>

<p></p>

<h1>Úvodník</h1>

<p>V dnešním pokračování seriálu o knihovně PyTorch si ukážeme tu nejdůležitější funkcionalitu poskytovanou tímto balíčkem. PyTorch totiž umožňuje konstrukci neuronových sítí zvoleného typu a tvaru, trénink těchto sítí a jejich následné použití pro řešení konkrétních úloh.</p>



<h2>Obsah</h2>

<p><a href="#k01">1. Realizace neuronových sítí s&nbsp;využitím knihovny PyTorch</a></p>
<p><a href="#k02">2. Idealizovaný model neuronu používaný v&nbsp;umělých neuronových sítích</a></p>
<p><a href="#k03">3. Role <i>biasu</i> ve výpočtech prováděných neurony</a></p>
<p><a href="#k04">4. Malá odbočka: neuronové sítě a operace skalárního součinu</a></p>
<p><a href="#k05">5. Aktivační funkce ve výpočtech prováděných neurony</a></p>
<p><a href="#k06">6. Vytvoření feed-forward sítě z&nbsp;jednotlivých neuronů</a></p>
<p><a href="#k07">7. Vstupní vrstva, výstupní vrstva a skryté vrstvy neuronů</a></p>
<p><a href="#k08">8. Trénink (učení) sítě s&nbsp;využitím trénovacích dat</a></p>
<p><a href="#k09">9. Praktická část</a></p>
<p><a href="#k10">10. Aktivační funkce vypočtené knihovnou NumPy</a></p>
<p><a href="#k11">11. Aktivační funkce vypočtené knihovnou PyTorch</a></p>
<p><a href="#k12">12. Porovnání průběhu aktivačních funkcí</a></p>
<p><a href="#k13">13. Vygenerování dat funkcí <strong>make_circles</strong></a></p>
<p><a href="#k14">14. Získání náhodnějších dat</a></p>
<p><a href="#k15">15. Náhodné rozdělení datové sady funkcí <strong>train_test_split</strong></a></p>
<p><a href="#k16">16. Realizace rozdělení datové sady na trénovací a testovací množinu</a></p>
<p><a href="#k17">17. Konverze původních dat z&nbsp;n-dimenzionálních polí do tenzorů</a></p>
<p><a href="#k18">18. Vygenerované sady tenzorů</a></p>
<p><a href="#k19">19. Repositář s&nbsp;demonstračními příklady</a></p>
<p><a href="#k20">20. Odkazy na Internetu</a></p>



<p><a name="k01"></a></p>
<h2 id="k01">1. Realizace neuronových sítí s&nbsp;využitím knihovny PyTorch</h2>

<p>V&nbsp;dnešním pokračování seriálu o knihovně <i>PyTorch</i> si ukážeme tu
nejdůležitější funkcionalitu poskytovanou tímto balíčkem. PyTorch totiž
umožňuje konstrukci neuronových sítí zvoleného typu a tvaru, trénink těchto
sítí a jejich následné použití pro řešení konkrétních úloh (a samozřejmě
podporuje i jejich validaci, i když zde si musíme vypomoci dalším kódem).
Zejména trénink neuronových sítí je přitom velmi náročný na výpočetní čas a
z&nbsp;tohoto důvodu knihovna PyTorch umožňuje realizovat tyto výpočty na GPU,
který je přesně pro tento typ úloh navržen (podporována je například CUDA
apod). V&nbsp;první části článku se seznámíme se základními pojmy, které se
vztahují k&nbsp;neuronovým sítím. Jedná se o krátké zopakování z&nbsp;článku o
knihovně <i>scikit-learn</i>. Druhá část článku bude zaměřena na praktické
použití PyTorche společně s&nbsp;dalšími knihovnami. Připravíme si data ve
formátu tenzorů, která budou následně použita pro natrénování i validaci
neuronové sítě.</p>



<p><a name="k02"></a></p>
<h2 id="k02">2. Idealizovaný model neuronu používaný v&nbsp;umělých neuronových sítích</h2>

<p>Při práci s&nbsp;umělými neuronovými sítěmi je vhodné alespoň do jisté míry
chápat, jakým způsobem je vlastně taková síť zkonstruována a z&nbsp;jakých
prvků se interně skládá. Základním stavebním prvkem těchto sítí je takzvaný
<i>umělý neuron</i>, resp.&nbsp;přesněji řečeno velmi zjednodušený a
idealizovaný model skutečného neuronu (čím více rozumíme lidskému mozku, tím
více je zřejmé, o jak primitivní model se vlastně jedná &ndash; to však zdá se
v&nbsp;praxi nevadí). Původní model neuronu byl navržen Warrenem McCullochem a
Walterem Pittsem (MCP) již ve čtyřicátých letech minulého století, z&nbsp;čehož
plyne, že neuronové sítě nejsou jen módním výstřelkem poslední doby (naopak,
moderní GPU umožňují jejich nasazení i tam, kde to dříve nebylo možné, to je
však téma na samostatný článek.). Na dalším obrázku jsou naznačeny prvky tohoto
zjednodušeného modelu neuronu:</p>

<img src="https://i.iinfo.cz/images/329/torch-nn1-1.png" class="image-312261" alt="&#160;" width="459" height="140" />
<p><i>Obrázek 1: Idealizovaný model neuronu.</i></p>

<p>Z&nbsp;obrázku je patrné, že neuron může mít libovolný počet vstupů (na
prvním obrázku jsou konkrétně zobrazeny tři vstupy
<strong>x<sub>1</sub></strong>, <strong>x<sub>2</sub></strong> a
<strong>x<sub>3</sub></strong>, ovšem může to být jen jeden vstup nebo naopak i
sto vstupů) a má pouze jeden výstup <strong>y</strong>. Vstupem a výstupem jsou
reálná čísla. Typicky přitom bývá výstup upraven <i>aktivační funkcí</i>
takovým způsobem, že leží v&nbsp;rozsahu &lt;-1..1&gt; nebo &lt;0..1&gt;. A i
vstupy jsou typicky v&nbsp;rozsahu -1..1 nebo 0..1.</p>

<p>Dále na schématu zobrazeném na prvním obrázku vidíme váhy
<strong>w<sub>1</sub></strong>, <strong>w<sub>2</sub></strong> a
<strong>w<sub>3</sub></strong>. Těmito váhami jsou vynásobeny vstupní hodnoty.
Váhy vlastně představují stav neuronu, tj.&nbsp;jde o funkci, na kterou byl
neuron natrénován (naučen). Vstupní hodnoty <strong>x<sub>1</sub></strong> až
<strong>x<sub>n</sub></strong> jsou tedy postupně vynásobeny váhami
<strong>w<sub>1</sub></strong> až <strong>w<sub>n</sub></strong> a výsledky
takto provedeného součinu jsou v&nbsp;neuronu sečteny, takže získáme jediné
reálné číslo. Toto číslo je zpracováno <i>aktivační funkcí</i> (ta již většinou
žádný stav nemá, ostatně stejně jako funkce pro výpočet sumy) výsledek této
funkce je poslán na výstup neuronu.</p>

<p>Idealizovaný a značně zjednodušený model neuronu tedy provádí tento
výpočet:</p>

<p>
y = f(w<sub>1</sub>x<sub>1</sub> + w<sub>2</sub>x<sub>2</sub> + ... + w<sub>n</sub>x<sub>n</sub>)
</p>

<p><div class="rs-tip-major">Poznámka: nejjednodušší neuronová síť obsahuje
jediný takový neuron. Taková síť se nazývá <i>perceptron</i>. Naopak síť
složená z&nbsp;více vrstev neuronů se nazývá vícevrstvý perceptron neboli
<i>Multi-Layer Perceptron (MLP)</i>.</div></p>



<p><a name="k03"></a></p>
<h2 id="k03">3. Role <i>biasu</i> ve výpočtech prováděných neurony</h2>

<p>Ve skutečnosti není stav neuronu pro <i>n</i> vstupů
<strong>x<sub>1</sub></strong> až <strong>x<sub>n</sub></strong> určen pouze
<i>n</i> vahami <strong>w<sub>1</sub></strong> až
<strong>w<sub>n</sub></strong> tak, jak jsme si to uvedli <a
href="#k02">v&nbsp;předchozí kapitole</a>. Navíc totiž musíme přidat ještě váhu
<strong>w<sub>0</sub></strong>, na kterou je připojena konstanta 1 (někdy se
proto můžeme setkat s&nbsp;nákresem neuronové sítě, v&nbsp;níž se nachází
speciální neurony bez vstupů a s&nbsp;jedničkou na výstupu). Idealizovaný model
neuronu se přidáním nového vstupu nepatrně zkomplikuje:</p>

<img src="https://i.iinfo.cz/images/329/torch-nn1-2.png" class="image-312262" alt="&#160;" width="465" height="145" />
<p><i>Obrázek 2: Idealizovaný model neuronu s&nbsp;biasem.</i></p>

<p>I výpočet bude vypadat (nepatrně) odlišně, neboť do něho přidáme nový člen
<strong>w<sub>0</sub></strong> (na začátku):</p>

<p>
y = f(<strong>w<sub>0</sub></strong> + w<sub>1</sub>x<sub>1</sub> + w<sub>2</sub>x<sub>2</sub> + ... + w<sub>n</sub>x<sub>n</sub>)
</p>

<p>Tato přidaná váha se někdy nazývá <i>bias</i>, protože vlastně umožňuje
posouvat <a
href="https://stackoverflow.com/questions/2480650/role-of-bias-in-neural-networks">průběh
aktivační funkce nalevo a napravo</a>, v&nbsp;závislosti na jeho hodnotě.</p>



<p><a name="k04"></a></p>
<h2 id="k04">4. Malá odbočka: neuronové sítě a operace skalárního součinu</h2>

<p>V&nbsp;paralelně běžícím seriálu o vektorových operacích podporovaných na
moderních mikroprocesorech s&nbsp;architekturou <i>x86</i> a <i>x86-64</i> jsme
si řekli, že s&nbsp;rozvojem neuronových sítí a umělé inteligence (ale nejenom
zde) se začal masivně využívat známý algoritmus pro výpočet skalárního součinu
(<i>dot product</i>, <i>scalar product</i>). Tento algoritmus se používá
v&nbsp;oblasti velkých jazykových modelů (LLM) pro zjišťování podobnosti
dlouhých vektorů s&nbsp;numerickými hodnotami (<i>vector similarity</i>). Kromě
klasického skalárního součinu se v&nbsp;této oblasti používá i
tzv.&nbsp;<i>cosinus similarity</i>, což je varianta skalárního součinu,
v&nbsp;níž nezáleží na délce vektorů, ale pouze na jejich vzájemné orientaci
(výpočet je tedy doplněn o normalizaci vektorů, ovšem základ zůstává stále
stejný). A toto porovnávání vektorů se v&nbsp;LLM provádí neustále a většinou
je optimalizováno a výpočty běží na GPU.</p>

<p>To však není zdaleka vše. Pokud se zaměříme na oblast klasických neuronových
sítí (<i>NN &ndash; neural networks</i>), což je téma dnešního článku,
zjistíme, že se tyto sítě skládají z&nbsp;takzvaných <i>perceptronů</i>, což je
vlastně značně zjednodušený model neuronů s&nbsp;jejich složením do několika
vrstev. A na vstup každého neuronu se přivádí nějaké množství numerických
vstupů popsaných <a href="#k03">v&nbsp;předchozí kapitole</a>. Každý
z&nbsp;těchto vstupů je váhován, tj.&nbsp;vynásoben určitou konstantou a
výsledky tohoto váhování jsou nakonec sečteny. Když se ovšem nad touto operací
zamyslíme, zjistíme, že se vlastně nejedná o nic jiného, než opět o aplikaci
výpočtu skalárního součinu. První z&nbsp;vektorů, který do tohoto součinu
vstupuje jako operand, jsou vstupy do neuronu, druhým vektorem je pak vektor
vah, které si neuron zapamatoval. A samotný trénink neuronové sítě vlastně není
nic jiného, než rekonfigurace těchto vah &ndash; vektorů.</p>



<p><a name="k05"></a></p>
<h2 id="k05">5. Aktivační funkce ve výpočtech prováděných neurony</h2>

<p>Bez aktivační funkce by se neuron choval velmi jednoduše a především
&bdquo;lineárně &ldquo;&ndash; spočítal by vážený součet vstupů a výsledek by
poslal na svůj výstup. Aktivační funkce, kterou jsme v&nbsp;předchozích
kapitolách označovali symbolem <i>f</i>, do celého výpočtu vnáší nelinearitu.
Ta je přitom pro praktické využití sítě velmi důležitá. Nejjednodušší aktivační
funkce může pro vstupní hodnoty &lt;0 vracet -1 a pro hodnoty &ge;0 vracet 1,
což vlastně říká, že je nutné dosáhnout určité hraniční hodnoty váženého součtu
vstupů, aby byl neuron aktivován (tj.&nbsp;na výstup vyslal jedničku a nikoli
-1). Ostatně právě zde znovu vidíme význam biasu, který onu hraniční hodnotu
posunuje.</p>

<img src="https://i.iinfo.cz/images/329/torch-nn1-3.png" class="image-312263" alt="&#160;" width="640" height="480" />
<p><i>Obrázek 3: Aktivační funkce ReLU.</i></p>

<p>V&nbsp;praxi je však aktivační funkce složitější, než zmíněný jednotkový
skok. Často se používá <i>ReLU</i>, <i>sigmoid</i> nebo <i>hyperbolický
tangent</i>. Pro specializovanější účely se však používají i další funkce,
které dokonce nemusí mít monotonní průběh. S&nbsp;dalšími podporovanými
funkcemi se seznámíme příště.</p>

<img src="https://i.iinfo.cz/images/329/torch-nn1-4.png" class="image-312264" alt="&#160;" width="640" height="480" />
<p><i>Obrázek 4: Aktivační funkce Tanh.</i></p>



<p><a name="k06"></a></p>
<h2 id="k06">6. Vytvoření feed-forward sítě z&nbsp;jednotlivých neuronů</h2>

<p>Samostatné neurony i s&nbsp;aktivační funkcí stále provádí velmi jednoduchou
činnost, ovšem aby se mohly stát součástí složitějšího systému (řekněme
automatického řízení auta nebo rozpoznávání obličejů), musíme z&nbsp;nich
vytvořit síť. Jedna z&nbsp;nejjednodušších forem umělé neuronové sítě se nazývá
<i>feed-forward</i>, a to z&nbsp;toho důvodu, že informace (tedy vstupní
hodnoty, mezihodnoty i hodnoty výstupní) touto sítí tečou jen jedním směrem
(při učení je tomu jinak, tehdy jsou naopak hodnoty posílány ve směru opačném).
Neurony jsou typicky uspořádány pravidelně do vrstev:</p>

<img src="https://i.iinfo.cz/images/329/torch-nn1-5.png" class="image-312265" alt="&#160;" width="600" height="400" />
<p><i>Obrázek 5: Uspořádání neuronů do vrstev ve feed-forward síti.</i></p>

<p>Kolečka na obrázku představují jednotlivé neurony, přičemž žlutě jsou
označeny neurony na vstupu, zeleně &bdquo;interní&ldquo; (skryté) neurony a
červeně neurony, které produkují kýžený výstup neuronové sítě.</p>

<p>Zcela nalevo jsou šipkami naznačeny vstupy. Jejich počet je prakticky zcela
závislý na řešeném problému. Může se jednat jen o několik vstupů (viz naše
testovací síť popsaná níže), ovšem pokud například budeme tvořit síť určenou
pro rozpoznání objektů v&nbsp;rastrovém obrázku, může být počet vstupů roven
počtu pixelů (což ovšem v&nbsp;praxi realizujeme odlišně &ndash; konkrétně
takzvanými <i>konvolučními sítěmi</i>).</p>



<p><a name="k07"></a></p>
<h2 id="k07">7. Vstupní vrstva, výstupní vrstva a skryté vrstvy neuronů</h2>

<p>Vraťme se ještě jednou k&nbsp;obrázku číslo 5.</p>

<p>Povšimněte si, že vstupní neurony mají vlastně zjednodušenou funkci, protože
mají jen jeden vstup. V&nbsp;mnoha typech sítí tyto neurony jen rozesílají
vstup na další neurony a neprovádí žádný složitější výpočet, například u nich
není použita aktivační funkce, ovšem to již záleží na konkrétní konfiguraci
sítě. Dále stojí za povšimnutí, že neurony posílají svůj výstup neuronům na
nejbližší další vrstvě; nejsou zde tedy žádné zkratky, žádné zpětné vazby
atd.</p>

<p>Existují samozřejmě složitější typy sítí, těmi se teď ale nebudeme zabývat.
Dále tato síť propojuje neurony na sousedních vrstvách systémem &bdquo;každý
s&nbsp;každým&ldquo;. V&nbsp;našem konkrétním příkladu mají neurony na
prostřední vrstvě dva vstupy, protože předchozí vrstva má jen dva neurony.
Ovšem neurony na poslední vrstvě již musí mít tři vstupy.</p>

<p><div class="rs-tip-major">Poznámka: může se stát, že síť bude po naučení
obsahovat neurony, jejichž váhy na vstupu budou nulové. To vlastně značí, že ze
sítě některé spoje (šipky) zmizí, protože vynásobením jakéhokoli vstupu nulou
dostaneme zase jen nulu.</div></p>

<p>První vrstva s&nbsp;jednoduchými (&bdquo;hloupými&ldquo;) neurony se nazývá
<i>vstupní vrstva</i>, poslední vrstva je <i>vrstva výstupní</i>. Vrstvy mezi
vrstvou vstupní a výstupní, kterých může být teoreticky libovolné množství, se
nazývají <i>skryté vrstvy</i>.</p>

<p>&bdquo;Paměť&ldquo; neuronové sítě je tvořena vahami na vstupech neuronů
(včetně biasu):</p>

<table>
<tr><th>Vrstva</th><th>Neuronů</th><th>Počet vstupů/neuron</th><th>Počet vah/neuron</th><th>Celkem</th></tr>
<tr><td>1</td><td>2</td><td>1</td><td>2</td><td>4</td></tr>
<tr><td>2</td><td>3</td><td>2</td><td>3</td><td>9</td></tr>
<tr><td>3</td><td>2</td><td>3</td><td>4</td><td>8</td></tr>
<tr><td>&sum;</td><td>7</td><td>&nbsp;</td><td>&nbsp;</td><td>21</td></tr>
</table>

<p>V&nbsp;praxi se používají sítě s&nbsp;více vrstvami a především
s&nbsp;větším počtem neuronů v&nbsp;každé vrstvě. Stavový prostor a tím i
schopnosti sítě se tak prudce rozšiřují (viz již zmíněná problematika
rozpoznávání objektů v&nbsp;rastrových obrázcích).</p>

<p><div class="rs-tip-major">Poznámka: někdy se počet neuronů v&nbsp;umělých
sítích porovnává s&nbsp;počtem neuronů v&nbsp;mozku, ale to je ve skutečnosti
dost zavádějící, neboť záleží spíše na uspořádání sítě, složitosti neuronů
(více výstupů) atd. A pravděpodobně mnoho věcí o mozku ještě neznáme.</div></p>



<p><a name="k08"></a></p>
<h2 id="k08">8. Trénink (učení) sítě s&nbsp;využitím trénovacích dat</h2>

<p>Nejzajímavější je proces tréninku (učení) sítě. Ten může probíhat několika
způsoby, ovšem nejčastější je učení založené na tom, že na vstup sítě přivedeme
data, u nichž dopředu známe očekávaný výsledek. Síť pro tato vstupní data
provede svůj odhad a na základě rozdílů mezi odhadem sítě a očekávaným
výsledkem se více či méně sofistikovanými algoritmy nepatrně pozmění váhy
<strong>w<sub>i</sub></strong> na vstupech do neuronů (včetně biasu, tedy
<strong>w<sub>0</sub></strong>).</p>

<p>Konkrétní míra změn váhy na vstupech neuronů je globálně řízena dalším
parametrem či parametry, z&nbsp;nichž ten nejdůležitější ovlivňuje rychlost
učení. Ta by neměla být příliš nízká (to vyžaduje objemná trénovací data nebo
jejich opakování), ale ani příliš vysoká. Základní algoritmus učení sítě se
jmenuje <i>backpropagation</i>, protože se váhy skutečně mění v&nbsp;opačném
směru &ndash; od výstupů (na něž se přivede vypočtená chyba) ke vstupům. Asi
nejlépe je tento koncept popsán v&nbsp;článku dostupném na adrese <a
href="https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/">https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/</a>.</p>



<p><a name="k09"></a></p>
<h2 id="k09">9. Praktická část</h2>

<p>Po teoretickém úvodu, ve kterém jsme si řekli skutečně jen základní, ale
nezbytné informace o problematice neuronových sítí, přejdeme k&nbsp;praktické
části, tj.&nbsp;postupně se pokusíme natrénovat neuronovou síť takovým
způsobem, aby dokázala řešit jednoduché problémy. A pochopitelně taktéž budeme
muset zvalidovat, jestli tato síť poskytuje korektní výsledky. Pokud výsledky
validace nebudou uspokojivé, může to znamenat, že je neuronová síť příliš
jednoduchá a nedokáže řešit složitější problémy; může být ale taktéž
nedotrénovaná (málo trénovacích dat) nebo naopak přetrénovaná. Pro realizaci
přípravy dat, trénink sítě a její validaci použijeme <a
href="https://www.root.cz/n/python/">programovací jazyk Python</a> a knihovny
<i>scikit-learn</i>, <i>PyTorch</i> a částečně taktéž <i>NumPy</i>. Ovšem
hlavní část řešení bude provedena v&nbsp;PyTorchi s&nbsp;tím, že výpočty
(především fáze učení neuronové sítě) může probíhat buď na CPU (tedy čistě
softwarově) nebo na GPU, tj.&nbsp;na grafickém procesoru.</p>



<p><a name="k10"></a></p>
<h2 id="k10">10. Aktivační funkce vypočtené knihovnou NumPy</h2>

<p>Začneme velmi pozvolna. Nejprve si necháme vypočítat a zobrazit všechny tři
aktivační funkce, které byly popsány <a href="#k05">v&nbsp;páté kapitole</a>.
První verze skriptů je založena na knihovně NumPy a pro vizualizaci je použita
knihovna Matplotlib.</p>

<pre>
<i># Výpočet a vykreslení aktivační funkce ReLU</i>
<i># Výpočet je proveden knihovnou NumPy</i>
&nbsp;
import matplotlib.pyplot as plt
import numpy as np
&nbsp;
<i># velikost obrázku s grafem</i>
plt.subplots(figsize=(6.4, 4.8))
&nbsp;
<i># hodnoty na x-ové ose</i>
x = np.linspace(-5, 5, 100)
&nbsp;
<i># výpočet aktivační funkce</i>
y = <strong>[max(0, i) for i in x]</strong>
&nbsp;
<i># vykreslení aktivační funkce</i>
plt.plot(x, y, label="ReLU")
&nbsp;
<i># zobrazení legendy</i>
plt.legend()
&nbsp;
<i># zobrazení mřížky</i>
plt.grid()
&nbsp;
<i># uložení do souboru</i>
plt.savefig("activation_function_relu_numpy.png")
&nbsp;
<i># zobrazení v novém okně</i>
plt.show()
</pre>

<img src="https://i.iinfo.cz/images/654/pytorch-nn-1-1.png" class="image-1187550" width="640" height="480" alt="pytorch-nn-1" data-description="&#160;" title="Autor: tisnik, podle licence: &lt;a href=&quot;http://en.wikipedia.org/wiki/Rights_Managed&quot;&gt;Rights Managed&lt;/a&gt;" />
<p><i>Obrázek 6: Aktivační funkce ReLU vypočtená knihovnou NumPy.</i></p>

<pre>
<i># Výpočet a vykreslení aktivační funkce sigmoid</i>
<i># Výpočet je proveden knihovnou NumPy</i>
&nbsp;
import matplotlib.pyplot as plt
import numpy as np
&nbsp;
<i># velikost obrázku s grafem</i>
plt.subplots(figsize=(6.4, 4.8))
&nbsp;
<i># hodnoty na x-ové ose</i>
x = np.linspace(-5, 5, 100)
&nbsp;
<i># výpočet aktivační funkce</i>
y = <strong>1/(1 + np.exp(-x))</strong>
&nbsp;
<i># vykreslení aktivační funkce</i>
plt.plot(x, y, label="Sigmoid")
&nbsp;
<i># zobrazení legendy</i>
plt.legend()
&nbsp;
<i># zobrazení mřížky</i>
plt.grid()
&nbsp;
<i># uložení do souboru</i>
plt.savefig("activation_function_sigmoid_numpy.png")
&nbsp;
<i># zobrazení v novém okně</i>
plt.show()
</pre>

<img src="https://i.iinfo.cz/images/654/pytorch-nn-1-2.png" class="image-1187553" width="640" height="480" alt="pytorch-nn-1" data-description="&#160;" title="Autor: tisnik, podle licence: &lt;a href=&quot;http://en.wikipedia.org/wiki/Rights_Managed&quot;&gt;Rights Managed&lt;/a&gt;" />
<p><i>Obrázek 7: Aktivační funkce sigmoid vypočtená knihovnou NumPy.</i></p>

<pre>
<i># Výpočet a vykreslení aktivační funkce tanh</i>
<i># Výpočet je proveden knihovnou NumPy</i>
&nbsp;
import matplotlib.pyplot as plt
import numpy as np
&nbsp;
<i># velikost obrázku s grafem</i>
plt.subplots(figsize=(6.4, 4.8))
&nbsp;
<i># hodnoty na x-ové ose</i>
x = np.linspace(-5, 5, 100)
&nbsp;
<i># výpočet aktivační funkce</i>
y = <strong>np.tanh(x)</strong>
&nbsp;
<i># vykreslení aktivační funkce</i>
plt.plot(x, y, label="Tanh")
&nbsp;
<i># zobrazení legendy</i>
plt.legend()
&nbsp;
<i># zobrazení mřížky</i>
plt.grid()
&nbsp;
<i># uložení do souboru</i>
plt.savefig("activation_function_tanh_numpy.png")
&nbsp;
<i># zobrazení v novém okně</i>
plt.show()
</pre>

<img src="https://i.iinfo.cz/images/654/pytorch-nn-1-3.png" class="image-1187556" width="640" height="480" alt="pytorch-nn-1" data-description="&#160;" title="Autor: tisnik, podle licence: &lt;a href=&quot;http://en.wikipedia.org/wiki/Rights_Managed&quot;&gt;Rights Managed&lt;/a&gt;" />
<p><i>Obrázek 8: Aktivační funkce tanh vypočtená knihovnou NumPy.</i></p>



<p><a name="k11"></a></p>
<h2 id="k11">11. Aktivační funkce vypočtené knihovnou PyTorch</h2>

<p>Aktivační funkce jsou v&nbsp;oblasti neuronových sítí velmi důležité, takže
asi nebude velkým překvapením, že podporu pro jejich výpočet nalezneme i přímo
v&nbsp;PyTorchi. Pokusme se tedy vypočítat a zobrazit ty stejné funkce, nyní
ovšem vypočtené PyTorchem s&nbsp;využitím tenzorů. Zdrojové kódy tedy vypadají
podobně, ale interně se výpočty značně odlišují:</p>

<pre>
<i># Výpočet a vykreslení aktivační funkce ReLU</i>
<i># Výpočet je proveden knihovnou PyTorch</i>
&nbsp;
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import numpy as np
&nbsp;
<i># velikost obrázku s grafem</i>
plt.subplots(figsize=(6.4, 4.8))
&nbsp;
<i># hodnoty na x-ové ose</i>
x = torch.linspace(-5, 5, 100)
&nbsp;
<i># výpočet aktivační funkce</i>
y = <strong>nn.ReLU()(x)</strong>
&nbsp;
<i># vykreslení aktivační funkce</i>
plt.plot(x, y, label="ReLU")
&nbsp;
<i># zobrazení legendy</i>
plt.legend()
&nbsp;
<i># zobrazení mřížky</i>
plt.grid()
&nbsp;
<i># uložení do souboru</i>
plt.savefig("activation_function_relu_pytorch.png")
&nbsp;
<i># zobrazení v novém okně</i>
plt.show()
</pre>

<img src="https://i.iinfo.cz/images/654/pytorch-nn-1-4.png" class="image-1187559" width="640" height="480" alt="pytorch-nn-1" data-description="&#160;" title="Autor: tisnik, podle licence: &lt;a href=&quot;http://en.wikipedia.org/wiki/Rights_Managed&quot;&gt;Rights Managed&lt;/a&gt;" />
<p><i>Obrázek 9: Aktivační funkce ReLU vypočtená knihovnou PyTorch.</i></p>

<pre>
<i># Výpočet a vykreslení aktivační funkce sigmoid</i>
<i># Výpočet je proveden knihovnou PyTorch</i>
&nbsp;
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import numpy as np
&nbsp;
<i># velikost obrázku s grafem</i>
plt.subplots(figsize=(6.4, 4.8))
&nbsp;
<i># hodnoty na x-ové ose</i>
x = torch.linspace(-5, 5, 100)
&nbsp;
<i># výpočet aktivační funkce</i>
y = <strong>nn.Sigmoid()(x)</strong>
&nbsp;
<i># vykreslení aktivační funkce</i>
plt.plot(x, y, label="Sigmoid")
&nbsp;
<i># zobrazení legendy</i>
plt.legend()
&nbsp;
<i># zobrazení mřížky</i>
plt.grid()
&nbsp;
<i># uložení do souboru</i>
plt.savefig("activation_function_sigmoid_pytorch.png")
&nbsp;
<i># zobrazení v novém okně</i>
plt.show()
</pre>

<img src="https://i.iinfo.cz/images/654/pytorch-nn-1-5.png" class="image-1187562" width="640" height="480" alt="pytorch-nn-1" data-description="&#160;" title="Autor: tisnik, podle licence: &lt;a href=&quot;http://en.wikipedia.org/wiki/Rights_Managed&quot;&gt;Rights Managed&lt;/a&gt;" />
<p><i>Obrázek 10: Aktivační funkce sigmoid vypočtená knihovnou PyTorch.</i></p>

<pre>
<i># Výpočet a vykreslení aktivační funkce tanh</i>
<i># Výpočet je proveden knihovnou PyTorch</i>
&nbsp;
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import numpy as np
&nbsp;
<i># velikost obrázku s grafem</i>
plt.subplots(figsize=(6.4, 4.8))
&nbsp;
<i># hodnoty na x-ové ose</i>
x = torch.linspace(-5, 5, 100)
&nbsp;
<i># výpočet aktivační funkce</i>
y = <strong>nn.Tanh()(x)</strong>
&nbsp;
<i># vykreslení aktivační funkce</i>
plt.plot(x, y, label="Tanh")
&nbsp;
<i># zobrazení legendy</i>
plt.legend()
&nbsp;
<i># zobrazení mřížky</i>
plt.grid()
&nbsp;
<i># uložení do souboru</i>
plt.savefig("activation_function_tanh_pytorch.png")
&nbsp;
<i># zobrazení v novém okně</i>
plt.show()
</pre>

<img src="https://i.iinfo.cz/images/654/pytorch-nn-1-6.png" class="image-1187565" width="640" height="480" alt="pytorch-nn-1" data-description="&#160;" title="Autor: tisnik, podle licence: &lt;a href=&quot;http://en.wikipedia.org/wiki/Rights_Managed&quot;&gt;Rights Managed&lt;/a&gt;" />
<p><i>Obrázek 11: Aktivační funkce tanh vypočtená knihovnou PyTorch.</i></p>



<p><a name="k12"></a></p>
<h2 id="k12">12. Porovnání průběhu aktivačních funkcí</h2>

<p>Jednotlivé aktivační funkce mají stejný definiční obor, ale odlišný průběh i
obor hodnot. Bude to patrné při zobrazení všech tří těchto funkcí na jediném
grafu. Opět si uvedeme dvě varianty výpočtu a zobrazení. Jedna z&nbsp;variant
je založená na knihovně NumPy, druhá na knihovně PyTorch:</p>

<pre>
<i># Výpočet a vykreslení všech aktivačních funkcí</i>
<i># Výpočet je proveden knihovnou NumPy</i>
&nbsp;
import matplotlib.pyplot as plt
import numpy as np
&nbsp;
<i># velikost obrázku s grafem</i>
plt.subplots(figsize=(6.4, 4.8))
&nbsp;
<i># hodnoty na x-ové ose</i>
x = np.linspace(-5, 5, 100)
&nbsp;
<i># výpočet všech tří aktivačních funkcí</i>
relu = <strong>[max(0, i) for i in x]</strong>
tanh = <strong>np.tanh(x)</strong>
sigmoid = <strong>1/(1 + np.exp(-x))</strong>
&nbsp;
<i># vykreslení všech tří aktivačních funkcí</i>
plt.plot(x, sigmoid, label="sigmoid")
plt.plot(x, tanh, label="tanh")
plt.plot(x, relu, label="ReLU")
plt.ylim(-1.5, 2)
&nbsp;
<i># zobrazení legendy</i>
plt.legend()
&nbsp;
<i># zobrazení mřížky</i>
plt.grid()
&nbsp;
<i># uložení do souboru</i>
plt.savefig("activation_functions_numpy.png")
&nbsp;
<i># zobrazení v novém okně</i>
plt.show()
</pre>

<img src="https://i.iinfo.cz/images/654/pytorch-nn-1-7.png" class="image-1187568" width="640" height="480" alt="pytorch-nn-1" data-description="&#160;" title="Autor: tisnik, podle licence: &lt;a href=&quot;http://en.wikipedia.org/wiki/Rights_Managed&quot;&gt;Rights Managed&lt;/a&gt;" />
<p><i>Obrázek 12: Porovnání aktivačních funkcí; vypočteno pomocí knihovny NumPy.</i></p>

<pre>
<i># Výpočet a vykreslení všech aktivačních funkcí</i>
<i># Výpočet je proveden knihovnou PyTorch</i>
&nbsp;
import torch
import torch.nn as nn
&nbsp;
import matplotlib.pyplot as plt
&nbsp;
<i># velikost obrázku s grafem</i>
plt.subplots(figsize=(6.4, 4.8))
&nbsp;
<i># hodnoty na x-ové ose</i>
x = torch.linspace(-5, 5, 100)
&nbsp;
<i># výpočet všech tří aktivačních funkcí</i>
relu = <strong>nn.ReLU()(x)</strong>
tanh = <strong>nn.Tanh()(x)</strong>
sigmoid = <strong>nn.Sigmoid()(x)</strong>
&nbsp;
<i># vykreslení všech tří aktivačních funkcí</i>
plt.plot(x, sigmoid, label="sigmoid")
plt.plot(x, tanh, label="tanh")
plt.plot(x, relu, label="ReLU")
plt.ylim(-1.5, 2)
&nbsp;
<i># zobrazení legendy</i>
plt.legend()
&nbsp;
<i># zobrazení mřížky</i>
plt.grid()
&nbsp;
<i># uložení do souboru</i>
plt.savefig("activation_functions_pytorch.png")
&nbsp;
<i># zobrazení v novém okně</i>
plt.show()
</pre>

<img src="https://i.iinfo.cz/images/654/pytorch-nn-1-8.png" class="image-1187571" width="640" height="480" alt="pytorch-nn-1" data-description="&#160;" title="Autor: tisnik, podle licence: &lt;a href=&quot;http://en.wikipedia.org/wiki/Rights_Managed&quot;&gt;Rights Managed&lt;/a&gt;" />
<p><i>Obrázek 13: Porovnání aktivačních funkcí; vypočteno pomocí knihovny PyTorch.</i></p>



<p><a name="k13"></a></p>
<h2 id="k13">13. Vygenerování dat funkcí <strong>make_circles</strong></h2>

<p>Pro trénink a validaci první verze jednoduché neuronové sítě použijeme data,
která jsou vygenerovaná s&nbsp;využitím knihovny <i>scikit-learn</i>, která již
byla (do určité míry) na Rootu popsána. V&nbsp;této knihovně nalezneme hned
několik více či méně vhodných generátorů dat. My dnes použijeme funkci nazvanou
<strong>make_circles</strong>. Jedná se o příhodný název, protože výsledek
skutečně připomíná soustředné kružnice. Tuto funkci lze zavolat následovně:</p>

<pre>
samples, labels = <strong>make_circles</strong>(
    n_samples=n_samples, factor=0.5, noise=0.05
)
</pre>

<p>kde parametr <strong>factor</strong> určuje rozdíl velikostí vnější a
vnitřní kružnice a parametrem <strong>noise</strong> se nastavuje směrodatná
odchylka při výpočtu pozice bodů.</p>

<p>Skript, který s&nbsp;využitím této funkce vygeneruje a následně zobrazí sadu
bodů v&nbsp;rovině, vypadá následovně:</p>

<pre>
<i># budeme provádět vykreslování de facto standardní knihovnou Matplotlib</i>
import matplotlib.pyplot as plt
&nbsp;
from sklearn.datasets import make_circles
&nbsp;
<i># velikost obrázku s grafem</i>
plt.subplots(figsize=(6.4, 6.4))
&nbsp;
<i># testovací data</i>
n_samples = 2000
&nbsp;
samples, labels = make_circles(
    n_samples=n_samples, factor=0.5, noise=0.05
)
&nbsp;
<i># vykreslení bodů v rovině</i>
plt.scatter(samples[:, 0], samples[:, 1], s=1.5, cmap=plt.cm.Set1)
&nbsp;
<i># uložení grafu do souboru</i>
plt.savefig("circles1.png")
&nbsp;
<i># vykreslení na obrazovku</i>
plt.show()
</pre>

<p>Výsledný graf s&nbsp;vygenerovanými body může vypadat následovně:</p>

<img src="https://i.iinfo.cz/images/654/pytorch-nn-1-9.png" class="image-1187574" width="640" height="640" alt="pytorch-nn-1" data-description="&#160;" title="Autor: tisnik, podle licence: &lt;a href=&quot;http://en.wikipedia.org/wiki/Rights_Managed&quot;&gt;Rights Managed&lt;/a&gt;" />
<p><i>Obrázek 14: Graf s&nbsp;body vygenerovanými funkcí
<strong>make_circles</strong>.</i></p>

<p>Jednotlivé body ovšem navíc spadají do nějaké skupiny (kategorie) určené
návěštím (<i>label</i>). I tuto skupinu/kategorii si můžeme velmi snadno
vizualizovat. Postačuje k&nbsp;tomu využít proměnnou <strong>labels</strong>,
která je naplněna druhou návratovou hodnotou funkce
<strong>make_circles</strong>. Výsledkem budou dvě skupiny bodů, přičemž
z&nbsp;barvy je ihned patrné, do které skupiny bod spadá:</p>

<pre>
<i># budeme provádět vykreslování de facto standardní knihovnou Matplotlib</i>
import matplotlib.pyplot as plt
&nbsp;
from sklearn.datasets import make_circles
&nbsp;
<i># velikost obrázku s grafem</i>
plt.subplots(figsize=(6.4, 6.4))
&nbsp;
<i># testovací data</i>
n_samples = 2000
&nbsp;
samples, labels = make_circles(
    n_samples=n_samples, factor=0.5, noise=0.05
)
&nbsp;
<i># vykreslení bodů v rovině</i>
plt.scatter(samples[:, 0], samples[:, 1], s=1.5, c=labels, cmap=plt.cm.Set1)
&nbsp;
<i># uložení grafu do souboru</i>
plt.savefig("circles2.png")
&nbsp;
<i># vykreslení na obrazovku</i>
plt.show()
</pre>

<p>Výsledek:</p>

<img src="https://i.iinfo.cz/images/654/pytorch-nn-1-10.png" class="image-1187577" width="640" height="640" alt="pytorch-nn-1" data-description="&#160;" title="Autor: tisnik, podle licence: &lt;a href=&quot;http://en.wikipedia.org/wiki/Rights_Managed&quot;&gt;Rights Managed&lt;/a&gt;" />
<p><i>Obrázek 15: Graf s&nbsp;body vygenerovanými funkcí
<strong>make_circles</strong>. Body jsou obarveny na základě své příslušnosti
k&nbsp;první nebo druhé skupině.</i></p>



<p><a name="k14"></a></p>
<h2 id="k14">14. Získání náhodnějších dat</h2>

<p>Pokusme se nyní zvýšit směrodatnou odchylku a zjistit, jak se tato změna
projeví ve výsledném obrazci složeném z&nbsp;bodů. Tím později poněkud ztížíme
trénink neuronové sítě, resp.&nbsp;přesněji řečeno bude muset být neuronová síť
naučena mnohem lépe v&nbsp;případě, kdy se budou body z&nbsp;obou skupin
dotýkat:</p>

<pre>
<i># budeme provádět vykreslování de facto standardní knihovnou Matplotlib</i>
import matplotlib.pyplot as plt
&nbsp;
from sklearn.datasets import make_circles
&nbsp;
<i># velikost obrázku s grafem</i>
plt.subplots(figsize=(6.4, 6.4))
&nbsp;
<i># testovací data</i>
n_samples = 2000
&nbsp;
samples, labels = make_circles(
    n_samples=n_samples, factor=0.5, noise=0.10
)
&nbsp;
<i># vykreslení bodů v rovině</i>
plt.scatter(samples[:, 0], samples[:, 1], s=1.5, c=labels, cmap=plt.cm.Set1)
&nbsp;
<i># uložení grafu do souboru</i>
plt.savefig("circles3.png")
&nbsp;
<i># vykreslení na obrazovku</i>
plt.show()
</pre>

<p>Výsledek:</p>

<img src="https://i.iinfo.cz/images/654/pytorch-nn-1-11.png" class="image-1187580" width="640" height="640" alt="pytorch-nn-1" data-description="&#160;" title="Autor: tisnik, podle licence: &lt;a href=&quot;http://en.wikipedia.org/wiki/Rights_Managed&quot;&gt;Rights Managed&lt;/a&gt;" />
<p><i>Obrázek 16: Graf s&nbsp;body vygenerovanými funkcí
<strong>make_circles</strong>. Body jsou obarveny na základě své příslušnosti
k&nbsp;první nebo druhé skupině.</i></p>

<p><div class="rs-tip-major">Poznámka: samozřejmě je možné hodnotu
<strong>noise</strong> ještě zvýšit, popř.&nbsp;si &bdquo;poladit&ldquo;
parametr <strong>factor</strong>.</div></p>



<p><a name="k15"></a></p>
<h2 id="k15">15. Náhodné rozdělení datové sady funkcí <strong>train_test_split</strong></h2>

<p>Pro rozdělení dat získaných voláním <strong>make_circles</strong> a data
trénovací a validační použijeme funkci <strong>train_test_split</strong>. Tato
funkce ve svém prvním parametru akceptuje přímo datovou sadu (nemusíme se tedy
snažit o ruční získání naměřených dat a očekávaných výsledků), dále velikost
testovacích a trénovacích dat (buď jako celé číslo, což je počet záznamů nebo
hodnotu typu float, což bude zlomek od 0 do 1, nepovinnou hodnotu, která zamezí
různým výsledkům pro několik volání této funkce a dále parametr povolující
zamíchání dat (ve výchozím nastavení je povolen):</p>

<pre>
train_test_split(*arrays, test_size=None, train_size=None, random_state=None, shuffle=True, stratify=None)
    Split arrays or matrices into random train and test subsets.
&nbsp;
    Quick utility that wraps input validation,
    ``next(ShuffleSplit().split(X, y))``, and application to input data
    into a single call for splitting (and optionally subsampling) data into a
    one-liner.
&nbsp;
    Read more in the :ref:`User Guide &lt;cross_validation&gt;`.
&nbsp;
    Parameters
    ----------
    *arrays : sequence of indexables with same length / shape[0]
        Allowed inputs are lists, numpy arrays, scipy-sparse
        matrices or pandas dataframes.
&nbsp;
    test_size : float or int, default=None
        If float, should be between 0.0 and 1.0 and represent the proportion
        of the dataset to include in the test split. If int, represents the
        absolute number of test samples. If None, the value is set to the
        complement of the train size. If ``train_size`` is also None, it will
        be set to 0.25.
&nbsp;
    train_size : float or int, default=None
        If float, should be between 0.0 and 1.0 and represent the
        proportion of the dataset to include in the train split. If
        int, represents the absolute number of train samples. If None,
        the value is automatically set to the complement of the test size.
&nbsp;
    random_state : int, RandomState instance or None, default=None
        Controls the shuffling applied to the data before applying the split.
        Pass an int for reproducible output across multiple function calls.
        See :term:`Glossary &lt;random_state&gt;`.
&nbsp;
    shuffle : bool, default=True
        Whether or not to shuffle the data before splitting. If shuffle=False
        then stratify must be None.
&nbsp;
    stratify : array-like, default=None
        If not None, data is split in a stratified fashion, using this as
        the class labels.
        Read more in the :ref:`User Guide &lt;stratification&gt;`.
</pre>



<p><a name="k16"></a></p>
<h2 id="k16">16. Realizace rozdělení datové sady na trénovací a testovací množinu</h2>

<p>Samotná realizace a rozdělení datové sady na část určenou pro trénink a na
část určenou pro testování nebo validaci, je poměrně jednoduchá. Konkrétně
použijeme <a href="#k15">výše popsanou funkci</a>
<strong>train_test_split</strong> následujícím způsobem:</p>

<pre>
samples, labels = make_circles(
    n_samples=n_samples, factor=0.5, noise=0.05
)
&nbsp;
<i># rozdělení na trénovací a testovací množinu</i>
X_train, X_test, y_train, y_test = <strong>train_test_split(samples, labels, test_size=1/3, random_state=26)</strong>
</pre>

<p>Výsledný skript na konci zobrazí jak trénovací, tak i testovací množinu.
Z&nbsp;obrázku by mělo být patrné, že jsou data vybrána skutečně náhodně (a
nikoli například stylem první třetina, druhé dvě třetiny):</p>

<pre>
<i># budeme provádět vykreslování de facto standardní knihovnou Matplotlib</i>
import matplotlib.pyplot as plt
&nbsp;
from sklearn.datasets import make_circles
from sklearn.model_selection import train_test_split
&nbsp;
<i># testovací data</i>
n_samples = 2000
&nbsp;
samples, labels = make_circles(
    n_samples=n_samples, factor=0.5, noise=0.05
)
&nbsp;
<i># rozdělení na trénovací a testovací množinu</i>
X_train, X_test, y_train, y_test = <strong>train_test_split(samples, labels, test_size=1/3, random_state=26)</strong>
&nbsp;
<i># vizualizace</i>
fig, (train_ax, test_ax) = plt.subplots(ncols=2, sharex=True, sharey=True, figsize=(10, 5))
train_ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=plt.cm.Set1)
train_ax.set_title("Trénovací data")
train_ax.set_xlabel("Feature #0")
train_ax.set_ylabel("Feature #1")
&nbsp;
test_ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=plt.cm.Set1)
test_ax.set_xlabel("Feature #0")
test_ax.set_title("Testovací data")
&nbsp;
<i># uložení grafu do souboru</i>
plt.savefig("circles4.png")
&nbsp;
<i># vykreslení na obrazovku</i>
plt.show()
</pre>

<a href="https://www.root.cz/obrazek/1187583/"><img src="https://i.iinfo.cz/images/654/pytorch-nn-1-12-prev.png" class="image-1187583" width="370" height="185" data-prev-filename="https://i.iinfo.cz/images/654/pytorch-nn-1-12-prev.png" data-prev-filename-webp="https://i.iinfo.cz/images/654/pytorch-nn-1-12-prev.webp" data-prev-width="370" data-prev-height="185" data-large-filename="https://i.iinfo.cz/images/654/pytorch-nn-1-12-large.png" data-large-filename-webp="https://i.iinfo.cz/images/654/pytorch-nn-1-12-large.webp" data-large-width="720" data-large-height="360" alt="pytorch-nn-1" data-description="&#160;" title="Autor: tisnik, podle licence: &lt;a href=&quot;http://en.wikipedia.org/wiki/Rights_Managed&quot;&gt;Rights Managed&lt;/a&gt;" /></a>
<p><i>Obrázek 17: Trénovací a testovací data získaná rozdělením bodů získaných
funkcí <strong>make_circles</strong>.</i></p>



<p><a name="k17"></a></p>
<h2 id="k17">17. Konverze původních dat z&nbsp;n-dimenzionálních polí do tenzorů</h2>

<p>V&nbsp;knihovně Pytorch se pracuje výhradně s&nbsp;tenzory, takže
v&nbsp;dalším kroku musíme převést trénovací a testovací data do odlišné
reprezentace. Prozatím máme obě sady dat uloženy ve formě n-dimenzionálních
polí knihovny NumPy, výsledkem má být sada tenzorů. Převod provedeme
následujícím skriptem (nejsem autorem &bdquo;datové části&ldquo;). Základem pro
převody je funkce <strong>torch.from_numpy</strong>. Následně provedeme
rozdělení dat na menší dávky, které využijeme příště pro trénink neuronové
sítě:</p>

<pre>
from sklearn.datasets import make_circles
from sklearn.model_selection import train_test_split
import torch
import numpy as np
from torch.utils.data import Dataset, DataLoader
&nbsp;
&nbsp;
<i># konverze původních dat z NumPy do tenzorů</i>
class <strong>Data</strong>(Dataset):
    def <strong>__init__</strong>(self, X, y):
        self.X = torch.from_numpy(X.astype(np.float32))
        self.y = torch.from_numpy(y.astype(np.float32))
        self.len = self.X.shape[0]
&nbsp; 
    def <strong>__getitem__</strong>(self, index):
        return self.X[index], self.y[index]
&nbsp;
    def <strong>__len__</strong>(self):
        return self.len
&nbsp;
&nbsp;
batch_size = 64
&nbsp;
<i># testovací data</i>
n_samples = 2000
&nbsp;
samples, labels = make_circles(
    n_samples=n_samples, factor=0.5, noise=0.05
)
&nbsp;
<i># rozdělení na trénovací a testovací množinu</i>
X_train, X_test, y_train, y_test = train_test_split(samples, labels, test_size=1/3, random_state=26)
&nbsp;
<i># trénovací a testovací sada</i>
&nbsp;
<i># trénovací sada</i>
train_data = Data(X_train, y_train)
train_dataloader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)
&nbsp;
<i># testovací sada</i>
test_data = Data(X_test, y_test)
test_dataloader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=True)
&nbsp;
<i># otestování funkcionality</i>
for batch, (X, y) in enumerate(train_dataloader):
    print(f"Dávka: {batch+1}")
    print(f"X shape: {X.shape}")
    print(f"y shape: {y.shape}")
    print()
</pre>



<p><a name="k18"></a></p>
<h2 id="k18">18. Vygenerované sady tenzorů</h2>

<p>Vytvořené sady (dávky) tenzorů by měly obsahovat tenzory s&nbsp;první
dimenzí rovnou maximálně 64 (nebo méně). O tom se snadno přesvědčíme:</p>

<pre>
Dávka: 1
X shape: torch.Size([64, 2])
y shape: torch.Size([64])
&nbsp;
Dávka: 2
X shape: torch.Size([64, 2])
y shape: torch.Size([64])
&nbsp;
Dávka: 3
X shape: torch.Size([64, 2])
y shape: torch.Size([64])
&nbsp;
Dávka: 4
X shape: torch.Size([64, 2])
y shape: torch.Size([64])
&nbsp;
Dávka: 5
X shape: torch.Size([64, 2])
y shape: torch.Size([64])
&nbsp;
Dávka: 6
X shape: torch.Size([64, 2])
y shape: torch.Size([64])
&nbsp;
Dávka: 7
X shape: torch.Size([64, 2])
y shape: torch.Size([64])
&nbsp;
Dávka: 8
X shape: torch.Size([64, 2])
y shape: torch.Size([64])
&nbsp;
Dávka: 9
X shape: torch.Size([64, 2])
y shape: torch.Size([64])
&nbsp;
Dávka: 10
X shape: torch.Size([64, 2])
y shape: torch.Size([64])
&nbsp;
Dávka: 11
X shape: torch.Size([64, 2])
y shape: torch.Size([64])
&nbsp;
Dávka: 12
X shape: torch.Size([64, 2])
y shape: torch.Size([64])
&nbsp;
Dávka: 13
X shape: torch.Size([64, 2])
y shape: torch.Size([64])
&nbsp;
Dávka: 14
X shape: torch.Size([64, 2])
y shape: torch.Size([64])
&nbsp;
Dávka: 15
X shape: torch.Size([64, 2])
y shape: torch.Size([64])
&nbsp;
Dávka: 16
X shape: torch.Size([64, 2])
y shape: torch.Size([64])
&nbsp;
Dávka: 17
X shape: torch.Size([64, 2])
y shape: torch.Size([64])
&nbsp;
Dávka: 18
X shape: torch.Size([64, 2])
y shape: torch.Size([64])
&nbsp;
Dávka: 19
X shape: torch.Size([64, 2])
y shape: torch.Size([64])
&nbsp;
Dávka: 20
X shape: torch.Size([64, 2])
y shape: torch.Size([64])
&nbsp;
Dávka: 21
X shape: torch.Size([53, 2])
y shape: torch.Size([53])
</pre>

<p>Proč je sad tolik? 20&times;64+53=1333. Toto číslo odpovídá 2000&times;2/3,
kde 2000 je celkový počet bodů a 2/3 je koeficient rozdělení vstupních dat na
trénovací část (1333) a testovací část (667).</p>



<p><a name="k19"></a></p>
<h2 id="k19">19. Repositář s&nbsp;demonstračními příklady</h2>

<p>Všechny demonstrační příklady využívající knihovnu PyTorch lze nalézt
v&nbsp;repositáři <a
href="https://github.com/tisnik/most-popular-python-libs">https://github.com/tisnik/most-popular-python-libs</a>.
Následují odkazy na jednotlivé příklady:</p>

<table>
<tr><th>#<th>Příklad</th><th>Stručný popis</th><th>Adresa příkladu</th></tr></i>
<tr><td> 1</td><td>tensor_constructor_scalar_1.py</td><td>konstrukce tenzoru nultého a prvního řádu</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_constructor_scalar_1.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_constructor_scalar_1.py</a></td></tr>
<tr><td> 2</td><td>tensor_constructor_scalar_2.py</td><td>inicializace tenzoru prvního řádu s&nbsp;jedním prvkem</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_constructor_scalar_2.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_constructor_scalar_2.py</a></td></tr>
<tr><td> 3</td><td>tensor_constructor_vector_1.py</td><td>konstrukce tenzoru prvního řádu (tříprvkový vektor)</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_constructor_vector_1.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_constructor_vector_1.py</a></td></tr>
<tr><td> 4</td><td>tensor_constructor_vector_2.py</td><td>konstrukce tenzoru prvního řádu s&nbsp;inicializací prvků</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_constructor_vector_2.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_constructor_vector_2.py</a></td></tr>
<tr><td> 5</td><td>tensor_constructor_vector_3.py</td><td>konstrukce tenzoru prvního řádu s&nbsp;využitím generátoru <strong>range</strong></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_constructor_vector_3.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_constructor_vector_3.py</a></td></tr>
<tr><td> 6</td><td>tensor_constructor_matrix_1.py</td><td>vytvoření a inicializace tenzoru druhého řádu, který může být reprezentován maticí</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_constructor_matrix_1.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_constructor_matrix_1.py</a></td></tr>
<tr><td> 7</td><td>tensor_constructor_matrix_2.py</td><td>inicializace prvků matice</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_constructor_matrix_2.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_constructor_matrix_2.py</a></td></tr>
<tr><td> 8</td><td>tensor_constructor_3D_1.py</td><td>tenzor třetího řádu reprezentovaný &bdquo;3D maticí&ldquo;</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_constructor_3D_1.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_constructor_3D_1.py</a></td></tr>
<tr><td> 9</td><td>tensor_constructor_3D_2.py</td><td>tenzor třetího řádu reprezentovaný &bdquo;3D maticí&ldquo; (jiná forma inicializace)</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_constructor_3D_2.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_constructor_3D_2.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>10</td><td>tensor_constructor_scalar_zero.py</td><td>vynulování prvků tenzoru nultého řádu</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_constructor_scalar_zero.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_constructor_scalar_zero.py</a></td></tr>
<tr><td>11</td><td>tensor_constructor_vector_zero.py</td><td>vynulování prvků tenzoru prvního řádu</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_constructor_vector_zero.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_constructor_vector_zero.py</a></td></tr>
<tr><td>12</td><td>tensor_constructor_matrix_zero.py</td><td>vynulování prvků tenzoru druhého řádu</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_constructor_matrix_zero.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_constructor_matrix_zero.py</a></td></tr>
<tr><td>13</td><td>tensor_constructor_3D_zero.py</td><td>vynulování prvků tenzoru třetího řádu</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_constructor_3D_zero.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_constructor_3D_zero.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>14</td><td>tensor_zeros_shape.py</td><td>použití konstruktoru <strong>zeros</strong> pro tenzory různých řádů a tvarů</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_zeros_shape.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_zeros_shape.py</a></td></tr>
<tr><td>15</td><td>tensor_ones_shape.py</td><td>použití konstruktoru <strong>ones</strong> pro tenzory různých řádů a tvarů</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_ones_shape.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_ones_shape.py</a></td></tr>
<tr><td>16</td><td>tensor_eye.py</td><td>konstrukce jednotkové matice</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_eye.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_eye.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>17</td><td>tensor_range.py</td><td>využití konstruktoru <strong>range</strong></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_range.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_range.py</a></td></tr>
<tr><td>18</td><td>tensor_arange.py</td><td>využití konstruktoru <strong>arange</strong></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_arange.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_arange.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>19</td><td>tensor_shape.py</td><td>zjištění tvaru tenzoru</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_shape.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_shape.py</a></td></tr>
<tr><td>20</td><td>tensor_zeros_shape.py</td><td>zjištění tvaru tenzoru vytvořeného konstruktorem <strong>zeros</strong></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_zeros_shape.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_zeros_shape.py</a></td></tr>
<tr><td>21</td><td>tensor_ones_shape.py</td><td>zjištění tvaru tenzoru vytvořeného konstruktorem <strong>ones</strong></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_ones_shape.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_ones_shape.py</a></td></tr>
<tr><td>22</td><td>tensor_read_dtype.py</td><td>zjištění, jakého typu jsou prvky tenzoru</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_read_dtype.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_read_dtype.py</a></td></tr>
<tr><td>23</td><td>tensor_set_dtype.py</td><td>nastavení či změna typu prvků tenzoru</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_set_dtype.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_set_dtype.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>24</td><td>tensor_storage_1.py</td><td>získání datové struktury se zdrojem dat pro tenzor</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_storage_1.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_storage_1.py</a></td></tr>
<tr><td>25</td><td>tensor_storage_2.py</td><td>získání datové struktury se zdrojem dat pro tenzor</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_storage_2.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_storage_2.py</a></td></tr>
<tr><td>26</td><td>tensor_storage_3.py</td><td>získání datové struktury se zdrojem dat pro tenzor</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_storage_3.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_storage_3.py</a></td></tr>
<tr><td>27</td><td>tensor_storage_casts.py</td><td>přetypování datové struktury se zdrojem dat pro tenzor</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_storage_casts.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_storage_casts.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>28</td><td>tensor_slice_operation_1.py</td><td>konstrukce řezu z&nbsp;tenzoru prvního řádu</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_slice_operation_1.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_slice_operation_1.py</a></td></tr>
<tr><td>29</td><td>tensor_slice_operation_2.py</td><td>konstrukce řezu z&nbsp;tenzoru druhého řádu (přes řádky a sloupce)</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_slice_operation_2.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_slice_operation_2.py</a></td></tr>
<tr><td>30</td><td>tensor_slice_operation_3.py</td><td>konstrukce řezu s&nbsp;jeho následnou modifikací</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_slice_operation_3.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_slice_operation_3.py</a></td></tr>
<tr><td>31</td><td>tensor_slice_operation_4.py</td><td>konstrukce řezu s&nbsp;jeho následnou modifikací (odlišné operace od předchozího příkladu)</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_slice_operation_4.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_slice_operation_4.py</a></td></tr>
<tr><td>32</td><td>tensor_is_slice.py</td><td>test základních vlastností řezů</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_is_slice.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_is_slice.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>33</td><td>tensor_stride_1.py</td><td>význam atributů <strong>stride</strong> a <strong>storage_offset</strong></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_stride_1.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_stride_1.py</a></td></tr>
<tr><td>34</td><td>tensor_stride_2.py</td><td>význam atributů <strong>stride</strong> a <strong>storage_offset</strong></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_stride_2.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_stride_2.py</a></td></tr>
<tr><td>35</td><td>tensor_stride_3.py</td><td>význam atributů <strong>stride</strong> a <strong>storage_offset</strong></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_stride_3.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_stride_3.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>36</td><td>tensor_reshape.py</td><td>změna tvaru tenzoru operací <strong>reshape</strong></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_reshape.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_reshape.py</a></td></tr>
<tr><td>37</td><td>tensor_reshape_2.py</td><td>změna tvaru tenzoru operací <strong>reshape</strong></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_reshape_2.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_reshape_2.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>38</td><td>tensor_narrow_operation_1.py</td><td>operace nad celým tenzorem typu <i>narrow</i>, první ukázka</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_narrow_operation_1.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_narrow_operation_1.py</a></td></tr>
<tr><td>39</td><td>tensor_narrow_operation_1_B.py</td><td>operace nad celým tenzorem typu <i>narrow</i>, první ukázka přepsaná do volání metody</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_narrow_operation_1_B.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_narrow_operation_1_B.py</a></td></tr>
<tr><td>40</td><td>tensor_narrow_operation_2.py</td><td>operace nad celým tenzorem typu <i>narrow</i>, druhá ukázka</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_narrow_operation_2.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_narrow_operation_2.py</a></td></tr>
<tr><td>41</td><td>tensor_narrow_operation_2_B.py</td><td>operace nad celým tenzorem typu <i>narrow</i>, druhá ukázka přepsaná do volání metody</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_narrow_operation_2_B.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_narrow_operation_2_B.py</a></td></tr>
<tr><td>42</td><td>tensor_narrow_operation_3.py</td><td>operace nad celým tenzorem typu <i>narrow</i>, třetí ukázka</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_narrow_operation_3.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_narrow_operation_3.py</a></td></tr>
<tr><td>43</td><td>tensor_narrow_operation_3_B.py</td><td>operace nad celým tenzorem typu <i>narrow</i>, třetí ukázka přepsaná do volání metody</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_narrow_operation_3_B.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_narrow_operation_3_B.py</a></td></tr>
<tr><td>44</td><td>tensor_narrow_operation_4.py</td><td>přepis původní matice přes pohled na ni (<i>view</i>)</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_narrow_operation_4.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_narrow_operation_4.py</a></td></tr>
<tr><td>45</td><td>tensor_narrow_operation_5.py</td><td>přepis původní matice přes pohled na ni (<i>view</i>)<i>narrow</i>, třetí ukázka</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_narrow_operation_5.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_narrow_operation_5.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>46</td><td>tensor_operator_add.py</td><td>součet dvou tenzorů prvek po prvku</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_operator_add.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_operator_add.py</a></td></tr>
<tr><td>47</td><td>tensor_operator_sub.py</td><td>rozdíl dvou tenzorů prvek po prvku</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_operator_sub.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_operator_sub.py</a></td></tr>
<tr><td>48</td><td>tensor_operator_mul.py</td><td>součin dvou tenzorů prvek po prvku</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_operator_mul.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_operator_mul.py</a></td></tr>
<tr><td>49</td><td>tensor_operator_div.py</td><td>podíl dvou tenzorů prvek po prvku</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_operator_div.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_operator_div.py</a></td></tr>
<tr><td>50</td><td>tensor_dot_product.py</td><td>skalární součin dvou tenzorů prvního řádu</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_dot_product.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_dot_product.py</a></td></tr>
<tr><td>50</td><td>tensor_operator_matmul.py</td><td>maticové násobení (dvou tenzorů druhého řádu)</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_operator_matmul.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_operator_matmul.py</a></td></tr>
<tr><td>51</td><td>tensor_operator_matmul_2.py</td><td>maticové násobení (dvou tenzorů druhého řádu)</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_operator_matmul_2.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_operator_matmul_2.py</a></td></tr>
<tr><td>52</td><td>tensor_operator_matmul_3.py</td><td>maticové násobení v&nbsp;případě nekompatibilních tvarů matic</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_operator_matmul_3.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_operator_matmul_3.py</a></td></tr>
<tr><td>53</td><td>tensor_operator_matmul_4.py</td><td>maticové násobení s&nbsp;broadcastingem</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_operator_matmul_4.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_operator_matmul_4.py</a></td></tr>
<tr><td>54</td><td>tensor_operator_matmul_5.py</td><td>násobení vektoru a matice</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_operator_matmul_5.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_operator_matmul_5.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>55</td><td>tensor_broadcast_1.py</td><td>operace <i>broadcast</i> (součin každého prvku tenzoru se skalárem)</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_broadcast_1.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_broadcast_1.py</a></td></tr>
<tr><td>56</td><td>tensor_broadcast_2.py</td><td>operace <i>broadcast</i> (součin tenzoru druhého řádu s vektorem)</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_broadcast_2.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_broadcast_2.py</a></td></tr>
<tr><td>57</td><td>tensor_broadcast_3.py</td><td>operace <i>broadcast</i> (součin vektoru s&nbsp;tenzorem druhého řádu)</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_broadcast_3.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_broadcast_3.py</a></td></tr>
<tr><td>58</td><td>tensor_broadcast_4.py</td><td>operace <i>broadcast</i> (součet tenzorů druhého a třetího řádu)</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_broadcast_4.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_broadcast_4.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>59</td><td>tensor_sparse.py</td><td>konstrukce řídkého tenzoru</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_sparse.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/tensor_sparse.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>60</td><td>activation_function_relu_.py</td><td>aktivační funkce ReLU vypočtená knihovnou NumPy</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/activation_function_relu_numpy.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/activation_function_relu_numpy.py</a></td></tr>
<tr><td>61</td><td>activation_function_relu_pytorch.py</td><td>aktivační funkce ReLU vypočtená knihovnou PyTorch</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/activation_function_relu_pytorch.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/activation_function_relu_pytorch.py</a></td></tr>
<tr><td>62</td><td>activation_function_sigmoid_.py</td><td>aktivační funkce sigmoid vypočtená knihovnou NumPy</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/activation_function_sigmoid_numpy.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/activation_function_sigmoid_numpy.py</a></td></tr>
<tr><td>63</td><td>activation_function_sigmoid_pytorch.py</td><td>aktivační funkce sigmoid vypočtená knihovnou PyTorch</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/activation_function_sigmoid_pytorch.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/activation_function_sigmoid_pytorch.py</a></td></tr>
<tr><td>64</td><td>activation_function_tanh_.py</td><td>aktivační funkce tanh vypočtená knihovnou NumPy</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/activation_function_tanh_numpy.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/activation_function_tanh_numpy.py</a></td></tr>
<tr><td>65</td><td>activation_function_tanh_pytorch.py</td><td>aktivační funkce tanh vypočtená knihovnou PyTorch</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/activation_function_tanh_pytorch.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/activation_function_tanh_pytorch.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>66</td><td>make_circles.py</td><td>vygenerování dat pro neuronovou síť funkcí <strong>make_circles</strong></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/make_circles.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/make_circles.py</a></td></tr>
<tr><td>67</td><td>make_circles_labels.py</td><td>vizualizace dat společně s&nbsp;jejich skupinou (ohodnocením)</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/make_circles_labels.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/make_circles_labels.py</a></td></tr>
<tr><td>68</td><td>make_more_noise_circles.py</td><td>získání náhodnějších dat funkcí <strong>make_circles</strong></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/make_more_noise_circles.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/make_more_noise_circles.py</a></td></tr>
<tr><td>69</td><td>make_data_set.py</td><td>náhodné rozdělení datové sady funkcí <strong>train_test_split</strong></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/make_data_set.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/make_data_set.py</a></td></tr>
<tr><td>70</td><td>prepare_for_training.py</td><td>konverze původních dat z&nbsp;n-dimenzionálních polí do tenzorů</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/prepare_for_training.py">https://github.com/tisnik/most-popular-python-libs/blob/master/PyTorch/prepare_for_training.py</a></td></tr>
</table>



<p><a name="k20"></a></p>
<h2 id="k20">20. Odkazy na Internetu</h2>

<ol>

<li>Seriál Programovací jazyk Lua na Rootu:<br />
<a href="https://www.root.cz/serialy/programovaci-jazyk-lua/">https://www.root.cz/serialy/programovaci-jazyk-lua/</a>
</li>

<li>PDM: moderní správce balíčků a virtuálních prostředí Pythonu:<br />
<a href="https://www.root.cz/clanky/pdm-moderni-spravce-balicku-a-virtualnich-prostredi-pythonu/">https://www.root.cz/clanky/pdm-moderni-spravce-balicku-a-virtualnich-prostredi-pythonu/</a>
</li>

<li>PyTorch Tutorial: Building a Simple Neural Network From Scratch<br />
<a href="https://www.datacamp.com/tutorial/pytorch-tutorial-building-a-simple-neural-network-from-scratch">https://www.datacamp.com/tutorial/pytorch-tutorial-building-a-simple-neural-network-from-scratch</a>
</li>

<li>Interní reprezentace numerických hodnot: od skutečného počítačového pravěku po IEEE 754–2008:<br />
<a href="https://www.root.cz/clanky/interni-reprezentace-numerickych-hodnot-od-skutecneho-pocitacoveho-praveku-po-ieee-754-2008/">https://www.root.cz/clanky/interni-reprezentace-numerickych-hodnot-od-skutecneho-pocitacoveho-praveku-po-ieee-754-2008/</a>
</li>

<li>Interní reprezentace numerických hodnot: od skutečného počítačového pravěku po IEEE 754–2008 (dokončení):<br />
<a href="https://www.root.cz/clanky/interni-reprezentace-numerickych-hodnot-od-skutecneho-pocitacoveho-praveku-po-ieee-754-2008-dokonceni/">https://www.root.cz/clanky/interni-reprezentace-numerickych-hodnot-od-skutecneho-pocitacoveho-praveku-po-ieee-754-2008-dokonceni/</a>
</li>

<li>Brain Floating Point &ndash; nový formát uložení čísel pro strojové učení a chytrá čidla:<br />
<a href="https://www.root.cz/clanky/brain-floating-point-ndash-novy-format-ulozeni-cisel-pro-strojove-uceni-a-chytra-cidla/">https://www.root.cz/clanky/brain-floating-point-ndash-novy-format-ulozeni-cisel-pro-strojove-uceni-a-chytra-cidla/</a>
</li>

<li>Stránky projektu PyTorch:<br />
<a href="https://pytorch.org/">https://pytorch.org/</a>
</li>

<li>Informace o instalaci PyTorche:<br />
<a href="https://pytorch.org/get-started/locally/">https://pytorch.org/get-started/locally/</a>
</li>

<li>Tenzor (Wikipedia):<br />
<a href="https://cs.wikipedia.org/wiki/Tenzor">https://cs.wikipedia.org/wiki/Tenzor</a>
</li>

<li>Introduction to Tensors:<br />
<a href="https://www.youtube.com/watch?v=uaQeXi4E7gA">https://www.youtube.com/watch?v=uaQeXi4E7gA</a>
</li>

<li>Introduction to Tensors: Transformation Rules:<br />
<a href="https://www.youtube.com/watch?v=j6DazQDbEhQ">https://www.youtube.com/watch?v=j6DazQDbEhQ</a>
</li>

<li>Tensor Attributes:<br />
<a href="https://pytorch.org/docs/stable/tensor_attributes.html">https://pytorch.org/docs/stable/tensor_attributes.html</a>
</li>

<li>Tensors Explained Intuitively: Covariant, Contravariant, Rank :<br />
<a href="https://www.youtube.com/watch?v=CliW7kSxxWU">https://www.youtube.com/watch?v=CliW7kSxxWU</a>
</li>

<li>What is the relationship between PyTorch and Torch?:<br />
<a href="https://stackoverflow.com/questions/44371560/what-is-the-relationship-between-pytorch-and-torch">https://stackoverflow.com/questions/44371560/what-is-the-relationship-between-pytorch-and-torch</a>
</li>

<li>What is a tensor anyway?? (from a mathematician):<br />
<a href="https://www.youtube.com/watch?v=K7f2pCQ3p3U">https://www.youtube.com/watch?v=K7f2pCQ3p3U</a>
</li>

<li>Visualization of tensors - part 1 :<br />
<a href="https://www.youtube.com/watch?v=YxXyN2ifK8A">https://www.youtube.com/watch?v=YxXyN2ifK8A</a>
</li>

<li>Visualization of tensors - part 2A:<br />
<a href="https://www.youtube.com/watch?v=A95jdIuUUW0">https://www.youtube.com/watch?v=A95jdIuUUW0</a>
</li>

<li>Visualization of tensors - part 2B:<br />
<a href="https://www.youtube.com/watch?v=A95jdIuUUW0">https://www.youtube.com/watch?v=A95jdIuUUW0</a>
</li>

<li>What the HECK is a Tensor?!?:<br />
<a href="https://www.youtube.com/watch?v=bpG3gqDM80w">https://www.youtube.com/watch?v=bpG3gqDM80w</a>
</li>

<li>Stránka projektu Torch<br />
<a href="http://torch.ch/">http://torch.ch/</a>
</li>

<li>Torch na GitHubu (několik repositářů)<br />
<a href="https://github.com/torch">https://github.com/torch</a>
</li>

<li>Torch (machine learning), Wikipedia<br />
<a href="https://en.wikipedia.org/wiki/Torch_%28machine_learning%29">https://en.wikipedia.org/wiki/Torch_%28machine_learning%29</a>
</li>

<li>Torch Package Reference Manual<br />
<a href="https://github.com/torch/torch7/blob/master/README.md">https://github.com/torch/torch7/blob/master/README.md</a>
</li>

<li>Torch Cheatsheet<br />
<a href="https://github.com/torch/torch7/wiki/Cheatsheet">https://github.com/torch/torch7/wiki/Cheatsheet</a>
</li>

<li>An Introduction to Tensors<br />
<a href="https://math.stackexchange.com/questions/10282/an-introduction-to-tensors">https://math.stackexchange.com/questions/10282/an-introduction-to-tensors</a>
</li>

<li>Differences between a matrix and a tensor<br />
<a href="https://math.stackexchange.com/questions/412423/differences-between-a-matrix-and-a-tensor">https://math.stackexchange.com/questions/412423/differences-between-a-matrix-and-a-tensor</a>
</li>

<li>Qualitatively, what is the difference between a matrix and a tensor?<br />
<a href="https://math.stackexchange.com/questions/1444412/qualitatively-what-is-the-difference-between-a-matrix-and-a-tensor?">https://math.stackexchange.com/questions/1444412/qualitatively-what-is-the-difference-between-a-matrix-and-a-tensor?</a>
</li>

<li>Tensors for Neural Networks, Clearly Explained!!!:<br />
<a href="https://www.youtube.com/watch?v=L35fFDpwIM4">https://www.youtube.com/watch?v=L35fFDpwIM4</a>
</li>

<li>Tensor Processing Unit:<br />
<a href="https://en.wikipedia.org/wiki/Tensor_Processing_Unit">https://en.wikipedia.org/wiki/Tensor_Processing_Unit</a>
</li>

<li>Třída Storage:<br />
<a href="http://docs.pytorch.wiki/en/storage.html">http://docs.pytorch.wiki/en/storage.html</a>
</li>

<li>Funkce torch.dot<br />
<a href="https://pytorch.org/docs/stable/generated/torch.dot.html#torch.dot">https://pytorch.org/docs/stable/generated/torch.dot.html#torch.dot</a>
</li>

<li>Funkce torch.narrow<br />
<a href="https://pytorch.org/docs/stable/generated/torch.narrow.html">https://pytorch.org/docs/stable/generated/torch.narrow.html</a>
</li>

<li>Funkce torch.matmul<br />
<a href="https://pytorch.org/docs/stable/generated/torch.matmul.html">https://pytorch.org/docs/stable/generated/torch.matmul.html</a>
</li>

<li>Funkce torch.reshape<br />
<a href="https://pytorch.org/docs/stable/generated/torch.reshape.html">https://pytorch.org/docs/stable/generated/torch.reshape.html</a>
</li>

<li>Funkce torch.arange<br />
<a href="https://pytorch.org/docs/stable/generated/torch.arange.html">https://pytorch.org/docs/stable/generated/torch.arange.html</a>
</li>

<li>Funkce torch.range<br />
<a href="https://pytorch.org/docs/stable/generated/torch.range.html">https://pytorch.org/docs/stable/generated/torch.range.html</a>
</li>

<li>Třída torch.Tensor<br />
<a href="https://pytorch.org/docs/stable/tensors.html">https://pytorch.org/docs/stable/tensors.html</a>
</li>

<li>Atributy tenzorů<br />
<a href="https://pytorch.org/docs/stable/tensor_attributes.html">https://pytorch.org/docs/stable/tensor_attributes.html</a>
</li>

<li>Pohledy vytvořené nad tenzory<br />
<a href="https://pytorch.org/docs/stable/tensor_view.html">https://pytorch.org/docs/stable/tensor_view.html</a>
</li>

<li>Broadcasting v&nbsp;knihovně <br />
<a href="https://.org/doc/stable/user/basics.broadcasting.html">https://numpy.org/doc/stable/user/basics.broadcasting.html</a>
</li>

<li>Broadcasting semantics (v&nbsp;knihovně PyTorch)<br />
<a href="https://pytorch.org/docs/stable/notes/broadcasting.html">https://pytorch.org/docs/stable/notes/broadcasting.html</a>
</li>

<li>Dot Product In Physics: What Is The Physical Meaning of It?<br />
<a href="https://profoundphysics.com/dot-product-in-physics-what-is-the-physical-meaning-of-it/">https://profoundphysics.com/dot-product-in-physics-what-is-the-physical-meaning-of-it/</a>
</li>

<li>scikit-learn: Getting Started<br />
<a href="https://scikit-learn.org/stable/getting_started.html">https://scikit-learn.org/stable/getting_started.html</a>
</li>

<li>Support Vector Machines<br />
<a href="https://scikit-learn.org/stable/modules/svm.html">https://scikit-learn.org/stable/modules/svm.html</a>
</li>

<li>Use Deep Learning to Detect Programming Languages<br />
<a href="http://searene.me/2017/11/26/use-neural-networks-to-detect-programming-languages/">http://searene.me/2017/11/26/use-neural-networks-to-detect-programming-languages/</a>
</li>

<li>Data pro neuronové sítě<br />
<a href="http://archive.ics.uci.edu/ml/index.php">http://archive.ics.uci.edu/ml/index.php</a>
</li>

<li>Feedforward neural network<br />
<a href="https://en.wikipedia.org/wiki/Feedforward_neural_network">https://en.wikipedia.org/wiki/Feedforward_neural_network</a>
</li>

<li>Biologické algoritmy (4) - Neuronové sítě<br />
<a href="https://www.root.cz/clanky/biologicke-algoritmy-4-neuronove-site/">https://www.root.cz/clanky/biologicke-algoritmy-4-neuronove-site/</a>
</li>

<li>Biologické algoritmy (5) - Neuronové sítě<br />
<a href="https://www.root.cz/clanky/biologicke-algoritmy-5-neuronove-site/">https://www.root.cz/clanky/biologicke-algoritmy-5-neuronove-site/</a>
</li>

<li>Umělá neuronová síť (Wikipedia)<br />
<a href="https://cs.wikipedia.org/wiki/Um%C4%9Bl%C3%A1_neuronov%C3%A1_s%C3%AD%C5%A5">https://cs.wikipedia.org/wiki/Um%C4%9Bl%C3%A1_neuronov%C3%A1_s%C3%AD%C5%A5</a>
</li>

<li>AI vs Machine Learning (Youtube)<br />
<a href="https://www.youtube.com/watch?v=4RixMPF4xis">https://www.youtube.com/watch?v=4RixMPF4xis</a>
</li>

<li>Machine Learning | What Is Machine Learning? | Introduction To Machine Learning | 2024 | Simplilearn (Youtube)<br />
<a href="https://www.youtube.com/watch?v=ukzFI9rgwfU">https://www.youtube.com/watch?v=ukzFI9rgwfU</a>
</li>

<li>A Gentle Introduction to Machine Learning (Youtube)<br />
<a href="https://www.youtube.com/watch?v=Gv9_4yMHFhI">https://www.youtube.com/watch?v=Gv9_4yMHFhI</a>
</li>

<li>Machine Learning vs Deep Learning<br />
<a href="https://www.youtube.com/watch?v=q6kJ71tEYqM">https://www.youtube.com/watch?v=q6kJ71tEYqM</a>
</li>

<li>Umělá inteligence (slajdy)<br />
<a href="https://slideplayer.cz/slide/12119218/">https://slideplayer.cz/slide/12119218/</a>
</li>

<li>Úvod do umělé inteligence<br />
<a href="https://slideplayer.cz/slide/2505525/">https://slideplayer.cz/slide/2505525/</a>
</li>

<li>Umělá inteligence I / Artificial Intelligence I<br />
<a href="https://ktiml.mff.cuni.cz/~bartak/ui/">https://ktiml.mff.cuni.cz/~bartak/ui/</a>
</li>

</ol>



<p></p><p></p>
<p><small>Autor: <a href="http://www.fit.vutbr.cz/~tisnovpa">Pavel Tišnovský</a> &nbsp; 2024</small></p>
</body>
</html>

