<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
        "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html>
<head>
<title>Využití knihovny scikit-learn pro zpracování a analýzu přirozeného jazyka (NLP), 3.část</title>
<meta name="Author" content="Pavel Tisnovsky" />
<meta name="Generator" content="vim" />
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<style type="text/css">
         body {color:#000000; background:#ffffff;}
         h1  {font-family: arial, helvetica, sans-serif; color:#ffffff; background-color:#c00000; text-align:center; padding-left:1em}
         h2  {font-family: arial, helvetica, sans-serif; color:#ffffff; background-color:#0000c0; padding-left:1em; text-align:left}
         h3  {font-family: arial, helvetica, sans-serif; color:#000000; background-color:#c0c0c0; padding-left:1em; text-align:left}
         h4  {font-family: arial, helvetica, sans-serif; color:#000000; background-color:#e0e0e0; padding-left:1em; text-align:left}
         a   {font-family: arial, helvetica, sans-serif;}
         li  {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify; width:450px;}
         ol  {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify; width:450px;}
         ul  {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify; width:450px;}
         p   {font-family: arial, helvetica, sans-serif; color:#000000; text-align:justify;}
         pre {background:#e0e0e0}
</style>
</head>

<body>

<h1>Využití knihovny scikit-learn pro zpracování a analýzu přirozeného jazyka (NLP), 3.část</h1>

<h3>Pavel Tišnovský</h3>

<p></p>

<h1>Úvodník</h1>

<p>Budeme se zabývat velmi často řešenou úlohou: analýzou, zda je předložený text spam nebo se jedná o jiný typ textu (ham). Při vektorizaci textu využijeme takzvané n-gramy.</p>



<h2>Obsah</h2>

<p><a href="#k01">1. Využití knihovny scikit-learn pro zpracování a analýzu přirozeného jazyka (NLP), 3.část</a></p>
<p><a href="#k02">2. Získání datové sady a slovníku se slopslovy</a></p>
<p><a href="#k03">3. Pokus o načtení datové sady do datového rámce (<i>dataframe</i>)</a></p>
<p><a href="#k04">4. Specifikace kódování znaků v&nbsp;CSV souboru při jeho načítání do datového rámce</a></p>
<p><a href="#k05">5. Zjištění statistiky o ohodnocení textu</a></p>
<p><a href="#k06">6. Sloupce s&nbsp;očekávanými výsledky i s&nbsp;vlastním textem SMS</a></p>
<p><a href="#k07">7. Vektorizace dat SMSek s&nbsp;výpisem výsledného slovníku</a></p>
<p><a href="#k08">8. Výpis obsahu slovníku ve více sloupcích</a></p>
<p><a href="#k09">9. Vektorizace všech SMSek z&nbsp;datové sady</a></p>
<p><a href="#k10">10. Preprocesing textů z&nbsp;SMSsek s&nbsp;vyčištěním dat</a></p>
<p><a href="#k11">11. Vektorizace textových dat po jejich filtraci</a></p>
<p><a href="#k12">12. Trénink a predikce modelu nad vektorizovanými daty založený na třídě <strong>CountVectorizer</strong></a></p>
<p><a href="#k13">13. Trénink a predikce modelu nad vektorizovanými daty založený na třídě <strong>TfidfVectorizer</strong></a></p>
<p><a href="#k14">14. Jak pracovat s&nbsp;kontextem: řešení založené na n-gramech</a></p>
<p><a href="#k15">15. Vektorizace textových dat s použitím n-gramů o délce 1-2 slov s&nbsp;výpisem výsledného slovníku</a></p>
<p><a href="#k16">16. Trénink a predikce modelu s&nbsp;využitím třídy <strong>CountVectorizer</strong> při vektorizaci n-gramů</a></p>
<p><a href="#k17">17. Trénink a predikce modelu s&nbsp;využitím třídy <strong>TfidfVectorizer</strong> při vektorizaci n-gramů</a></p>
<p><a href="#k18">18. Zjištění vlivu minimální a maximální délky n-gramů na kvalitu modelu</a></p>
<p><a href="#k19">19. Repositář s&nbsp;demonstračními příklady</a></p>
<p><a href="#k20">20. Odkazy na Internetu</a></p>



<p><a name="k01"></a></p>
<h2 id="k01">1. Využití knihovny scikit-learn pro zpracování a analýzu přirozeného jazyka (NLP), 3.část</h2>

<p>Již potřetí se dnes vrátíme k&nbsp;problematice zpracování a analýzy
přirozeného jazyka (<i>Natural Language Processing</i> neboli <i>NLP</i>)
v&nbsp;Pythonu s&nbsp;využitím knihovny <i>scikit-learn</i>. Budeme se zabývat
velmi často řešenou úlohou &ndash; a to konkrétně analýzou, zda je předložený
text <i>spam</i> nebo se jedná o jiný typ textu (označuje se termínem
<i>ham</i>). A aby nebyl objem zpracovávaných dat obrovský, budeme tuto analýzu
provádět nad datovou sadou obsahující anglicky psané SMSky, tj.&nbsp;relativně
krátké texty. Jedná se tedy o zadání, které se do jisté míry podobá úloze,
kterou jsme již řešili, konkrétně k&nbsp;vytvoření modelu pro zjištění, zda
jsou tweety laděny pozitivně, negativně nebo neutrálně.</p>

<p>Při řešení dnes použijeme novou techniku vektorizace, která není založena na
vektorizaci jednotlivých slov, ale na zjištění, jaké kombinace slov se
v&nbsp;textu vyskytují. Vektorizovat se budou právě tyto kombinace slov,
tj.&nbsp;například &bdquo;dobrý den&ldquo; by byl samostatný záznam ve
slovníku. Díky tomu bude možné lépe podchytit kontext a (možná) tak zlepšit
predikční schopnosti modelu (i když, jak uvidíme dále, pro krátké SMSky od
mnoha autorů to nebude tak výrazné). Ovšem v&nbsp;oboru NLP se s&nbsp;n-gramy
pracuje poměrně často.</p>

<div class="rs-tip-major">Poznámka: udává se, že nejúspěšnější je použití
<i>bigramů</i>, tj.&nbsp;dvojic slov.</div>


<p><a name="k02"></a></p>
<h2 id="k02">2. Získání datové sady a slovníku se slopslovy</h2>

<p>V&nbsp;prvním kroku si, jak je již v&nbsp;tomto seriálu zvykem, stáhneme
datovou sadu, kterou použijeme pro trénink a validaci modelu. Tato datová sada
je tvořena jediným souborem ve formátu CSV, který je dostupný na adrese <a href="https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset?resource=download">https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset?resource=download</a>
(ve skutečnosti je nutné si projít potvrzovacím dialogem, na druhou stranu celá
platforma Kaggle nabízí i další užitečné materiály a dokonce i celé Jupyter
notebooky, takže je vhodné tento krok &bdquo;přetrpět&ldquo;).</p>

<a href="https://www.root.cz/obrazek/1155735/"><img src="https://i.iinfo.cz/images/651/scikit-learn-nlp-3-1-prev.webp" class="image-1155735" width="370" height="155" data-prev-filename="https://i.iinfo.cz/images/651/scikit-learn-nlp-3-1-prev.webp" data-prev-width="370" data-prev-height="155" data-large-filename="https://i.iinfo.cz/images/651/scikit-learn-nlp-3-1-large.webp" data-large-width="720" data-large-height="301" alt="&nbsp;" title="Autor: tisnik, podle licence: &lt;a href=&quot;http://en.wikipedia.org/wiki/Rights_Managed&quot;&gt;Rights Managed&lt;/a&gt;" /></a>
<p><i>Obrázek 1: Obsah staženého souboru CSV po jeho importu do spreadsheetu.</i></p>

<p>Dále budeme v&nbsp;některých skriptech vyžadovat slovník s&nbsp;takzvanými
stopslovy (<i>stopwords</i>), o nichž jsme se již zmínili předminule a minule.
Jedná se o slova, která v&nbsp;textu nenesou žádnou skutečně užitečnou
informaci a mohou být odfiltrována. Tento slovník stáhneme jednoduchým skriptem
založeným na knihovně NLTK. Vše se stáhne do adresáře
<strong>ntkl_data</strong> umístěného v&nbsp;domovském adresáři:</p>

<pre>
<i># Stažení slovníku, který bude použit pro předzpracování textu v dalších</i>
<i># demonstračních příkladech.</i>
&nbsp;
import nltk
&nbsp;
<i># tento příkaz zajistí stažení příslušných datových souborů</i>
nltk.download("stopwords")
</pre>



<p><a name="k03"></a></p>
<h2 id="k03">3. Pokus o načtení datové sady do datového rámce (<i>dataframe</i>)</h2>

<p>Již při analýze datové sady s&nbsp;tweety o dopravcích jsme celý soubor ve
formátu CSV nejprve (před jeho dalším zpracováním) načetli do datového rámce
(<i>dataframe</i>) s&nbsp;využitím knihovny Pandas. Toto načtení se obešlo bez
problémů, takže se pokusme o provedení naprosto stejné operace, ovšem nyní nad
souborem <strong>spam.csv</strong> získaným v&nbsp;rámci <a href="#k02">předchozí kapitoly</a>:</p>

<pre>
<i># Pokus o načtení datové sady a zjištění základních údajů</i>
&nbsp;
import pandas as pd 
&nbsp;
<i># načtení tabulky do datového rámce</i>
spam = pd.read_csv("spam.csv")
&nbsp;
<i># základní informace o datovém rámci</i>
print(spam.describe())
</pre>

<p>Nyní ovšem tato operace vyvolá výjimku, která by měla vypadat zhruba
následovně:</p>

<pre>
Traceback (most recent call last):
  File "/home/ptisnovs/xy/205_spam_read.py", line 6, in &lt;module&gt;
    spam = pd.read_csv("spam.csv")
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ptisnovs/.local/lib/python3.11/site-packages/pandas/io/parsers/readers.py", line 1026, in read_csv
    return _read(filepath_or_buffer, kwds)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ptisnovs/.local/lib/python3.11/site-packages/pandas/io/parsers/readers.py", line 620, in _read
    parser = TextFileReader(filepath_or_buffer, **kwds)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ptisnovs/.local/lib/python3.11/site-packages/pandas/io/parsers/readers.py", line 1620, in __init__
    self._engine = self._make_engine(f, self.engine)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ptisnovs/.local/lib/python3.11/site-packages/pandas/io/parsers/readers.py", line 1898, in _make_engine
    return mapping[engine](f, **self.options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ptisnovs/.local/lib/python3.11/site-packages/pandas/io/parsers/c_parser_wrapper.py", line 93, in __init__
    self._reader = parsers.TextReader(src, **kwds)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "parsers.pyx", line 574, in pandas._libs.parsers.TextReader.__cinit__
  File "parsers.pyx", line 663, in pandas._libs.parsers.TextReader._get_header
  File "parsers.pyx", line 874, in pandas._libs.parsers.TextReader._tokenize_rows
  File "parsers.pyx", line 891, in pandas._libs.parsers.TextReader._check_tokenize_status
  File "parsers.pyx", line 2053, in pandas._libs.parsers.raise_parser_error
UnicodeDecodeError: 'utf-8' codec can't decode bytes in position 606-607: invalid continuation byte
</pre>

<div class="rs-tip-major">Poznámka: tato výjimka možná není příliš čitelná,
protože si musíme uvědomit, že knihovnu Pandas používají i lidé, pro které není
vývoj primárním cílem práce. Ovšem z&nbsp;posledního řádku je patrné, jaký
problém nastal &ndash; v&nbsp;souboru CSV se objevují sekvence bajtů, které
není možné dekódovat jako UTF-8 znak. Konkrétně se jedná o tento úsek:
&bdquo;å£1.5&ldquo;0, ale s&nbsp;velkou pravděpodobností takových problémů
nalezneme větší množství.</div>


<p><a name="k04"></a></p>
<h2 id="k04">4. Specifikace kódování znaků v&nbsp;CSV souboru při jeho načítání do datového rámce</h2>

<p>Pokusme se nyní o načtení souboru ve formátu CSV do datového rámce se
specifikací kódování znaků. Již víme, že soubor, který jsme stáhli, nepoužívá
UTF-8 a vlastně i nepřímo víme, že se nejedná o čisté ASCII (to je totiž
podmnožinou UTF-8). Pokusme se tedy použít nějaké osmibitové kódování, kdy
Pandas ve skutečnosti nemůže zjistit žádné chyby &ndash; každý bajt je převeden
do jednoho znaku, ať již obsahuje jakoukoli hodnotu. Případné
&bdquo;paznaky&ldquo; později odfiltrujeme. Vzhledem k&nbsp;tomu, že kódování
jen odhadujeme, zkusme například zadat <strong>latin1</strong>, což je jedno ze
základních osmibitových rozšíření původní sedmibitové znakové sady ASCII:</p>

<pre>
<i># Načtení datové sady a zjištění základních údajů o načteném datovém rámci</i>
&nbsp;
import pandas as pd 
&nbsp;
<i># načtení tabulky do datového rámce, specifikace kódování souboru</i>
spam = pd.read_csv("spam.csv", <strong>encoding="latin1"</strong>)
&nbsp;
<i># základní informace o datovém rámci</i>
print(spam.describe())
</pre>

<p>Nyní se soubor načte a vytvoří se z&nbsp;něho kýžený datový rámec.
Z&nbsp;jeho popisu je patrné, že obsahuje dva skutečné datové sloupce nazvané
<strong>v1</strong> a <strong>v2</strong> a taktéž je patrné, že některé řádky
(ale je jich jen 12 z&nbsp;celkového počtu 5572) jsou uloženy tak netypickým
způsobem, že byl programový kód načítající CSV při dekódování tak zmaten, že
předpokládal, že věta obsahuje oddělovač jednotlivých buněk (což je ostatně pro
formát CSV typické &ndash; tento formát nabízí velkou volnost a není tak dobře
přenositelný, jak bychom si přáli):</p>

<pre>
          v1                      v2  ...             Unnamed: 3 Unnamed: 4
count   5572                    5572  ...                     12          6
unique     2                    5169  ...                     10          5
top      ham  Sorry, I'll call later  ...   MK17 92H. 450Ppw 16"    GNT:-)"
freq    4825                      30  ...                      2          2
&nbsp;
[4 rows x 5 columns]
</pre>

<p>Pro zajímavost se na jeden z&nbsp;problémových řádků podívejme:</p>

<pre>
spam,"Your free ringtone is waiting to be collected. Simply text the password \MIX\"" to 85069 to verify. Get Usher and Britney. FML",  PO Box 5249," MK17 92H. 450Ppw 16""",
</pre>

<p>To je skutečně pro většinou &bdquo;načítačů&ldquo; CSV matoucí.</p>


<p><a name="k05"></a></p>
<h2 id="k05">5. Zjištění statistiky o ohodnocení textu</h2>

<p>Další postup již vlastně známe. Zjistíme, jaké údaje jsou zapsány
v&nbsp;prvním sloupci datového rámce. Předpokladem přitom je, že tento sloupec
bude obsahovat pouze dvě možné hodnoty, konkrétně &bdquo;ham&ldquo; nebo
&bdquo;spam&ldquo;. Údaje o tom, jaké hodnoty jsou v&nbsp;tomto sloupci uloženy
a navíc i informace o počtu těchto hodnot (frekvenci) zjistíme s&nbsp;využitím
metody <strong>value_counts()</strong> zavolané nad objektem představujícím
sloupec <strong>v1</strong> (což je objekt typu datové řady &ndash;
<strong>Serie</strong>)</p>

<pre>
<i># Načtení datové sady do datového rámce a zjištění statistiky o ohodnocení textu</i>
&nbsp;
import pandas as pd 
&nbsp;
<i># načtení tabulky do datového rámce, specifikace kódování souboru</i>
spam = pd.read_csv("spam.csv", encoding="latin1")
&nbsp;
<i># početm spamů a hamů</i>
print(<strong>spam.v1.value_counts()</strong>)
</pre>

<p>Z&nbsp;výsledků je patrné, že v&nbsp;tomto sloupci jsou skutečně (korektně)
uloženy pouze dvě možné hodnoty. Navíc je patrné, že více SMS není ohodnoceno
jako spam (spamu je pouze 15%), což v&nbsp;důsledku ovlivní i trénink modelu a
vypočtené matice záměn (viz další text):</p>

<pre>
v1
ham     4825
spam     747
Name: count, dtype: int64
</pre>



<p><a name="k06"></a></p>
<h2 id="k06">6. Sloupce s&nbsp;očekávanými výsledky i s&nbsp;vlastním textem SMS</h2>

<p>Vzhledem k&nbsp;tomu, že sloupce datové sady (přesněji řečeno <i>první
dva</i> sloupce) mají taková jména, která jsou v&nbsp;jazyce Python platnými
identifikátory, můžeme snadno přečíst hodnoty uložené v&nbsp;těchto sloupcích
&ndash; názvy těchto sloupců jsou totiž současně i názvy atributů datového
rámce <strong>spam</strong>. První sloupec přitom obsahuje očekávané výsledky
(ohodnocení textu) a druhý sloupec vlastní, nijak nezpracovaný text SMS:</p>

<pre>
<i># hodnocení (spam/ham)</i>
labels = <strong>spam.v1.values</strong>
&nbsp;
<i># vlastní text SMS</i>
features = <strong>spam.v2.values</strong>
</pre>

<p>Další skript zobrazí jak několik vybraných hodnot z&nbsp;obou sloupců, tak i
počet zde uložených hodnot:</p>

<pre>
<i># Načtení datové sady, charakteristiky sloupců s návěstím a vlastním textem</i>
&nbsp;
import pandas as pd 
&nbsp;
<i># načtení tabulky do datového rámce, specifikace kódování souboru</i>
spam = pd.read_csv("spam.csv", encoding="latin1")
&nbsp;
<i># hodnocení (spam/ham)</i>
labels = <strong>spam.v1.values</strong>
&nbsp;
<i># vlastní text SMS</i>
features = <strong>spam.v2.values</strong>
&nbsp;
<i># hodnoty použité později pro trénink modelu</i>
print("Labels:")
print(labels)
print("Number of labels:", len(labels))
print()
&nbsp;
print("Features:")
print(features)
print("Number of features:", len(features))
</pre>

<p>První sloupec (včetně počtu hodnot):</p>

<pre>
Labels:
['ham' 'ham' 'spam' ... 'ham' 'ham' 'ham']
Number of labels: 5572
</pre>

<p>Druhý sloupec:</p>

<pre>
Features:
['Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...'
 'Ok lar... Joking wif u oni...'
 "Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&amp;C's apply 08452810075over18's"
 ... 'Pity, * was in mood for that. So...any other suggestions?'
 "The guy did some bitching but I acted like i'd be interested in buying something else next week and he gave it to us for free"
 'Rofl. Its true to its name']
Number of features: 5572
</pre>



<p><a name="k07"></a></p>
<h2 id="k07">7. Vektorizace dat SMSek s&nbsp;výpisem výsledného slovníku</h2>

<p>Nyní provedeme vektorizaci SMSek, tj.&nbsp;textových hodnot uložených ve
druhém sloupci nazvaném <strong>v2</strong>. Samotnou základní vektorizaci již
dobře známe. Pro tento účel zpočátku použijeme třídu
<strong>CountVectorizer</strong>, zkonstruuje slovník z&nbsp;celého korpusu
(tedy ze všech textů). Tento slovník lze z&nbsp;výsledného objektu získat
metodou <strong>get_feature_names_out</strong>. A podobně lze získat řídkou
matici s&nbsp;frekvencemi jednotlivých slov.</p>

<p>Nás však nejprve bude zajímat pouze samotný slovník, tj.&nbsp;jaká slova
jsou v&nbsp;něm uložena:</p>

<pre>
<i># Vektorizace textových dat, výpis výsledného slovníku</i>
&nbsp;
import pandas as pd 
import re
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import CountVectorizer
&nbsp;
<i># načtení tabulky do datového rámce, specifikace kódování souboru</i>
spam = pd.read_csv("spam.csv", encoding="latin1")
&nbsp;
<i># hodnocení (spam/ham)</i>
labels = spam.v1.values
&nbsp;
<i># vlastní text SMS</i>
features = spam.v2.values
&nbsp;
<i># hodnoty použité později pro trénink modelu</i>
print("Labels:")
print(labels)
print("Number of labels:", len(labels))
print()
&nbsp;
print("Features:")
print(features)
print("Number of features:", len(features))
print()
&nbsp;
<i># vektorizace textu</i>
vectorizer = CountVectorizer(
    max_features=2500,
    min_df=7, max_df=0.8, stop_words=stopwords.words("english")
)
vectorized_features = vectorizer.fit_transform(features).toarray()
&nbsp;
<i># slova pro dekódování vah</i>
feature_names = vectorizer.get_feature_names_out()
print("Feature names count:", len(feature_names))
print("Feature names:")
for feature_name in feature_names:
    print(feature_name)
</pre>

<p>Z&nbsp;výpisu (zde notně zkráceného) je patrné, že kromě skutečných slov zde
nalezneme čísla a na konci různé &bdquo;paznaky&ldquo;, které možná vznikly až
při exportu SMSek:</p>

<pre>
Feature names count: 1267
Feature names:
00
000
02
03
04
06
0800
08000839402
08000930705
0870
08712460324
08718720201
10
100
1000
10am
10p
11
11mths
12
12hrs
1327
150
150p
150ppm
...
...
...
leaves
leaving
lect
left
leh
lei
lemme
less
lesson
lessons
let
lets
liao
library
life
lift
light
...
...
...
us
use
used
user
usf
using
usual
valentine
valentines
valid
valued
via
video
vikky
visit
vl
voice
...
...
...
xx
xxx
xy
ya
yahoo
yar
yeah
year
years
yep
yes
yesterday
yet
yijue
yo
yr
yrs
yup
ì_
ìï
û_
</pre>

<div class="rs-tip-major">Poznámka: poslední tři údaje ve slovníku skutečně
nereprezentují reálná slova.</div>


<p><a name="k08"></a></p>
<h2 id="k08">8. Výpis obsahu slovníku ve více sloupcích</h2>

<p>Pro zajímavost si ukažme trik použitelný pro tisk obsahu slovníku ve více
sloupcích. Trik spočívá ve výběru každého n-tého prvku (pro čtyři sloupce bude
<i>n=4</i>) postupně s&nbsp;offsetem 0, 1 a 2:</p>

<pre>
columns = 4
&nbsp;
c1 = feature_names[::columns],
c1 = feature_names[1::columns],
c1 = feature_names[2::columns],
c1 = feature_names[3::columns],
</pre>

<p>Tyto struktury by teoreticky mělo být možné vytisknout s&nbsp;využitím
standardní funkce <strong>zip</strong>:</p>

<pre>
columns = 4
&nbsp;
for c1, c2, c3, c4 in zip(
    feature_names[::columns],
    feature_names[1::columns],
    feature_names[2::columns],
    feature_names[3::columns],
):
    ...
    ...
    ...
</pre>

<p>To však nebude plně korektní ve chvíli, kdy budou mít sloupce odlišný počet
prvků, tj.&nbsp;když původní počet prvků ve slovníku nebude dělitelný čtyřmi.
Oprava spočívá v&nbsp;tom, že namísto funkce <strong>zip</strong> použijeme
funkci <strong>zip_longest</strong> naimportovanou ze standardní knihovny
<strong>itertools</strong>:</p>

<pre>
columns = 4
&nbsp;
for c1, c2, c3, c4 in zip_longest(
    feature_names[::columns],
    feature_names[1::columns],
    feature_names[2::columns],
    feature_names[3::columns],
):
    ...
    ...
    ...
</pre>

<p>Upravený skript pro výpočet a zobrazení slovníku bude mít tuto podobu:</p>

<pre>
<i># Vektorizace textových dat, výpis výsledného slovníku</i>
&nbsp;
import pandas as pd
import re
from itertools import zip_longest
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import CountVectorizer
&nbsp;
<i># načtení tabulky do datového rámce, specifikace kódování souboru</i>
spam = pd.read_csv("spam.csv", encoding="latin1")
&nbsp;
<i># hodnocení (spam/ham)</i>
labels = spam.v1.values
&nbsp;
<i># vlastní text SMS</i>
features = spam.v2.values
&nbsp;
<i># hodnoty použité později pro trénink modelu</i>
print("Labels:")
print(labels)
print("Number of labels:", len(labels))
print()
&nbsp;
print("Features:")
print(features)
print("Number of features:", len(features))
print()
&nbsp;
<i># vektorizace textu</i>
vectorizer = CountVectorizer(
    max_features=2500, min_df=7, max_df=0.8, stop_words=stopwords.words("english")
)
vectorized_features = vectorizer.fit_transform(features).toarray()
&nbsp;
<i># slova pro dekódování vah</i>
feature_names = vectorizer.get_feature_names_out()
print("Feature names count:", len(feature_names))
print("Feature names:")
&nbsp;
columns = 4
for c1, c2, c3, c4 in zip_longest(
    feature_names[::columns],
    feature_names[1::columns],
    feature_names[2::columns],
    feature_names[3::columns],
):
    print(f"{c1: &lt;20}{c2: &lt;20}{c3: &lt;20}{c4}")
</pre>

<p>A takto vypadá výsledný slovník, resp.&nbsp;jeho vybraná část při tisku do
čtyř sloupců:</p>

<pre>
Feature names count: 1267
Feature names:
00                  000                 02                  03
04                  06                  0800                08000839402
08000930705         0870                08712460324         08718720201
10                  100                 1000                10am
10p                 11                  11mths              12
12hrs               1327                150                 150p
150ppm              16                  18                  1st
20                  200                 2000                2003
2004                20p                 25                  250
25p                 2day                2lands              2nd
2nite               30                  3030                350
36504               3g                  4u                  50
...
...
...
dont                door                double              download
draw                dream               dreams              drink
drive               driving             drop                drugs
dude                dun                 dunno               dvd
earlier             early               easy                eat
eating              eg                  eh                  either
else                em                  email               end
ending              ends                energy              england
enjoy               enough              enter               entered
entitled            entry               especially          etc
eve                 even                evening             ever
every               everyone            everything          ex
exam                excellent           excuse              experience
expires             extra               eyes                face
facebook            fact                family              fancy
fantastic           far                 fast                fat
father              fault               feb                 feel
feeling             feels               felt                figure
film                final               finally             find
fine                fingers             finish              finished
first               fixed               following           fone
food                forever             forget              forgot
forward             forwarded           found               fr
free                freemsg             freephone           frens
fri                 friday              friend              friends
...
...
....
meeting             meh                 member              men
merry               message             messages            met
mid                 midnight            might               min
mind                mine                mins                minute
minutes             miracle             miss                missed
missing             mistake             mm                  mo
mob                 mobile              mobiles             mobileupd8
mode                model               mom                 moment
mon                 monday              money               month
months              mood                moon                moral
morning             mother              motorola            move
movie               movies              mp3                 mr
mrng                mrt                 msg                 msgs
mu                  much                mum                 murdered
murderer            music               must                muz
na                  nah                 naked               name
national            naughty             near                need
needs               net                 network             neva
never               new                 news                next
ni8                 nice                nigeria             night
...
...
...
slave               sleep               sleeping            slept
slow                slowly              small               smile
smiling             smoke               sms                 smth
snow                sofa                sol                 somebody
someone             something           sometimes           somewhere
song                sony                soon                sorry
sort                sound               sounds              south
sp                  space               speak               special
specially           spend               spent               spree
st                  stand               start               started
starting            starts              statement           station
stay                staying             std                 still
stop                store               story               street
...
...
...
wk                  wkly                woke                wonder
wonderful           wondering           wont                word
words               work                workin              working
world               worried             worries             worry
worse               worth               wot                 would
wow                 write               wrong               wun
www                 xmas                xx                  xxx
xy                  ya                  yahoo               yar
yeah                year                years               yep
yes                 yesterday           yet                 yijue
yo                  yr                  yrs                 yup
ì_                  ìï                  û_                  None
</pre>

<div class="rs-tip-major">Poznámka: poslední hodnota <strong>None</strong>
odpovídá chybějícímu prvku.</div>


<p><a name="k09"></a></p>
<h2 id="k09">9. Vektorizace všech SMSek z&nbsp;datové sady</h2>

<p>Pro vlastní vektorizaci SMSsek opět, stejně jako v&nbsp;předchozích dvou
příkladech, použijeme třídu <strong>CountVectorizer</strong>, která nejprve
zkonstruuje slovník z&nbsp;celého korpusu (tedy ze všech textů) a následně
každou SMS nahradí vektorem obsahujícím v&nbsp;<i>i</i>-tém prvku počet výskytů
slova z&nbsp;<i>i</i>-tého místa ve slovníku v&nbsp;dané SMS. Z&nbsp;těchto
vektorů se pak vytvoří matice, přičemž výsledné rozměry matice jsou <i>n</i>
sloupců a <i>m</i> řádků, kde <i>n</i> odpovídá velikosti slovníku a <i>m</i>
počtu SMSsek, které se zpracovaly (což odpovídá velikosti korpusu).</p>

<pre>
<i># Vektorizace textových dat</i>
&nbsp;
import pandas as pd 
import re
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import CountVectorizer
&nbsp;
<i># načtení tabulky do datového rámce, specifikace kódování souboru</i>
spam = pd.read_csv("spam.csv", encoding="latin1")
&nbsp;
<i># hodnocení (spam/ham)</i>
labels = spam.v1.values
&nbsp;
<i># vlastní text SMS</i>
features = spam.v2.values
&nbsp;
<i># hodnoty použité později pro trénink modelu</i>
print("Labels:")
print(labels)
print("Number of labels:", len(labels))
print()
&nbsp;
print("Features:")
print(features)
print("Number of features:", len(features))
&nbsp;
<i># vektorizace textu</i>
vectorizer = CountVectorizer(
    max_features=2500,
    min_df=7, max_df=0.8, stop_words=stopwords.words("english")
)
vectorized_features = vectorizer.fit_transform(features).toarray()
&nbsp;
<i># slova pro dekódování vah</i>
feature_names = vectorizer.get_feature_names_out()
print("Feature names count:", len(feature_names))
print()
&nbsp;
<i># vlastní výsledek vektorizace</i>
print("Sparse matrix of size", vectorized_features.shape, ":")
print()
&nbsp;
<i># ukázka způsobu zakódování</i>
print("Selected tweet:")
print("Original:     ", features[2])
print("Vectorized:   ", vectorized_features[2])
print()
&nbsp;
print("word# weight meaning")
for i, f in enumerate(vectorized_features[2]):
    if f &gt; 0:
        print(f"{i:4}  {f:5}  {feature_names[i]}")
</pre>

<p>Tento skript nejprve vypíše ohodnocení SMSek:</p>

<pre>
Labels:
['ham' 'ham' 'spam' ... 'ham' 'ham' 'ham']
Number of labels: 5572
</pre>

<p>Dále vypíše část samotných obsahů SMSsek:</p>

<pre>
Features:
['Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...'
 'Ok lar... Joking wif u oni...'
 "Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&amp;C's apply 08452810075over18's"
 ... 'Pity, * was in mood for that. So...any other suggestions?'
 "The guy did some bitching but I acted like i'd be interested in buying something else next week and he gave it to us for free"
 'Rofl. Its true to its name']
Number of features: 5572
</pre>

<p>Důležitější je však údaj o rozměrech řídké matice. Ta má celkem 1276 sloupců
(počet slov ve slovníku) a 5572 řádků (počet SMSek):</p>

<pre>
Feature names count: 1267
Sparse matrix of size (5572, 1267) :
</pre>

<p>V&nbsp;dalším kroku je ukázána vybraná SMSka. Nejprve je zobrazen její
původní text:</p>

<pre>
Selected tweet:
Original:      Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&amp;C's apply 08452810075over18's
</pre>

<p>Poté se zobrazí vektor odpovídající slovům v&nbsp;SMSce. Tento vektor
obsahuje prakticky samé nuly, takže se nejedná o praktický výpis:</p>

<pre>
Vectorized:    [0 0 0 ... 0 0 0]
</pre>

<p>A následně se zobrazí pouze ta slova ve vektoru, která se ve vybrané SMSce
vyskytují alespoň jedenkrát:</p>

<pre>
word# weight meaning
 104      1  apply
 238      1  comp
 271      1  cup
 357      2  entry
 393      1  final
 412      1  free
 664      1  may
 867      1  question
 874      1  rate
 888      1  receive
1030      1  std
1073      1  text
1133      1  txt
1210      1  win
1221      1  wkly
</pre>



<p><a name="k10"></a></p>
<h2 id="k10">10. Preprocesing textů z&nbsp;SMSsek s&nbsp;vyčištěním dat</h2>

<p>Vraťme se ještě ke slovníku, který jsme získali a vypsali v&nbsp;příkladu
uvedeném <a href="#k08">v&nbsp;osmé kapitole</a>. Jak je patrné, obsahuje tento
slovník velké množství částí textu, který nelze považovat za běžná slova. Bude
tedy vhodné provést nějaký preprocesing dat, v&nbsp;jehož rámci odstraníme ze
vstupních textů různé &bdquo;paznaky&ldquo; (ostatně zpracováváme anglický
text), čísla atd. Pro tento účel sice existují specializované knihovny, my si
ovšem prozatím vystačíme se sekvencí regulárních výrazů, odstraněním znaků,
které nepatří do ASCII pomocí &bdquo;pipeline&ldquo; představované
příkazem:</p>

<pre>
processed_feature = processed_feature.encode("ascii", errors="ignore").decode()
</pre>

<p>který je následován konverzí SMSky na malá písmena s&nbsp;odstraněním
přebytečných bílých znaků na začátku a na konci zprávy:</p>

<pre>
processed_feature.strip().lower()
</pre>

<p>Celý skript, který tuto činnost provádí, vypadá následovně:</p>

<pre>
<i># Vektorizace textových dat, výpis výsledného slovníku</i>
&nbsp;
import pandas as pd
import re
from itertools import zip_longest
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import CountVectorizer
&nbsp;
<i># načtení tabulky do datového rámce, specifikace kódování souboru</i>
spam = pd.read_csv("spam.csv", encoding="latin1")
&nbsp;
<i># hodnocení (spam/ham)</i>
labels = spam.v1.values
&nbsp;
<i># vlastní text SMS</i>
features = spam.v2.values
&nbsp;
<i># hodnoty použité později pro trénink modelu</i>
print("Labels:")
print(labels)
print("Number of labels:", len(labels))
print()
&nbsp;
print("Features:")
print(features)
print("Number of features:", len(features))
print()
&nbsp;
&nbsp;
def <strong>process_feature</strong>(feature):
    <i>"""Preprocesing textových dat."""</i>
    <i># odstranění speciálních znaků a dalšího smetí</i>
    processed_feature = re.sub(r"\W", " ", feature)
&nbsp;
    <i># odstranění samostatných znaků (oddělených bílými znaky)</i>
    processed_feature = re.sub(r"\s+[a-zA-Z]\s+", " ", processed_feature)
&nbsp;
    <i># odstranění samostatných znaků na začátku vět</i>
    processed_feature = re.sub(r"\^[a-zA-Z]\s+", " ", processed_feature)
&nbsp;
    <i># náhrada více mezer (nebo jiných bílých znaků) za jedinou mezeru</i>
    processed_feature = re.sub(r"\s+", " ", processed_feature, flags=re.I)
&nbsp;
    <i># odstranění slov s číslicemi</i>
    processed_feature = re.sub("\w*\d\w*", "", processed_feature)
&nbsp;
    <i># odstranění prefixů ^b</i>
    processed_feature = re.sub(r"^b\s+", "", processed_feature)
&nbsp;
    <i># odstranění znaků, které nejsou ASCII</i>
    processed_feature = processed_feature.encode("ascii", errors="ignore").decode()
&nbsp;
    <i># konverze výsledku na malá písmena</i>
    return processed_feature.strip().lower()
&nbsp;
&nbsp;
<i># preprocesing všech hodnocení</i>
processed_features = [process_feature(feature) for feature in features]
&nbsp;
<i># vektorizace textu</i>
vectorizer = CountVectorizer(
    max_features=2500, min_df=7, max_df=0.8, stop_words=stopwords.words("english")
)
vectorized_features = vectorizer.fit_transform(processed_features).toarray()
&nbsp;
<i># slova pro dekódování vah</i>
feature_names = vectorizer.get_feature_names_out()
print("Feature names count:", len(feature_names))
print("Feature names:")
&nbsp;
columns = 4
for c1, c2, c3, c4 in zip_longest(
    feature_names[::columns],
    feature_names[1::columns],
    feature_names[2::columns],
    feature_names[3::columns],
):
    print(f"{str(c1): &lt;20}{str(c2): &lt;20}{str(c3): &lt;20}{c4}")
</pre>

<p>Výsledkem bude slovník, z&nbsp;něhož byla skutečně odstraněna
&bdquo;neslova&ldquo;. Slovník bude pochopitelně kratší a tím pádem bude menší
i výsledná řídká matice získaná vektorizací:</p>

<pre>
Feature names:
abiola              able                abt                 ac
access              account             across              actually
add                 address             admirer             advance
aft                 afternoon           age                 ago
ah                  aha                 ahead               aight
al                  alex                almost              alone
already             alright             alrite              also
always              amp                 angry               another
ans                 answer              anymore             anyone
...
...
...
wrong               wun                 www                 xmas
xx                  xxx                 xy                  ya
yahoo               yar                 yeah                year
years               yep                 yes                 yesterday
yet                 yijue               yo                  yr
yrs                 yup                 None                None
</pre>




<p><a name="k11"></a></p>
<h2 id="k11">11. Vektorizace textových dat po jejich filtraci</h2>

<p>Opět se pokusme o vektorizaci SMSsek (tj.&nbsp;našeho korpusu), nyní ovšem
s&nbsp;provedením filtrace textu, který z&nbsp;SMSek odstraní
&bdquo;neslova&ldquo;, přičemž samotná vektorizace bude provedena až nad takto
zpracovanými SMSkami. Nejprve si uvedeme samotný skript a poté porovnáme jeho
výstup s&nbsp;výstupem skriptu, který pracoval s&nbsp;nefiltrovanými
SMSkami:</p>

<pre>
<i># Vektorizace textových dat po jejich filtraci s využitím série regulárních výrazů</i>
&nbsp;
import pandas as pd
import re
from itertools import zip_longest
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import CountVectorizer
&nbsp;
<i># načtení tabulky do datového rámce, specifikace kódování souboru</i>
spam = pd.read_csv("spam.csv", encoding="latin1")
&nbsp;
<i># hodnocení (spam/ham)</i>
labels = spam.v1.values
&nbsp;
<i># vlastní text SMS</i>
features = spam.v2.values
&nbsp;
<i># hodnoty použité později pro trénink modelu</i>
print("Labels:")
print(labels)
print("Number of labels:", len(labels))
print()
&nbsp;
print("Features:")
print(features)
print("Number of features:", len(features))
print()
&nbsp;
&nbsp;
def <strong>process_feature</strong>(feature):
    <i>"""Preprocesing textových dat."""</i>
    <i># odstranění speciálních znaků a dalšího smetí</i>
    processed_feature = re.sub(r"\W", " ", feature)
&nbsp;
    <i># odstranění samostatných znaků (oddělených bílými znaky)</i>
    processed_feature = re.sub(r"\s+[a-zA-Z]\s+", " ", processed_feature)
&nbsp;
    <i># odstranění samostatných znaků na začátku vět</i>
    processed_feature = re.sub(r"\^[a-zA-Z]\s+", " ", processed_feature)
&nbsp;
    <i># náhrada více mezer (nebo jiných bílých znaků) za jedinou mezeru</i>
    processed_feature = re.sub(r"\s+", " ", processed_feature, flags=re.I)
&nbsp;
    <i># odstranění slov s číslicemi</i>
    processed_feature = re.sub("\w*\d\w*", "", processed_feature)
&nbsp;
    <i># odstranění prefixů ^b</i>
    processed_feature = re.sub(r"^b\s+", "", processed_feature)
&nbsp;
    <i># odstranění znaků, které nejsou ASCII</i>
    processed_feature = processed_feature.encode("ascii", errors="ignore").decode()
&nbsp;
    <i># konverze výsledku na malá písmena</i>
    return processed_feature.strip().lower()
&nbsp;
&nbsp;
<i># preprocesing všech hodnocení</i>
processed_features = [process_feature(feature) for feature in features]
&nbsp;
<i># vektorizace textu</i>
vectorizer = CountVectorizer(
    max_features=2500, min_df=7, max_df=0.8, stop_words=stopwords.words("english")
)
vectorized_features = vectorizer.fit_transform(processed_features).toarray()
&nbsp;
<i># slova pro dekódování vah</i>
feature_names = vectorizer.get_feature_names_out()
print("Feature names count:", len(feature_names))
print()
&nbsp;
<i># vlastní výsledek vektorizace</i>
print("Sparse matrix of size", vectorized_features.shape, ":")
print()
&nbsp;
<i># ukázka způsobu zakódování</i>
print("Selected tweet:")
print("Original:     ", features[2])
print("Processed:    ", processed_features[2])
print("Vectorized:   ", vectorized_features[2])
print()
&nbsp;
print("word# weight meaning")
for i, f in enumerate(vectorized_features[2]):
    if f &gt; 0:
        print(f"{i:4}  {f:5}  {feature_names[i]}")
</pre>

<p>Skript nyní vypíše, že ve slovníku má 1186 slov a tím pádem i řídká matice
má rozměry 5572×1186 prvků:</p>

<pre>
Feature names count: 1186
&nbsp;
Sparse matrix of size (5572, 1186) :
</pre>

<p>Připomeňme si, že bez provedení filtrace jsou výsledky odlišné:</p>

<pre>
Feature names count: 1267
&nbsp;
Sparse matrix of size (5572, 1267) :
</pre>

<p>Zajímavější bude zjištění, jak se liší originální SMSka od SMSky
přefiltrované (<strong>Original</strong>
vs.&nbsp;<strong>Processed</strong>):</p>

<pre>
Selected tweet:
Original:      Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&amp;C's apply 08452810075over18's
Processed:     free entry in  wkly comp to win fa cup final tkts  may  text fa to  to receive entry question std txt rate c apply  s
Vectorized:    [0 0 0 ... 0 0 0]
</pre>

<p>Slova použitá ve vektorizované SMSce:</p>

<pre>
word# weight meaning
  40      1  apply
 171      1  comp
 202      1  cup
 288      2  entry
 324      1  final
 343      1  free
 594      1  may
 792      1  question
 799      1  rate
 813      1  receive
 954      1  std
 996      1  text
1056      1  txt
1132      1  win
1143      1  wkly
</pre>

<div class="rs-tip-major">Poznámka: tímto postupem tedy zajistíme, že model
se bude učit na menší matici a navíc nebude učící algoritmus zatížen
&bdquo;šumem&ldquo; v&nbsp;datech.</div>



<p><a name="k12"></a></p>
<h2 id="k12">12. Trénink a predikce modelu nad vektorizovanými daty založený na třídě <strong>CountVectorizer</strong></h2>

<p>Ve chvíli, kdy již máme k&nbsp;dispozici jak očekávané odpovědi (první
sloupec datové sady) i vektorizované SMSky, můžeme přistoupit k&nbsp;tréninku
modelu. V&nbsp;prvním demonstračním příkladu pro vektorizaci použijeme třídu
<strong>CountVectorizer</strong> a model bude představován třídou
<strong>KNeighborsClassifier</strong>. Model natrénujeme s&nbsp;využitím 80%
dat z&nbsp;datové sady, přičemž zbylých 10% bude použito pro otestování kvality
předpovědi modelu. A nakonec si necháme zobrazit <i>matici záměn</i>, která
nyní bude jednoduchá &ndash; její velikost bude 2×2 prvky, protože jsou
k&nbsp;dispozici jen dvě možné odpovědi:</p>

<pre>
<i># Trénink a predikce modelu nad vektorizovanými daty, založeno na třídě CountVectorizer</i>
&nbsp;
import pandas as pd
import re
import matplotlib.pyplot as plt
from nltk.corpus import stopwords
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.metrics import classification_report, accuracy_score
from sklearn.neighbors import KNeighborsClassifier
&nbsp;
<i># načtení tabulky do datového rámce, specifikace kódování souboru</i>
spam = pd.read_csv("spam.csv", encoding="latin1")
&nbsp;
<i># hodnocení (spam/ham)</i>
labels = spam.v1.values
&nbsp;
<i># vlastní text SMS</i>
features = spam.v2.values
&nbsp;
<i># hodnoty použité později pro trénink modelu</i>
print("Labels:")
print(labels)
print("Number of labels:", len(labels))
print()
&nbsp;
print("Features:")
print(features)
print("Number of features:", len(features))
print()
&nbsp;
&nbsp;
def <strong>process_feature</strong>(feature):
    <i>"""Preprocesing textových dat."""</i>
    <i># odstranění speciálních znaků a dalšího smetí</i>
    processed_feature = re.sub(r"\W", " ", feature)
&nbsp;
    <i># odstranění samostatných znaků (oddělených bílými znaky)</i>
    processed_feature = re.sub(r"\s+[a-zA-Z]\s+", " ", processed_feature)
&nbsp;
    <i># odstranění samostatných znaků na začátku vět</i>
    processed_feature = re.sub(r"\^[a-zA-Z]\s+", " ", processed_feature)
&nbsp;
    <i># náhrada více mezer (nebo jiných bílých znaků) za jedinou mezeru</i>
    processed_feature = re.sub(r"\s+", " ", processed_feature, flags=re.I)
&nbsp;
    <i># odstranění slov s číslicemi</i>
    processed_feature = re.sub("\w*\d\w*", "", processed_feature)
&nbsp;
    <i># odstranění prefixů ^b</i>
    processed_feature = re.sub(r"^b\s+", "", processed_feature)
&nbsp;
    <i># odstranění znaků, které nejsou ASCII</i>
    processed_feature = processed_feature.encode("ascii", errors="ignore").decode()
&nbsp;
    <i># konverze výsledku na malá písmena</i>
    return processed_feature.strip().lower()
&nbsp;
&nbsp;
<i># preprocesing všech hodnocení</i>
processed_features = [process_feature(feature) for feature in features]
&nbsp;
<i># vektorizace textu</i>
vectorizer = CountVectorizer(
    max_features=2500, min_df=7, max_df=0.8, stop_words=stopwords.words("english")
)
vectorized_features = vectorizer.fit_transform(processed_features).toarray()
&nbsp;
<i># klasické rozdělení datové sady na trénovací a testovací část</i>
trainX, testX, trainY, testY = train_test_split(
    vectorized_features, labels, test_size=0.2, random_state=0
)
&nbsp;
<i># konstrukce vybraného modelu s předáním hyperparametrů</i>
classifier = KNeighborsClassifier(n_neighbors=1)
&nbsp;
<i># trénink modelu</i>
classifier.fit(trainX, trainY)
&nbsp;
<i># predikce modelu pro testovací vstupy (ne pro trénovací data)</i>
predictions = classifier.predict(testX)
&nbsp;
<i># vyhodnocení kvality modelu</i>
print(classification_report(testY, predictions))
print("Accuracy score:", accuracy_score(testY, predictions))
print()
&nbsp;
<i># matice záměn - absolutní hodnoty</i>
disp = ConfusionMatrixDisplay.from_estimator(
    classifier,
    testX,
    testY,
    cmap=plt.cm.Blues,
    normalize=None,
)
&nbsp;
<i># zobrazení matice v textové podobě</i>
print(disp.confusion_matrix)
print()
&nbsp;
<i># uložení výsledků ve formě rastrového obrázku</i>
plt.savefig("214_1.png")
&nbsp;
<i># vizualizace matice</i>
plt.show()
&nbsp;
<i># matice záměn - relativní hodnoty</i>
disp = ConfusionMatrixDisplay.from_estimator(
    classifier,
    testX,
    testY,
    cmap=plt.cm.Blues,
    normalize="true",
)
&nbsp;
<i># zobrazení matice v textové podobě</i>
print(disp.confusion_matrix)
&nbsp;
<i># uložení výsledků ve formě rastrového obrázku</i>
plt.savefig("214_2.png")
&nbsp;
<i># vizualizace matice</i>
plt.show()
</pre>

<p>Vyhodnocení kvality modelu:</p>

<pre>
Number of features: 5572
&nbsp;
              precision    recall  f1-score   support
&nbsp;
         ham       0.95      1.00      0.98       949
        spam       0.99      0.73      0.84       166
&nbsp;
    accuracy                           0.96      1115
   macro avg       0.97      0.86      0.91      1115
weighted avg       0.96      0.96      0.96      1115
</pre>

<p>Zajímavější je vyjádření přesnosti odpovědí modelu, která dosahuje velmi
pěkných 95-96%:</p>

<pre>
Accuracy score: 0.9587443946188341
</pre>

<p>A pochopitelně si můžeme nechat zobrazit matice záměn, a to jak
v&nbsp;absolutní, tak i relativní podobě:</p>

<pre>
[[948   1]
 [ 45 121]]
&nbsp;
[[0.99894626 0.00105374]
 [0.27108434 0.72891566]]
</pre>

<p>Naprostá většina hodnot leží na hlavní diagonále, což odpovídá (poměrně)
kvalitnímu modelu!</p>

<img src="https://i.iinfo.cz/images/651/scikit-learn-nlp-3-2.webp" class="image-1155738" width="640" height="480" alt="&nbsp;" title="Autor: tisnik, podle licence: &lt;a href=&quot;http://en.wikipedia.org/wiki/Rights_Managed&quot;&gt;Rights Managed&lt;/a&gt;" />
<p><i>Obrázek 2: Matice záměn s&nbsp;absolutními hodnotami.</i></p>

<img src="https://i.iinfo.cz/images/651/scikit-learn-nlp-3-3.webp" class="image-1155741" width="640" height="480" alt="&nbsp;" title="Autor: tisnik, podle licence: &lt;a href=&quot;http://en.wikipedia.org/wiki/Rights_Managed&quot;&gt;Rights Managed&lt;/a&gt;" />
<p><i>Obrázek 3: Matice záměn s&nbsp;relativními hodnotami.</i></p>


<p><a name="k13"></a></p>
<h2 id="k13">13. Trénink a predikce modelu nad vektorizovanými daty založený na třídě <strong>TfidfVectorizer</strong></h2>

<p>Se třídou <strong>TfidfVectorizer</strong>, která do výsledné matice ukládá
nikoli frekvence slov, ale jejich hodnoty <i>tf-idf</i> (tj.&nbsp;numericky
vyjádřenou specifičnost slov vůči dokumentu i celému korpusu), jsme se již
setkali. Teoreticky by měl model natrénovaný s&nbsp;maticí obsahující prvky
<i>tf-idf</i> dávat lepší výsledky v&nbsp;porovnání s&nbsp;použitím matice
s&nbsp;frekvencemi slov. Jestli tomu tak bude i při analýze spamu
v&nbsp;SMSkách, se můžeme snadno přesvědčit:</p>

<pre>
<i># Trénink a predikce modelu nad vektorizovanými daty, založeno na třídě TfidfVectorizer</i>
&nbsp;
import pandas as pd
import re
import matplotlib.pyplot as plt
from nltk.corpus import stopwords
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.metrics import classification_report, accuracy_score
from sklearn.neighbors import KNeighborsClassifier
&nbsp;
<i># načtení tabulky do datového rámce, specifikace kódování souboru</i>
spam = pd.read_csv("spam.csv", encoding="latin1")
&nbsp;
<i># hodnocení (spam/ham)</i>
labels = spam.v1.values
&nbsp;
<i># vlastní text SMS</i>
features = spam.v2.values
&nbsp;
<i># hodnoty použité později pro trénink modelu</i>
print("Labels:")
print(labels)
print("Number of labels:", len(labels))
print()
&nbsp;
print("Features:")
print(features)
print("Number of features:", len(features))
print()
&nbsp;
&nbsp;
def <strong>process_feature</strong>(feature):
    <i>"""Preprocesing textových dat."""</i>
    <i># odstranění speciálních znaků a dalšího smetí</i>
    processed_feature = re.sub(r"\W", " ", feature)
&nbsp;
    <i># odstranění samostatných znaků (oddělených bílými znaky)</i>
    processed_feature = re.sub(r"\s+[a-zA-Z]\s+", " ", processed_feature)
&nbsp;
    <i># odstranění samostatných znaků na začátku vět</i>
    processed_feature = re.sub(r"\^[a-zA-Z]\s+", " ", processed_feature)
&nbsp;
    <i># náhrada více mezer (nebo jiných bílých znaků) za jedinou mezeru</i>
    processed_feature = re.sub(r"\s+", " ", processed_feature, flags=re.I)
&nbsp;
    <i># odstranění slov s číslicemi</i>
    processed_feature = re.sub("\w*\d\w*", "", processed_feature)
&nbsp;
    <i># odstranění prefixů ^b</i>
    processed_feature = re.sub(r"^b\s+", "", processed_feature)
&nbsp;
    <i># odstranění znaků, které nejsou ASCII</i>
    processed_feature = processed_feature.encode("ascii", errors="ignore").decode()
&nbsp;
    <i># konverze výsledku na malá písmena</i>
    return processed_feature.strip().lower()
&nbsp;
&nbsp;
<i># preprocesing všech hodnocení</i>
processed_features = [process_feature(feature) for feature in features]
&nbsp;
<i># vektorizace textu</i>
vectorizer = TfidfVectorizer(
    max_features=2500, min_df=7, max_df=0.8, stop_words=stopwords.words("english")
)
vectorized_features = vectorizer.fit_transform(processed_features).toarray()
&nbsp;
<i># klasické rozdělení datové sady na trénovací a testovací část</i>
trainX, testX, trainY, testY = train_test_split(
    vectorized_features, labels, test_size=0.2, random_state=0
)
&nbsp;
<i># konstrukce vybraného modelu s předáním hyperparametrů</i>
classifier = KNeighborsClassifier(n_neighbors=1)
&nbsp;
<i># trénink modelu</i>
classifier.fit(trainX, trainY)
&nbsp;
<i># predikce modelu pro testovací vstupy (ne pro trénovací data)</i>
predictions = classifier.predict(testX)
&nbsp;
<i># vyhodnocení kvality modelu</i>
print(classification_report(testY, predictions))
print("Accuracy score:", accuracy_score(testY, predictions))
print()
&nbsp;
<i># matice záměn - absolutní hodnoty</i>
disp = ConfusionMatrixDisplay.from_estimator(
    classifier,
    testX,
    testY,
    cmap=plt.cm.Blues,
    normalize=None,
)
&nbsp;
<i># zobrazení matice v textové podobě</i>
print(disp.confusion_matrix)
print()
&nbsp;
<i># uložení výsledků ve formě rastrového obrázku</i>
plt.savefig("215_1.png")
&nbsp;
<i># vizualizace matice</i>
plt.show()
&nbsp;
<i># matice záměn - relativní hodnoty</i>
disp = ConfusionMatrixDisplay.from_estimator(
    classifier,
    testX,
    testY,
    cmap=plt.cm.Blues,
    normalize="true",
)
&nbsp;
<i># zobrazení matice v textové podobě</i>
print(disp.confusion_matrix)
&nbsp;
<i># uložení výsledků ve formě rastrového obrázku</i>
plt.savefig("215_2.png")
&nbsp;
<i># vizualizace matice</i>
plt.show()
</pre>

<p>Ve skutečnosti bude mít tento model nepatrně horší výsledky, než model
předchozí. Ovšem rozdíly se v&nbsp;tomto případě skutečně pohybují hluboko pod
hranicí statistické odchylky (což je ovšem zajímavé &ndash; evidetně se model
nenaučil mnoho slov vyloženě specifických pro spam či naopak):</p>

<pre>
              precision    recall  f1-score   support
&nbsp;
         ham       0.96      1.00      0.98       949
        spam       0.97      0.73      0.84       166
&nbsp;
    accuracy                           0.96      1115
   macro avg       0.96      0.87      0.91      1115
weighted avg       0.96      0.96      0.95      1115
&nbsp;
Accuracy score: 0.95695067264574
&nbsp;
[[945   4]
 [ 44 122]]
&nbsp;
[[0.99578504 0.00421496]
 [0.26506024 0.73493976]]
</pre>

<p>Z&nbsp;maticí záměn je patrné, že je tento model nepatrně horší, protože
čísla na hlavní diagonále jsou v&nbsp;součtu menší, než tomu bylo u předchozího
modelu:</p>

<img src="https://i.iinfo.cz/images/651/scikit-learn-nlp-3-4.webp" class="image-1155744" width="640" height="480" alt="&nbsp;" title="Autor: tisnik, podle licence: &lt;a href=&quot;http://en.wikipedia.org/wiki/Rights_Managed&quot;&gt;Rights Managed&lt;/a&gt;" />
<p><i>Obrázek 4: Matice záměn s&nbsp;absolutními hodnotami.</i></p>

<img src="https://i.iinfo.cz/images/651/scikit-learn-nlp-3-5.webp" class="image-1155747" width="640" height="480" alt="&nbsp;" title="Autor: tisnik, podle licence: &lt;a href=&quot;http://en.wikipedia.org/wiki/Rights_Managed&quot;&gt;Rights Managed&lt;/a&gt;" />
<p><i>Obrázek 5: Matice záměn s&nbsp;relativními hodnotami.</i></p>



<p><a name="k14"></a></p>
<h2 id="k14">14. Jak pracovat s&nbsp;kontextem: řešení založené na n-gramech</h2>

<p>Již <a href="https://www.root.cz/clanky/vyuziti-knihovny-scikit-learn-pro-zpracovani-a-analyzu-prirozeneho-jazyka-nlp-2-cast/">minule</a>
jsme se zmínili o tom, že po vektorizaci vlastně ztrácíme informaci o pořadí
slov v&nbsp;jednotlivých dokumentech. A to může znamenat, že modely nebudou
natrénovány tak kvalitně, jak by to bylo možné při použití více sofistikovaných
metod. Ovšem samotná vektorizace nemusí probíhat pouze nad slovníkem, který
obsahuje jednotlivá slova. Namísto jednotlivých slov (či navíc
k&nbsp;jednotlivým slovům) můžeme doplnit i dvojice, trojice atd. často
používaných skupin slov. Takovým n-ticím se někdy říká <i>n-gramy</i>, i když
je nutné poznamenat, že samotný název <i>n-gram</i> má více významů (může se
například jednat o skupinu znaků). Ovšem my se budeme zabývat pouze n-gramy ve
smyslu &bdquo;n-tice slov&ldquo;. Knihovna <i>scikit-learn</i> podporuje tvorbu
slovníků, jehož prvky mohou být n-gramy, takže si můžeme vyzkoušet, zda tento
alternativní způsob konstrukce slovníků povede k&nbsp;lepšímu nebo naopak
k&nbsp;horšímu modelu.</p>

<div class="rs-tip-major">Poznámka: n-gramy lze s&nbsp;výhodou použít
například pro detekci autora textu. Ostatně v&nbsp;dalších příkladech se ukáže,
že náš korpus složený z&nbsp;různých SMS není tou nejvhodnější datovou sadu pro
použití n-gramů.</div>




<p><a name="k15"></a></p>
<h2 id="k15">15. Vektorizace textových dat s použitím n-gramů o délce 1-2 slov s&nbsp;výpisem výsledného slovníku</h2>

<p>Specifikace minimálního a maximálního počtu slov v&nbsp;n-gramech je snadná,
protože do třídy <strong>CountVectorizer</strong> nebo
<strong>TfidfVectorizer</strong> je možné předat nepovinný parametr
<strong>ngram_range</strong>, který musí obsahovat dvojici <i>(minimální_délka,
maximální_délka)</i>. Pokusme se tedy zkonstruovat slovník, ve kterém budou
obsažena jak jednotlivá slova, tak i jejich dvojice:</p>

<pre>
<i># vektorizace textu</i>
vectorizer = CountVectorizer(
    max_features=2500, min_df=7, max_df=0.8, stop_words=stopwords.words("english"), ngram_range=(1,2)
)
vectorized_features = vectorizer.fit_transform(processed_features).toarray()
</pre>

<p>Výše uvedený konstruktor třídy <strong>CountVectorizer</strong> je zařazen
do dalšího skriptu, který po vektorizaci vypíše nový slovník
s&nbsp;jednotlivými slovy i jejich dvojicemi:</p>

<pre>
<i># Vektorizace textových dat s použitím n-gramů o délce 1-2 slov, výpis výsledného slovníku</i>
&nbsp;
import pandas as pd
import re
from itertools import zip_longest
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import CountVectorizer
&nbsp;
<i># načtení tabulky do datového rámce, specifikace kódování souboru</i>
spam = pd.read_csv("spam.csv", encoding="latin1")
&nbsp;
<i># hodnocení (spam/ham)</i>
labels = spam.v1.values
&nbsp;
<i># vlastní text SMS</i>
features = spam.v2.values
&nbsp;
<i># hodnoty použité později pro trénink modelu</i>
print("Labels:")
print(labels)
print("Number of labels:", len(labels))
print()
&nbsp;
print("Features:")
print(features)
print("Number of features:", len(features))
print()
&nbsp;
&nbsp;
def <strong>process_feature</strong>(feature):
    <i>"""Preprocesing textových dat."""</i>
    <i># odstranění speciálních znaků a dalšího smetí</i>
    processed_feature = re.sub(r"\W", " ", feature)
&nbsp;
    <i># odstranění samostatných znaků (oddělených bílými znaky)</i>
    processed_feature = re.sub(r"\s+[a-zA-Z]\s+", " ", processed_feature)
&nbsp;
    <i># odstranění samostatných znaků na začátku vět</i>
    processed_feature = re.sub(r"\^[a-zA-Z]\s+", " ", processed_feature)
&nbsp;
    <i># náhrada více mezer (nebo jiných bílých znaků) za jedinou mezeru</i>
    processed_feature = re.sub(r"\s+", " ", processed_feature, flags=re.I)
&nbsp;
    <i># odstranění slov s číslicemi</i>
    processed_feature = re.sub("\w*\d\w*", "", processed_feature)
&nbsp;
    <i># odstranění prefixů ^b</i>
    processed_feature = re.sub(r"^b\s+", "", processed_feature)
&nbsp;
    <i># odstranění znaků, které nejsou ASCII</i>
    processed_feature = processed_feature.encode("ascii", errors="ignore").decode()
&nbsp;
    <i># konverze výsledku na malá písmena</i>
    return processed_feature.strip().lower()
&nbsp;
&nbsp;
<i># preprocesing všech hodnocení</i>
processed_features = [process_feature(feature) for feature in features]
&nbsp;
<i># vektorizace textu</i>
vectorizer = CountVectorizer(
    max_features=2500, min_df=7, max_df=0.8, stop_words=stopwords.words("english"), ngram_range=(1,2)
)
vectorized_features = vectorizer.fit_transform(processed_features).toarray()
&nbsp;
<i># slova pro dekódování vah</i>
feature_names = vectorizer.get_feature_names_out()
print("Feature names count:", len(feature_names))
print()
&nbsp;
print("Feature names:")
for feature_name in feature_names:
    print(feature_name)
print()
&nbsp;
<i># vlastní výsledek vektorizace</i>
print("Sparse matrix of size", vectorized_features.shape, ":")
print()
&nbsp;
<i># ukázka způsobu zakódování</i>
print("Selected tweet:")
print("Original:     ", features[2])
print("Processed:    ", processed_features[2])
print("Vectorized:   ", vectorized_features[2])
print()
&nbsp;
print("word# weight meaning")
for i, f in enumerate(vectorized_features[2]):
    if f &gt; 0:
        print(f"{i:4}  {f:5}  {feature_names[i]}")
</pre>

<p>Nyní bude do slovníku zařazeno 1475 prvků:</p>

<pre>
Feature names count: 1475
</pre>

<p>Z&nbsp;výpisu obsahu slovníku (zde pochopitelně zkráceného) je patrné, že
v&nbsp;něm skutečně nalezneme i dvojice slov. Příkladem může být častá
kombinace <strong>call + další slovo</strong>:</p>

<pre>
Feature names:
abiola
able
abt
ac
access
account
account statement
across
across sea
actually
add
...
...
...
call
call back
call claim
call customer
call free
call identifier
call land
call landline
call later
call min
call mobile
call per
call reply
...
...
...
worth
worth discount
yes
yes see
</pre>

<p>Zvětší se pochopitelně i matice získaná vektorizací:</p>

<pre>
Sparse matrix of size (5572, 1475) :
</pre>

<p>Ukázka vektorizace jedné SMSky:</p>

<pre>
Selected tweet:
Original:      Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&amp;C's apply 08452810075over18's
Processed:     free entry in  wkly comp to win fa cup final tkts  may  text fa to  to receive entry question std txt rate c apply  s
Vectorized:    [0 0 0 ... 0 0 0]
</pre>

<p>Zajímavé je, že i tato SMSka obsahuje dvojici &bdquo;free entry&ldquo;
zařazenou do slovníku:</p>

<pre>
word# weight meaning
  45      1  apply
 220      1  comp
 258      1  cup
 360      2  entry
 400      1  final
 421      1  free
 424      1  <strong>free entry</strong>
 742      1  may
 989      1  question
 996      1  rate
1010      1  receive
1180      1  std
1229      1  text
1303      1  txt
1414      1  win
1427      1  wkly
</pre>



<p><a name="k16"></a></p>
<h2 id="k16">16. Trénink a predikce modelu s&nbsp;využitím třídy <strong>CountVectorizer</strong> při vektorizaci n-gramů</h2>

<p>Nový model nyní natrénujeme takovým způsobem, že použijeme třídu
<strong>CountVectorizer</strong>, ovšem umožníme, aby se do slovníku nevkládala
pouze jednotlivá slova, ale i jejich dvojice. Tím by se teoreticky měla zvýšit
výsledná kvalita modelu. Praktické výsledky ovšem mohou být odlišné od
předpokladů, takže si vždy musíme předpoklady ověřit měřením (natrénováním a
otestováním modelu):</p>

<pre>
<i># Trénink a predikce modelu nad vektorizovanými daty, založeno na třídě CountVectorizer, použití ngramů</i>
&nbsp;
import pandas as pd
import re
import matplotlib.pyplot as plt
from nltk.corpus import stopwords
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.metrics import classification_report, accuracy_score
from sklearn.neighbors import KNeighborsClassifier
&nbsp;
<i># načtení tabulky do datového rámce, specifikace kódování souboru</i>
spam = pd.read_csv("spam.csv", encoding="latin1")
&nbsp;
<i># hodnocení (spam/ham)</i>
labels = spam.v1.values
&nbsp;
<i># vlastní text SMS</i>
features = spam.v2.values
&nbsp;
<i># hodnoty použité později pro trénink modelu</i>
print("Labels:")
print(labels)
print("Number of labels:", len(labels))
print()
&nbsp;
print("Features:")
print(features)
print("Number of features:", len(features))
print()
&nbsp;
&nbsp;
def <strong>process_feature</strong>(feature):
    <i>"""Preprocesing textových dat."""</i>
    <i># odstranění speciálních znaků a dalšího smetí</i>
    processed_feature = re.sub(r"\W", " ", feature)
&nbsp;
    <i># odstranění samostatných znaků (oddělených bílými znaky)</i>
    processed_feature = re.sub(r"\s+[a-zA-Z]\s+", " ", processed_feature)
&nbsp;
    <i># odstranění samostatných znaků na začátku vět</i>
    processed_feature = re.sub(r"\^[a-zA-Z]\s+", " ", processed_feature)
&nbsp;
    <i># náhrada více mezer (nebo jiných bílých znaků) za jedinou mezeru</i>
    processed_feature = re.sub(r"\s+", " ", processed_feature, flags=re.I)
&nbsp;
    <i># odstranění slov s číslicemi</i>
    processed_feature = re.sub("\w*\d\w*", "", processed_feature)
&nbsp;
    <i># odstranění prefixů ^b</i>
    processed_feature = re.sub(r"^b\s+", "", processed_feature)
&nbsp;
    <i># odstranění znaků, které nejsou ASCII</i>
    processed_feature = processed_feature.encode("ascii", errors="ignore").decode()
&nbsp;
    <i># konverze výsledku na malá písmena</i>
    return processed_feature.strip().lower()
&nbsp;
&nbsp;
<i># preprocesing všech hodnocení</i>
processed_features = [process_feature(feature) for feature in features]
&nbsp;
<i># vektorizace textu</i>
vectorizer = CountVectorizer(
    max_features=2500, min_df=7, max_df=0.8, stop_words=stopwords.words("english"), ngram_range=(1, 2)
)
vectorized_features = vectorizer.fit_transform(processed_features).toarray()
&nbsp;
<i># klasické rozdělení datové sady na trénovací a testovací část</i>
trainX, testX, trainY, testY = train_test_split(
    vectorized_features, labels, test_size=0.2, random_state=0
)
&nbsp;
<i># konstrukce vybraného modelu s předáním hyperparametrů</i>
classifier = KNeighborsClassifier(n_neighbors=1)
&nbsp;
<i># trénink modelu</i>
classifier.fit(trainX, trainY)
&nbsp;
<i># predikce modelu pro testovací vstupy (ne pro trénovací data)</i>
predictions = classifier.predict(testX)
&nbsp;
<i># vyhodnocení kvality modelu</i>
print(classification_report(testY, predictions))
print("Accuracy score:", accuracy_score(testY, predictions))
print()
&nbsp;
<i># matice záměn - absolutní hodnoty</i>
disp = ConfusionMatrixDisplay.from_estimator(
    classifier,
    testX,
    testY,
    cmap=plt.cm.Blues,
    normalize=None,
)
&nbsp;
<i># zobrazení matice v textové podobě</i>
print(disp.confusion_matrix)
print()
&nbsp;
<i># uložení výsledků ve formě rastrového obrázku</i>
plt.savefig("217_1.png")
&nbsp;
<i># vizualizace matice</i>
plt.show()
&nbsp;
<i># matice záměn - relativní hodnoty</i>
disp = ConfusionMatrixDisplay.from_estimator(
    classifier,
    testX,
    testY,
    cmap=plt.cm.Blues,
    normalize="true",
)
&nbsp;
<i># zobrazení matice v textové podobě</i>
print(disp.confusion_matrix)
&nbsp;
<i># uložení výsledků ve formě rastrového obrázku</i>
plt.savefig("217_2.png")
&nbsp;
<i># vizualizace matice</i>
plt.show()
</pre>

<p>Výsledky v&nbsp;tomto případě kupodivu nebudou o mnoho lepší
v&nbsp;porovnání s&nbsp;modelem natrénovaným pro jednotlivá slova. To je sice
poněkud neintuitivní, ovšem je dobré si uvědomit, že zpracováváme jen krátké
SMSky a nikoli delší dokumenty, v&nbsp;nichž by se daly najít různé
specifičnosti (každý autor například používá podobná slovní spojení atd.):</p>

<pre>
               precision    recall  f1-score   support
&nbsp;
         ham       0.95      1.00      0.98       949
        spam       0.99      0.72      0.84       166
&nbsp;
    accuracy                           0.96      1115
   macro avg       0.97      0.86      0.91      1115
weighted avg       0.96      0.96      0.96      1115
&nbsp;
Accuracy score: 0.957847533632287
&nbsp;
[[948   1]
 [ 46 120]]
&nbsp;
[[0.99894626 0.00105374]
 [0.27710843 0.72289157]]
</pre>

<p>Matice záměn zobrazené formou grafu:</p>

<img src="https://i.iinfo.cz/images/651/scikit-learn-nlp-3-6.webp" class="image-1155750" width="640" height="480" alt="&nbsp;" title="Autor: tisnik, podle licence: &lt;a href=&quot;http://en.wikipedia.org/wiki/Rights_Managed&quot;&gt;Rights Managed&lt;/a&gt;" />
<p><i>Obrázek 6: Matice záměn s&nbsp;absolutními hodnotami.</i></p>

<img src="https://i.iinfo.cz/images/651/scikit-learn-nlp-3-7.webp" class="image-1155753" width="640" height="480" alt="&nbsp;" title="Autor: tisnik, podle licence: &lt;a href=&quot;http://en.wikipedia.org/wiki/Rights_Managed&quot;&gt;Rights Managed&lt;/a&gt;" />
<p><i>Obrázek 7: Matice záměn s&nbsp;relativními hodnotami.</i></p>



<p><a name="k17"></a></p>
<h2 id="k17">17. Trénink a predikce modelu s&nbsp;využitím třídy <strong>TfidfVectorizer</strong> při vektorizaci n-gramů</h2>

<p>Na závěr si ještě upravme demonstrační příklad <a href="#k16">z&nbsp;předchozí kapitoly</a> do podoby, v&nbsp;níž je vektorizace
provedena třídou <strong>TfidfVectorizer</strong>. Výsledkem budou prvky
obsahující informaci o specifičnosti jednotlivých slov i jejich dvojic, což by
teoreticky mělo znamenat, že model bude mít lepší predikce, protože dvojice
slov již může poměrně přesně určovat, jestli se jedná o spam nebo ham. Pojďme
si to vyzkoušet:</p>

<pre>
<i># Trénink a predikce modelu nad vektorizovanými daty, založeno na třídě TfidfVectorizer, použití ngramů</i>
&nbsp;
import pandas as pd
import re
import matplotlib.pyplot as plt
from nltk.corpus import stopwords
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.metrics import classification_report, accuracy_score
from sklearn.neighbors import KNeighborsClassifier
&nbsp;
<i># načtení tabulky do datového rámce, specifikace kódování souboru</i>
spam = pd.read_csv("spam.csv", encoding="latin1")
&nbsp;
<i># hodnocení (spam/ham)</i>
labels = spam.v1.values
&nbsp;
<i># vlastní text SMS</i>
features = spam.v2.values
&nbsp;
<i># hodnoty použité později pro trénink modelu</i>
print("Labels:")
print(labels)
print("Number of labels:", len(labels))
print()
&nbsp;
print("Features:")
print(features)
print("Number of features:", len(features))
print()
&nbsp;
&nbsp;
def <strong>process_feature</strong>(feature):
    <i>"""Preprocesing textových dat."""</i>
    <i># odstranění speciálních znaků a dalšího smetí</i>
    processed_feature = re.sub(r"\W", " ", feature)
&nbsp;
    <i># odstranění samostatných znaků (oddělených bílými znaky)</i>
    processed_feature = re.sub(r"\s+[a-zA-Z]\s+", " ", processed_feature)
&nbsp;
    <i># odstranění samostatných znaků na začátku vět</i>
    processed_feature = re.sub(r"\^[a-zA-Z]\s+", " ", processed_feature)
&nbsp;
    <i># náhrada více mezer (nebo jiných bílých znaků) za jedinou mezeru</i>
    processed_feature = re.sub(r"\s+", " ", processed_feature, flags=re.I)
&nbsp;
    <i># odstranění slov s číslicemi</i>
    processed_feature = re.sub("\w*\d\w*", "", processed_feature)
&nbsp;
    <i># odstranění prefixů ^b</i>
    processed_feature = re.sub(r"^b\s+", "", processed_feature)
&nbsp;
    <i># odstranění znaků, které nejsou ASCII</i>
    processed_feature = processed_feature.encode("ascii", errors="ignore").decode()
&nbsp;
    <i># konverze výsledku na malá písmena</i>
    return processed_feature.strip().lower()
&nbsp;
&nbsp;
<i># preprocesing všech hodnocení</i>
processed_features = [process_feature(feature) for feature in features]
&nbsp;
<i># vektorizace textu</i>
vectorizer = TfidfVectorizer(
    max_features=2500, min_df=7, max_df=0.8, stop_words=stopwords.words("english"), ngram_range=(1, 2)
)
vectorized_features = vectorizer.fit_transform(processed_features).toarray()
&nbsp;
<i># klasické rozdělení datové sady na trénovací a testovací část</i>
trainX, testX, trainY, testY = train_test_split(
    vectorized_features, labels, test_size=0.2, random_state=0
)
&nbsp;
<i># konstrukce vybraného modelu s předáním hyperparametrů</i>
classifier = KNeighborsClassifier(n_neighbors=1)
&nbsp;
<i># trénink modelu</i>
classifier.fit(trainX, trainY)
&nbsp;
<i># predikce modelu pro testovací vstupy (ne pro trénovací data)</i>
predictions = classifier.predict(testX)
&nbsp;
<i># vyhodnocení kvality modelu</i>
print(classification_report(testY, predictions))
print("Accuracy score:", accuracy_score(testY, predictions))
print()
&nbsp;
<i># matice záměn - absolutní hodnoty</i>
disp = ConfusionMatrixDisplay.from_estimator(
    classifier,
    testX,
    testY,
    cmap=plt.cm.Blues,
    normalize=None,
)
&nbsp;
<i># zobrazení matice v textové podobě</i>
print(disp.confusion_matrix)
print()
&nbsp;
<i># uložení výsledků ve formě rastrového obrázku</i>
plt.savefig("218_1.png")
&nbsp;
<i># vizualizace matice</i>
plt.show()
&nbsp;
<i># matice záměn - relativní hodnoty</i>
disp = ConfusionMatrixDisplay.from_estimator(
    classifier,
    testX,
    testY,
    cmap=plt.cm.Blues,
    normalize="true",
)
&nbsp;
<i># zobrazení matice v textové podobě</i>
print(disp.confusion_matrix)
&nbsp;
<i># uložení výsledků ve formě rastrového obrázku</i>
plt.savefig("218_2.png")
&nbsp;
<i># vizualizace matice</i>
plt.show()
</pre>

<p>Výsledky nového modelu jsou skutečně lepší, než tomu bylo v&nbsp;předchozím
příkladu, i když se opět jedná spíše o statistickou odchylku (tedy záleží,
která data jsou vybrána do trénovací a která do testovací množiny):</p>

<pre>
              precision    recall  f1-score   support
&nbsp;
         ham       0.96      1.00      0.98       949
        spam       0.98      0.74      0.84       166
&nbsp;
    accuracy                           0.96      1115
   macro avg       0.97      0.87      0.91      1115
weighted avg       0.96      0.96      0.96      1115
&nbsp;
Accuracy score: 0.9587443946188341
&nbsp;
[[946   3]
 [ 43 123]]
&nbsp;
[[0.99683878 0.00316122]
 [0.25903614 0.74096386]]
</pre>

<p>Matice záměn ve vizuální podobě vypadají takto:</p>

<img src="https://i.iinfo.cz/images/651/scikit-learn-nlp-3-8.webp" class="image-1155756" width="640" height="480" alt="&nbsp;" title="Autor: tisnik, podle licence: &lt;a href=&quot;http://en.wikipedia.org/wiki/Rights_Managed&quot;&gt;Rights Managed&lt;/a&gt;" />
<p><i>Obrázek 8: Matice záměn s&nbsp;absolutními hodnotami.</i></p>

<img src="https://i.iinfo.cz/images/651/scikit-learn-nlp-3-9.webp" class="image-1155759" width="640" height="480" alt="&nbsp;" title="Autor: tisnik, podle licence: &lt;a href=&quot;http://en.wikipedia.org/wiki/Rights_Managed&quot;&gt;Rights Managed&lt;/a&gt;" />
<p><i>Obrázek 9: Matice záměn s&nbsp;relativními hodnotami.</i></p>



<p><a name="k18"></a></p>
<h2 id="k18">18. Zjištění vlivu minimální a maximální délky n-gramů na kvalitu modelu</h2>

<p>V&nbsp;dnešním posledním demonstračním příkladu se pokusíme zjistit, jaká je
(pro danou vstupní datovou sadu a pouze pro ni!) vhodná minimální a maximální
délka n-gramů. Může totiž nastat situace, že bude nejvýhodnější model
natrénovat pouze s&nbsp;využitím dvojic slov, nebo s&nbsp;využitím dvojic a
trojic slov atd. Dopředu není možné přesně odhadnout, která kombinace (<i>min,
max</i>) bude nejvhodnější, takže se opět uchýlíme k&nbsp;měření. To je
realizováno v&nbsp;dalším skriptu, který pro každou legální kombinaci minimální
a maximální délky n-gramů vypíše počet záznamů ve slovníku i celkové dosažené
skóre modelu, tedy jeho předikční schopnosti:</p>



<p><a name="k19"></a></p>
<h2 id="k19">19. Repositář s&nbsp;demonstračními příklady</h2>

<p>Všechny demonstrační příklady využívající knihovnu Scikit-learn lze nalézt
v&nbsp;repositáři <a href="https://github.com/tisnik/most-popular-python-libs">https://github.com/tisnik/most-popular-python-libs</a>.
Následují odkazy na jednotlivé příklady i na (Jupyter) diáře s&nbsp;postupem
výpočtů a analýz:</p>

<table>
<tr><th>#</th><th>Příklad</th><th>Stručný popis</th><th>Adresa příkladu</th></tr>
<tr><td> 1</td><td>01_show_matrix.py</td><td>kooperace mezi knihovnami Matplotlib a NumPy: vizualizace obsahu 2D matice</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/01_show_matrix.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/01_show_matrix.py</a></td></tr>
<tr><td> 2</td><td>02_get_digits.py</td><td>datová množina obsahující naskenované ručně napsané číslice</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/02_get_digits.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/02_get_digits.py</a></td></tr>
<tr><td> 3</td><td>03_get_features.py</td><td>další atributy datové množiny, které použijeme při trénování</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/03_get_features.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/03_get_features.py</a></td></tr>
<tr><td> 4</td><td>04_get_images.py</td><td>přečtení a následné vykreslení jednotlivých ručně nakreslených číslic</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/04_get_images.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/04_get_images.py</a></td></tr>
<tr><td> 5</td><td>05_show_grayscale_matrix.py</td><td>odstranění umělé aplikované barvové palety (obrázky ve stupních šedi)</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/05_show_grayscale_matrix.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/05_show_grayscale_matrix.py</a></td></tr>
<tr><td> 6</td><td>06_grayscale_images.py</td><td>vykreslení ručně nakreslených číslic ve formě obrázků ve stupních šedi</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/06_grayscale_images.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/06_grayscale_images.py</a></td></tr>
<tr><td> 7</td><td>07_multiplot.py</td><td>rozdělení plochy grafu do oblastí; vykreslení více obrázků do jediného grafu</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/07_multiplot.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/07_multiplot.py</a></td></tr>
<tr><td> 8</td><td>08_model_preperation_1.py</td><td>obrázky s&nbsp;jejich ohodnocením</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/08_model_preperation_1.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/08_model_preperation_1.py</a></td></tr>
<tr><td> 9</td><td>09_training_set.py</td><td>příprava dat pro trénink</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/09_training_set.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/09_training_set.py</a></td></tr>
<tr><td>10</td><td>10_classification.py</td><td>klasifikace obrázků</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/10_classification.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/10_classification.py</a></td></tr>
<tr><td>11</td><td>11_results.py</td><td>vykreslení obrázků společně s&nbsp;jejich klasifikací</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/11_results.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/11_results.py</a></td></tr>
<tr><td>12</td><td>12_change_training_set.py</td><td>změna poměru rozdělení dat na tréninkovou a testovací množinu</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/12_change_training_set.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/12_change_training_set.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>13</td><td>13_blobs.py</td><td>použití funkce <strong>make_blobs</strong> pro vygenerování sady bodů v&nbsp;rovině sdružených do oblastí</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/13_blobs.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/13_blobs.py</a></td></tr>
<tr><td>14</td><td>14_swap_coords.py</td><td>úprava předchozího příkladu: prohození souřadnic na osách</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/14_swap_coords.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/14_swap_coords.py</a></td></tr>
<tr><td>15</td><td>15_blobs_scatter_plot.py</td><td>základní podoba bodového diagramu (<i>scatter plot</i>)</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/15_blobs_scatter_plot.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/15_blobs_scatter_plot.py</a></td></tr>
<tr><td>16</td><td>16_blobs_scatter_plot.py</td><td>úprava bodového diagramu při zobrazení většího množství bodů</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/16_blobs_scatter_plot.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/16_blobs_scatter_plot.py</a></td></tr>
<tr><td>17</td><td>17_colorized_blobs.py</td><td>obarvení bodů podle oblastí</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/17_colorized_blobs.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/17_colorized_blobs.py</a></td></tr>
<tr><td>18</td><td>18_k-means.py</td><td>základní použití algoritmu K-means pro clustering</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/18_k-means.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/18_k-means.py</a></td></tr>
<tr><td>19</td><td>19_combination.py</td><td>zobrazení centroidů společně s&nbsp;původními body</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/19_combination.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/19_combination.py</a></td></tr>
<tr><td>20</td><td>20_combinations.py</td><td>vizualizace clusteringu původní množiny bodů</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/20_combinations.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/20_combinations.py</a></td></tr>
<tr><td>21</td><td>21_other_settings.py</td><td>vizualizace clusteringu původní množiny bodů pro odlišnou množinu</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/21_other_settings.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/21_other_settings.py</a></td></tr>
<tr><td>22</td><td>22_random_points.py</td><td>clustering pro náhodná data</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/22_random_points.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/22_random_points.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>23</td><td>23_circles.py</td><td>pseudonáhodné rozmístění bodů do kružnic, menší náhodnost výsledku</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/23_circles.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/23_circles.py</a></td></tr>
<tr><td>24</td><td>24_more_noise_circles.py</td><td>pseudonáhodné rozmístění bodů do kružnic, větší náhodnost výsledku</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/24_more_noise_circles.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/24_more_noise_circles.py</a></td></tr>
<tr><td>25</td><td>25_moons.py</td><td>pseudonáhodné rozmístění bodů do tvaru dvou půlměsíců, menší náhodnost</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/25_moons.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/25_moons.py</a></td></tr>
<tr><td>26</td><td>26_more_noisy_moons.py</td><td>pseudonáhodné rozmístění bodů do tvaru dvou půlměsíců, větší náhodnost</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/26_more_noisy_moons.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/26_more_noisy_moons.py</a></td></tr>
<tr><td>27</td><td>27_circles_kmeans.py</td><td>výsledek clusteringu provedeného algoritmem K-means na &bdquo;kružnice&ldquo;</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/27_circles_kmeans.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/27_circles_kmeans.py</a></td></tr>
<tr><td>28</td><td>28_moons_kmeans.py</td><td>výsledek clusteringu provedeného algoritmem K-means na &bdquo;půlměsíce&ldquo;</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/28_moons_kmeans.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/28_moons_kmeans.py</a></td></tr>
<tr><td>29</td><td>29_blobs_spectral_clustering.py</td><td>spectral clustering pro body rozmístěné pomocí <strong>make_blobs</strong></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/29_blobs_spectral_clustering.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/29_blobs_spectral_clustering.py</a></td></tr>
<tr><td>30</td><td>30_circles_spectral_clustering.py</td><td>spectral clustering pro body rozmístěné do kružnic</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/30_circles_spectral_clustering.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/30_circles_spectral_clustering.py</a></td></tr>
<tr><td>31</td><td>31_moons_spectral_clustering.py</td><td>spectral clustering pro body rozmístěné do půlměsíců </td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/31_moons_spectral_clustering.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/31_moons_spectral_clustering.py</a></td></tr>
<tr><td>32</td><td>32_moons_spectral_clustering_limits.py</td><td>vyhledání limitů algoritmu spectral clustering</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/32_moons_spectral_clustering_limits.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/32_moons_spectral_clustering_limits.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>33</td><td>33_particles_load.py</td><td>načtení souřadnic částic uložených v&nbsp;souboru formátu CSV</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/33_particles_load.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/33_particles_load.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>34</td><td>34_lorenz_attractor.py</td><td>zobrazení Lorenzova atraktoru formou bodů propojených úsečkami</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/34_lorenz_attractor.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/34_lorenz_attractor.py</a></td></tr>
<tr><td>35</td><td>35_lorenz_attractor_points.py</td><td>Lorenzův atraktor vykreslený formou jednotlivých bodů s&nbsp;definovaným stylem zobrazení a velikostí stopy</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/35_lorenz_attractor_points.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/35_lorenz_attractor_points.py</a></td></tr>
<tr><td>36</td><td>36_blobs_3d.py</td><td>vygenerování a zobrazení sady bodů v&nbsp;3D prostoru</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/36_blobs_3d.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/36_blobs_3d.py</a></td></tr>
<tr><td>37</td><td>37_spread_blobs_3d.py</td><td>vygenerování a zobrazení sady bodů v&nbsp;3D prostoru, odlišné parametry při generování</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/37_spread_blobs_3d.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/37_spread_blobs_3d.py</a></td></tr>
<tr><td>38</td><td>38_views.py</td><td>různé pohledy na 3D graf</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/38_views.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/38_views.py</a></td></tr>
<tr><td>39</td><td>39_colorized_3d_blobs.py</td><td>obarvení bodů v&nbsp;prostoru na základě vstupních dat</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/39_colorized_3d_blobs.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/39_colorized_3d_blobs.py</a></td></tr>
<tr><td>40</td><td>40_kmeans_3d_blobs.py</td><td>shluková analýza v&nbsp;3D prostoru</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/40_kmeans_3d_blobs.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/40_kmeans_3d_blobs.py</a></td></tr>
<tr><td>41</td><td>41_kmeans_spread_3d_blobs.py</td><td>shluková analýza v&nbsp;3D prostoru pro odlišnou množinu bodů</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/41_kmeans_spread_3d_blobs.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/41_kmeans_spread_3d_blobs.py</a></td></tr>
<tr><td>42</td><td>42_kmeans_random_3d.py</td><td>shluková analýza pro body rozmístěné zcela náhodně v&nbsp;omezeném prostoru</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/42_kmeans_random_3d.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/42_kmeans_random_3d.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>43</td><td>43_speed_measurements.py</td><td>benchmark pro postupně rostoucí počet bodů tvořících shluky</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/43_speed_measurements.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/43_speed_measurements.py</a></td></tr>
<tr><td>44</td><td>44_speed_measurements.py</td><td>benchmark pro postupně rostoucí počet bodů rozmístěných náhodně</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/44_speed_measurements.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/44_speed_measurements.py</a></td></tr>
<tr><td>45</td><td>45_speed_measurements.py</td><td>benchmark pro stále stejný počet bodů, u jejichž rozmístění v&nbsp;prostoru se používá stále větší směrodatná odchylka</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/45_speed_measurements.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/45_speed_measurements.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>46</td><td>46_iris_dataset.py</td><td>načtení datové kolekce</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/46_iris_dataset.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/46_iris_dataset.py</a></td></tr>
<tr><td>47</td><td>47_iris_description.py</td><td>metadata o datové kolekci</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/47_iris_description.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/47_iris_description.py</a></td></tr>
<tr><td>48</td><td>48_iris_data.py</td><td>tvar dat &ndash; počet záznamů a počet proměnných</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/48_iris_data.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/48_iris_data.py</a></td></tr>
<tr><td>49</td><td>49_iris_targets.py</td><td>jména atributů, vztah mezi numerickou hodnotou atributu a jeho jménem</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/49_iris_targets.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/49_iris_targets.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>50</td><td>50_iris_scatter_plot_1.py</td><td>korelační diagram pro dvojici vybraných proměnných</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/50_iris_scatter_plot_1.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/50_iris_scatter_plot_1.py</a></td></tr>
<tr><td>51</td><td>51_iris_scatter_plot_2.py</td><td>příprava pro tvorbu složitějších grafů</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/51_iris_scatter_plot_2.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/51_iris_scatter_plot_2.py</a></td></tr>
<tr><td>52</td><td>52_iris_mutliplot.py</td><td>mřížka obsahující více korelačních diagramů</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/52_iris_mutliplot.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/52_iris_mutliplot.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>53</td><td>53_iris_histograms.py</td><td>zobrazení základního histogramu pro data v&nbsp;sadě Iris</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/53_iris_histograms.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/53_iris_histograms.py</a></td></tr>
<tr><td>54</td><td>54_iris_histograms.py</td><td>úprava histogramu</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/54_iris_histograms.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/54_iris_histograms.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>55</td><td>55_pca.py</td><td>analýza hlavních komponent (PCA), výsledek zobrazený v&nbsp;2D grafu</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/55_pca.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/55_pca.py</a></td></tr>
<tr><td>56</td><td>56_pca_3d.py</td><td>analýza hlavních komponent (PCA), výsledek zobrazený v&nbsp;3D grafu</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/56_pca_3d.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/56_pca_3d.py</a></td></tr>
<tr><td>57</td><td>57_kmeans.py</td><td>základní shluková analýza</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/57_kmeans.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/57_kmeans.py</a></td></tr>
<tr><td>58</td><td>58_multiple_kmeans.py</td><td>větší množství výsledků shlukové analýzy pro různé atributy</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/58_multiple_kmeans.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/58_multiple_kmeans.py</a></td></tr>
<tr><td>59</td><td>59_kmeans_errors.py</td><td>korektní a nekorektní výsledky základní shlukové analýzy</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/59_kmeans_errors.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/59_kmeans_errors.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>60</td><td>60_basic_classifier.py</td><td>aplikace jednoduchého modelu</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/60_basic_classifier.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/60_basic_classifier.py</a></td></tr>
<tr><td>61</td><td>61_changed_model_parameters.py</td><td>změna parametrů modelu pro zjištění druhů rostil</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/61_changed_model_parameters.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/61_changed_model_parameters.py</a></td></tr>
<tr><td>62</td><td>62_different_model.py</td><td>použití odlišného modelu pro zjištění druhů rostlin</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/62_different_model.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/62_different_model.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>63</td><td>63_verify_on_whole_data_1.py</td><td>otestování naučeného modelu s&nbsp;využitím tréninkových dat</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/63_verify_on_whole_data_1.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/63_verify_on_whole_data_1.py</a></td></tr>
<tr><td>64</td><td>64_verify_on_whole_data_2.py</td><td>využití funkce <strong>metrics.accuracy_score</strong> pro zjištění kvality modelu</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/64_verify_on_whole_data_2.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/64_verify_on_whole_data_2.py</a></td></tr>
<tr><td>65</td><td>65_basic_comparison.py</td><td>porovnání vlastností různých modelů (prozatím nekorektní řešení)</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/65_basic_comparison.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/65_basic_comparison.py</a></td></tr>
<tr><td>66</td><td>66_training_testing_split_1.py</td><td>rozdělení datové sady na trénovací data a testovací data (základní varianta)</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/66_training_testing_split_1.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/66_training_testing_split_1.py</a></td></tr>
<tr><td>67</td><td>67_training_testing_split_2.py</td><td>rozdělení datové sady na trénovací data a testovací data (náhodné rozdělení sady)</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/67_training_testing_split_2.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/67_training_testing_split_2.py</a></td></tr>
<tr><td>68</td><td>68_training_testing_split_3.py</td><td>rozdělení datové sady na trénovací data a testovací data (využití vestavěné funkce)</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/68_training_testing_split_3.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/68_training_testing_split_3.py</a></td></tr>
<tr><td>69</td><td>69_better_comparison.py</td><td>vylepšené porovnání vlastností různých modelů</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/69_better_comparison.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/69_better_comparison.py</a></td></tr>
<tr><td>70</td><td>70_multiple_runs.py</td><td>vliv generátoru náhodných čísel na změřené výsledky</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/70_multiple_runs.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/70_multiple_runs.py</a></td></tr>
<tr><td>71</td><td>71_stable_multiple_runs.py</td><td>generátor náhodných čísel a použití hodnoty <strong>random_state</strong></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/71_stable_multiple_runs.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/71_stable_multiple_runs.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>72</td><td>72_housings_dataset.py</td><td>načtení datové sady <i>California housings</i></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/72_housings_dataset.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/72_housings_dataset.py</a></td></tr>
<tr><td>73</td><td>73_housings_dataset_description.py</td><td>metainformace o datové sadě <i>California housings</i></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/73_housings_dataset_description.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/73_housings_dataset_description.py</a></td></tr>
<tr><td>74</td><td>74_housings_data.py</td><td>n-rozměrné pole s&nbsp;atributy jednotlivých domů/bloků</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/74_housings_data.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/74_housings_data.py</a></td></tr>
<tr><td>75</td><td>75_housings_targets.py</td><td>jména atributů, ceny domů atd.</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/75_housings_targets.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/75_housings_targets.py</a></td></tr>
<tr><td>76</td><td>76_housings_scatter_plot.py</td><td>korelační diagram pro dvojici vybraných proměnných</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/76_housings_scatter_plot.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/76_housings_scatter_plot.py</a></td></tr>
<tr><td>77</td><td>77_housings_mutliplot.py</td><td>korelační diagram pro všechny kombinace dvojic proměnných</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/77_housings_mutliplot.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/77_housings_mutliplot.py</a></td></tr>
<tr><td>78</td><td>78_scatter.py</td><td>dvourozměrné hodnoty reprezentované jako dvojice atributů</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/78_scatter.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/78_scatter.py</a></td></tr>
<tr><td>79</td><td>79_linear_regression_gen_data.py</td><td>model <i>LinearRegression</i> nad uměle vytvořenými daty</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/79_linear_regression_gen_data.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/79_linear_regression_gen_data.py</a></td></tr>
<tr><td>80</td><td>80_linear_regression_predictions.py</td><td>predikce modelu provádějícího lineární regresi</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/80_linear_regression_predictions.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/80_linear_regression_predictions.py</a></td></tr>
<tr><td>81</td><td>81_linear_regression_random_data.py</td><td>chování modelu pro zcela náhodná data</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/81_linear_regression_random_data.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/81_linear_regression_random_data.py</a></td></tr>
<tr><td>82</td><td>82_linear_regression_housings.py</td><td>model <i>LinearRegression</i> pro datovou sadu <i>California housings</i></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/82_linear_regression_housings.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/82_linear_regression_housings.py</a></td></tr>
<tr><td>83</td><td>83_polynomial_regression_gen_data.py</td><td>polynomiální regrese (základní příklad)</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/83_polynomial_regression_gen_data.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/83_polynomial_regression_gen_data.py</a></td></tr>
<tr><td>84</td><td>84_polynomial_regression_housings.py</td><td>polynomiální regrese a datová sada <i>California housings</i>, první příklad</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/84_polynomial_regression_housings.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/84_polynomial_regression_housings.py</a></td></tr>
<tr><td>85</td><td>85_polynomial_regression_housings_2.py</td><td>polynomiální regrese a datová sada <i>California housings</i>, druhý příklad</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/85_polynomial_regression_housings_2.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/85_polynomial_regression_housings_2.py</a></td></tr>
<tr><td>86</td><td>86_polynomial_regression_housings_3.py</td><td>polynomiální regrese a datová sada <i>California housings</i>, třetí příklad</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/86_polynomial_regression_housings_3.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/86_polynomial_regression_housings_3.py</a></td></tr>
<tr><td>87</td><td>87_linear_regression_errors.py</td><td>výpočet chyby a skóre modelu lineární regrese</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/87_linear_regression_errors.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/87_linear_regression_errors.py</a></td></tr>
<tr><td>88</td><td>88_linear_regression_non_linear_data.py</td><td>lineární regrese nad nelineárními daty</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/88_linear_regression_non_linear_data.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/88_linear_regression_non_linear_data.py</a></td></tr>
<tr><td>89</td><td>89_polynomial_regression_error.py</td><td></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/89_polynomial_regression_error.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/89_polynomial_regression_error.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>90</td><td>90_housings_prediction_1.py</td><td>regresní analýza nad daty <i>California housings</i></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/90_housings_prediction_1.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/90_housings_prediction_1.py</a></td></tr>
<tr><td>91</td><td>91_housings_prediction_2.py</td><td>korektní natrénování modelu pro regresi</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/91_housings_prediction_2.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/91_housings_prediction_2.py</a></td></tr>
<tr><td>92</td><td>92_housings_prediction_3.py</td><td>omezení množství atributů (proměnných), na kterých je model natrénován</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/92_housings_prediction_3.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/92_housings_prediction_3.py</a></td></tr>
<tr><td>93</td><td>93_housings_prediction_errors_1.py</td><td>chybně natrénovaný model při náhodné volbě dat</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/93_housings_prediction_errors_1.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/93_housings_prediction_errors_1.py</a></td></tr>
<tr><td>94</td><td>94_housings_prediction_errors_2.py</td><td>omezení atributů + chybně natrénovaný model</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/94_housings_prediction_errors_2.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/94_housings_prediction_errors_2.py</a></td></tr>
<tr><td>95</td><td>95_housings_histograms.py</td><td>histogramy pro jednotlivé atributy (proměnné)</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/95_housings_histograms.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/95_housings_histograms.py</a></td></tr>
<tr><td>96</td><td>96_housings_statistic.py</td><td>statistické údaje pro jednotlivé atributy (proměnné)</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/96_housings_statistic.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/96_housings_statistic.py</a></td></tr>
<tr><td>97</td><td>97_housings_statistic_normalized.py</td><td>statistické údaje získané po normalizaci</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/97_housings_statistic_normalized.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/97_housings_statistic_normalized.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td> 98</td><td>98_k_fold_help.py</td><td>zobrazení nápovědy ke třídě s&nbsp;realizací k-foldingu</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/98_k_fold_help.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/98_k_fold_help.py</a></td></tr>
<tr><td> 99</td><td>99_k_fold_old.py</td><td>původní (nepodporovaná) varianta provedení k-foldingu</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/99_k_fold_old.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/99_k_fold_old.py</a></td></tr>
<tr><td>100</td><td>100_k_fold_1.py</td><td>interní chování algoritmu k-foldingu (základní parametry)</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/100_k_fold_1.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/100_k_fold_1.py</a></td></tr>
<tr><td>101</td><td>101_k_fold_2.py</td><td>interní chování algoritmu k-foldingu (odlišné parametry)</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/101_k_fold_2.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/101_k_fold_2.py</a></td></tr>
<tr><td>102</td><td>102_k_fold_selection.py</td><td>k-folding a výběr dat pro otestování modelů</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/102_k_fold_selection.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/102_k_fold_selection.py</a></td></tr>
<tr><td>103</td><td>103_average_score.py</td><td>realizace výpočtu průměrného skóre pro otestování modelů</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/103_average_score.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/103_average_score.py</a></td></tr>
<tr><td>104</td><td>104_hyperparams_score.py</td><td>změna hyperparametrů s&nbsp;výpočtem průměrného skóre (tabulka)</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/104_hyperparams_score.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/104_hyperparams_score.py</a></td></tr>
<tr><td>105</td><td>105_hyperparams_score_plot.py</td><td>změna hyperparametrů s&nbsp;výpočtem průměrného skóre (graf)</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/105_hyperparams_score_plot.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/105_hyperparams_score_plot.py</a></td></tr>
<tr><td>106</td><td>106_model_selection.py</td><td>výběr nejlepšího modelu s&nbsp;využitím k-foldingu</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/106_model_selection.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/106_model_selection.py</a></td></tr>
<tr><td>107</td><td>107_features_selection_basic.py</td><td>výběr atributů (proměnných) pro trénink modelu (základní varianta)</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/107_features_selection_basic.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/107_features_selection_basic.py</a></td></tr>
<tr><td>108</td><td>108_features_selection_iris.py</td><td>výběr atributů (proměnných) pro trénink modelu (datová sada Iris)</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/108_features_selection_iris.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/108_features_selection_iris.py</a></td></tr>
<tr><td>109</td><td>109_features_selection_houses.py</td><td>výběr atributů (proměnných) pro trénink modelu (datová sada California Housings)</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/109_features_selection_houses.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/109_features_selection_houses.py</a></td></tr>
<tr><td>110</td><td>110_best_features_selection_houses.py</td><td>získání nejlepší sady atributů (proměnných)</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/110_best_features_selection_houses.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/110_best_features_selection_houses.py</a></td></tr>
<tr><td>111</td><td>111_features_selection_graphical.py</td><td>výběr atributů (proměnných) pro trénink modelu (datová sada Iris), grafický výstup</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/111_features_selection_graphical.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/111_features_selection_graphical.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>112</td><td>112_simplest_linear_regression.py</td><td>lineární regrese bodů ležících v&nbsp;rovině</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/112_simplest_linear_regression.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/112_simplest_linear_regression.py</a></td></tr>
<tr><td>113</td><td>113_linear_regression_no_intercept.py</td><td>lineární regrese při vynucení <i>w<sub>0</sub>=0</i> pro obecná data</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/113_linear_regression_no_intercept.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/113_linear_regression_no_intercept.py</a></td></tr>
<tr><td>114</td><td>114_linear_regression_from_0_0.py</td><td>lineární regrese při vynucení <i>w<sub>0</sub>=0</i> v&nbsp;případě, že vstupní body obsahují počátek souřadného systému</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/114_linear_regression_from_0_0.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/114_linear_regression_from_0_0.py</a></td></tr>
<tr><td>115</td><td>115_linear_regression_multiple_y.py</td><td>model předpovídající pro každou vstupní hodnotu dvě výstupní hodnoty (odpovědi)</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/115_linear_regression_multiple_y.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/115_linear_regression_multiple_y.py</a></td></tr>
<tr><td>116</td><td>116_grid_operations.py</td><td>konstrukce matice obsahující souřadnice bodů v&nbsp;mřížce</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/116_grid_operations.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/116_grid_operations.py</a></td></tr>
<tr><td>117</td><td>117_linear_regression_multiple_x.py</td><td>proložení bodů v&nbsp;prostoru rovinou</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/117_linear_regression_multiple_x.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/117_linear_regression_multiple_x.py</a></td></tr>
<tr><td>118</td><td>118_linear_regression_multiple_x.py</td><td>proložení bodů s&nbsp;náhodnou výškou rovinou</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/118_linear_regression_multiple_x.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/118_linear_regression_multiple_x.py</a></td></tr>
<tr><td>119</td><td>119_linear_regression_multiple_x_and_y.py</td><td>proložení dvou sad bodů dvojicí rovin</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/119_linear_regression_multiple_x_and_y.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/119_linear_regression_multiple_x_and_y.py</a></td></tr>
<tr><td>120</td><td>120_linear_regression_multiple_x_and_y.py</td><td>proložení dvou sad bodů dvojicí rovin</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/120_linear_regression_multiple_x_and_y.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/120_linear_regression_multiple_x_and_y.py</a></td></tr>
<tr><td>121</td><td>121_linear_regression_poly.py</td><td>základní polynomická regrese</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/121_linear_regression_poly.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/121_linear_regression_poly.py</a></td></tr>
<tr><td>122</td><td>122_linear_regression_poly_multiple_x.py</td><td>polynomická regrese a body v&nbsp;prostoru, první příklad</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/122_linear_regression_poly_multiple_x.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/122_linear_regression_poly_multiple_x.py</a></td></tr>
<tr><td>123</td><td>123_linear_regression_poly_multiple_x.py</td><td>polynomická regrese a body v&nbsp;prostoru, druhý příklad</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/123_linear_regression_poly_multiple_x.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/123_linear_regression_poly_multiple_x.py</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>124</td><td>124_iris_set_statistic.py</td><td>získání statistických informací o datové sadě <i>Iris</i></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/124_iris_set_statistic.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/124_iris_set_statistic.py</a></td></tr>
<tr><td>125</td><td>125_california_housings_statistic.py</td><td>získání statistických informací o datové sadě <i>California Housings</i></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/125_california_housings_statistic.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/125_california_housings_statistic.py</a></td></tr>
<tr><td>126</td><td>126_variance_threshold_1.py</td><td>výběr atributů pro trénink modelu pomocí <strong>VarianceThreshold</strong></td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/126_variance_threshold_1.py">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/126_variance_threshold_1.py</a></td></tr>
</table>

<p>V&nbsp;repositáři nalezneme taktéž projektový soubor a Jupyter Notebook
s&nbsp;vysvětlením, jak lze modely využít pro rozpoznávání obsahu rastrových
obrázků:</p>

<table>
<tr><th>#</th><th>Příklad</th><th>Stručný popis</th><th>Adresa příkladu</th></tr>
<tr><td>1</td><td>pyproject.toml</td><td>projektový soubor (pro PDM) se všemi závislostmi</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/pyproject.toml">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/pyproject.toml</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>2</td><td>pdm.lock</td><td>lock soubor s&nbsp;konkrétními verzemi všech přímých i tranzitivních závislostí</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/pdm.lock">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/pdm.lock</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>3</td><td>Rozpoznání_obrazu_scikit-learn.ipynb</td><td>Jupyter notebook s&nbsp;celým postupem</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/Rozpoznání_obrazu_scikit-learn.ipynb">https://github.com/tisnik/most-popular-python-libs/blob/master/sklearn/Rozpoznání_obrazu_scikit-learn.ipynb</a></td></tr>
<tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>
<tr><td>4</td><td>particle_life.py</td><td>emergence: příklad vzniku struktury</td><td><a href="https://github.com/tisnik/most-popular-python-libs/blob/master/particles/particle_life.py">https://github.com/tisnik/most-popular-python-libs/blob/master/particles/particle_life.py</a></td></tr>
</table>



<p><a name="k20"></a></p>
<h2 id="k20">20. Odkazy na Internetu</h2>

<ol>

<li>Python for NLP: Sentiment Analysis with Scikit-Learn<br />
<a href="https://stackabuse.com/python-for-nlp-sentiment-analysis-with-scikit-learn/">https://stackabuse.com/python-for-nlp-sentiment-analysis-with-scikit-learn/</a>
</li>

<li>Datová sada - hodnocení leteckých dopravců<br />
<a href="https://raw.githubusercontent.com/satyajeetkrjha/kaggle-Twitter-US-Airline-Sentiment-/refs/heads/master/Tweets.csv">https://raw.githubusercontent.com/satyajeetkrjha/kaggle-Twitter-US-Airline-Sentiment-/refs/heads/master/Tweets.csv</a>
</li>

<li>Twitter_US_Airline_Sentiment_Analysis <br />
<a href="https://github.com/rustagijanvi/Twitter_US_Airline_Sentiment_Analysis/tree/main">https://github.com/rustagijanvi/Twitter_US_Airline_Sentiment_Analysis/tree/main</a>
</li>

<li>Shluková analýza (clustering) a knihovna Scikit-learn<br />
<a href="https://www.root.cz/clanky/shlukova-analyza-clustering-a-knihovna-scikit-learn/">https://www.root.cz/clanky/shlukova-analyza-clustering-a-knihovna-scikit-learn/</a>
</li>

<li>Shluková analýza (clustering) a knihovna Scikit-learn (2)<br />
<a href="https://www.root.cz/clanky/shlukova-analyza-clustering-a-knihovna-scikit-learn-2/">https://www.root.cz/clanky/shlukova-analyza-clustering-a-knihovna-scikit-learn-2/</a>
</li>

<li>Shluková analýza (clustering) a knihovna Scikit-learn (z plochy do 3D prostoru)<br />
<a href="https://www.root.cz/clanky/shlukova-analyza-clustering-a-knihovna-scikit-learn-z-plochy-do-3d-prostoru/">https://www.root.cz/clanky/shlukova-analyza-clustering-a-knihovna-scikit-learn-z-plochy-do-3d-prostoru/</a>
</li>

<li>Rozpoznávání obrázků knihovnou Scikit-learn: první kroky<br />
<a href="https://www.root.cz/clanky/rozpoznavani-obrazku-knihovnou-scikit-learn-prvni-kroky/">https://www.root.cz/clanky/rozpoznavani-obrazku-knihovnou-scikit-learn-prvni-kroky/</a>
</li>

<li>scikit-learn: Machine Learning in Python<br />
<a href="https://scikit-learn.org/stable/index.html">https://scikit-learn.org/stable/index.html</a>
</li>

<li>Sklearn-pandas<br />
<a href="https://github.com/scikit-learn-contrib/sklearn-pandas">https://github.com/scikit-learn-contrib/sklearn-pandas</a>
</li>

<li>sklearn-xarray<br />
<a href="https://github.com/phausamann/sklearn-xarray/">https://github.com/phausamann/sklearn-xarray/</a>
</li>

<li>Clustering<br />
<a href="https://scikit-learn.org/stable/modules/clustering.html">https://scikit-learn.org/stable/modules/clustering.html</a>
</li>

<li>Cluster analysis (Wikipedia)<br />
<a href="https://en.wikipedia.org/wiki/Cluster_analysis">https://en.wikipedia.org/wiki/Cluster_analysis</a>
</li>

<li>Shluková analýza (Wikipedia)<br />
<a href="https://cs.wikipedia.org/wiki/Shlukov%C3%A1_anal%C3%BDza">https://cs.wikipedia.org/wiki/Shlukov%C3%A1_anal%C3%BDza</a>
</li>

<li>K-means<br />
<a href="https://cs.wikipedia.org/wiki/K-means">https://cs.wikipedia.org/wiki/K-means</a>
</li>

<li>k-means clustering<br />
<a href="https://en.wikipedia.org/wiki/K-means_clustering">https://en.wikipedia.org/wiki/K-means_clustering</a>
</li>

<li>Spectral clustering<br />
<a href="https://en.wikipedia.org/wiki/Spectral_clustering">https://en.wikipedia.org/wiki/Spectral_clustering</a>
</li>

<li>Emergence<br />
<a href="https://cs.wikipedia.org/wiki/Emergence">https://cs.wikipedia.org/wiki/Emergence</a>
</li>

<li>Particle Life: Vivid structures from rudimentary rules<br />
<a href="https://particle-life.com/">https://particle-life.com/</a>
</li>

<li>Hertzsprungův–Russellův diagram<br />
<a href="https://cs.wikipedia.org/wiki/Hertzsprung%C5%AFv%E2%80%93Russell%C5%AFv_diagram">https://cs.wikipedia.org/wiki/Hertzsprung%C5%AFv%E2%80%93Russell%C5%AFv_diagram</a>
</li>

<li>Using Machine Learning in an HR Diagram<br />
<a href="https://cocalc.com/share/public_paths/08b6e03583cbdef3cdb9813a54ec68ff773c747f">https://cocalc.com/share/public_paths/08b6e03583cbdef3cdb9813a54ec68ff773c747f</a>
</li>

<li>Gaia H-R diagrams: Querying Gaia data for one million nearby stars<br />
<a href="https://vlas.dev/post/gaia-dr2-hrd/">https://vlas.dev/post/gaia-dr2-hrd/</a>
</li>

<li>The Hertzsprung–Russell diagram<br />
<a href="https://scipython.com/book2/chapter-9-data-analysis-with-pandas/problems/p92/the-hertzsprung-russell-diagram/">https://scipython.com/book2/chapter-9-data-analysis-with-pandas/problems/p92/the-hertzsprung-russell-diagram/</a>
</li>

<li>Animated Hertzsprung-Russell Diagram with 119,614 datapoints<br />
<a href="https://github.com/zonination/h-r-diagram">https://github.com/zonination/h-r-diagram</a>
</li>

<li>Neuraxle Pipelines<br />
<a href="https://github.com/Neuraxio/Neuraxle">https://github.com/Neuraxio/Neuraxle</a>
</li>

<li>scikit-learn: Getting Started<br />
<a href="https://scikit-learn.org/stable/getting_started.html">https://scikit-learn.org/stable/getting_started.html</a>
</li>

<li>Support Vector Machines<br />
<a href="https://scikit-learn.org/stable/modules/svm.html">https://scikit-learn.org/stable/modules/svm.html</a>
</li>

<li>Use Deep Learning to Detect Programming Languages<br />
<a href="http://searene.me/2017/11/26/use-neural-networks-to-detect-programming-languages/">http://searene.me/2017/11/26/use-neural-networks-to-detect-programming-languages/</a>
</li>

<li>Natural-language processing<br />
<a href="https://en.wikipedia.org/wiki/Natural-language_processing">https://en.wikipedia.org/wiki/Natural-language_processing</a>
</li>

<li>THE MNIST DATABASE of handwritten digits<br />
<a href="http://yann.lecun.com/exdb/mnist/">http://yann.lecun.com/exdb/mnist/</a>
</li>

<li>MNIST database (Wikipedia)<br />
<a href="https://en.wikipedia.org/wiki/MNIST_database">https://en.wikipedia.org/wiki/MNIST_database</a>
</li>

<li>MNIST For ML Beginners<br />
<a href="https://www.tensorflow.org/get_started/mnist/beginners">https://www.tensorflow.org/get_started/mnist/beginners</a>
</li>

<li>Stránka projektu Torch<br />
<a href="http://torch.ch/">http://torch.ch/</a>
</li>

<li>Torch: Serialization<br />
<a href="https://github.com/torch/torch7/blob/master/doc/serialization.md">https://github.com/torch/torch7/blob/master/doc/serialization.md</a>
</li>

<li>Torch: modul image<br />
<a href="https://github.com/torch/image/blob/master/README.md">https://github.com/torch/image/blob/master/README.md</a>
</li>

<li>Data pro neuronové sítě<br />
<a href="http://archive.ics.uci.edu/ml/index.php">http://archive.ics.uci.edu/ml/index.php</a>
</li>

<li>Torch na GitHubu (několik repositářů)<br />
<a href="https://github.com/torch">https://github.com/torch</a>
</li>

<li>Torch (machine learning), Wikipedia<br />
<a href="https://en.wikipedia.org/wiki/Torch_%28machine_learning%29">https://en.wikipedia.org/wiki/Torch_%28machine_learning%29</a>
</li>

<li>Torch Package Reference Manual<br />
<a href="https://github.com/torch/torch7/blob/master/README.md">https://github.com/torch/torch7/blob/master/README.md</a>
</li>

<li>Torch Cheatsheet<br />
<a href="https://github.com/torch/torch7/wiki/Cheatsheet">https://github.com/torch/torch7/wiki/Cheatsheet</a>
</li>

<li>Neural network containres (Torch)<br />
<a href="https://github.com/torch/nn/blob/master/doc/containers.md">https://github.com/torch/nn/blob/master/doc/containers.md</a>
</li>

<li>Simple layers<br />
<a href="https://github.com/torch/nn/blob/master/doc/simple.md#nn.Linear">https://github.com/torch/nn/blob/master/doc/simple.md#nn.Linear</a>
</li>

<li>Transfer Function Layers<br />
<a href="https://github.com/torch/nn/blob/master/doc/transfer.md#nn.transfer.dok">https://github.com/torch/nn/blob/master/doc/transfer.md#nn.transfer.dok</a>
</li>

<li>Feedforward neural network<br />
<a href="https://en.wikipedia.org/wiki/Feedforward_neural_network">https://en.wikipedia.org/wiki/Feedforward_neural_network</a>
</li>

<li>Biologické algoritmy (4) - Neuronové sítě<br />
<a href="https://www.root.cz/clanky/biologicke-algoritmy-4-neuronove-site/">https://www.root.cz/clanky/biologicke-algoritmy-4-neuronove-site/</a>
</li>

<li>Biologické algoritmy (5) - Neuronové sítě<br />
<a href="https://www.root.cz/clanky/biologicke-algoritmy-5-neuronove-site/">https://www.root.cz/clanky/biologicke-algoritmy-5-neuronove-site/</a>
</li>

<li>Umělá neuronová síť (Wikipedia)<br />
<a href="https://cs.wikipedia.org/wiki/Um%C4%9Bl%C3%A1_neuronov%C3%A1_s%C3%AD%C5%A5">https://cs.wikipedia.org/wiki/Um%C4%9Bl%C3%A1_neuronov%C3%A1_s%C3%AD%C5%A5</a>
</li>

<li>PyTorch<br />
<a href="http://pytorch.org/">http://pytorch.org/</a>
</li>

<li>JupyterLite na PyPi<br />
<a href="https://pypi.org/project/jupyterlite/">https://pypi.org/project/jupyterlite/</a>
</li>

<li>JupyterLite na GitHubu<br />
<a href="https://github.com/jupyterlite/jupyterlite">https://github.com/jupyterlite/jupyterlite</a>
</li>

<li>Dokumentace k&nbsp;projektu JupyterLite<br />
<a href="https://github.com/jupyterlite/jupyterlite">https://github.com/jupyterlite/jupyterlite</a>
</li>

<li>Matplotlib Home Page<br />
<a href="http://matplotlib.org/">http://matplotlib.org/</a>
</li>

<li>Matplotlib (Wikipedia)<br />
<a href="https://en.wikipedia.org/wiki/Matplotlib">https://en.wikipedia.org/wiki/Matplotlib</a>
</li>

<li>Popis barvových map modulu matplotlib.cm<br />
<a href="https://gist.github.com/endolith/2719900#id7">https://gist.github.com/endolith/2719900#id7</a>
</li>

<li>Ukázky (palety) barvových map modulu matplotlib.cm<br />
<a href="http://matplotlib.org/examples/color/colormaps_reference.html">http://matplotlib.org/examples/color/colormaps_reference.html</a>
</li>

<li>Galerie grafů vytvořených v&nbsp;Matplotlibu<br />
<a href="https://matplotlib.org/3.2.1/gallery/">https://matplotlib.org/3.2.1/gallery/</a>
</li>

<li>3D rendering<br />
<a href="https://en.wikipedia.org/wiki/3D_rendering">https://en.wikipedia.org/wiki/3D_rendering</a>
</li>

<li>3D computer graphics<br />
<a href="https://en.wikipedia.org/wiki/3D_computer_graphics">https://en.wikipedia.org/wiki/3D_computer_graphics</a>
</li>

<li>Primary 3D view planes<br />
<a href="https://matplotlib.org/stable/gallery/mplot3d/view_planes_3d.html">https://matplotlib.org/stable/gallery/mplot3d/view_planes_3d.html</a>
</li>

<li>Getting started in scikit-learn with the famous iris dataset<br />
<a href="https://www.youtube.com/watch?v=hd1W4CyPX58">https://www.youtube.com/watch?v=hd1W4CyPX58</a>
</li>

<li>Training a machine learning model with scikit-learn<br />
<a href="https://www.youtube.com/watch?v=RlQuVL6-qe8">https://www.youtube.com/watch?v=RlQuVL6-qe8</a>
</li>

<li>Iris (plant)<br />
<a href="https://en.wikipedia.org/wiki/Iris_(plant)">https://en.wikipedia.org/wiki/Iris_(plant)</a>
</li>

<li>Kosatec<br />
<a href="https://cs.wikipedia.org/wiki/Kosatec">https://cs.wikipedia.org/wiki/Kosatec</a>
</li>

<li>Iris setosa<br />
<a href="https://en.wikipedia.org/wiki/Iris_setosa">https://en.wikipedia.org/wiki/Iris_setosa</a>
</li>

<li>Iris versicolor<br />
<a href="https://en.wikipedia.org/wiki/Iris_versicolor">https://en.wikipedia.org/wiki/Iris_versicolor</a>
</li>

<li>Iris virginica<br />
<a href="https://en.wikipedia.org/wiki/Iris_virginica">https://en.wikipedia.org/wiki/Iris_virginica</a>
</li>

<li>Druh<br />
<a href="https://cs.wikipedia.org/wiki/Druh">https://cs.wikipedia.org/wiki/Druh</a>
</li>

<li>Iris subg. Limniris<br />
<a href="https://en.wikipedia.org/wiki/Iris_subg._Limniris">https://en.wikipedia.org/wiki/Iris_subg._Limniris</a>
</li>

<li>Iris Dataset Classification with Python: A Tutorial<br />
<a href="https://www.pycodemates.com/2022/05/iris-dataset-classification-with-python.html">https://www.pycodemates.com/2022/05/iris-dataset-classification-with-python.html</a>
</li>

<li>Iris flower data set<br />
<a href="https://en.wikipedia.org/wiki/Iris_flower_data_set">https://en.wikipedia.org/wiki/Iris_flower_data_set</a>
</li>

<li>List of datasets for machine-learning research<br />
<a href="https://en.wikipedia.org/wiki/List_of_datasets_for_machine-learning_research">https://en.wikipedia.org/wiki/List_of_datasets_for_machine-learning_research</a>
</li>

<li>Analýza hlavních komponent<br />
<a href="https://cs.wikipedia.org/wiki/Anal%C3%BDza_hlavn%C3%ADch_komponent">https://cs.wikipedia.org/wiki/Anal%C3%BDza_hlavn%C3%ADch_komponent</a>
</li>

<li>Principal component analysis<br />
<a href="https://en.wikipedia.org/wiki/Principal_component_analysis">https://en.wikipedia.org/wiki/Principal_component_analysis</a>
</li>

<li>Scikit-learn Crash Course - Machine Learning Library for Python<br />
<a href="https://www.youtube.com/watch?v=0B5eIE_1vpU">https://www.youtube.com/watch?v=0B5eIE_1vpU</a>
</li>

<li>calm-notebooks<br />
<a href="https://github.com/koaning/calm-notebooks">https://github.com/koaning/calm-notebooks</a>
</li>

<li>Should you teach Python or R for data science?<br />
<a href="https://www.dataschool.io/python-or-r-for-data-science/">https://www.dataschool.io/python-or-r-for-data-science/</a>
</li>

<li>nbviewer: A simple way to share Jupyter Notebooks<br />
<a href="https://nbviewer.org/">https://nbviewer.org/</a>
</li>

<li>AI vs Machine Learning (Youtube)<br />
<a href="https://www.youtube.com/watch?v=4RixMPF4xis">https://www.youtube.com/watch?v=4RixMPF4xis</a>
</li>

<li>Machine Learning | What Is Machine Learning? | Introduction To Machine Learning | 2024 | Simplilearn (Youtube)<br />
<a href="https://www.youtube.com/watch?v=ukzFI9rgwfU">https://www.youtube.com/watch?v=ukzFI9rgwfU</a>
</li>

<li>A Gentle Introduction to Machine Learning (Youtube)<br />
<a href="https://www.youtube.com/watch?v=Gv9_4yMHFhI">https://www.youtube.com/watch?v=Gv9_4yMHFhI</a>
</li>

<li>Machine Learning vs Deep Learning<br />
<a href="https://www.youtube.com/watch?v=q6kJ71tEYqM">https://www.youtube.com/watch?v=q6kJ71tEYqM</a>
</li>

<li>Umělá inteligence (slajdy)<br />
<a href="https://slideplayer.cz/slide/12119218/">https://slideplayer.cz/slide/12119218/</a>
</li>

<li>Úvod do umělé inteligence<br />
<a href="https://slideplayer.cz/slide/2505525/">https://slideplayer.cz/slide/2505525/</a>
</li>

<li>Umělá inteligence I / Artificial Intelligence I<br />
<a href="https://ktiml.mff.cuni.cz/~bartak/ui/">https://ktiml.mff.cuni.cz/~bartak/ui/</a>
</li>

<li>Matplotlib vs. seaborn vs. Plotly vs. MATLAB vs. ggplot2 vs. pandas<br />
<a href="https://ritza.co/articles/matplotlib-vs-seaborn-vs-plotly-vs-MATLAB-vs-ggplot2-vs-pandas/">https://ritza.co/articles/matplotlib-vs-seaborn-vs-plotly-vs-MATLAB-vs-ggplot2-vs-pandas/</a>
</li>

<li>Matplotlib, Seaborn or Plotnine?<br />
<a href="https://www.reddit.com/r/datascience/comments/jvrqxt/matplotlib_seaborn_or_plotnine/">https://www.reddit.com/r/datascience/comments/jvrqxt/matplotlib_seaborn_or_plotnine/</a>
</li>

<li>@Rabeez: Rabeez/plotting_comparison.ipynb<br />
<a href="https://gist.github.com/Rabeez/ffc0b59d4a41e20fa8d944c44a96adbc">https://gist.github.com/Rabeez/ffc0b59d4a41e20fa8d944c44a96adbc</a>
</li>

<li>Matplotlib, Seaborn, Plotly and Plotnine Comparison<br />
<a href="https://python.plainenglish.io/matplotlib-seaborn-plotly-and-plotnine-comparison-baf2db5a9c40">https://python.plainenglish.io/matplotlib-seaborn-plotly-and-plotnine-comparison-baf2db5a9c40</a>
</li>

<li>Data Visualization 101: How to Choose a Python Plotting Library<br />
<a href="https://towardsdatascience.com/data-visualization-101-how-to-choose-a-python-plotting-library-853460a08a8a">https://towardsdatascience.com/data-visualization-101-how-to-choose-a-python-plotting-library-853460a08a8a</a>
</li>

<li>Data science in Python: pandas, seaborn, scikit-learn<br />
<a href="https://www.youtube.com/watch?v=3ZWuPVWq7p4">https://www.youtube.com/watch?v=3ZWuPVWq7p4</a>
</li>

<li>7.2. Real world datasets<br />
<a href="https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset">https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset</a>
</li>

<li>7.2.7. California Housing dataset<br />
<a href="https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset">https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset</a>
</li>

<li>Comprehensive Guide to Classification Models in Scikit-Learn<br />
<a href="https://www.geeksforgeeks.org/comprehensive-guide-to-classification-models-in-scikit-learn/">https://www.geeksforgeeks.org/comprehensive-guide-to-classification-models-in-scikit-learn/</a>
</li>

<li>Tidy Data Visualization: ggplot2 vs seaborn<br />
<a href="https://blog.tidy-intelligence.com/posts/ggplot2-vs-seaborn/">https://blog.tidy-intelligence.com/posts/ggplot2-vs-seaborn/</a>
</li>

<li>seaborn: statistical data visualization<br />
<a href="https://seaborn.pydata.org/">https://seaborn.pydata.org/</a>
</li>

<li>Linear regression (Wikipedia)<br />
<a href="https://en.wikipedia.org/wiki/Linear_regression">https://en.wikipedia.org/wiki/Linear_regression</a>
</li>

<li>Lineární regrese (Wikipedia)<br />
<a href="https://cs.wikipedia.org/wiki/Line%C3%A1rn%C3%AD_regrese">https://cs.wikipedia.org/wiki/Line%C3%A1rn%C3%AD_regrese</a>
</li>

<li>Iris Flower Classification with MLP Classifier<br />
<a href="https://www.metriccoders.com/post/iris-flower-classification-with-mlp-classifier">https://www.metriccoders.com/post/iris-flower-classification-with-mlp-classifier</a>
</li>

<li>SMS Spam Collection Dataset<br />
<a href="https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset">https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset</a>
</li>

</ol>



<p></p><p></p>
<p><small>Autor: <a href="http://www.fit.vutbr.cz/~tisnovpa">Pavel Tišnovský</a> &nbsp; 2024</small></p>
</body>
</html>

